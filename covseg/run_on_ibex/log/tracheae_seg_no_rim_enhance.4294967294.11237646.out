Loading anaconda...
...Anaconda env loaded
directing: X rim_enhanced: False test_id 0
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9414 # image files with weight 9372
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2468 # image files with weight 2460
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9372
Using 4 GPUs
best_f1 is: 0.32580495971267653
Going to train epochs [4-33]
Training epoch 4
	step [1/147], loss=52.3330
	step [2/147], loss=52.8949
	step [3/147], loss=49.3992
	step [4/147], loss=53.9985
	step [5/147], loss=53.6015
	step [6/147], loss=51.5309
	step [7/147], loss=52.5312
	step [8/147], loss=51.0736
	step [9/147], loss=53.6189
	step [10/147], loss=50.5913
	step [11/147], loss=52.4290
	step [12/147], loss=49.1331
	step [13/147], loss=49.9117
	step [14/147], loss=51.7727
	step [15/147], loss=50.7260
	step [16/147], loss=51.5849
	step [17/147], loss=51.1657
	step [18/147], loss=50.2311
	step [19/147], loss=50.4103
	step [20/147], loss=50.7149
	step [21/147], loss=52.2280
	step [22/147], loss=53.1114
	step [23/147], loss=50.9510
	step [24/147], loss=50.8454
	step [25/147], loss=50.7195
	step [26/147], loss=50.3808
	step [27/147], loss=51.3254
	step [28/147], loss=51.9879
	step [29/147], loss=49.2369
	step [30/147], loss=49.3942
	step [31/147], loss=49.8715
	step [32/147], loss=50.1956
	step [33/147], loss=50.0862
	step [34/147], loss=49.6502
	step [35/147], loss=49.1975
	step [36/147], loss=51.1203
	step [37/147], loss=49.9914
	step [38/147], loss=50.0323
	step [39/147], loss=50.4345
	step [40/147], loss=50.4449
	step [41/147], loss=47.3469
	step [42/147], loss=48.9353
	step [43/147], loss=49.2710
	step [44/147], loss=48.2221
	step [45/147], loss=49.0322
	step [46/147], loss=52.2295
	step [47/147], loss=47.4435
	step [48/147], loss=49.5844
	step [49/147], loss=49.6510
	step [50/147], loss=50.6756
	step [51/147], loss=48.4000
	step [52/147], loss=48.9903
	step [53/147], loss=48.7562
	step [54/147], loss=49.6127
	step [55/147], loss=48.1220
	step [56/147], loss=47.1664
	step [57/147], loss=48.3084
	step [58/147], loss=48.3642
	step [59/147], loss=49.8988
	step [60/147], loss=46.4603
	step [61/147], loss=48.1930
	step [62/147], loss=50.3466
	step [63/147], loss=48.4604
	step [64/147], loss=47.7972
	step [65/147], loss=48.3220
	step [66/147], loss=47.0328
	step [67/147], loss=48.0736
	step [68/147], loss=47.2996
	step [69/147], loss=47.7868
	step [70/147], loss=48.2021
	step [71/147], loss=48.8822
	step [72/147], loss=47.1347
	step [73/147], loss=46.3354
	step [74/147], loss=46.4869
	step [75/147], loss=46.2856
	step [76/147], loss=47.3456
	step [77/147], loss=44.1581
	step [78/147], loss=46.7303
	step [79/147], loss=47.2534
	step [80/147], loss=46.7752
	step [81/147], loss=47.2617
	step [82/147], loss=47.3683
	step [83/147], loss=46.1815
	step [84/147], loss=49.0859
	step [85/147], loss=46.8049
	step [86/147], loss=46.1129
	step [87/147], loss=45.7347
	step [88/147], loss=45.6659
	step [89/147], loss=45.5192
	step [90/147], loss=47.4427
	step [91/147], loss=46.3865
	step [92/147], loss=47.3811
	step [93/147], loss=47.1236
	step [94/147], loss=46.3653
	step [95/147], loss=46.8358
	step [96/147], loss=48.6024
	step [97/147], loss=45.8145
	step [98/147], loss=45.4364
	step [99/147], loss=47.7508
	step [100/147], loss=45.4373
	step [101/147], loss=49.3852
	step [102/147], loss=44.9862
	step [103/147], loss=44.2515
	step [104/147], loss=46.2185
	step [105/147], loss=45.0429
	step [106/147], loss=44.7267
	step [107/147], loss=44.8392
	step [108/147], loss=45.2196
	step [109/147], loss=46.8743
	step [110/147], loss=44.8175
	step [111/147], loss=46.1131
	step [112/147], loss=46.0421
	step [113/147], loss=44.6985
	step [114/147], loss=45.9520
	step [115/147], loss=45.9221
	step [116/147], loss=43.8451
	step [117/147], loss=45.6602
	step [118/147], loss=43.1943
	step [119/147], loss=47.4203
	step [120/147], loss=44.7760
	step [121/147], loss=44.7341
	step [122/147], loss=42.8755
	step [123/147], loss=43.2391
	step [124/147], loss=44.3968
	step [125/147], loss=45.6120
	step [126/147], loss=44.7905
	step [127/147], loss=45.4892
	step [128/147], loss=43.9466
	step [129/147], loss=44.4907
	step [130/147], loss=44.1458
	step [131/147], loss=44.2756
	step [132/147], loss=43.5882
	step [133/147], loss=44.0784
	step [134/147], loss=43.7318
	step [135/147], loss=44.4999
	step [136/147], loss=43.4070
	step [137/147], loss=45.2928
	step [138/147], loss=44.0696
	step [139/147], loss=42.8985
	step [140/147], loss=43.2090
	step [141/147], loss=44.4658
	step [142/147], loss=44.5936
	step [143/147], loss=41.8313
	step [144/147], loss=45.6334
	step [145/147], loss=41.5046
	step [146/147], loss=42.5544
	step [147/147], loss=19.2515
	Evaluating
	loss=0.1494, precision=0.1875, recall=0.9953, f1=0.3155
Training epoch 5
	step [1/147], loss=43.8647
	step [2/147], loss=41.7690
	step [3/147], loss=44.8795
	step [4/147], loss=42.6045
	step [5/147], loss=42.4880
	step [6/147], loss=44.2019
	step [7/147], loss=41.8714
	step [8/147], loss=42.3573
	step [9/147], loss=43.4583
	step [10/147], loss=40.9715
	step [11/147], loss=43.0041
	step [12/147], loss=43.5198
	step [13/147], loss=41.2627
	step [14/147], loss=42.9269
	step [15/147], loss=41.8326
	step [16/147], loss=44.0053
	step [17/147], loss=41.9753
	step [18/147], loss=42.0293
	step [19/147], loss=43.4588
	step [20/147], loss=41.6575
	step [21/147], loss=43.4803
	step [22/147], loss=42.1867
	step [23/147], loss=44.5583
	step [24/147], loss=42.0267
	step [25/147], loss=42.2843
	step [26/147], loss=42.6972
	step [27/147], loss=43.3197
	step [28/147], loss=41.2661
	step [29/147], loss=43.1388
	step [30/147], loss=41.4249
	step [31/147], loss=41.2100
	step [32/147], loss=43.5621
	step [33/147], loss=41.5760
	step [34/147], loss=41.2976
	step [35/147], loss=42.3396
	step [36/147], loss=41.4211
	step [37/147], loss=41.3964
	step [38/147], loss=41.6147
	step [39/147], loss=41.0313
	step [40/147], loss=40.9673
	step [41/147], loss=40.1292
	step [42/147], loss=42.6343
	step [43/147], loss=41.8746
	step [44/147], loss=41.4748
	step [45/147], loss=42.0192
	step [46/147], loss=41.3032
	step [47/147], loss=41.4865
	step [48/147], loss=43.3625
	step [49/147], loss=41.4280
	step [50/147], loss=41.6597
	step [51/147], loss=41.1306
	step [52/147], loss=41.5765
	step [53/147], loss=40.7567
	step [54/147], loss=39.9499
	step [55/147], loss=41.5536
	step [56/147], loss=40.9127
	step [57/147], loss=39.6219
	step [58/147], loss=39.8980
	step [59/147], loss=41.4620
	step [60/147], loss=41.0097
	step [61/147], loss=41.3484
	step [62/147], loss=39.3844
	step [63/147], loss=39.9889
	step [64/147], loss=40.1066
	step [65/147], loss=39.7794
	step [66/147], loss=39.2127
	step [67/147], loss=40.3084
	step [68/147], loss=39.3715
	step [69/147], loss=39.4983
	step [70/147], loss=40.6495
	step [71/147], loss=38.7432
	step [72/147], loss=38.7280
	step [73/147], loss=41.6218
	step [74/147], loss=38.8369
	step [75/147], loss=40.8031
	step [76/147], loss=39.8281
	step [77/147], loss=39.6093
	step [78/147], loss=39.3787
	step [79/147], loss=41.2590
	step [80/147], loss=38.3745
	step [81/147], loss=40.2626
	step [82/147], loss=39.9799
	step [83/147], loss=38.7175
	step [84/147], loss=39.6160
	step [85/147], loss=38.9605
	step [86/147], loss=39.2965
	step [87/147], loss=39.2587
	step [88/147], loss=38.9286
	step [89/147], loss=38.3454
	step [90/147], loss=41.0594
	step [91/147], loss=38.2371
	step [92/147], loss=38.5373
	step [93/147], loss=38.5696
	step [94/147], loss=38.3505
	step [95/147], loss=38.6940
	step [96/147], loss=39.3290
	step [97/147], loss=40.6732
	step [98/147], loss=37.3308
	step [99/147], loss=37.9104
	step [100/147], loss=37.8159
	step [101/147], loss=39.3299
	step [102/147], loss=37.2225
	step [103/147], loss=38.5752
	step [104/147], loss=39.8408
	step [105/147], loss=37.3464
	step [106/147], loss=37.1409
	step [107/147], loss=37.3902
	step [108/147], loss=37.0304
	step [109/147], loss=36.1553
	step [110/147], loss=37.9154
	step [111/147], loss=38.1222
	step [112/147], loss=38.6805
	step [113/147], loss=39.4736
	step [114/147], loss=39.3636
	step [115/147], loss=41.0025
	step [116/147], loss=37.6326
	step [117/147], loss=37.5357
	step [118/147], loss=37.2043
	step [119/147], loss=37.0570
	step [120/147], loss=39.3620
	step [121/147], loss=36.1506
	step [122/147], loss=38.0064
	step [123/147], loss=37.4815
	step [124/147], loss=37.0982
	step [125/147], loss=37.6353
	step [126/147], loss=38.5717
	step [127/147], loss=36.4181
	step [128/147], loss=37.5487
	step [129/147], loss=38.3808
	step [130/147], loss=36.7395
	step [131/147], loss=38.7265
	step [132/147], loss=37.9676
	step [133/147], loss=37.5353
	step [134/147], loss=37.0241
	step [135/147], loss=36.6706
	step [136/147], loss=37.4501
	step [137/147], loss=36.1583
	step [138/147], loss=38.0194
	step [139/147], loss=38.1025
	step [140/147], loss=38.5685
	step [141/147], loss=35.8803
	step [142/147], loss=38.4814
	step [143/147], loss=36.1474
	step [144/147], loss=36.6046
	step [145/147], loss=37.6176
	step [146/147], loss=36.6595
	step [147/147], loss=17.1986
	Evaluating
	loss=0.1279, precision=0.1692, recall=0.9958, f1=0.2893
Training epoch 6
	step [1/147], loss=35.9340
	step [2/147], loss=36.1837
	step [3/147], loss=38.1650
	step [4/147], loss=36.6949
	step [5/147], loss=37.2776
	step [6/147], loss=34.7367
	step [7/147], loss=35.3382
	step [8/147], loss=36.9057
	step [9/147], loss=36.2474
	step [10/147], loss=34.8155
	step [11/147], loss=37.2005
	step [12/147], loss=36.3475
	step [13/147], loss=36.7771
	step [14/147], loss=35.7045
	step [15/147], loss=36.7503
	step [16/147], loss=35.8102
	step [17/147], loss=37.5196
	step [18/147], loss=37.0368
	step [19/147], loss=36.6152
	step [20/147], loss=34.6154
	step [21/147], loss=35.7206
	step [22/147], loss=35.5666
	step [23/147], loss=36.9676
	step [24/147], loss=34.1909
	step [25/147], loss=34.0322
	step [26/147], loss=36.2220
	step [27/147], loss=35.9515
	step [28/147], loss=35.6333
	step [29/147], loss=35.5301
	step [30/147], loss=35.1136
	step [31/147], loss=36.8148
	step [32/147], loss=35.4195
	step [33/147], loss=34.5360
	step [34/147], loss=36.4087
	step [35/147], loss=35.7087
	step [36/147], loss=35.6994
	step [37/147], loss=35.1415
	step [38/147], loss=34.6120
	step [39/147], loss=36.0018
	step [40/147], loss=36.1723
	step [41/147], loss=34.3707
	step [42/147], loss=34.7239
	step [43/147], loss=34.9557
	step [44/147], loss=33.9073
	step [45/147], loss=35.5889
	step [46/147], loss=33.5816
	step [47/147], loss=35.6457
	step [48/147], loss=35.3860
	step [49/147], loss=34.6924
	step [50/147], loss=36.0508
	step [51/147], loss=35.6361
	step [52/147], loss=35.4520
	step [53/147], loss=34.9996
	step [54/147], loss=34.4895
	step [55/147], loss=35.4584
	step [56/147], loss=34.9746
	step [57/147], loss=34.0062
	step [58/147], loss=33.9422
	step [59/147], loss=34.1238
	step [60/147], loss=31.9535
	step [61/147], loss=36.1926
	step [62/147], loss=35.3153
	step [63/147], loss=33.8468
	step [64/147], loss=33.9669
	step [65/147], loss=35.2179
	step [66/147], loss=35.3801
	step [67/147], loss=35.9074
	step [68/147], loss=35.1758
	step [69/147], loss=33.2160
	step [70/147], loss=32.4252
	step [71/147], loss=34.8696
	step [72/147], loss=33.3482
	step [73/147], loss=35.0371
	step [74/147], loss=33.6729
	step [75/147], loss=34.5369
	step [76/147], loss=34.0648
	step [77/147], loss=33.1416
	step [78/147], loss=34.8065
	step [79/147], loss=36.2231
	step [80/147], loss=33.4896
	step [81/147], loss=34.0048
	step [82/147], loss=31.9598
	step [83/147], loss=33.6121
	step [84/147], loss=33.4112
	step [85/147], loss=32.5956
	step [86/147], loss=34.2296
	step [87/147], loss=34.3421
	step [88/147], loss=34.2091
	step [89/147], loss=32.7147
	step [90/147], loss=33.7026
	step [91/147], loss=33.9191
	step [92/147], loss=33.4106
	step [93/147], loss=32.3105
	step [94/147], loss=34.7806
	step [95/147], loss=33.8112
	step [96/147], loss=34.3234
	step [97/147], loss=32.0323
	step [98/147], loss=35.4467
	step [99/147], loss=32.9877
	step [100/147], loss=30.5813
	step [101/147], loss=33.2757
	step [102/147], loss=35.4190
	step [103/147], loss=32.4359
	step [104/147], loss=36.2286
	step [105/147], loss=31.4795
	step [106/147], loss=33.7793
	step [107/147], loss=32.6051
	step [108/147], loss=32.2949
	step [109/147], loss=33.1630
	step [110/147], loss=33.5363
	step [111/147], loss=32.0035
	step [112/147], loss=32.2276
	step [113/147], loss=31.6133
	step [114/147], loss=32.8634
	step [115/147], loss=31.3710
	step [116/147], loss=31.7646
	step [117/147], loss=32.2115
	step [118/147], loss=34.2448
	step [119/147], loss=33.2618
	step [120/147], loss=31.0128
	step [121/147], loss=33.2591
	step [122/147], loss=33.2575
	step [123/147], loss=34.1365
	step [124/147], loss=32.5551
	step [125/147], loss=30.9654
	step [126/147], loss=32.3826
	step [127/147], loss=33.0059
	step [128/147], loss=33.1495
	step [129/147], loss=31.9546
	step [130/147], loss=31.7903
	step [131/147], loss=31.8887
	step [132/147], loss=31.6973
	step [133/147], loss=33.1273
	step [134/147], loss=33.0190
	step [135/147], loss=31.1868
	step [136/147], loss=32.0960
	step [137/147], loss=30.3263
	step [138/147], loss=30.7184
	step [139/147], loss=32.3275
	step [140/147], loss=33.3749
	step [141/147], loss=31.2805
	step [142/147], loss=31.7122
	step [143/147], loss=31.8695
	step [144/147], loss=31.6330
	step [145/147], loss=30.4173
	step [146/147], loss=33.1062
	step [147/147], loss=14.1425
	Evaluating
	loss=0.1060, precision=0.1831, recall=0.9954, f1=0.3093
Training epoch 7
	step [1/147], loss=30.6757
	step [2/147], loss=30.7926
	step [3/147], loss=32.4033
	step [4/147], loss=31.8197
	step [5/147], loss=30.4164
	step [6/147], loss=33.3369
	step [7/147], loss=30.7203
	step [8/147], loss=31.4491
	step [9/147], loss=31.7946
	step [10/147], loss=29.5553
	step [11/147], loss=31.3223
	step [12/147], loss=32.3798
	step [13/147], loss=29.9753
	step [14/147], loss=30.5907
	step [15/147], loss=29.8001
	step [16/147], loss=30.7272
	step [17/147], loss=32.1592
	step [18/147], loss=31.4727
	step [19/147], loss=31.8027
	step [20/147], loss=32.8295
	step [21/147], loss=30.6436
	step [22/147], loss=31.6013
	step [23/147], loss=31.7735
	step [24/147], loss=30.2994
	step [25/147], loss=30.7686
	step [26/147], loss=30.8972
	step [27/147], loss=31.9054
	step [28/147], loss=31.5758
	step [29/147], loss=28.7539
	step [30/147], loss=31.0859
	step [31/147], loss=29.7138
	step [32/147], loss=31.3166
	step [33/147], loss=32.1380
	step [34/147], loss=31.0311
	step [35/147], loss=30.2895
	step [36/147], loss=31.3281
	step [37/147], loss=30.0983
	step [38/147], loss=30.7155
	step [39/147], loss=29.8872
	step [40/147], loss=29.3890
	step [41/147], loss=31.1704
	step [42/147], loss=29.8336
	step [43/147], loss=30.5796
	step [44/147], loss=31.9616
	step [45/147], loss=32.2134
	step [46/147], loss=30.3665
	step [47/147], loss=29.9596
	step [48/147], loss=29.7718
	step [49/147], loss=28.7355
	step [50/147], loss=30.0777
	step [51/147], loss=33.3944
	step [52/147], loss=29.8886
	step [53/147], loss=30.1861
	step [54/147], loss=29.7948
	step [55/147], loss=30.0257
	step [56/147], loss=29.1782
	step [57/147], loss=30.4570
	step [58/147], loss=30.0660
	step [59/147], loss=29.4618
	step [60/147], loss=30.5522
	step [61/147], loss=30.9605
	step [62/147], loss=31.6129
	step [63/147], loss=30.6090
	step [64/147], loss=30.5887
	step [65/147], loss=31.3625
	step [66/147], loss=29.4221
	step [67/147], loss=30.3668
	step [68/147], loss=27.9026
	step [69/147], loss=29.4382
	step [70/147], loss=30.5348
	step [71/147], loss=29.2938
	step [72/147], loss=31.1773
	step [73/147], loss=30.9972
	step [74/147], loss=30.1718
	step [75/147], loss=29.3544
	step [76/147], loss=31.2648
	step [77/147], loss=30.3771
	step [78/147], loss=29.5139
	step [79/147], loss=29.3835
	step [80/147], loss=29.4437
	step [81/147], loss=30.3677
	step [82/147], loss=30.8115
	step [83/147], loss=27.4947
	step [84/147], loss=28.9394
	step [85/147], loss=30.4046
	step [86/147], loss=30.4093
	step [87/147], loss=29.4684
	step [88/147], loss=28.3564
	step [89/147], loss=29.4724
	step [90/147], loss=28.7082
	step [91/147], loss=29.9774
	step [92/147], loss=29.0173
	step [93/147], loss=26.8019
	step [94/147], loss=30.4090
	step [95/147], loss=29.2943
	step [96/147], loss=27.9108
	step [97/147], loss=29.0913
	step [98/147], loss=26.4678
	step [99/147], loss=28.4395
	step [100/147], loss=30.7786
	step [101/147], loss=29.8734
	step [102/147], loss=31.6150
	step [103/147], loss=28.3298
	step [104/147], loss=27.2717
	step [105/147], loss=30.1748
	step [106/147], loss=29.0883
	step [107/147], loss=27.6400
	step [108/147], loss=28.6528
	step [109/147], loss=29.1377
	step [110/147], loss=28.4231
	step [111/147], loss=28.3232
	step [112/147], loss=29.9867
	step [113/147], loss=28.5414
	step [114/147], loss=29.2010
	step [115/147], loss=32.2931
	step [116/147], loss=28.1541
	step [117/147], loss=27.3466
	step [118/147], loss=28.8643
	step [119/147], loss=26.8368
	step [120/147], loss=28.4526
	step [121/147], loss=28.1607
	step [122/147], loss=27.7643
	step [123/147], loss=30.0310
	step [124/147], loss=28.2387
	step [125/147], loss=29.6200
	step [126/147], loss=28.2744
	step [127/147], loss=29.0328
	step [128/147], loss=28.5180
	step [129/147], loss=27.3879
	step [130/147], loss=27.7905
	step [131/147], loss=27.0563
	step [132/147], loss=29.6736
	step [133/147], loss=26.2183
	step [134/147], loss=30.5577
	step [135/147], loss=28.3841
	step [136/147], loss=27.5220
	step [137/147], loss=27.3623
	step [138/147], loss=27.2043
	step [139/147], loss=28.8734
	step [140/147], loss=26.0650
	step [141/147], loss=28.3158
	step [142/147], loss=26.8969
	step [143/147], loss=26.6560
	step [144/147], loss=29.4259
	step [145/147], loss=27.5269
	step [146/147], loss=26.7094
	step [147/147], loss=12.3711
	Evaluating
	loss=0.0898, precision=0.2003, recall=0.9949, f1=0.3335
saving model as: 0_saved_model.pth
Training epoch 8
	step [1/147], loss=26.9307
	step [2/147], loss=25.6683
	step [3/147], loss=30.9304
	step [4/147], loss=29.0496
	step [5/147], loss=27.6669
	step [6/147], loss=27.0594
	step [7/147], loss=27.8330
	step [8/147], loss=26.6685
	step [9/147], loss=27.9067
	step [10/147], loss=27.8837
	step [11/147], loss=28.6781
	step [12/147], loss=27.9461
	step [13/147], loss=26.7243
	step [14/147], loss=27.3649
	step [15/147], loss=25.0161
	step [16/147], loss=25.6766
	step [17/147], loss=29.0440
	step [18/147], loss=27.6654
	step [19/147], loss=27.6068
	step [20/147], loss=27.9373
	step [21/147], loss=28.4670
	step [22/147], loss=26.6090
	step [23/147], loss=27.9522
	step [24/147], loss=25.2957
	step [25/147], loss=26.7621
	step [26/147], loss=27.7551
	step [27/147], loss=27.0469
	step [28/147], loss=28.6885
	step [29/147], loss=27.7565
	step [30/147], loss=29.2646
	step [31/147], loss=28.5445
	step [32/147], loss=28.3760
	step [33/147], loss=29.6513
	step [34/147], loss=27.0434
	step [35/147], loss=27.6422
	step [36/147], loss=29.8070
	step [37/147], loss=26.9076
	step [38/147], loss=26.5714
	step [39/147], loss=27.9561
	step [40/147], loss=27.3339
	step [41/147], loss=26.1880
	step [42/147], loss=25.8411
	step [43/147], loss=26.5569
	step [44/147], loss=26.9301
	step [45/147], loss=27.6799
	step [46/147], loss=26.9644
	step [47/147], loss=25.5729
	step [48/147], loss=26.7447
	step [49/147], loss=27.4120
	step [50/147], loss=27.4439
	step [51/147], loss=27.1740
	step [52/147], loss=28.0284
	step [53/147], loss=26.7351
	step [54/147], loss=26.9337
	step [55/147], loss=27.0052
	step [56/147], loss=26.2500
	step [57/147], loss=26.9777
	step [58/147], loss=28.5308
	step [59/147], loss=26.0514
	step [60/147], loss=26.7071
	step [61/147], loss=25.2021
	step [62/147], loss=27.0376
	step [63/147], loss=27.1063
	step [64/147], loss=27.1264
	step [65/147], loss=29.3578
	step [66/147], loss=27.9686
	step [67/147], loss=28.0232
	step [68/147], loss=26.5808
	step [69/147], loss=25.4480
	step [70/147], loss=25.3126
	step [71/147], loss=26.2744
	step [72/147], loss=27.3952
	step [73/147], loss=26.8477
	step [74/147], loss=28.5613
	step [75/147], loss=26.1251
	step [76/147], loss=25.1683
	step [77/147], loss=25.2715
	step [78/147], loss=29.4074
	step [79/147], loss=27.2961
	step [80/147], loss=27.4470
	step [81/147], loss=27.2045
	step [82/147], loss=27.0620
	step [83/147], loss=26.7325
	step [84/147], loss=26.0341
	step [85/147], loss=25.2049
	step [86/147], loss=27.1526
	step [87/147], loss=25.2400
	step [88/147], loss=25.7135
	step [89/147], loss=27.1960
	step [90/147], loss=28.0072
	step [91/147], loss=26.2607
	step [92/147], loss=26.6491
	step [93/147], loss=25.7637
	step [94/147], loss=26.5589
	step [95/147], loss=26.5949
	step [96/147], loss=27.7348
	step [97/147], loss=25.3964
	step [98/147], loss=24.5423
	step [99/147], loss=26.8274
	step [100/147], loss=24.2992
	step [101/147], loss=26.5104
	step [102/147], loss=27.5012
	step [103/147], loss=24.7318
	step [104/147], loss=25.0439
	step [105/147], loss=25.5571
	step [106/147], loss=24.2441
	step [107/147], loss=25.7239
	step [108/147], loss=24.8240
	step [109/147], loss=26.6217
	step [110/147], loss=24.6962
	step [111/147], loss=26.8586
	step [112/147], loss=26.6857
	step [113/147], loss=26.1316
	step [114/147], loss=25.9853
	step [115/147], loss=27.1324
	step [116/147], loss=25.0980
	step [117/147], loss=26.3173
	step [118/147], loss=24.2673
	step [119/147], loss=25.2331
	step [120/147], loss=24.1057
	step [121/147], loss=24.9865
	step [122/147], loss=25.7347
	step [123/147], loss=24.8547
	step [124/147], loss=24.9219
	step [125/147], loss=23.4881
	step [126/147], loss=24.4322
	step [127/147], loss=23.4686
	step [128/147], loss=25.4148
	step [129/147], loss=25.4372
	step [130/147], loss=25.0261
	step [131/147], loss=26.2422
	step [132/147], loss=23.9708
	step [133/147], loss=24.5350
	step [134/147], loss=24.0920
	step [135/147], loss=24.6130
	step [136/147], loss=25.2603
	step [137/147], loss=26.2064
	step [138/147], loss=24.6450
	step [139/147], loss=23.8472
	step [140/147], loss=25.5830
	step [141/147], loss=23.8275
	step [142/147], loss=24.7676
	step [143/147], loss=25.4268
	step [144/147], loss=26.0972
	step [145/147], loss=24.6795
	step [146/147], loss=25.5279
	step [147/147], loss=10.6581
	Evaluating
	loss=0.0777, precision=0.1982, recall=0.9950, f1=0.3306
Training epoch 9
	step [1/147], loss=24.6797
	step [2/147], loss=23.1886
	step [3/147], loss=24.6165
	step [4/147], loss=25.0873
	step [5/147], loss=24.0791
	step [6/147], loss=24.6945
	step [7/147], loss=22.9489
	step [8/147], loss=26.6399
	step [9/147], loss=24.1876
	step [10/147], loss=24.4899
	step [11/147], loss=26.0415
	step [12/147], loss=25.2580
	step [13/147], loss=24.1997
	step [14/147], loss=24.5325
	step [15/147], loss=26.5946
	step [16/147], loss=24.6730
	step [17/147], loss=25.0711
	step [18/147], loss=24.0719
	step [19/147], loss=24.7359
	step [20/147], loss=25.6453
	step [21/147], loss=23.6022
	step [22/147], loss=23.6637
	step [23/147], loss=24.9787
	step [24/147], loss=25.4614
	step [25/147], loss=24.0179
	step [26/147], loss=23.9067
	step [27/147], loss=24.7749
	step [28/147], loss=23.6759
	step [29/147], loss=27.3341
	step [30/147], loss=24.8495
	step [31/147], loss=24.3678
	step [32/147], loss=23.9874
	step [33/147], loss=23.7316
	step [34/147], loss=24.9432
	step [35/147], loss=26.3198
	step [36/147], loss=23.6170
	step [37/147], loss=24.4895
	step [38/147], loss=23.4286
	step [39/147], loss=24.1251
	step [40/147], loss=22.7134
	step [41/147], loss=24.3242
	step [42/147], loss=23.6550
	step [43/147], loss=25.0852
	step [44/147], loss=25.1276
	step [45/147], loss=25.8439
	step [46/147], loss=24.1569
	step [47/147], loss=23.3689
	step [48/147], loss=22.9032
	step [49/147], loss=24.1194
	step [50/147], loss=23.2605
	step [51/147], loss=24.3851
	step [52/147], loss=22.2072
	step [53/147], loss=22.9430
	step [54/147], loss=23.3395
	step [55/147], loss=25.3696
	step [56/147], loss=24.4875
	step [57/147], loss=23.4973
	step [58/147], loss=23.0334
	step [59/147], loss=24.3472
	step [60/147], loss=23.6107
	step [61/147], loss=22.6753
	step [62/147], loss=22.9068
	step [63/147], loss=24.0384
	step [64/147], loss=24.3914
	step [65/147], loss=23.4648
	step [66/147], loss=22.7965
	step [67/147], loss=23.4218
	step [68/147], loss=24.8305
	step [69/147], loss=22.1062
	step [70/147], loss=23.7282
	step [71/147], loss=23.6098
	step [72/147], loss=25.3224
	step [73/147], loss=22.4878
	step [74/147], loss=22.2396
	step [75/147], loss=21.7326
	step [76/147], loss=22.4615
	step [77/147], loss=22.4016
	step [78/147], loss=21.9132
	step [79/147], loss=23.7530
	step [80/147], loss=23.6129
	step [81/147], loss=22.9602
	step [82/147], loss=23.4033
	step [83/147], loss=21.5849
	step [84/147], loss=25.7910
	step [85/147], loss=23.6605
	step [86/147], loss=21.7493
	step [87/147], loss=23.0241
	step [88/147], loss=23.8968
	step [89/147], loss=22.6241
	step [90/147], loss=22.9774
	step [91/147], loss=23.7435
	step [92/147], loss=24.1089
	step [93/147], loss=23.4580
	step [94/147], loss=22.3942
	step [95/147], loss=21.3379
	step [96/147], loss=24.2639
	step [97/147], loss=22.4570
	step [98/147], loss=22.8150
	step [99/147], loss=22.2937
	step [100/147], loss=22.8315
	step [101/147], loss=25.6595
	step [102/147], loss=24.4026
	step [103/147], loss=22.3100
	step [104/147], loss=23.5792
	step [105/147], loss=22.8334
	step [106/147], loss=24.6487
	step [107/147], loss=22.5005
	step [108/147], loss=21.2889
	step [109/147], loss=22.0009
	step [110/147], loss=22.5719
	step [111/147], loss=23.1044
	step [112/147], loss=24.9864
	step [113/147], loss=22.3172
	step [114/147], loss=24.3351
	step [115/147], loss=24.2869
	step [116/147], loss=24.0120
	step [117/147], loss=23.6538
	step [118/147], loss=23.8045
	step [119/147], loss=23.1264
	step [120/147], loss=23.3510
	step [121/147], loss=23.3310
	step [122/147], loss=21.5372
	step [123/147], loss=24.9458
	step [124/147], loss=23.7514
	step [125/147], loss=24.1760
	step [126/147], loss=21.7960
	step [127/147], loss=21.5172
	step [128/147], loss=21.9086
	step [129/147], loss=22.1526
	step [130/147], loss=23.4022
	step [131/147], loss=21.7617
	step [132/147], loss=22.5632
	step [133/147], loss=24.2563
	step [134/147], loss=21.8250
	step [135/147], loss=23.5433
	step [136/147], loss=26.6683
	step [137/147], loss=22.3171
	step [138/147], loss=22.3662
	step [139/147], loss=23.3680
	step [140/147], loss=20.4740
	step [141/147], loss=21.0248
	step [142/147], loss=22.9731
	step [143/147], loss=22.7308
	step [144/147], loss=20.8985
	step [145/147], loss=22.5114
	step [146/147], loss=22.0423
	step [147/147], loss=9.9690
	Evaluating
	loss=0.0685, precision=0.2130, recall=0.9940, f1=0.3509
saving model as: 0_saved_model.pth
Training epoch 10
	step [1/147], loss=21.9699
	step [2/147], loss=22.4025
	step [3/147], loss=21.6721
	step [4/147], loss=24.0335
	step [5/147], loss=20.9294
	step [6/147], loss=21.2269
	step [7/147], loss=23.5079
	step [8/147], loss=24.5209
	step [9/147], loss=22.5993
	step [10/147], loss=22.2319
	step [11/147], loss=23.9858
	step [12/147], loss=21.8752
	step [13/147], loss=22.1907
	step [14/147], loss=21.2519
	step [15/147], loss=20.7778
	step [16/147], loss=23.3534
	step [17/147], loss=21.9954
	step [18/147], loss=20.3391
	step [19/147], loss=21.0822
	step [20/147], loss=22.6130
	step [21/147], loss=22.5953
	step [22/147], loss=20.4972
	step [23/147], loss=22.0005
	step [24/147], loss=22.7311
	step [25/147], loss=22.6185
	step [26/147], loss=22.4394
	step [27/147], loss=22.2499
	step [28/147], loss=21.5081
	step [29/147], loss=22.0050
	step [30/147], loss=19.8040
	step [31/147], loss=20.2093
	step [32/147], loss=21.8811
	step [33/147], loss=22.3818
	step [34/147], loss=22.9730
	step [35/147], loss=21.0059
	step [36/147], loss=22.6544
	step [37/147], loss=22.3095
	step [38/147], loss=21.0467
	step [39/147], loss=23.2423
	step [40/147], loss=23.4960
	step [41/147], loss=20.5594
	step [42/147], loss=21.4539
	step [43/147], loss=23.9938
	step [44/147], loss=21.2794
	step [45/147], loss=22.8467
	step [46/147], loss=22.0128
	step [47/147], loss=21.5848
	step [48/147], loss=21.4417
	step [49/147], loss=22.4335
	step [50/147], loss=20.3183
	step [51/147], loss=21.7483
	step [52/147], loss=20.9686
	step [53/147], loss=22.8273
	step [54/147], loss=21.8705
	step [55/147], loss=23.9092
	step [56/147], loss=21.6246
	step [57/147], loss=21.2331
	step [58/147], loss=22.9678
	step [59/147], loss=21.2529
	step [60/147], loss=22.4728
	step [61/147], loss=21.7298
	step [62/147], loss=21.0792
	step [63/147], loss=21.9353
	step [64/147], loss=20.6918
	step [65/147], loss=22.7939
	step [66/147], loss=20.4656
	step [67/147], loss=20.8557
	step [68/147], loss=20.7625
	step [69/147], loss=21.3695
	step [70/147], loss=19.7349
	step [71/147], loss=22.2260
	step [72/147], loss=22.3694
	step [73/147], loss=20.4711
	step [74/147], loss=21.4779
	step [75/147], loss=21.2582
	step [76/147], loss=21.4515
	step [77/147], loss=22.1436
	step [78/147], loss=20.6227
	step [79/147], loss=21.2313
	step [80/147], loss=19.5990
	step [81/147], loss=19.5327
	step [82/147], loss=21.3909
	step [83/147], loss=21.9089
	step [84/147], loss=23.1604
	step [85/147], loss=19.8608
	step [86/147], loss=22.0018
	step [87/147], loss=21.4946
	step [88/147], loss=20.2260
	step [89/147], loss=22.0596
	step [90/147], loss=20.6878
	step [91/147], loss=20.4028
	step [92/147], loss=19.4659
	step [93/147], loss=19.5321
	step [94/147], loss=21.9389
	step [95/147], loss=20.5709
	step [96/147], loss=19.8918
	step [97/147], loss=22.5173
	step [98/147], loss=22.0363
	step [99/147], loss=21.1782
	step [100/147], loss=20.2731
	step [101/147], loss=22.3568
	step [102/147], loss=21.6198
	step [103/147], loss=25.5881
	step [104/147], loss=21.4529
	step [105/147], loss=20.8768
	step [106/147], loss=22.5822
	step [107/147], loss=22.8695
	step [108/147], loss=21.0279
	step [109/147], loss=22.0447
	step [110/147], loss=21.2306
	step [111/147], loss=20.8630
	step [112/147], loss=22.1516
	step [113/147], loss=18.8891
	step [114/147], loss=20.3045
	step [115/147], loss=19.8737
	step [116/147], loss=20.1417
	step [117/147], loss=20.6256
	step [118/147], loss=21.4995
	step [119/147], loss=22.6473
	step [120/147], loss=20.6108
	step [121/147], loss=21.0889
	step [122/147], loss=20.1514
	step [123/147], loss=20.5078
	step [124/147], loss=24.1874
	step [125/147], loss=20.8407
	step [126/147], loss=20.7247
	step [127/147], loss=20.2183
	step [128/147], loss=21.4315
	step [129/147], loss=21.0594
	step [130/147], loss=21.3817
	step [131/147], loss=19.9356
	step [132/147], loss=22.3099
	step [133/147], loss=21.3494
	step [134/147], loss=20.6648
	step [135/147], loss=21.5293
	step [136/147], loss=22.5891
	step [137/147], loss=20.6434
	step [138/147], loss=19.6630
	step [139/147], loss=19.8569
	step [140/147], loss=22.0000
	step [141/147], loss=21.6800
	step [142/147], loss=21.2035
	step [143/147], loss=20.7663
	step [144/147], loss=20.3109
	step [145/147], loss=19.7733
	step [146/147], loss=20.6388
	step [147/147], loss=9.4901
	Evaluating
	loss=0.0640, precision=0.2011, recall=0.9944, f1=0.3345
Training epoch 11
	step [1/147], loss=20.2726
	step [2/147], loss=20.6646
	step [3/147], loss=19.4595
	step [4/147], loss=22.9500
	step [5/147], loss=20.1754
	step [6/147], loss=20.7436
	step [7/147], loss=21.0079
	step [8/147], loss=20.3630
	step [9/147], loss=20.9017
	step [10/147], loss=22.5713
	step [11/147], loss=21.5434
	step [12/147], loss=20.2296
	step [13/147], loss=20.0605
	step [14/147], loss=19.7402
	step [15/147], loss=20.4842
	step [16/147], loss=19.8094
	step [17/147], loss=19.2524
	step [18/147], loss=21.6203
	step [19/147], loss=19.8339
	step [20/147], loss=21.8226
	step [21/147], loss=22.0604
	step [22/147], loss=20.7178
	step [23/147], loss=20.3884
	step [24/147], loss=20.1508
	step [25/147], loss=23.2035
	step [26/147], loss=20.9498
	step [27/147], loss=21.2839
	step [28/147], loss=20.0080
	step [29/147], loss=19.4090
	step [30/147], loss=19.9906
	step [31/147], loss=20.7750
	step [32/147], loss=20.8051
	step [33/147], loss=19.4021
	step [34/147], loss=20.2387
	step [35/147], loss=21.7172
	step [36/147], loss=20.8154
	step [37/147], loss=21.6378
	step [38/147], loss=21.8458
	step [39/147], loss=23.5355
	step [40/147], loss=20.7290
	step [41/147], loss=21.0234
	step [42/147], loss=20.0275
	step [43/147], loss=20.1162
	step [44/147], loss=21.7626
	step [45/147], loss=19.7462
	step [46/147], loss=19.4664
	step [47/147], loss=19.1445
	step [48/147], loss=18.6620
	step [49/147], loss=20.2637
	step [50/147], loss=19.3704
	step [51/147], loss=19.0271
	step [52/147], loss=19.0474
	step [53/147], loss=20.0813
	step [54/147], loss=20.1598
	step [55/147], loss=21.7190
	step [56/147], loss=21.0318
	step [57/147], loss=20.2202
	step [58/147], loss=19.4667
	step [59/147], loss=19.4703
	step [60/147], loss=18.9661
	step [61/147], loss=18.4534
	step [62/147], loss=18.2848
	step [63/147], loss=21.7936
	step [64/147], loss=18.1635
	step [65/147], loss=19.1224
	step [66/147], loss=21.4974
	step [67/147], loss=20.4640
	step [68/147], loss=21.7546
	step [69/147], loss=20.5194
	step [70/147], loss=22.0418
	step [71/147], loss=20.3737
	step [72/147], loss=19.8264
	step [73/147], loss=19.4996
	step [74/147], loss=20.9942
	step [75/147], loss=18.0671
	step [76/147], loss=16.5745
	step [77/147], loss=20.2738
	step [78/147], loss=21.2296
	step [79/147], loss=19.1139
	step [80/147], loss=19.8262
	step [81/147], loss=19.5703
	step [82/147], loss=19.2792
	step [83/147], loss=20.2810
	step [84/147], loss=20.6132
	step [85/147], loss=19.6891
	step [86/147], loss=19.2287
	step [87/147], loss=20.8185
	step [88/147], loss=18.4698
	step [89/147], loss=19.6589
	step [90/147], loss=18.4105
	step [91/147], loss=18.0796
	step [92/147], loss=20.6629
	step [93/147], loss=19.6289
	step [94/147], loss=20.5259
	step [95/147], loss=18.7713
	step [96/147], loss=17.8179
	step [97/147], loss=19.5216
	step [98/147], loss=18.4246
	step [99/147], loss=20.1426
	step [100/147], loss=21.7462
	step [101/147], loss=18.4220
	step [102/147], loss=18.0836
	step [103/147], loss=19.0133
	step [104/147], loss=18.8291
	step [105/147], loss=20.8946
	step [106/147], loss=17.9187
	step [107/147], loss=18.9031
	step [108/147], loss=19.0214
	step [109/147], loss=20.6215
	step [110/147], loss=20.3605
	step [111/147], loss=21.8122
	step [112/147], loss=18.4779
	step [113/147], loss=19.8603
	step [114/147], loss=17.4661
	step [115/147], loss=17.7764
	step [116/147], loss=19.4584
	step [117/147], loss=19.2356
	step [118/147], loss=17.6336
	step [119/147], loss=20.0330
	step [120/147], loss=18.9925
	step [121/147], loss=19.1639
	step [122/147], loss=19.0087
	step [123/147], loss=19.4030
	step [124/147], loss=16.9191
	step [125/147], loss=19.7267
	step [126/147], loss=18.8320
	step [127/147], loss=21.3470
	step [128/147], loss=17.1525
	step [129/147], loss=18.0920
	step [130/147], loss=19.3234
	step [131/147], loss=17.1458
	step [132/147], loss=18.3494
	step [133/147], loss=19.1367
	step [134/147], loss=18.3828
	step [135/147], loss=19.1673
	step [136/147], loss=19.1883
	step [137/147], loss=18.8550
	step [138/147], loss=20.3792
	step [139/147], loss=19.1353
	step [140/147], loss=19.9238
	step [141/147], loss=18.4651
	step [142/147], loss=18.4235
	step [143/147], loss=18.4809
	step [144/147], loss=19.6393
	step [145/147], loss=19.5444
	step [146/147], loss=19.3832
	step [147/147], loss=9.1400
	Evaluating
	loss=0.0749, precision=0.1715, recall=0.9955, f1=0.2926
Training epoch 12
	step [1/147], loss=20.8986
	step [2/147], loss=18.6944
	step [3/147], loss=18.8149
	step [4/147], loss=19.2055
	step [5/147], loss=18.9238
	step [6/147], loss=17.5797
	step [7/147], loss=18.0120
	step [8/147], loss=19.5950
	step [9/147], loss=18.1398
	step [10/147], loss=19.9569
	step [11/147], loss=18.6799
	step [12/147], loss=18.6496
	step [13/147], loss=18.0661
	step [14/147], loss=18.7704
	step [15/147], loss=19.2074
	step [16/147], loss=20.4182
	step [17/147], loss=18.2760
	step [18/147], loss=19.6702
	step [19/147], loss=18.7789
	step [20/147], loss=16.9430
	step [21/147], loss=19.3439
	step [22/147], loss=17.7867
	step [23/147], loss=21.0957
	step [24/147], loss=18.0296
	step [25/147], loss=19.9438
	step [26/147], loss=18.1224
	step [27/147], loss=19.4740
	step [28/147], loss=18.6597
	step [29/147], loss=18.5638
	step [30/147], loss=18.7663
	step [31/147], loss=16.8749
	step [32/147], loss=21.0349
	step [33/147], loss=20.9878
	step [34/147], loss=19.7134
	step [35/147], loss=18.1860
	step [36/147], loss=20.8288
	step [37/147], loss=18.6252
	step [38/147], loss=19.9749
	step [39/147], loss=18.6216
	step [40/147], loss=17.7317
	step [41/147], loss=17.0438
	step [42/147], loss=18.8029
	step [43/147], loss=20.9448
	step [44/147], loss=18.3736
	step [45/147], loss=17.7888
	step [46/147], loss=18.7618
	step [47/147], loss=17.7150
	step [48/147], loss=18.6424
	step [49/147], loss=18.8022
	step [50/147], loss=18.0679
	step [51/147], loss=17.4066
	step [52/147], loss=18.1393
	step [53/147], loss=18.6961
	step [54/147], loss=19.0097
	step [55/147], loss=17.6717
	step [56/147], loss=20.0417
	step [57/147], loss=17.1721
	step [58/147], loss=18.6446
	step [59/147], loss=18.9283
	step [60/147], loss=18.3921
	step [61/147], loss=17.7941
	step [62/147], loss=18.7090
	step [63/147], loss=17.2760
	step [64/147], loss=18.2653
	step [65/147], loss=17.6339
	step [66/147], loss=17.7047
	step [67/147], loss=18.8741
	step [68/147], loss=18.2115
	step [69/147], loss=19.9802
	step [70/147], loss=17.8390
	step [71/147], loss=18.5590
	step [72/147], loss=18.8015
	step [73/147], loss=16.4890
	step [74/147], loss=18.2408
	step [75/147], loss=19.0286
	step [76/147], loss=17.3529
	step [77/147], loss=20.7594
	step [78/147], loss=17.3938
	step [79/147], loss=17.8652
	step [80/147], loss=18.2344
	step [81/147], loss=18.1490
	step [82/147], loss=17.7674
	step [83/147], loss=16.7678
	step [84/147], loss=17.2479
	step [85/147], loss=15.7051
	step [86/147], loss=19.6141
	step [87/147], loss=18.7519
	step [88/147], loss=17.9890
	step [89/147], loss=18.4069
	step [90/147], loss=17.9405
	step [91/147], loss=20.3020
	step [92/147], loss=18.0044
	step [93/147], loss=18.6892
	step [94/147], loss=20.0309
	step [95/147], loss=17.6827
	step [96/147], loss=20.0366
	step [97/147], loss=17.7092
	step [98/147], loss=17.4661
	step [99/147], loss=18.9673
	step [100/147], loss=16.3084
	step [101/147], loss=17.2734
	step [102/147], loss=18.4405
	step [103/147], loss=18.3119
	step [104/147], loss=17.9791
	step [105/147], loss=17.8474
	step [106/147], loss=16.8994
	step [107/147], loss=17.6062
	step [108/147], loss=16.7298
	step [109/147], loss=17.8931
	step [110/147], loss=17.8160
	step [111/147], loss=17.3589
	step [112/147], loss=18.7924
	step [113/147], loss=18.2299
	step [114/147], loss=19.9656
	step [115/147], loss=17.1232
	step [116/147], loss=17.0864
	step [117/147], loss=18.6739
	step [118/147], loss=18.8528
	step [119/147], loss=17.7506
	step [120/147], loss=17.3717
	step [121/147], loss=20.1393
	step [122/147], loss=17.1026
	step [123/147], loss=17.4575
	step [124/147], loss=17.6104
	step [125/147], loss=17.3736
	step [126/147], loss=18.7550
	step [127/147], loss=18.3898
	step [128/147], loss=15.4581
	step [129/147], loss=20.3371
	step [130/147], loss=18.1139
	step [131/147], loss=16.7762
	step [132/147], loss=16.4446
	step [133/147], loss=17.9791
	step [134/147], loss=19.1884
	step [135/147], loss=17.1897
	step [136/147], loss=17.7188
	step [137/147], loss=16.7343
	step [138/147], loss=17.8740
	step [139/147], loss=18.4796
	step [140/147], loss=18.2975
	step [141/147], loss=18.2728
	step [142/147], loss=17.2913
	step [143/147], loss=18.4069
	step [144/147], loss=17.4537
	step [145/147], loss=16.7400
	step [146/147], loss=19.4927
	step [147/147], loss=7.2516
	Evaluating
	loss=0.0508, precision=0.2156, recall=0.9945, f1=0.3544
saving model as: 0_saved_model.pth
Training epoch 13
	step [1/147], loss=16.6844
	step [2/147], loss=19.2137
	step [3/147], loss=18.6717
	step [4/147], loss=16.6054
	step [5/147], loss=17.7797
	step [6/147], loss=18.2038
	step [7/147], loss=16.3361
	step [8/147], loss=18.3436
	step [9/147], loss=17.9488
	step [10/147], loss=17.7793
	step [11/147], loss=16.5705
	step [12/147], loss=15.6956
	step [13/147], loss=15.8482
	step [14/147], loss=17.7583
	step [15/147], loss=17.7770
	step [16/147], loss=15.3530
	step [17/147], loss=16.4690
	step [18/147], loss=16.8149
	step [19/147], loss=18.2579
	step [20/147], loss=16.9678
	step [21/147], loss=16.4432
	step [22/147], loss=16.0257
	step [23/147], loss=17.5124
	step [24/147], loss=16.7185
	step [25/147], loss=18.1636
	step [26/147], loss=17.5651
	step [27/147], loss=18.3138
	step [28/147], loss=17.4126
	step [29/147], loss=16.7097
	step [30/147], loss=16.5263
	step [31/147], loss=16.5683
	step [32/147], loss=17.5494
	step [33/147], loss=18.3126
	step [34/147], loss=16.8713
	step [35/147], loss=16.1549
	step [36/147], loss=15.4673
	step [37/147], loss=17.6736
	step [38/147], loss=18.0711
	step [39/147], loss=16.9635
	step [40/147], loss=17.4172
	step [41/147], loss=17.4830
	step [42/147], loss=16.4531
	step [43/147], loss=18.9255
	step [44/147], loss=16.6214
	step [45/147], loss=18.3565
	step [46/147], loss=17.5309
	step [47/147], loss=18.6384
	step [48/147], loss=17.4632
	step [49/147], loss=15.6119
	step [50/147], loss=17.5494
	step [51/147], loss=18.0935
	step [52/147], loss=18.1544
	step [53/147], loss=16.7524
	step [54/147], loss=16.1524
	step [55/147], loss=18.7096
	step [56/147], loss=16.7924
	step [57/147], loss=15.4691
	step [58/147], loss=17.6532
	step [59/147], loss=17.1073
	step [60/147], loss=17.5160
	step [61/147], loss=16.3215
	step [62/147], loss=16.2736
	step [63/147], loss=17.6758
	step [64/147], loss=15.6679
	step [65/147], loss=17.4137
	step [66/147], loss=18.1774
	step [67/147], loss=15.0621
	step [68/147], loss=18.0777
	step [69/147], loss=16.3170
	step [70/147], loss=16.0182
	step [71/147], loss=17.0067
	step [72/147], loss=19.4192
	step [73/147], loss=17.4702
	step [74/147], loss=15.9775
	step [75/147], loss=17.4597
	step [76/147], loss=16.9445
	step [77/147], loss=18.1109
	step [78/147], loss=17.3766
	step [79/147], loss=18.6610
	step [80/147], loss=17.4460
	step [81/147], loss=17.9687
	step [82/147], loss=17.4535
	step [83/147], loss=17.1935
	step [84/147], loss=17.4454
	step [85/147], loss=17.0742
	step [86/147], loss=16.5652
	step [87/147], loss=16.5594
	step [88/147], loss=16.4620
	step [89/147], loss=16.5601
	step [90/147], loss=17.4766
	step [91/147], loss=16.0074
	step [92/147], loss=15.1377
	step [93/147], loss=16.9318
	step [94/147], loss=16.9610
	step [95/147], loss=17.0043
	step [96/147], loss=16.7967
	step [97/147], loss=18.0667
	step [98/147], loss=17.0225
	step [99/147], loss=16.1115
	step [100/147], loss=16.5535
	step [101/147], loss=18.6595
	step [102/147], loss=17.3681
	step [103/147], loss=16.6264
	step [104/147], loss=19.0873
	step [105/147], loss=15.8319
	step [106/147], loss=16.8815
	step [107/147], loss=16.8859
	step [108/147], loss=15.7215
	step [109/147], loss=16.3240
	step [110/147], loss=18.4389
	step [111/147], loss=18.3837
	step [112/147], loss=16.1103
	step [113/147], loss=17.0041
	step [114/147], loss=17.1169
	step [115/147], loss=17.0917
	step [116/147], loss=16.0850
	step [117/147], loss=18.5330
	step [118/147], loss=18.2967
	step [119/147], loss=15.6680
	step [120/147], loss=18.0636
	step [121/147], loss=16.3109
	step [122/147], loss=15.0759
	step [123/147], loss=16.7038
	step [124/147], loss=15.7723
	step [125/147], loss=16.9378
	step [126/147], loss=18.1736
	step [127/147], loss=15.5361
	step [128/147], loss=15.6731
	step [129/147], loss=16.2966
	step [130/147], loss=16.1819
	step [131/147], loss=15.7338
	step [132/147], loss=15.2814
	step [133/147], loss=16.7828
	step [134/147], loss=18.3682
	step [135/147], loss=15.9931
	step [136/147], loss=17.1883
	step [137/147], loss=15.3311
	step [138/147], loss=16.8462
	step [139/147], loss=15.0332
	step [140/147], loss=15.2463
	step [141/147], loss=14.9764
	step [142/147], loss=17.6784
	step [143/147], loss=19.5856
	step [144/147], loss=16.1100
	step [145/147], loss=16.9899
	step [146/147], loss=16.0676
	step [147/147], loss=8.5450
	Evaluating
	loss=0.0484, precision=0.2008, recall=0.9944, f1=0.3342
Training epoch 14
	step [1/147], loss=15.8544
	step [2/147], loss=16.6276
	step [3/147], loss=15.4567
	step [4/147], loss=16.2697
	step [5/147], loss=16.1925
	step [6/147], loss=16.6871
	step [7/147], loss=15.0571
	step [8/147], loss=16.0730
	step [9/147], loss=15.9136
	step [10/147], loss=15.1836
	step [11/147], loss=17.2137
	step [12/147], loss=14.6410
	step [13/147], loss=16.7067
	step [14/147], loss=15.1768
	step [15/147], loss=15.3414
	step [16/147], loss=16.6452
	step [17/147], loss=15.0410
	step [18/147], loss=15.6410
	step [19/147], loss=18.7833
	step [20/147], loss=18.9651
	step [21/147], loss=16.0508
	step [22/147], loss=15.8701
	step [23/147], loss=14.2228
	step [24/147], loss=15.1945
	step [25/147], loss=16.1173
	step [26/147], loss=18.0929
	step [27/147], loss=15.5597
	step [28/147], loss=16.7252
	step [29/147], loss=16.9713
	step [30/147], loss=15.4278
	step [31/147], loss=16.4758
	step [32/147], loss=16.0789
	step [33/147], loss=15.4255
	step [34/147], loss=17.3938
	step [35/147], loss=16.4885
	step [36/147], loss=16.7420
	step [37/147], loss=14.2477
	step [38/147], loss=15.7137
	step [39/147], loss=15.9021
	step [40/147], loss=15.9457
	step [41/147], loss=15.8214
	step [42/147], loss=17.0766
	step [43/147], loss=16.1852
	step [44/147], loss=15.7821
	step [45/147], loss=17.3558
	step [46/147], loss=14.8771
	step [47/147], loss=16.1850
	step [48/147], loss=16.0776
	step [49/147], loss=15.7091
	step [50/147], loss=15.3921
	step [51/147], loss=15.1119
	step [52/147], loss=17.7116
	step [53/147], loss=14.6761
	step [54/147], loss=19.8252
	step [55/147], loss=15.5119
	step [56/147], loss=16.6281
	step [57/147], loss=15.9454
	step [58/147], loss=15.5009
	step [59/147], loss=15.6508
	step [60/147], loss=16.3513
	step [61/147], loss=16.5851
	step [62/147], loss=13.5823
	step [63/147], loss=15.7615
	step [64/147], loss=16.9452
	step [65/147], loss=17.1242
	step [66/147], loss=16.6512
	step [67/147], loss=15.7552
	step [68/147], loss=14.6692
	step [69/147], loss=14.0267
	step [70/147], loss=17.4833
	step [71/147], loss=16.4464
	step [72/147], loss=16.6636
	step [73/147], loss=15.6271
	step [74/147], loss=16.0997
	step [75/147], loss=18.4477
	step [76/147], loss=15.6369
	step [77/147], loss=14.6754
	step [78/147], loss=15.6143
	step [79/147], loss=16.3089
	step [80/147], loss=16.2674
	step [81/147], loss=15.3630
	step [82/147], loss=15.5227
	step [83/147], loss=14.5583
	step [84/147], loss=16.8781
	step [85/147], loss=16.7007
	step [86/147], loss=14.6422
	step [87/147], loss=15.1616
	step [88/147], loss=16.3269
	step [89/147], loss=16.0680
	step [90/147], loss=14.8262
	step [91/147], loss=14.5331
	step [92/147], loss=15.5402
	step [93/147], loss=17.2293
	step [94/147], loss=15.3263
	step [95/147], loss=16.2548
	step [96/147], loss=17.0935
	step [97/147], loss=15.7368
	step [98/147], loss=15.4765
	step [99/147], loss=17.7626
	step [100/147], loss=15.0456
	step [101/147], loss=15.0863
	step [102/147], loss=16.9297
	step [103/147], loss=15.4457
	step [104/147], loss=14.0347
	step [105/147], loss=15.9997
	step [106/147], loss=15.4934
	step [107/147], loss=14.3238
	step [108/147], loss=16.3474
	step [109/147], loss=15.2437
	step [110/147], loss=16.1697
	step [111/147], loss=14.3552
	step [112/147], loss=16.4991
	step [113/147], loss=15.3898
	step [114/147], loss=14.5711
	step [115/147], loss=17.2502
	step [116/147], loss=16.5636
	step [117/147], loss=15.4031
	step [118/147], loss=16.4395
	step [119/147], loss=16.0237
	step [120/147], loss=14.1124
	step [121/147], loss=15.5516
	step [122/147], loss=15.8317
	step [123/147], loss=16.1937
	step [124/147], loss=17.3224
	step [125/147], loss=14.0119
	step [126/147], loss=16.8092
	step [127/147], loss=15.3903
	step [128/147], loss=18.4149
	step [129/147], loss=15.8184
	step [130/147], loss=15.5983
	step [131/147], loss=16.0940
	step [132/147], loss=18.5950
	step [133/147], loss=17.3224
	step [134/147], loss=16.4043
	step [135/147], loss=13.7805
	step [136/147], loss=15.7103
	step [137/147], loss=14.7443
	step [138/147], loss=14.8475
	step [139/147], loss=15.2787
	step [140/147], loss=15.2908
	step [141/147], loss=14.7156
	step [142/147], loss=17.1118
	step [143/147], loss=14.8867
	step [144/147], loss=15.5421
	step [145/147], loss=14.7753
	step [146/147], loss=14.0496
	step [147/147], loss=7.3030
	Evaluating
	loss=0.0467, precision=0.1843, recall=0.9948, f1=0.3110
Training epoch 15
	step [1/147], loss=14.8806
	step [2/147], loss=15.4149
	step [3/147], loss=14.5180
	step [4/147], loss=16.1975
	step [5/147], loss=13.6774
	step [6/147], loss=16.3732
	step [7/147], loss=17.2885
	step [8/147], loss=15.5651
	step [9/147], loss=15.4598
	step [10/147], loss=14.0830
	step [11/147], loss=16.9630
	step [12/147], loss=16.0140
	step [13/147], loss=14.7205
	step [14/147], loss=14.3627
	step [15/147], loss=14.1666
	step [16/147], loss=14.6840
	step [17/147], loss=14.1329
	step [18/147], loss=16.1635
	step [19/147], loss=15.5751
	step [20/147], loss=15.8488
	step [21/147], loss=14.0335
	step [22/147], loss=14.4048
	step [23/147], loss=15.0180
	step [24/147], loss=14.3927
	step [25/147], loss=15.4623
	step [26/147], loss=14.9386
	step [27/147], loss=13.7440
	step [28/147], loss=15.8349
	step [29/147], loss=15.3478
	step [30/147], loss=15.4880
	step [31/147], loss=14.3453
	step [32/147], loss=13.9570
	step [33/147], loss=15.8325
	step [34/147], loss=15.0258
	step [35/147], loss=15.3991
	step [36/147], loss=15.0338
	step [37/147], loss=14.6454
	step [38/147], loss=14.8939
	step [39/147], loss=13.9610
	step [40/147], loss=15.5949
	step [41/147], loss=16.4818
	step [42/147], loss=16.1491
	step [43/147], loss=15.7037
	step [44/147], loss=15.2728
	step [45/147], loss=15.9482
	step [46/147], loss=14.3917
	step [47/147], loss=14.8390
	step [48/147], loss=16.3761
	step [49/147], loss=16.5449
	step [50/147], loss=15.7412
	step [51/147], loss=15.8586
	step [52/147], loss=15.7521
	step [53/147], loss=15.1341
	step [54/147], loss=15.4879
	step [55/147], loss=15.9522
	step [56/147], loss=15.7504
	step [57/147], loss=15.7339
	step [58/147], loss=13.8140
	step [59/147], loss=12.8179
	step [60/147], loss=14.6886
	step [61/147], loss=16.5186
	step [62/147], loss=17.6172
	step [63/147], loss=13.9798
	step [64/147], loss=16.0067
	step [65/147], loss=13.3950
	step [66/147], loss=14.5254
	step [67/147], loss=15.6340
	step [68/147], loss=15.6683
	step [69/147], loss=15.3492
	step [70/147], loss=16.0901
	step [71/147], loss=12.9657
	step [72/147], loss=15.9366
	step [73/147], loss=13.4436
	step [74/147], loss=15.2462
	step [75/147], loss=16.0881
	step [76/147], loss=13.9955
	step [77/147], loss=15.2002
	step [78/147], loss=16.1442
	step [79/147], loss=14.8466
	step [80/147], loss=15.9229
	step [81/147], loss=17.3048
	step [82/147], loss=15.1224
	step [83/147], loss=14.6380
	step [84/147], loss=14.4765
	step [85/147], loss=14.3681
	step [86/147], loss=13.5833
	step [87/147], loss=13.8564
	step [88/147], loss=13.0906
	step [89/147], loss=15.9822
	step [90/147], loss=15.5549
	step [91/147], loss=16.5387
	step [92/147], loss=15.3843
	step [93/147], loss=16.0524
	step [94/147], loss=14.9633
	step [95/147], loss=14.5939
	step [96/147], loss=14.7162
	step [97/147], loss=13.4852
	step [98/147], loss=14.3230
	step [99/147], loss=13.7069
	step [100/147], loss=16.2736
	step [101/147], loss=15.4581
	step [102/147], loss=14.4329
	step [103/147], loss=15.3373
	step [104/147], loss=14.3713
	step [105/147], loss=14.2639
	step [106/147], loss=15.3176
	step [107/147], loss=15.1859
	step [108/147], loss=14.1648
	step [109/147], loss=13.7663
	step [110/147], loss=13.6212
	step [111/147], loss=14.2270
	step [112/147], loss=14.3637
	step [113/147], loss=14.3600
	step [114/147], loss=13.5461
	step [115/147], loss=14.1521
	step [116/147], loss=15.0799
	step [117/147], loss=13.8791
	step [118/147], loss=14.5977
	step [119/147], loss=16.3021
	step [120/147], loss=14.8249
	step [121/147], loss=14.0611
	step [122/147], loss=14.2105
	step [123/147], loss=14.2453
	step [124/147], loss=13.1486
	step [125/147], loss=14.3086
	step [126/147], loss=14.3492
	step [127/147], loss=14.5438
	step [128/147], loss=13.6904
	step [129/147], loss=14.4093
	step [130/147], loss=14.6487
	step [131/147], loss=14.8606
	step [132/147], loss=15.0909
	step [133/147], loss=15.0469
	step [134/147], loss=14.4037
	step [135/147], loss=15.5333
	step [136/147], loss=17.1834
	step [137/147], loss=13.3165
	step [138/147], loss=13.0640
	step [139/147], loss=16.1900
	step [140/147], loss=13.3041
	step [141/147], loss=14.2495
	step [142/147], loss=14.0112
	step [143/147], loss=13.7249
	step [144/147], loss=13.5567
	step [145/147], loss=15.7384
	step [146/147], loss=16.3538
	step [147/147], loss=5.7236
	Evaluating
	loss=0.0451, precision=0.1793, recall=0.9946, f1=0.3038
Training epoch 16
	step [1/147], loss=14.6386
	step [2/147], loss=14.1580
	step [3/147], loss=14.4429
	step [4/147], loss=12.7445
	step [5/147], loss=13.5879
	step [6/147], loss=13.4417
	step [7/147], loss=14.1350
	step [8/147], loss=14.9743
	step [9/147], loss=15.9252
	step [10/147], loss=14.6607
	step [11/147], loss=13.9471
	step [12/147], loss=15.1906
	step [13/147], loss=13.3691
	step [14/147], loss=13.9959
	step [15/147], loss=14.9893
	step [16/147], loss=15.4746
	step [17/147], loss=12.4745
	step [18/147], loss=12.3759
	step [19/147], loss=14.6772
	step [20/147], loss=15.5314
	step [21/147], loss=14.8482
	step [22/147], loss=13.7850
	step [23/147], loss=15.1356
	step [24/147], loss=14.2622
	step [25/147], loss=13.3026
	step [26/147], loss=14.7499
	step [27/147], loss=15.8372
	step [28/147], loss=15.4334
	step [29/147], loss=15.7529
	step [30/147], loss=16.4496
	step [31/147], loss=14.1890
	step [32/147], loss=13.6072
	step [33/147], loss=14.5391
	step [34/147], loss=16.8723
	step [35/147], loss=14.5607
	step [36/147], loss=12.6574
	step [37/147], loss=14.2342
	step [38/147], loss=14.1403
	step [39/147], loss=14.7294
	step [40/147], loss=15.0244
	step [41/147], loss=13.3755
	step [42/147], loss=13.8818
	step [43/147], loss=13.8474
	step [44/147], loss=14.2800
	step [45/147], loss=13.1484
	step [46/147], loss=14.0241
	step [47/147], loss=14.7231
	step [48/147], loss=14.4428
	step [49/147], loss=13.9070
	step [50/147], loss=13.7499
	step [51/147], loss=14.5169
	step [52/147], loss=13.5844
	step [53/147], loss=14.9877
	step [54/147], loss=14.1221
	step [55/147], loss=13.1012
	step [56/147], loss=17.0461
	step [57/147], loss=15.7501
	step [58/147], loss=13.3358
	step [59/147], loss=13.8493
	step [60/147], loss=13.6590
	step [61/147], loss=16.1565
	step [62/147], loss=13.8300
	step [63/147], loss=15.0973
	step [64/147], loss=14.6380
	step [65/147], loss=13.5875
	step [66/147], loss=14.4147
	step [67/147], loss=16.2273
	step [68/147], loss=14.0190
	step [69/147], loss=15.3846
	step [70/147], loss=14.7296
	step [71/147], loss=13.2439
	step [72/147], loss=13.4281
	step [73/147], loss=14.4131
	step [74/147], loss=12.9789
	step [75/147], loss=14.8500
	step [76/147], loss=14.8176
	step [77/147], loss=14.6034
	step [78/147], loss=12.8487
	step [79/147], loss=14.1698
	step [80/147], loss=14.4894
	step [81/147], loss=14.4970
	step [82/147], loss=14.8211
	step [83/147], loss=15.0266
	step [84/147], loss=14.2014
	step [85/147], loss=13.5266
	step [86/147], loss=13.1890
	step [87/147], loss=15.8211
	step [88/147], loss=13.2537
	step [89/147], loss=14.2968
	step [90/147], loss=13.3452
	step [91/147], loss=13.7003
	step [92/147], loss=14.6528
	step [93/147], loss=15.7660
	step [94/147], loss=13.4560
	step [95/147], loss=12.3399
	step [96/147], loss=14.3097
	step [97/147], loss=14.3725
	step [98/147], loss=12.7876
	step [99/147], loss=13.3468
	step [100/147], loss=14.1050
	step [101/147], loss=12.7291
	step [102/147], loss=14.0869
	step [103/147], loss=13.4898
	step [104/147], loss=13.2426
	step [105/147], loss=12.6741
	step [106/147], loss=12.4166
	step [107/147], loss=13.8171
	step [108/147], loss=13.6574
	step [109/147], loss=13.5093
	step [110/147], loss=13.9038
	step [111/147], loss=14.4331
	step [112/147], loss=12.8510
	step [113/147], loss=11.7654
	step [114/147], loss=14.3407
	step [115/147], loss=13.6488
	step [116/147], loss=13.8460
	step [117/147], loss=12.7925
	step [118/147], loss=12.5839
	step [119/147], loss=14.0254
	step [120/147], loss=14.2535
	step [121/147], loss=14.3129
	step [122/147], loss=12.7581
	step [123/147], loss=14.2689
	step [124/147], loss=12.4909
	step [125/147], loss=15.5868
	step [126/147], loss=14.0643
	step [127/147], loss=13.4031
	step [128/147], loss=14.5899
	step [129/147], loss=13.8794
	step [130/147], loss=14.0010
	step [131/147], loss=14.3172
	step [132/147], loss=13.9616
	step [133/147], loss=13.0271
	step [134/147], loss=14.5068
	step [135/147], loss=14.0954
	step [136/147], loss=14.0017
	step [137/147], loss=13.5100
	step [138/147], loss=12.0763
	step [139/147], loss=14.3087
	step [140/147], loss=14.1866
	step [141/147], loss=13.7787
	step [142/147], loss=14.1531
	step [143/147], loss=12.5057
	step [144/147], loss=13.6802
	step [145/147], loss=12.7957
	step [146/147], loss=12.7744
	step [147/147], loss=5.6469
	Evaluating
	loss=0.0367, precision=0.2289, recall=0.9935, f1=0.3721
saving model as: 0_saved_model.pth
Training epoch 17
	step [1/147], loss=14.0732
	step [2/147], loss=12.5547
	step [3/147], loss=12.7972
	step [4/147], loss=13.9989
	step [5/147], loss=12.2658
	step [6/147], loss=12.2998
	step [7/147], loss=14.9359
	step [8/147], loss=12.5964
	step [9/147], loss=12.7383
	step [10/147], loss=14.2815
	step [11/147], loss=13.2933
	step [12/147], loss=13.8963
	step [13/147], loss=14.0322
	step [14/147], loss=14.8427
	step [15/147], loss=13.1537
	step [16/147], loss=14.6953
	step [17/147], loss=14.0675
	step [18/147], loss=12.4347
	step [19/147], loss=13.4344
	step [20/147], loss=13.8968
	step [21/147], loss=11.3127
	step [22/147], loss=12.7399
	step [23/147], loss=13.2018
	step [24/147], loss=13.5404
	step [25/147], loss=11.8728
	step [26/147], loss=13.9460
	step [27/147], loss=13.5214
	step [28/147], loss=14.6694
	step [29/147], loss=13.6878
	step [30/147], loss=12.4381
	step [31/147], loss=14.1651
	step [32/147], loss=13.6808
	step [33/147], loss=13.2745
	step [34/147], loss=13.0351
	step [35/147], loss=13.5885
	step [36/147], loss=14.7599
	step [37/147], loss=13.9144
	step [38/147], loss=12.9180
	step [39/147], loss=13.2636
	step [40/147], loss=15.9770
	step [41/147], loss=12.4984
	step [42/147], loss=13.2762
	step [43/147], loss=11.9430
	step [44/147], loss=13.4124
	step [45/147], loss=12.2531
	step [46/147], loss=14.4069
	step [47/147], loss=13.0076
	step [48/147], loss=13.4257
	step [49/147], loss=13.6336
	step [50/147], loss=14.9868
	step [51/147], loss=12.6247
	step [52/147], loss=13.0913
	step [53/147], loss=12.5910
	step [54/147], loss=13.8406
	step [55/147], loss=13.5589
	step [56/147], loss=14.8979
	step [57/147], loss=14.6260
	step [58/147], loss=13.1214
	step [59/147], loss=13.2612
	step [60/147], loss=13.5745
	step [61/147], loss=12.1204
	step [62/147], loss=12.9758
	step [63/147], loss=14.6219
	step [64/147], loss=12.2641
	step [65/147], loss=12.5637
	step [66/147], loss=15.1024
	step [67/147], loss=12.6840
	step [68/147], loss=12.7043
	step [69/147], loss=12.9612
	step [70/147], loss=13.8300
	step [71/147], loss=13.7785
	step [72/147], loss=13.3375
	step [73/147], loss=12.6395
	step [74/147], loss=12.5425
	step [75/147], loss=12.5728
	step [76/147], loss=13.4343
	step [77/147], loss=12.6793
	step [78/147], loss=15.0316
	step [79/147], loss=13.7187
	step [80/147], loss=12.9388
	step [81/147], loss=13.4010
	step [82/147], loss=11.8356
	step [83/147], loss=12.6878
	step [84/147], loss=13.0960
	step [85/147], loss=13.5820
	step [86/147], loss=15.4485
	step [87/147], loss=12.9066
	step [88/147], loss=13.7428
	step [89/147], loss=13.4640
	step [90/147], loss=13.7979
	step [91/147], loss=13.6729
	step [92/147], loss=14.8066
	step [93/147], loss=14.2658
	step [94/147], loss=12.8702
	step [95/147], loss=14.1584
	step [96/147], loss=13.8856
	step [97/147], loss=11.1077
	step [98/147], loss=13.7883
	step [99/147], loss=14.0694
	step [100/147], loss=14.1792
	step [101/147], loss=13.5501
	step [102/147], loss=12.7299
	step [103/147], loss=10.6462
	step [104/147], loss=12.3836
	step [105/147], loss=12.4411
	step [106/147], loss=13.8172
	step [107/147], loss=15.2578
	step [108/147], loss=13.2040
	step [109/147], loss=12.8760
	step [110/147], loss=12.3649
	step [111/147], loss=12.6618
	step [112/147], loss=13.0743
	step [113/147], loss=12.9597
	step [114/147], loss=13.1060
	step [115/147], loss=13.3378
	step [116/147], loss=12.8009
	step [117/147], loss=15.9652
	step [118/147], loss=13.1113
	step [119/147], loss=13.8082
	step [120/147], loss=13.5835
	step [121/147], loss=11.5807
	step [122/147], loss=14.4528
	step [123/147], loss=13.4185
	step [124/147], loss=12.4051
	step [125/147], loss=15.4757
	step [126/147], loss=13.8141
	step [127/147], loss=12.2154
	step [128/147], loss=13.7935
	step [129/147], loss=12.0216
	step [130/147], loss=12.4357
	step [131/147], loss=11.3712
	step [132/147], loss=14.1681
	step [133/147], loss=12.1075
	step [134/147], loss=14.7237
	step [135/147], loss=13.4068
	step [136/147], loss=12.4481
	step [137/147], loss=13.9993
	step [138/147], loss=14.2536
	step [139/147], loss=13.9131
	step [140/147], loss=12.0548
	step [141/147], loss=12.7498
	step [142/147], loss=13.3021
	step [143/147], loss=13.7348
	step [144/147], loss=11.7718
	step [145/147], loss=14.1611
	step [146/147], loss=11.9640
	step [147/147], loss=4.8015
	Evaluating
	loss=0.0344, precision=0.2397, recall=0.9930, f1=0.3862
saving model as: 0_saved_model.pth
Training epoch 18
	step [1/147], loss=13.6993
	step [2/147], loss=14.1274
	step [3/147], loss=11.4289
	step [4/147], loss=11.7138
	step [5/147], loss=13.9409
	step [6/147], loss=13.4646
	step [7/147], loss=12.9964
	step [8/147], loss=12.5023
	step [9/147], loss=13.0832
	step [10/147], loss=13.1615
	step [11/147], loss=13.8148
	step [12/147], loss=12.3095
	step [13/147], loss=12.9636
	step [14/147], loss=12.7987
	step [15/147], loss=13.3253
	step [16/147], loss=11.7969
	step [17/147], loss=12.7094
	step [18/147], loss=14.4585
	step [19/147], loss=12.9639
	step [20/147], loss=11.8217
	step [21/147], loss=11.6752
	step [22/147], loss=11.4257
	step [23/147], loss=13.0421
	step [24/147], loss=13.3052
	step [25/147], loss=13.4062
	step [26/147], loss=12.6054
	step [27/147], loss=13.8247
	step [28/147], loss=13.9104
	step [29/147], loss=12.7490
	step [30/147], loss=14.0358
	step [31/147], loss=13.6430
	step [32/147], loss=14.1914
	step [33/147], loss=12.5196
	step [34/147], loss=12.2797
	step [35/147], loss=14.7116
	step [36/147], loss=12.6658
	step [37/147], loss=12.3349
	step [38/147], loss=11.8049
	step [39/147], loss=11.8492
	step [40/147], loss=13.1355
	step [41/147], loss=12.9280
	step [42/147], loss=11.2821
	step [43/147], loss=12.9843
	step [44/147], loss=11.8807
	step [45/147], loss=14.3169
	step [46/147], loss=13.8802
	step [47/147], loss=12.5700
	step [48/147], loss=14.1858
	step [49/147], loss=12.2396
	step [50/147], loss=14.5452
	step [51/147], loss=13.6448
	step [52/147], loss=12.8158
	step [53/147], loss=11.8023
	step [54/147], loss=13.1477
	step [55/147], loss=12.9914
	step [56/147], loss=11.8727
	step [57/147], loss=13.2620
	step [58/147], loss=11.9751
	step [59/147], loss=12.5378
	step [60/147], loss=11.3108
	step [61/147], loss=11.3465
	step [62/147], loss=12.2695
	step [63/147], loss=11.8908
	step [64/147], loss=11.8562
	step [65/147], loss=12.2012
	step [66/147], loss=12.4221
	step [67/147], loss=13.2388
	step [68/147], loss=14.0541
	step [69/147], loss=12.9406
	step [70/147], loss=12.0199
	step [71/147], loss=11.5159
	step [72/147], loss=12.1506
	step [73/147], loss=13.5983
	step [74/147], loss=11.7804
	step [75/147], loss=12.7326
	step [76/147], loss=13.4823
	step [77/147], loss=11.5217
	step [78/147], loss=12.8516
	step [79/147], loss=12.7371
	step [80/147], loss=12.8435
	step [81/147], loss=13.5800
	step [82/147], loss=11.0452
	step [83/147], loss=14.7655
	step [84/147], loss=11.3523
	step [85/147], loss=12.3838
	step [86/147], loss=12.9621
	step [87/147], loss=13.0352
	step [88/147], loss=11.3362
	step [89/147], loss=12.7005
	step [90/147], loss=11.2028
	step [91/147], loss=11.6812
	step [92/147], loss=12.8284
	step [93/147], loss=10.7762
	step [94/147], loss=12.5316
	step [95/147], loss=10.8240
	step [96/147], loss=11.6898
	step [97/147], loss=13.2987
	step [98/147], loss=12.1755
	step [99/147], loss=14.2383
	step [100/147], loss=14.2861
	step [101/147], loss=12.4456
	step [102/147], loss=11.8656
	step [103/147], loss=12.5154
	step [104/147], loss=12.1060
	step [105/147], loss=13.2483
	step [106/147], loss=12.6002
	step [107/147], loss=12.4869
	step [108/147], loss=13.1245
	step [109/147], loss=13.3827
	step [110/147], loss=10.9174
	step [111/147], loss=12.7890
	step [112/147], loss=12.3205
	step [113/147], loss=11.2232
	step [114/147], loss=12.2854
	step [115/147], loss=11.5860
	step [116/147], loss=11.2451
	step [117/147], loss=13.7307
	step [118/147], loss=11.3765
	step [119/147], loss=11.3992
	step [120/147], loss=13.1459
	step [121/147], loss=12.4205
	step [122/147], loss=12.4328
	step [123/147], loss=12.6529
	step [124/147], loss=11.9033
	step [125/147], loss=12.6055
	step [126/147], loss=13.7585
	step [127/147], loss=14.8556
	step [128/147], loss=12.6389
	step [129/147], loss=11.9810
	step [130/147], loss=14.4470
	step [131/147], loss=11.8151
	step [132/147], loss=12.2939
	step [133/147], loss=12.8884
	step [134/147], loss=14.0561
	step [135/147], loss=11.3395
	step [136/147], loss=11.5988
	step [137/147], loss=11.7697
	step [138/147], loss=11.3815
	step [139/147], loss=14.6656
	step [140/147], loss=12.3618
	step [141/147], loss=13.8404
	step [142/147], loss=11.1930
	step [143/147], loss=11.8635
	step [144/147], loss=12.6143
	step [145/147], loss=11.9675
	step [146/147], loss=12.3892
	step [147/147], loss=4.8509
	Evaluating
	loss=0.0398, precision=0.1860, recall=0.9953, f1=0.3134
Training epoch 19
	step [1/147], loss=12.9813
	step [2/147], loss=13.0921
	step [3/147], loss=14.1449
	step [4/147], loss=13.1200
	step [5/147], loss=11.7997
	step [6/147], loss=10.7135
	step [7/147], loss=11.4822
	step [8/147], loss=12.7983
	step [9/147], loss=12.3058
	step [10/147], loss=12.3373
	step [11/147], loss=11.1431
	step [12/147], loss=14.1438
	step [13/147], loss=12.6923
	step [14/147], loss=10.9298
	step [15/147], loss=11.4480
	step [16/147], loss=11.3911
	step [17/147], loss=13.9532
	step [18/147], loss=12.1139
	step [19/147], loss=13.2770
	step [20/147], loss=12.0148
	step [21/147], loss=13.4346
	step [22/147], loss=11.9370
	step [23/147], loss=11.5499
	step [24/147], loss=12.1259
	step [25/147], loss=14.0955
	step [26/147], loss=10.0737
	step [27/147], loss=12.9582
	step [28/147], loss=11.2007
	step [29/147], loss=12.1699
	step [30/147], loss=11.0264
	step [31/147], loss=11.4126
	step [32/147], loss=12.7603
	step [33/147], loss=13.1938
	step [34/147], loss=12.9897
	step [35/147], loss=12.3304
	step [36/147], loss=11.9819
	step [37/147], loss=11.5378
	step [38/147], loss=12.8416
	step [39/147], loss=10.1049
	step [40/147], loss=14.3263
	step [41/147], loss=12.0123
	step [42/147], loss=13.4801
	step [43/147], loss=12.4032
	step [44/147], loss=12.3490
	step [45/147], loss=13.3188
	step [46/147], loss=12.3948
	step [47/147], loss=12.6197
	step [48/147], loss=10.5827
	step [49/147], loss=13.6128
	step [50/147], loss=13.2099
	step [51/147], loss=13.3760
	step [52/147], loss=12.9089
	step [53/147], loss=12.7021
	step [54/147], loss=11.0803
	step [55/147], loss=11.7036
	step [56/147], loss=13.8710
	step [57/147], loss=12.7132
	step [58/147], loss=10.9595
	step [59/147], loss=11.1438
	step [60/147], loss=12.0954
	step [61/147], loss=12.3439
	step [62/147], loss=11.2489
	step [63/147], loss=12.3271
	step [64/147], loss=12.0108
	step [65/147], loss=12.0663
	step [66/147], loss=12.0936
	step [67/147], loss=13.1420
	step [68/147], loss=12.2452
	step [69/147], loss=12.1694
	step [70/147], loss=12.6727
	step [71/147], loss=10.9169
	step [72/147], loss=11.5303
	step [73/147], loss=12.2481
	step [74/147], loss=11.5339
	step [75/147], loss=12.2974
	step [76/147], loss=12.2755
	step [77/147], loss=11.8534
	step [78/147], loss=12.8423
	step [79/147], loss=13.5736
	step [80/147], loss=12.9140
	step [81/147], loss=12.0770
	step [82/147], loss=11.6221
	step [83/147], loss=11.0987
	step [84/147], loss=11.1718
	step [85/147], loss=10.0722
	step [86/147], loss=11.5076
	step [87/147], loss=10.1202
	step [88/147], loss=12.5054
	step [89/147], loss=12.5596
	step [90/147], loss=12.3333
	step [91/147], loss=12.3931
	step [92/147], loss=12.2128
	step [93/147], loss=12.6450
	step [94/147], loss=13.4431
	step [95/147], loss=13.1480
	step [96/147], loss=11.5646
	step [97/147], loss=12.6624
	step [98/147], loss=13.3479
	step [99/147], loss=10.6557
	step [100/147], loss=11.5658
	step [101/147], loss=11.1622
	step [102/147], loss=13.1394
	step [103/147], loss=11.4209
	step [104/147], loss=12.0469
	step [105/147], loss=12.4387
	step [106/147], loss=12.9328
	step [107/147], loss=12.3275
	step [108/147], loss=11.4547
	step [109/147], loss=11.6752
	step [110/147], loss=12.2470
	step [111/147], loss=10.8526
	step [112/147], loss=13.0297
	step [113/147], loss=11.2456
	step [114/147], loss=9.4837
	step [115/147], loss=10.2097
	step [116/147], loss=11.0668
	step [117/147], loss=10.8750
	step [118/147], loss=11.4625
	step [119/147], loss=10.6187
	step [120/147], loss=11.5344
	step [121/147], loss=11.2665
	step [122/147], loss=11.7566
	step [123/147], loss=11.6004
	step [124/147], loss=10.4751
	step [125/147], loss=10.8730
	step [126/147], loss=11.5269
	step [127/147], loss=10.3713
	step [128/147], loss=11.2680
	step [129/147], loss=10.3831
	step [130/147], loss=12.4404
	step [131/147], loss=11.3558
	step [132/147], loss=11.9915
	step [133/147], loss=12.0004
	step [134/147], loss=11.2501
	step [135/147], loss=11.2239
	step [136/147], loss=12.0784
	step [137/147], loss=11.1651
	step [138/147], loss=11.9221
	step [139/147], loss=11.9410
	step [140/147], loss=12.4841
	step [141/147], loss=12.4865
	step [142/147], loss=12.2729
	step [143/147], loss=10.7726
	step [144/147], loss=10.9220
	step [145/147], loss=10.8065
	step [146/147], loss=12.1252
	step [147/147], loss=5.4464
	Evaluating
	loss=0.0380, precision=0.1849, recall=0.9951, f1=0.3119
Training epoch 20
	step [1/147], loss=10.5041
	step [2/147], loss=12.2096
	step [3/147], loss=12.3604
	step [4/147], loss=11.1998
	step [5/147], loss=11.1282
	step [6/147], loss=11.2789
	step [7/147], loss=11.8918
	step [8/147], loss=12.1568
	step [9/147], loss=11.8621
	step [10/147], loss=11.0294
	step [11/147], loss=11.4447
	step [12/147], loss=12.7720
	step [13/147], loss=12.3065
	step [14/147], loss=11.7227
	step [15/147], loss=10.8478
	step [16/147], loss=11.0229
	step [17/147], loss=11.9766
	step [18/147], loss=10.9629
	step [19/147], loss=11.9192
	step [20/147], loss=11.5399
	step [21/147], loss=12.2277
	step [22/147], loss=11.4524
	step [23/147], loss=12.9278
	step [24/147], loss=11.5894
	step [25/147], loss=11.1095
	step [26/147], loss=12.5599
	step [27/147], loss=10.7075
	step [28/147], loss=10.4223
	step [29/147], loss=10.1034
	step [30/147], loss=12.4164
	step [31/147], loss=11.4751
	step [32/147], loss=12.1826
	step [33/147], loss=13.6195
	step [34/147], loss=11.4210
	step [35/147], loss=10.1423
	step [36/147], loss=11.6098
	step [37/147], loss=12.5112
	step [38/147], loss=10.5225
	step [39/147], loss=13.3363
	step [40/147], loss=11.8988
	step [41/147], loss=11.5859
	step [42/147], loss=14.9369
	step [43/147], loss=11.3889
	step [44/147], loss=12.6501
	step [45/147], loss=11.9832
	step [46/147], loss=11.8563
	step [47/147], loss=11.5680
	step [48/147], loss=12.0031
	step [49/147], loss=10.7427
	step [50/147], loss=12.9607
	step [51/147], loss=11.0913
	step [52/147], loss=10.6012
	step [53/147], loss=11.2699
	step [54/147], loss=14.6973
	step [55/147], loss=10.1674
	step [56/147], loss=12.4570
	step [57/147], loss=11.0102
	step [58/147], loss=10.9445
	step [59/147], loss=10.9073
	step [60/147], loss=11.1402
	step [61/147], loss=11.8630
	step [62/147], loss=11.0593
	step [63/147], loss=12.4491
	step [64/147], loss=11.4467
	step [65/147], loss=12.1066
	step [66/147], loss=11.4454
	step [67/147], loss=11.6065
	step [68/147], loss=13.0244
	step [69/147], loss=11.9185
	step [70/147], loss=11.0782
	step [71/147], loss=13.3160
	step [72/147], loss=11.2025
	step [73/147], loss=11.2958
	step [74/147], loss=12.0298
	step [75/147], loss=10.7282
	step [76/147], loss=11.8785
	step [77/147], loss=11.8640
	step [78/147], loss=11.2832
	step [79/147], loss=10.9633
	step [80/147], loss=11.4988
	step [81/147], loss=11.7422
	step [82/147], loss=11.6457
	step [83/147], loss=11.2745
	step [84/147], loss=11.4563
	step [85/147], loss=10.6859
	step [86/147], loss=9.8176
	step [87/147], loss=11.7154
	step [88/147], loss=11.7168
	step [89/147], loss=12.1004
	step [90/147], loss=11.2014
	step [91/147], loss=9.8459
	step [92/147], loss=10.8036
	step [93/147], loss=12.0899
	step [94/147], loss=11.2889
	step [95/147], loss=12.8448
	step [96/147], loss=12.9709
	step [97/147], loss=12.0958
	step [98/147], loss=12.1622
	step [99/147], loss=12.6063
	step [100/147], loss=11.2490
	step [101/147], loss=11.7210
	step [102/147], loss=11.5854
	step [103/147], loss=11.7882
	step [104/147], loss=12.3078
	step [105/147], loss=11.2922
	step [106/147], loss=11.6958
	step [107/147], loss=11.5564
	step [108/147], loss=12.2732
	step [109/147], loss=11.2202
	step [110/147], loss=10.7600
	step [111/147], loss=10.8008
	step [112/147], loss=11.0019
	step [113/147], loss=11.8603
	step [114/147], loss=11.2860
	step [115/147], loss=10.2883
	step [116/147], loss=11.7767
	step [117/147], loss=12.4916
	step [118/147], loss=11.1903
	step [119/147], loss=10.6286
	step [120/147], loss=10.1673
	step [121/147], loss=11.4103
	step [122/147], loss=11.7866
	step [123/147], loss=11.2870
	step [124/147], loss=11.8867
	step [125/147], loss=11.0193
	step [126/147], loss=11.7303
	step [127/147], loss=11.0891
	step [128/147], loss=12.6857
	step [129/147], loss=11.1505
	step [130/147], loss=9.9972
	step [131/147], loss=12.9436
	step [132/147], loss=10.7109
	step [133/147], loss=10.9455
	step [134/147], loss=11.3882
	step [135/147], loss=9.8250
	step [136/147], loss=10.0651
	step [137/147], loss=12.1830
	step [138/147], loss=10.6178
	step [139/147], loss=11.8097
	step [140/147], loss=11.1435
	step [141/147], loss=12.5254
	step [142/147], loss=9.9132
	step [143/147], loss=12.6971
	step [144/147], loss=13.4855
	step [145/147], loss=11.4658
	step [146/147], loss=10.9663
	step [147/147], loss=4.2314
	Evaluating
	loss=0.0417, precision=0.1624, recall=0.9956, f1=0.2792
Training epoch 21
	step [1/147], loss=10.2839
	step [2/147], loss=11.2901
	step [3/147], loss=11.0512
	step [4/147], loss=11.0251
	step [5/147], loss=11.6033
	step [6/147], loss=10.6360
	step [7/147], loss=10.7686
	step [8/147], loss=11.5455
	step [9/147], loss=11.7873
	step [10/147], loss=9.9667
	step [11/147], loss=10.5522
	step [12/147], loss=11.6573
	step [13/147], loss=10.2912
	step [14/147], loss=10.2202
	step [15/147], loss=10.7678
	step [16/147], loss=10.2883
	step [17/147], loss=11.3658
	step [18/147], loss=10.4520
	step [19/147], loss=9.6017
	step [20/147], loss=11.1144
	step [21/147], loss=10.4451
	step [22/147], loss=11.4862
	step [23/147], loss=10.0380
	step [24/147], loss=12.7475
	step [25/147], loss=9.2805
	step [26/147], loss=11.3015
	step [27/147], loss=10.0648
	step [28/147], loss=10.3598
	step [29/147], loss=12.7366
	step [30/147], loss=9.8639
	step [31/147], loss=10.5798
	step [32/147], loss=12.1358
	step [33/147], loss=11.4028
	step [34/147], loss=10.5706
	step [35/147], loss=10.8096
	step [36/147], loss=9.7723
	step [37/147], loss=11.6415
	step [38/147], loss=11.2365
	step [39/147], loss=11.4223
	step [40/147], loss=10.9824
	step [41/147], loss=10.2663
	step [42/147], loss=10.5973
	step [43/147], loss=10.4480
	step [44/147], loss=11.4192
	step [45/147], loss=10.8800
	step [46/147], loss=11.6031
	step [47/147], loss=11.1243
	step [48/147], loss=9.6023
	step [49/147], loss=10.6718
	step [50/147], loss=10.1046
	step [51/147], loss=10.7804
	step [52/147], loss=10.3727
	step [53/147], loss=10.3739
	step [54/147], loss=10.4072
	step [55/147], loss=11.7513
	step [56/147], loss=12.3674
	step [57/147], loss=10.7846
	step [58/147], loss=11.5087
	step [59/147], loss=10.3307
	step [60/147], loss=10.6978
	step [61/147], loss=10.1221
	step [62/147], loss=10.6630
	step [63/147], loss=9.9740
	step [64/147], loss=9.9730
	step [65/147], loss=13.2858
	step [66/147], loss=11.2212
	step [67/147], loss=10.2729
	step [68/147], loss=11.2196
	step [69/147], loss=9.9837
	step [70/147], loss=10.0616
	step [71/147], loss=11.2665
	step [72/147], loss=12.5293
	step [73/147], loss=9.3580
	step [74/147], loss=9.8577
	step [75/147], loss=10.6065
	step [76/147], loss=12.2343
	step [77/147], loss=9.8018
	step [78/147], loss=12.6046
	step [79/147], loss=11.5719
	step [80/147], loss=12.0015
	step [81/147], loss=11.2324
	step [82/147], loss=10.2289
	step [83/147], loss=11.2400
	step [84/147], loss=11.1290
	step [85/147], loss=10.2018
	step [86/147], loss=10.8846
	step [87/147], loss=10.3461
	step [88/147], loss=10.2379
	step [89/147], loss=11.5848
	step [90/147], loss=11.0841
	step [91/147], loss=10.4082
	step [92/147], loss=12.3550
	step [93/147], loss=11.2777
	step [94/147], loss=10.2953
	step [95/147], loss=12.1509
	step [96/147], loss=13.3684
	step [97/147], loss=11.6842
	step [98/147], loss=10.6034
	step [99/147], loss=10.9015
	step [100/147], loss=12.2219
	step [101/147], loss=11.5125
	step [102/147], loss=11.3643
	step [103/147], loss=10.0501
	step [104/147], loss=11.5047
	step [105/147], loss=10.5381
	step [106/147], loss=10.3813
	step [107/147], loss=11.2643
	step [108/147], loss=10.7047
	step [109/147], loss=11.0188
	step [110/147], loss=11.2914
	step [111/147], loss=11.7286
	step [112/147], loss=11.0608
	step [113/147], loss=10.7009
	step [114/147], loss=11.8664
	step [115/147], loss=10.5614
	step [116/147], loss=10.5687
	step [117/147], loss=11.4226
	step [118/147], loss=10.3686
	step [119/147], loss=11.6969
	step [120/147], loss=9.9671
	step [121/147], loss=11.1845
	step [122/147], loss=11.1405
	step [123/147], loss=10.4074
	step [124/147], loss=10.1895
	step [125/147], loss=10.9107
	step [126/147], loss=10.3627
	step [127/147], loss=10.9165
	step [128/147], loss=9.6662
	step [129/147], loss=11.9731
	step [130/147], loss=12.1665
	step [131/147], loss=10.6796
	step [132/147], loss=12.2215
	step [133/147], loss=10.8001
	step [134/147], loss=10.7489
	step [135/147], loss=12.1356
	step [136/147], loss=10.4207
	step [137/147], loss=9.8840
	step [138/147], loss=10.6595
	step [139/147], loss=10.6085
	step [140/147], loss=11.0576
	step [141/147], loss=9.8259
	step [142/147], loss=11.6699
	step [143/147], loss=13.2610
	step [144/147], loss=10.8757
	step [145/147], loss=10.0427
	step [146/147], loss=11.4706
	step [147/147], loss=5.5828
	Evaluating
	loss=0.0381, precision=0.1776, recall=0.9952, f1=0.3014
Training epoch 22
	step [1/147], loss=11.9715
	step [2/147], loss=11.2823
	step [3/147], loss=11.1941
	step [4/147], loss=11.0899
	step [5/147], loss=10.5770
	step [6/147], loss=10.5562
	step [7/147], loss=10.4143
	step [8/147], loss=11.4410
	step [9/147], loss=10.3739
	step [10/147], loss=10.8085
	step [11/147], loss=10.0559
	step [12/147], loss=11.2438
	step [13/147], loss=11.7269
	step [14/147], loss=9.4141
	step [15/147], loss=10.1395
	step [16/147], loss=10.4120
	step [17/147], loss=11.9803
	step [18/147], loss=10.3247
	step [19/147], loss=10.6735
	step [20/147], loss=10.6924
	step [21/147], loss=11.5568
	step [22/147], loss=10.3965
	step [23/147], loss=9.7450
	step [24/147], loss=9.5999
	step [25/147], loss=10.2247
	step [26/147], loss=9.9973
	step [27/147], loss=8.8095
	step [28/147], loss=10.8035
	step [29/147], loss=10.8672
	step [30/147], loss=10.3942
	step [31/147], loss=9.1460
	step [32/147], loss=9.8018
	step [33/147], loss=10.5194
	step [34/147], loss=11.2024
	step [35/147], loss=10.6698
	step [36/147], loss=9.9162
	step [37/147], loss=10.6454
	step [38/147], loss=9.8354
	step [39/147], loss=10.7805
	step [40/147], loss=10.4258
	step [41/147], loss=11.2256
	step [42/147], loss=10.4986
	step [43/147], loss=10.6080
	step [44/147], loss=10.3815
	step [45/147], loss=10.3116
	step [46/147], loss=10.5721
	step [47/147], loss=12.9906
	step [48/147], loss=10.3049
	step [49/147], loss=10.6511
	step [50/147], loss=9.7659
	step [51/147], loss=8.8634
	step [52/147], loss=11.2120
	step [53/147], loss=10.3491
	step [54/147], loss=11.5920
	step [55/147], loss=11.1141
	step [56/147], loss=10.3087
	step [57/147], loss=9.5361
	step [58/147], loss=10.8323
	step [59/147], loss=9.9840
	step [60/147], loss=10.0343
	step [61/147], loss=9.5358
	step [62/147], loss=10.8423
	step [63/147], loss=11.0603
	step [64/147], loss=9.8216
	step [65/147], loss=9.3479
	step [66/147], loss=12.1287
	step [67/147], loss=10.0212
	step [68/147], loss=11.6543
	step [69/147], loss=9.8010
	step [70/147], loss=11.0229
	step [71/147], loss=11.3489
	step [72/147], loss=10.2655
	step [73/147], loss=10.7706
	step [74/147], loss=12.1645
	step [75/147], loss=10.6378
	step [76/147], loss=10.1272
	step [77/147], loss=10.1983
	step [78/147], loss=11.1523
	step [79/147], loss=10.6808
	step [80/147], loss=10.4839
	step [81/147], loss=10.5023
	step [82/147], loss=11.4088
	step [83/147], loss=9.9384
	step [84/147], loss=11.5316
	step [85/147], loss=10.8827
	step [86/147], loss=12.1839
	step [87/147], loss=10.5312
	step [88/147], loss=11.7338
	step [89/147], loss=11.2502
	step [90/147], loss=11.0477
	step [91/147], loss=10.2129
	step [92/147], loss=10.7051
	step [93/147], loss=10.2087
	step [94/147], loss=11.7693
	step [95/147], loss=11.6903
	step [96/147], loss=10.8991
	step [97/147], loss=10.0148
	step [98/147], loss=11.1650
	step [99/147], loss=10.5893
	step [100/147], loss=9.4276
	step [101/147], loss=10.2390
	step [102/147], loss=10.4236
	step [103/147], loss=11.0013
	step [104/147], loss=9.7418
	step [105/147], loss=10.5419
	step [106/147], loss=9.1359
	step [107/147], loss=10.1712
	step [108/147], loss=9.8112
	step [109/147], loss=10.3296
	step [110/147], loss=9.3957
	step [111/147], loss=10.9637
	step [112/147], loss=11.6793
	step [113/147], loss=11.2767
	step [114/147], loss=10.1587
	step [115/147], loss=9.8104
	step [116/147], loss=9.1622
	step [117/147], loss=8.8357
	step [118/147], loss=10.3457
	step [119/147], loss=9.7235
	step [120/147], loss=10.4014
	step [121/147], loss=9.8860
	step [122/147], loss=10.4576
	step [123/147], loss=9.0068
	step [124/147], loss=10.8073
	step [125/147], loss=11.5001
	step [126/147], loss=12.4521
	step [127/147], loss=11.0656
	step [128/147], loss=10.6584
	step [129/147], loss=12.0176
	step [130/147], loss=9.9783
	step [131/147], loss=9.8897
	step [132/147], loss=11.0219
	step [133/147], loss=9.2870
	step [134/147], loss=10.3972
	step [135/147], loss=11.3367
	step [136/147], loss=10.7348
	step [137/147], loss=11.9563
	step [138/147], loss=11.7732
	step [139/147], loss=9.8636
	step [140/147], loss=11.3804
	step [141/147], loss=9.4094
	step [142/147], loss=11.5441
	step [143/147], loss=9.5782
	step [144/147], loss=10.2728
	step [145/147], loss=9.5817
	step [146/147], loss=9.7452
	step [147/147], loss=5.1641
	Evaluating
	loss=0.0291, precision=0.2358, recall=0.9916, f1=0.3810
Training epoch 23
	step [1/147], loss=10.0774
	step [2/147], loss=10.4046
	step [3/147], loss=9.9683
	step [4/147], loss=9.1563
	step [5/147], loss=10.0730
	step [6/147], loss=9.8432
	step [7/147], loss=12.2592
	step [8/147], loss=9.9940
	step [9/147], loss=9.5010
	step [10/147], loss=9.5171
	step [11/147], loss=10.7431
	step [12/147], loss=10.7917
	step [13/147], loss=10.7440
	step [14/147], loss=9.7352
	step [15/147], loss=9.2194
	step [16/147], loss=11.0926
	step [17/147], loss=9.7463
	step [18/147], loss=9.7443
	step [19/147], loss=8.9894
	step [20/147], loss=11.0380
	step [21/147], loss=10.2223
	step [22/147], loss=9.2417
	step [23/147], loss=11.2364
	step [24/147], loss=10.3120
	step [25/147], loss=11.1112
	step [26/147], loss=10.3207
	step [27/147], loss=9.1576
	step [28/147], loss=9.8060
	step [29/147], loss=9.1385
	step [30/147], loss=11.5151
	step [31/147], loss=9.3643
	step [32/147], loss=9.6314
	step [33/147], loss=11.2398
	step [34/147], loss=11.5390
	step [35/147], loss=10.3511
	step [36/147], loss=9.3449
	step [37/147], loss=10.2263
	step [38/147], loss=8.7375
	step [39/147], loss=11.2416
	step [40/147], loss=10.4606
	step [41/147], loss=11.4730
	step [42/147], loss=11.0115
	step [43/147], loss=10.2681
	step [44/147], loss=10.4323
	step [45/147], loss=9.8377
	step [46/147], loss=9.3220
	step [47/147], loss=10.5993
	step [48/147], loss=10.4977
	step [49/147], loss=10.6515
	step [50/147], loss=10.0659
	step [51/147], loss=9.9502
	step [52/147], loss=10.7932
	step [53/147], loss=10.6159
	step [54/147], loss=9.5063
	step [55/147], loss=10.6225
	step [56/147], loss=9.5501
	step [57/147], loss=10.0562
	step [58/147], loss=10.1759
	step [59/147], loss=12.3720
	step [60/147], loss=9.5510
	step [61/147], loss=10.7204
	step [62/147], loss=11.4844
	step [63/147], loss=9.9866
	step [64/147], loss=10.4035
	step [65/147], loss=11.7494
	step [66/147], loss=10.0032
	step [67/147], loss=9.5215
	step [68/147], loss=9.6214
	step [69/147], loss=9.8249
	step [70/147], loss=9.4273
	step [71/147], loss=10.0700
	step [72/147], loss=10.0851
	step [73/147], loss=9.3535
	step [74/147], loss=10.1892
	step [75/147], loss=9.2901
	step [76/147], loss=9.9133
	step [77/147], loss=10.2866
	step [78/147], loss=10.5475
	step [79/147], loss=8.8288
	step [80/147], loss=10.3856
	step [81/147], loss=9.6339
	step [82/147], loss=9.9402
	step [83/147], loss=10.4315
	step [84/147], loss=11.2092
	step [85/147], loss=9.2643
	step [86/147], loss=9.2526
	step [87/147], loss=10.9594
	step [88/147], loss=10.0295
	step [89/147], loss=9.2197
	step [90/147], loss=8.6535
	step [91/147], loss=10.2292
	step [92/147], loss=10.0192
	step [93/147], loss=10.5847
	step [94/147], loss=10.3242
	step [95/147], loss=11.1002
	step [96/147], loss=10.6292
	step [97/147], loss=10.7533
	step [98/147], loss=9.5063
	step [99/147], loss=9.6736
	step [100/147], loss=10.8349
	step [101/147], loss=10.0888
	step [102/147], loss=10.5082
	step [103/147], loss=11.3522
	step [104/147], loss=10.8940
	step [105/147], loss=11.1684
	step [106/147], loss=10.0646
	step [107/147], loss=8.5440
	step [108/147], loss=10.7447
	step [109/147], loss=10.2223
	step [110/147], loss=9.2267
	step [111/147], loss=8.4714
	step [112/147], loss=9.7394
	step [113/147], loss=9.1189
	step [114/147], loss=11.0098
	step [115/147], loss=10.4903
	step [116/147], loss=9.5147
	step [117/147], loss=11.7251
	step [118/147], loss=10.6639
	step [119/147], loss=9.4357
	step [120/147], loss=9.7522
	step [121/147], loss=12.1431
	step [122/147], loss=10.1198
	step [123/147], loss=10.6357
	step [124/147], loss=9.6810
	step [125/147], loss=9.1138
	step [126/147], loss=10.5209
	step [127/147], loss=9.9570
	step [128/147], loss=10.6176
	step [129/147], loss=11.6920
	step [130/147], loss=8.8462
	step [131/147], loss=10.0458
	step [132/147], loss=9.9715
	step [133/147], loss=9.9110
	step [134/147], loss=10.5155
	step [135/147], loss=9.6443
	step [136/147], loss=9.8113
	step [137/147], loss=9.4047
	step [138/147], loss=8.6955
	step [139/147], loss=10.2341
	step [140/147], loss=9.7708
	step [141/147], loss=12.2332
	step [142/147], loss=10.7832
	step [143/147], loss=9.9813
	step [144/147], loss=9.5129
	step [145/147], loss=10.6174
	step [146/147], loss=10.4182
	step [147/147], loss=3.7751
	Evaluating
	loss=0.0289, precision=0.2241, recall=0.9937, f1=0.3658
Training epoch 24
	step [1/147], loss=9.6980
	step [2/147], loss=9.6385
	step [3/147], loss=11.4544
	step [4/147], loss=10.3592
	step [5/147], loss=9.7334
	step [6/147], loss=9.4375
	step [7/147], loss=10.3238
	step [8/147], loss=9.2407
	step [9/147], loss=8.8884
	step [10/147], loss=11.8322
	step [11/147], loss=10.0357
	step [12/147], loss=9.8370
	step [13/147], loss=9.3787
	step [14/147], loss=10.3891
	step [15/147], loss=9.2271
	step [16/147], loss=10.0233
	step [17/147], loss=10.0117
	step [18/147], loss=9.3057
	step [19/147], loss=10.3704
	step [20/147], loss=10.0250
	step [21/147], loss=8.9214
	step [22/147], loss=8.9153
	step [23/147], loss=11.0759
	step [24/147], loss=10.5393
	step [25/147], loss=8.6781
	step [26/147], loss=9.0697
	step [27/147], loss=9.6253
	step [28/147], loss=10.8540
	step [29/147], loss=10.2638
	step [30/147], loss=9.0816
	step [31/147], loss=9.3580
	step [32/147], loss=10.9672
	step [33/147], loss=10.0805
	step [34/147], loss=9.3627
	step [35/147], loss=9.7051
	step [36/147], loss=8.6197
	step [37/147], loss=9.9593
	step [38/147], loss=10.3059
	step [39/147], loss=10.3363
	step [40/147], loss=9.2702
	step [41/147], loss=8.8872
	step [42/147], loss=9.1193
	step [43/147], loss=10.0699
	step [44/147], loss=10.3111
	step [45/147], loss=7.7956
	step [46/147], loss=9.6665
	step [47/147], loss=9.5364
	step [48/147], loss=8.3959
	step [49/147], loss=10.7611
	step [50/147], loss=10.2724
	step [51/147], loss=8.8278
	step [52/147], loss=8.7124
	step [53/147], loss=9.7727
	step [54/147], loss=8.4265
	step [55/147], loss=10.3745
	step [56/147], loss=8.6409
	step [57/147], loss=11.1812
	step [58/147], loss=9.0860
	step [59/147], loss=9.3719
	step [60/147], loss=8.4491
	step [61/147], loss=11.0728
	step [62/147], loss=9.4433
	step [63/147], loss=9.7536
	step [64/147], loss=8.8006
	step [65/147], loss=10.9502
	step [66/147], loss=9.6801
	step [67/147], loss=9.3831
	step [68/147], loss=10.2339
	step [69/147], loss=9.2951
	step [70/147], loss=9.5132
	step [71/147], loss=9.8684
	step [72/147], loss=10.2228
	step [73/147], loss=9.9883
	step [74/147], loss=9.5778
	step [75/147], loss=8.9785
	step [76/147], loss=11.3016
	step [77/147], loss=9.3835
	step [78/147], loss=10.5053
	step [79/147], loss=10.5131
	step [80/147], loss=10.9316
	step [81/147], loss=8.5189
	step [82/147], loss=9.3807
	step [83/147], loss=9.8654
	step [84/147], loss=9.4121
	step [85/147], loss=8.3577
	step [86/147], loss=10.8500
	step [87/147], loss=10.5290
	step [88/147], loss=9.4324
	step [89/147], loss=8.9403
	step [90/147], loss=10.3239
	step [91/147], loss=9.9245
	step [92/147], loss=10.1443
	step [93/147], loss=9.4444
	step [94/147], loss=9.4044
	step [95/147], loss=10.2370
	step [96/147], loss=9.7049
	step [97/147], loss=9.2433
	step [98/147], loss=10.0916
	step [99/147], loss=9.7230
	step [100/147], loss=9.7093
	step [101/147], loss=9.0564
	step [102/147], loss=10.2904
	step [103/147], loss=10.4977
	step [104/147], loss=9.1126
	step [105/147], loss=8.8843
	step [106/147], loss=10.1172
	step [107/147], loss=8.8800
	step [108/147], loss=10.1303
	step [109/147], loss=10.2643
	step [110/147], loss=9.0700
	step [111/147], loss=8.9932
	step [112/147], loss=9.7842
	step [113/147], loss=8.9471
	step [114/147], loss=9.8915
	step [115/147], loss=10.3964
	step [116/147], loss=10.8251
	step [117/147], loss=9.2538
	step [118/147], loss=10.8580
	step [119/147], loss=9.5509
	step [120/147], loss=9.7275
	step [121/147], loss=10.6634
	step [122/147], loss=8.7072
	step [123/147], loss=9.9338
	step [124/147], loss=9.6761
	step [125/147], loss=9.1997
	step [126/147], loss=9.1292
	step [127/147], loss=10.6541
	step [128/147], loss=8.7029
	step [129/147], loss=9.6135
	step [130/147], loss=8.6911
	step [131/147], loss=10.7226
	step [132/147], loss=9.7566
	step [133/147], loss=9.9264
	step [134/147], loss=9.0580
	step [135/147], loss=8.8087
	step [136/147], loss=9.8445
	step [137/147], loss=9.2322
	step [138/147], loss=10.8249
	step [139/147], loss=9.1393
	step [140/147], loss=9.9893
	step [141/147], loss=8.8738
	step [142/147], loss=8.9959
	step [143/147], loss=9.4711
	step [144/147], loss=8.4799
	step [145/147], loss=9.9628
	step [146/147], loss=10.4318
	step [147/147], loss=4.4646
	Evaluating
	loss=0.0256, precision=0.2504, recall=0.9912, f1=0.3998
saving model as: 0_saved_model.pth
Training epoch 25
	step [1/147], loss=10.1374
	step [2/147], loss=9.8914
	step [3/147], loss=9.5065
	step [4/147], loss=11.4508
	step [5/147], loss=8.9612
	step [6/147], loss=8.7159
	step [7/147], loss=9.2674
	step [8/147], loss=10.4438
	step [9/147], loss=10.1042
	step [10/147], loss=10.0168
	step [11/147], loss=8.9643
	step [12/147], loss=11.0248
	step [13/147], loss=9.7606
	step [14/147], loss=9.4771
	step [15/147], loss=9.7934
	step [16/147], loss=8.0509
	step [17/147], loss=8.2863
	step [18/147], loss=8.9004
	step [19/147], loss=8.1868
	step [20/147], loss=9.2598
	step [21/147], loss=10.3972
	step [22/147], loss=8.9571
	step [23/147], loss=9.4380
	step [24/147], loss=10.3038
	step [25/147], loss=9.7325
	step [26/147], loss=9.9212
	step [27/147], loss=10.3033
	step [28/147], loss=9.1651
	step [29/147], loss=9.1870
	step [30/147], loss=10.3469
	step [31/147], loss=11.4028
	step [32/147], loss=9.4640
	step [33/147], loss=10.9364
	step [34/147], loss=9.6550
	step [35/147], loss=10.1313
	step [36/147], loss=10.1309
	step [37/147], loss=9.1654
	step [38/147], loss=8.9509
	step [39/147], loss=9.7446
	step [40/147], loss=9.8259
	step [41/147], loss=10.7118
	step [42/147], loss=8.6691
	step [43/147], loss=9.6439
	step [44/147], loss=9.3075
	step [45/147], loss=8.3607
	step [46/147], loss=9.6439
	step [47/147], loss=9.2274
	step [48/147], loss=7.7586
	step [49/147], loss=8.8061
	step [50/147], loss=9.9382
	step [51/147], loss=9.3090
	step [52/147], loss=9.0694
	step [53/147], loss=9.5465
	step [54/147], loss=9.7409
	step [55/147], loss=9.5860
	step [56/147], loss=9.4160
	step [57/147], loss=10.7013
	step [58/147], loss=10.2299
	step [59/147], loss=8.1502
	step [60/147], loss=10.1521
	step [61/147], loss=9.7115
	step [62/147], loss=8.0132
	step [63/147], loss=9.4603
	step [64/147], loss=9.3780
	step [65/147], loss=11.0725
	step [66/147], loss=9.2865
	step [67/147], loss=8.1227
	step [68/147], loss=8.5438
	step [69/147], loss=8.2943
	step [70/147], loss=9.1041
	step [71/147], loss=8.9220
	step [72/147], loss=10.8419
	step [73/147], loss=9.3325
	step [74/147], loss=8.7082
	step [75/147], loss=9.6028
	step [76/147], loss=8.7282
	step [77/147], loss=9.1835
	step [78/147], loss=9.7945
	step [79/147], loss=8.9291
	step [80/147], loss=8.9025
	step [81/147], loss=9.7377
	step [82/147], loss=8.9074
	step [83/147], loss=9.0566
	step [84/147], loss=9.0210
	step [85/147], loss=9.3688
	step [86/147], loss=9.4155
	step [87/147], loss=9.1279
	step [88/147], loss=9.7207
	step [89/147], loss=8.5692
	step [90/147], loss=8.5579
	step [91/147], loss=9.1634
	step [92/147], loss=9.7863
	step [93/147], loss=9.9411
	step [94/147], loss=9.0185
	step [95/147], loss=10.4376
	step [96/147], loss=11.6134
	step [97/147], loss=8.7947
	step [98/147], loss=10.2298
	step [99/147], loss=8.8206
	step [100/147], loss=9.5550
	step [101/147], loss=9.1160
	step [102/147], loss=9.3080
	step [103/147], loss=9.6705
	step [104/147], loss=9.7937
	step [105/147], loss=9.2895
	step [106/147], loss=9.2588
	step [107/147], loss=9.6745
	step [108/147], loss=10.5779
	step [109/147], loss=9.6137
	step [110/147], loss=9.4583
	step [111/147], loss=10.3189
	step [112/147], loss=10.5426
	step [113/147], loss=9.4655
	step [114/147], loss=9.4683
	step [115/147], loss=9.4742
	step [116/147], loss=10.1530
	step [117/147], loss=9.3188
	step [118/147], loss=9.2528
	step [119/147], loss=9.4021
	step [120/147], loss=10.0446
	step [121/147], loss=9.3570
	step [122/147], loss=9.3671
	step [123/147], loss=9.0435
	step [124/147], loss=8.7162
	step [125/147], loss=8.2716
	step [126/147], loss=9.5708
	step [127/147], loss=9.9076
	step [128/147], loss=9.2442
	step [129/147], loss=9.2699
	step [130/147], loss=8.8917
	step [131/147], loss=8.3326
	step [132/147], loss=10.2286
	step [133/147], loss=10.6961
	step [134/147], loss=8.9494
	step [135/147], loss=9.1461
	step [136/147], loss=7.1787
	step [137/147], loss=10.1941
	step [138/147], loss=8.3373
	step [139/147], loss=8.8044
	step [140/147], loss=9.9632
	step [141/147], loss=8.5320
	step [142/147], loss=9.7089
	step [143/147], loss=7.8338
	step [144/147], loss=9.1802
	step [145/147], loss=9.6551
	step [146/147], loss=8.9842
	step [147/147], loss=4.4178
	Evaluating
	loss=0.0305, precision=0.2145, recall=0.9942, f1=0.3529
Training epoch 26
	step [1/147], loss=9.0034
	step [2/147], loss=8.9090
	step [3/147], loss=9.4434
	step [4/147], loss=8.8066
	step [5/147], loss=10.3771
	step [6/147], loss=8.9383
	step [7/147], loss=9.3008
	step [8/147], loss=9.5640
	step [9/147], loss=10.1690
	step [10/147], loss=8.6601
	step [11/147], loss=8.3862
	step [12/147], loss=8.7423
	step [13/147], loss=9.5225
	step [14/147], loss=11.3467
	step [15/147], loss=8.7927
	step [16/147], loss=9.3549
	step [17/147], loss=9.1854
	step [18/147], loss=8.3390
	step [19/147], loss=8.5634
	step [20/147], loss=10.6167
	step [21/147], loss=8.5475
	step [22/147], loss=9.6635
	step [23/147], loss=9.3091
	step [24/147], loss=8.8972
	step [25/147], loss=7.9355
	step [26/147], loss=7.6537
	step [27/147], loss=9.6094
	step [28/147], loss=9.7780
	step [29/147], loss=7.4613
	step [30/147], loss=8.5985
	step [31/147], loss=8.7666
	step [32/147], loss=10.8196
	step [33/147], loss=8.0404
	step [34/147], loss=10.5429
	step [35/147], loss=8.6409
	step [36/147], loss=8.6536
	step [37/147], loss=11.6817
	step [38/147], loss=9.9933
	step [39/147], loss=8.0621
	step [40/147], loss=9.0632
	step [41/147], loss=8.6798
	step [42/147], loss=9.7422
	step [43/147], loss=10.7315
	step [44/147], loss=11.2464
	step [45/147], loss=10.2755
	step [46/147], loss=9.5087
	step [47/147], loss=9.0083
	step [48/147], loss=9.6685
	step [49/147], loss=8.5149
	step [50/147], loss=8.7658
	step [51/147], loss=9.4674
	step [52/147], loss=8.3530
	step [53/147], loss=8.9415
	step [54/147], loss=8.1274
	step [55/147], loss=10.2458
	step [56/147], loss=9.8142
	step [57/147], loss=9.0440
	step [58/147], loss=9.1359
	step [59/147], loss=9.6535
	step [60/147], loss=7.9757
	step [61/147], loss=8.7841
	step [62/147], loss=9.9640
	step [63/147], loss=10.0938
	step [64/147], loss=9.6599
	step [65/147], loss=8.8647
	step [66/147], loss=9.3865
	step [67/147], loss=8.8795
	step [68/147], loss=9.6640
	step [69/147], loss=9.8937
	step [70/147], loss=8.8958
	step [71/147], loss=10.1969
	step [72/147], loss=7.8693
	step [73/147], loss=8.5623
	step [74/147], loss=8.5838
	step [75/147], loss=8.6984
	step [76/147], loss=9.3094
	step [77/147], loss=10.2600
	step [78/147], loss=9.2946
	step [79/147], loss=8.7066
	step [80/147], loss=7.9674
	step [81/147], loss=8.0328
	step [82/147], loss=9.6549
	step [83/147], loss=9.7113
	step [84/147], loss=9.7541
	step [85/147], loss=8.2441
	step [86/147], loss=9.4245
	step [87/147], loss=8.8661
	step [88/147], loss=9.0102
	step [89/147], loss=9.1842
	step [90/147], loss=8.6842
	step [91/147], loss=9.1502
	step [92/147], loss=8.3234
	step [93/147], loss=8.1925
	step [94/147], loss=9.3538
	step [95/147], loss=8.3753
	step [96/147], loss=8.2943
	step [97/147], loss=8.8161
	step [98/147], loss=8.5246
	step [99/147], loss=9.3376
	step [100/147], loss=9.4054
	step [101/147], loss=9.8571
	step [102/147], loss=9.4193
	step [103/147], loss=9.0464
	step [104/147], loss=9.0223
	step [105/147], loss=8.2677
	step [106/147], loss=8.9787
	step [107/147], loss=8.5888
	step [108/147], loss=9.7942
	step [109/147], loss=8.4323
	step [110/147], loss=8.6148
	step [111/147], loss=7.9923
	step [112/147], loss=9.1376
	step [113/147], loss=8.7379
	step [114/147], loss=8.2411
	step [115/147], loss=9.2457
	step [116/147], loss=8.5198
	step [117/147], loss=7.8571
	step [118/147], loss=8.8206
	step [119/147], loss=9.9257
	step [120/147], loss=8.3701
	step [121/147], loss=10.1165
	step [122/147], loss=9.9804
	step [123/147], loss=9.0691
	step [124/147], loss=9.1966
	step [125/147], loss=8.2466
	step [126/147], loss=11.3975
	step [127/147], loss=8.6057
	step [128/147], loss=10.4186
	step [129/147], loss=8.1782
	step [130/147], loss=8.9063
	step [131/147], loss=8.3740
	step [132/147], loss=10.0686
	step [133/147], loss=9.5054
	step [134/147], loss=8.9858
	step [135/147], loss=9.1880
	step [136/147], loss=9.4858
	step [137/147], loss=8.3953
	step [138/147], loss=8.6558
	step [139/147], loss=10.1058
	step [140/147], loss=9.1109
	step [141/147], loss=9.1192
	step [142/147], loss=8.9108
	step [143/147], loss=9.5297
	step [144/147], loss=9.3993
	step [145/147], loss=8.9393
	step [146/147], loss=7.9318
	step [147/147], loss=3.4323
	Evaluating
	loss=0.0284, precision=0.2223, recall=0.9936, f1=0.3633
Training epoch 27
	step [1/147], loss=8.9082
	step [2/147], loss=8.1458
	step [3/147], loss=8.7290
	step [4/147], loss=8.6297
	step [5/147], loss=8.1748
	step [6/147], loss=10.0737
	step [7/147], loss=7.3927
	step [8/147], loss=8.3716
	step [9/147], loss=8.3691
	step [10/147], loss=8.3715
	step [11/147], loss=8.9721
	step [12/147], loss=8.6659
	step [13/147], loss=9.0194
	step [14/147], loss=8.0434
	step [15/147], loss=8.5270
	step [16/147], loss=8.0006
	step [17/147], loss=7.9632
	step [18/147], loss=9.4170
	step [19/147], loss=8.9765
	step [20/147], loss=9.9327
	step [21/147], loss=9.0517
	step [22/147], loss=8.9432
	step [23/147], loss=8.7665
	step [24/147], loss=9.3277
	step [25/147], loss=9.8395
	step [26/147], loss=8.9860
	step [27/147], loss=10.1877
	step [28/147], loss=8.9233
	step [29/147], loss=10.0334
	step [30/147], loss=9.9331
	step [31/147], loss=7.9191
	step [32/147], loss=8.1098
	step [33/147], loss=9.3484
	step [34/147], loss=8.6276
	step [35/147], loss=9.6103
	step [36/147], loss=8.9574
	step [37/147], loss=8.5271
	step [38/147], loss=9.5507
	step [39/147], loss=9.1813
	step [40/147], loss=8.5646
	step [41/147], loss=8.5203
	step [42/147], loss=9.8950
	step [43/147], loss=8.0003
	step [44/147], loss=9.0290
	step [45/147], loss=9.4138
	step [46/147], loss=7.3814
	step [47/147], loss=9.0699
	step [48/147], loss=7.7303
	step [49/147], loss=7.6188
	step [50/147], loss=7.3990
	step [51/147], loss=8.5796
	step [52/147], loss=8.2839
	step [53/147], loss=8.4870
	step [54/147], loss=9.1132
	step [55/147], loss=10.2836
	step [56/147], loss=10.6428
	step [57/147], loss=7.9088
	step [58/147], loss=9.1352
	step [59/147], loss=8.8369
	step [60/147], loss=7.9923
	step [61/147], loss=8.2994
	step [62/147], loss=9.7999
	step [63/147], loss=9.8207
	step [64/147], loss=9.1414
	step [65/147], loss=8.7304
	step [66/147], loss=10.0670
	step [67/147], loss=8.4224
	step [68/147], loss=9.4398
	step [69/147], loss=9.1365
	step [70/147], loss=8.9304
	step [71/147], loss=9.8279
	step [72/147], loss=8.2893
	step [73/147], loss=8.6231
	step [74/147], loss=7.8397
	step [75/147], loss=10.0838
	step [76/147], loss=9.3961
	step [77/147], loss=8.6931
	step [78/147], loss=8.3429
	step [79/147], loss=7.7347
	step [80/147], loss=9.1771
	step [81/147], loss=8.5990
	step [82/147], loss=8.8022
	step [83/147], loss=8.5984
	step [84/147], loss=7.7546
	step [85/147], loss=9.0700
	step [86/147], loss=8.4150
	step [87/147], loss=8.9171
	step [88/147], loss=7.5636
	step [89/147], loss=7.3341
	step [90/147], loss=7.5147
	step [91/147], loss=9.3237
	step [92/147], loss=7.0256
	step [93/147], loss=7.9350
	step [94/147], loss=9.3457
	step [95/147], loss=8.9327
	step [96/147], loss=9.6348
	step [97/147], loss=9.6519
	step [98/147], loss=8.6627
	step [99/147], loss=8.9592
	step [100/147], loss=8.9311
	step [101/147], loss=8.8888
	step [102/147], loss=9.9429
	step [103/147], loss=9.4945
	step [104/147], loss=8.4745
	step [105/147], loss=9.8637
	step [106/147], loss=10.2061
	step [107/147], loss=8.5923
	step [108/147], loss=8.5113
	step [109/147], loss=8.2276
	step [110/147], loss=10.5648
	step [111/147], loss=9.6181
	step [112/147], loss=9.5304
	step [113/147], loss=7.9986
	step [114/147], loss=9.3687
	step [115/147], loss=10.0474
	step [116/147], loss=8.2866
	step [117/147], loss=8.2114
	step [118/147], loss=8.1538
	step [119/147], loss=8.4891
	step [120/147], loss=8.3319
	step [121/147], loss=7.1879
	step [122/147], loss=8.2957
	step [123/147], loss=8.0000
	step [124/147], loss=9.3984
	step [125/147], loss=9.0138
	step [126/147], loss=8.8269
	step [127/147], loss=8.9359
	step [128/147], loss=10.1476
	step [129/147], loss=9.3817
	step [130/147], loss=7.4243
	step [131/147], loss=9.0051
	step [132/147], loss=8.4460
	step [133/147], loss=7.5848
	step [134/147], loss=10.4853
	step [135/147], loss=8.2647
	step [136/147], loss=8.8172
	step [137/147], loss=8.4276
	step [138/147], loss=9.1914
	step [139/147], loss=9.1167
	step [140/147], loss=8.2337
	step [141/147], loss=7.0749
	step [142/147], loss=9.0935
	step [143/147], loss=9.8232
	step [144/147], loss=8.5499
	step [145/147], loss=8.8288
	step [146/147], loss=9.4790
	step [147/147], loss=3.8565
	Evaluating
	loss=0.0256, precision=0.2454, recall=0.9912, f1=0.3933
Training epoch 28
	step [1/147], loss=8.9255
	step [2/147], loss=8.7116
	step [3/147], loss=7.4181
	step [4/147], loss=7.7806
	step [5/147], loss=9.0954
	step [6/147], loss=8.9306
	step [7/147], loss=8.4618
	step [8/147], loss=9.4810
	step [9/147], loss=8.7961
	step [10/147], loss=8.8875
	step [11/147], loss=9.0709
	step [12/147], loss=8.8979
	step [13/147], loss=9.0095
	step [14/147], loss=7.8865
	step [15/147], loss=9.8592
	step [16/147], loss=7.6600
	step [17/147], loss=7.2967
	step [18/147], loss=9.7740
	step [19/147], loss=9.0156
	step [20/147], loss=8.5925
	step [21/147], loss=9.0439
	step [22/147], loss=10.0834
	step [23/147], loss=8.2563
	step [24/147], loss=8.7024
	step [25/147], loss=8.6452
	step [26/147], loss=8.4151
	step [27/147], loss=7.5170
	step [28/147], loss=7.7714
	step [29/147], loss=8.7903
	step [30/147], loss=8.6585
	step [31/147], loss=9.3666
	step [32/147], loss=8.7033
	step [33/147], loss=8.3779
	step [34/147], loss=8.2988
	step [35/147], loss=8.1056
	step [36/147], loss=8.7136
	step [37/147], loss=9.0586
	step [38/147], loss=7.7255
	step [39/147], loss=9.1756
	step [40/147], loss=8.2781
	step [41/147], loss=9.2202
	step [42/147], loss=8.6400
	step [43/147], loss=8.8649
	step [44/147], loss=8.2699
	step [45/147], loss=9.8135
	step [46/147], loss=8.2306
	step [47/147], loss=8.3986
	step [48/147], loss=8.6972
	step [49/147], loss=7.7157
	step [50/147], loss=8.0069
	step [51/147], loss=8.7314
	step [52/147], loss=9.1775
	step [53/147], loss=8.2899
	step [54/147], loss=9.5855
	step [55/147], loss=8.3699
	step [56/147], loss=9.3093
	step [57/147], loss=8.3003
	step [58/147], loss=8.2343
	step [59/147], loss=10.2818
	step [60/147], loss=8.2870
	step [61/147], loss=8.4787
	step [62/147], loss=8.2754
	step [63/147], loss=9.1244
	step [64/147], loss=8.2371
	step [65/147], loss=7.9632
	step [66/147], loss=9.6537
	step [67/147], loss=8.9042
	step [68/147], loss=7.6882
	step [69/147], loss=8.5958
	step [70/147], loss=8.0764
	step [71/147], loss=8.3158
	step [72/147], loss=10.0287
	step [73/147], loss=8.5053
	step [74/147], loss=7.3584
	step [75/147], loss=8.1275
	step [76/147], loss=7.9659
	step [77/147], loss=8.5051
	step [78/147], loss=8.7210
	step [79/147], loss=8.1333
	step [80/147], loss=6.6649
	step [81/147], loss=8.7573
	step [82/147], loss=8.7904
	step [83/147], loss=8.4794
	step [84/147], loss=8.8993
	step [85/147], loss=7.4391
	step [86/147], loss=7.9910
	step [87/147], loss=7.8512
	step [88/147], loss=7.7941
	step [89/147], loss=8.0606
	step [90/147], loss=8.4252
	step [91/147], loss=7.5047
	step [92/147], loss=8.1605
	step [93/147], loss=8.7896
	step [94/147], loss=8.9131
	step [95/147], loss=8.4656
	step [96/147], loss=9.0305
	step [97/147], loss=8.3922
	step [98/147], loss=9.4955
	step [99/147], loss=8.6348
	step [100/147], loss=8.0717
	step [101/147], loss=8.5582
	step [102/147], loss=8.7627
	step [103/147], loss=9.2547
	step [104/147], loss=8.9128
	step [105/147], loss=8.9168
	step [106/147], loss=8.7678
	step [107/147], loss=8.8152
	step [108/147], loss=8.7149
	step [109/147], loss=7.3019
	step [110/147], loss=8.0813
	step [111/147], loss=8.4711
	step [112/147], loss=8.9061
	step [113/147], loss=9.0210
	step [114/147], loss=8.4598
	step [115/147], loss=8.3054
	step [116/147], loss=7.7568
	step [117/147], loss=7.9653
	step [118/147], loss=7.7203
	step [119/147], loss=8.7352
	step [120/147], loss=9.0040
	step [121/147], loss=7.6731
	step [122/147], loss=7.6986
	step [123/147], loss=7.8094
	step [124/147], loss=7.5426
	step [125/147], loss=9.0311
	step [126/147], loss=10.1904
	step [127/147], loss=8.5694
	step [128/147], loss=9.0476
	step [129/147], loss=7.8224
	step [130/147], loss=8.1841
	step [131/147], loss=8.1717
	step [132/147], loss=8.0788
	step [133/147], loss=8.3193
	step [134/147], loss=8.3398
	step [135/147], loss=7.5678
	step [136/147], loss=8.3407
	step [137/147], loss=9.9653
	step [138/147], loss=8.6118
	step [139/147], loss=9.4039
	step [140/147], loss=8.3611
	step [141/147], loss=7.8767
	step [142/147], loss=8.6049
	step [143/147], loss=8.4613
	step [144/147], loss=10.0485
	step [145/147], loss=8.7052
	step [146/147], loss=8.3522
	step [147/147], loss=3.9471
	Evaluating
	loss=0.0263, precision=0.2407, recall=0.9913, f1=0.3874
Training epoch 29
	step [1/147], loss=7.2116
	step [2/147], loss=8.7940
	step [3/147], loss=8.7955
	step [4/147], loss=8.6191
	step [5/147], loss=8.1921
	step [6/147], loss=8.2198
	step [7/147], loss=8.7895
	step [8/147], loss=7.8155
	step [9/147], loss=9.0758
	step [10/147], loss=8.7908
	step [11/147], loss=9.2011
	step [12/147], loss=8.4018
	step [13/147], loss=8.8071
	step [14/147], loss=6.7947
	step [15/147], loss=9.0646
	step [16/147], loss=9.1334
	step [17/147], loss=8.3427
	step [18/147], loss=7.3638
	step [19/147], loss=8.7497
	step [20/147], loss=7.7107
	step [21/147], loss=8.2695
	step [22/147], loss=8.1847
	step [23/147], loss=8.3486
	step [24/147], loss=7.2931
	step [25/147], loss=8.9941
	step [26/147], loss=8.6943
	step [27/147], loss=7.3776
	step [28/147], loss=8.0211
	step [29/147], loss=9.3929
	step [30/147], loss=8.6598
	step [31/147], loss=8.0941
	step [32/147], loss=8.1209
	step [33/147], loss=7.5112
	step [34/147], loss=7.5866
	step [35/147], loss=9.0928
	step [36/147], loss=8.5785
	step [37/147], loss=9.1841
	step [38/147], loss=8.7022
	step [39/147], loss=8.2927
	step [40/147], loss=8.1965
	step [41/147], loss=7.5921
	step [42/147], loss=7.3349
	step [43/147], loss=7.8212
	step [44/147], loss=8.0083
	step [45/147], loss=7.4555
	step [46/147], loss=9.6228
	step [47/147], loss=7.8814
	step [48/147], loss=7.6672
	step [49/147], loss=8.5312
	step [50/147], loss=8.4320
	step [51/147], loss=7.5097
	step [52/147], loss=8.8632
	step [53/147], loss=8.5210
	step [54/147], loss=9.2165
	step [55/147], loss=8.6273
	step [56/147], loss=7.2076
	step [57/147], loss=6.6850
	step [58/147], loss=8.5464
	step [59/147], loss=8.1523
	step [60/147], loss=7.6537
	step [61/147], loss=7.9581
	step [62/147], loss=7.8599
	step [63/147], loss=9.1327
	step [64/147], loss=7.3966
	step [65/147], loss=7.2904
	step [66/147], loss=7.9994
	step [67/147], loss=7.8822
	step [68/147], loss=8.4737
	step [69/147], loss=7.9708
	step [70/147], loss=7.5861
	step [71/147], loss=8.5441
	step [72/147], loss=7.9931
	step [73/147], loss=8.9521
	step [74/147], loss=8.1430
	step [75/147], loss=7.8721
	step [76/147], loss=8.7754
	step [77/147], loss=9.4625
	step [78/147], loss=7.8133
	step [79/147], loss=7.4355
	step [80/147], loss=10.7099
	step [81/147], loss=7.3012
	step [82/147], loss=8.4312
	step [83/147], loss=8.4786
	step [84/147], loss=8.1643
	step [85/147], loss=8.5881
	step [86/147], loss=7.8885
	step [87/147], loss=7.6173
	step [88/147], loss=7.4178
	step [89/147], loss=7.9566
	step [90/147], loss=7.7140
	step [91/147], loss=7.8130
	step [92/147], loss=7.4021
	step [93/147], loss=7.6671
	step [94/147], loss=7.1129
	step [95/147], loss=7.5563
	step [96/147], loss=7.6825
	step [97/147], loss=6.9598
	step [98/147], loss=9.3308
	step [99/147], loss=8.9538
	step [100/147], loss=8.9740
	step [101/147], loss=9.5026
	step [102/147], loss=7.7277
	step [103/147], loss=8.7055
	step [104/147], loss=8.9136
	step [105/147], loss=7.9428
	step [106/147], loss=8.4942
	step [107/147], loss=8.1489
	step [108/147], loss=8.5761
	step [109/147], loss=7.4501
	step [110/147], loss=8.3740
	step [111/147], loss=8.5011
	step [112/147], loss=8.8443
	step [113/147], loss=8.9601
	step [114/147], loss=7.8069
	step [115/147], loss=8.4234
	step [116/147], loss=8.1299
	step [117/147], loss=7.7954
	step [118/147], loss=8.0797
	step [119/147], loss=7.8008
	step [120/147], loss=7.7281
	step [121/147], loss=7.9407
	step [122/147], loss=8.7038
	step [123/147], loss=8.1010
	step [124/147], loss=8.3151
	step [125/147], loss=7.0246
	step [126/147], loss=8.6391
	step [127/147], loss=8.6036
	step [128/147], loss=9.1134
	step [129/147], loss=8.7396
	step [130/147], loss=8.2199
	step [131/147], loss=7.9088
	step [132/147], loss=7.8756
	step [133/147], loss=9.5440
	step [134/147], loss=8.0291
	step [135/147], loss=6.8519
	step [136/147], loss=7.3936
	step [137/147], loss=8.4604
	step [138/147], loss=9.2424
	step [139/147], loss=8.1317
	step [140/147], loss=8.4066
	step [141/147], loss=8.4854
	step [142/147], loss=7.5728
	step [143/147], loss=9.7827
	step [144/147], loss=9.4535
	step [145/147], loss=8.4332
	step [146/147], loss=7.6850
	step [147/147], loss=3.6100
	Evaluating
	loss=0.0216, precision=0.2594, recall=0.9915, f1=0.4113
saving model as: 0_saved_model.pth
Training epoch 30
	step [1/147], loss=8.0626
	step [2/147], loss=8.9837
	step [3/147], loss=7.2622
	step [4/147], loss=7.9165
	step [5/147], loss=8.2897
	step [6/147], loss=7.6038
	step [7/147], loss=8.0688
	step [8/147], loss=8.1724
	step [9/147], loss=9.0845
	step [10/147], loss=8.2100
	step [11/147], loss=7.4067
	step [12/147], loss=7.4783
	step [13/147], loss=8.8457
	step [14/147], loss=8.4543
	step [15/147], loss=7.9777
	step [16/147], loss=7.4314
	step [17/147], loss=9.0714
	step [18/147], loss=8.3072
	step [19/147], loss=7.8517
	step [20/147], loss=8.7128
	step [21/147], loss=7.8486
	step [22/147], loss=7.6732
	step [23/147], loss=7.7832
	step [24/147], loss=8.1731
	step [25/147], loss=8.0625
	step [26/147], loss=9.0423
	step [27/147], loss=7.6168
	step [28/147], loss=8.1304
	step [29/147], loss=8.4979
	step [30/147], loss=6.8900
	step [31/147], loss=8.0976
	step [32/147], loss=8.4117
	step [33/147], loss=7.6516
	step [34/147], loss=7.9920
	step [35/147], loss=7.7319
	step [36/147], loss=7.8989
	step [37/147], loss=8.1176
	step [38/147], loss=7.8434
	step [39/147], loss=7.9918
	step [40/147], loss=7.9791
	step [41/147], loss=7.4758
	step [42/147], loss=7.8359
	step [43/147], loss=8.5142
	step [44/147], loss=8.0360
	step [45/147], loss=7.7004
	step [46/147], loss=7.3045
	step [47/147], loss=8.3914
	step [48/147], loss=7.0170
	step [49/147], loss=8.2201
	step [50/147], loss=8.1142
	step [51/147], loss=7.0648
	step [52/147], loss=7.9162
	step [53/147], loss=7.9312
	step [54/147], loss=8.6948
	step [55/147], loss=7.9814
	step [56/147], loss=7.9462
	step [57/147], loss=8.2658
	step [58/147], loss=7.8428
	step [59/147], loss=7.7002
	step [60/147], loss=7.7579
	step [61/147], loss=7.8903
	step [62/147], loss=8.3058
	step [63/147], loss=6.6666
	step [64/147], loss=8.2628
	step [65/147], loss=7.3397
	step [66/147], loss=8.2751
	step [67/147], loss=7.0736
	step [68/147], loss=7.9468
	step [69/147], loss=7.7980
	step [70/147], loss=7.4318
	step [71/147], loss=8.7680
	step [72/147], loss=8.1954
	step [73/147], loss=8.9104
	step [74/147], loss=7.9410
	step [75/147], loss=8.0101
	step [76/147], loss=8.2431
	step [77/147], loss=9.2614
	step [78/147], loss=8.1028
	step [79/147], loss=8.3845
	step [80/147], loss=9.0075
	step [81/147], loss=7.3165
	step [82/147], loss=8.3202
	step [83/147], loss=6.7351
	step [84/147], loss=8.6217
	step [85/147], loss=8.0253
	step [86/147], loss=8.6383
	step [87/147], loss=8.2416
	step [88/147], loss=8.9487
	step [89/147], loss=8.6406
	step [90/147], loss=7.8061
	step [91/147], loss=7.7826
	step [92/147], loss=8.6400
	step [93/147], loss=9.4609
	step [94/147], loss=7.9869
	step [95/147], loss=7.2338
	step [96/147], loss=8.0216
	step [97/147], loss=9.6609
	step [98/147], loss=9.8413
	step [99/147], loss=7.8821
	step [100/147], loss=7.0169
	step [101/147], loss=8.1538
	step [102/147], loss=7.2662
	step [103/147], loss=8.8984
	step [104/147], loss=7.6590
	step [105/147], loss=7.4244
	step [106/147], loss=8.7739
	step [107/147], loss=8.6558
	step [108/147], loss=7.9485
	step [109/147], loss=8.2146
	step [110/147], loss=8.1958
	step [111/147], loss=9.1787
	step [112/147], loss=7.9829
	step [113/147], loss=7.9136
	step [114/147], loss=8.8559
	step [115/147], loss=7.3974
	step [116/147], loss=8.1900
	step [117/147], loss=8.9770
	step [118/147], loss=9.9290
	step [119/147], loss=8.1509
	step [120/147], loss=7.8521
	step [121/147], loss=8.2386
	step [122/147], loss=6.3486
	step [123/147], loss=7.4703
	step [124/147], loss=7.6649
	step [125/147], loss=9.1161
	step [126/147], loss=7.8006
	step [127/147], loss=8.4158
	step [128/147], loss=7.8512
	step [129/147], loss=8.3851
	step [130/147], loss=7.7833
	step [131/147], loss=7.9506
	step [132/147], loss=6.8784
	step [133/147], loss=7.2919
	step [134/147], loss=7.3282
	step [135/147], loss=7.3207
	step [136/147], loss=8.5247
	step [137/147], loss=7.8777
	step [138/147], loss=6.4044
	step [139/147], loss=9.3519
	step [140/147], loss=7.4719
	step [141/147], loss=7.4828
	step [142/147], loss=8.0759
	step [143/147], loss=7.9808
	step [144/147], loss=8.5360
	step [145/147], loss=7.8714
	step [146/147], loss=7.4754
	step [147/147], loss=4.1935
	Evaluating
	loss=0.0225, precision=0.2662, recall=0.9916, f1=0.4197
saving model as: 0_saved_model.pth
Training epoch 31
	step [1/147], loss=7.4889
	step [2/147], loss=7.6021
	step [3/147], loss=7.5803
	step [4/147], loss=8.6797
	step [5/147], loss=7.2511
	step [6/147], loss=6.5688
	step [7/147], loss=6.8982
	step [8/147], loss=8.3310
	step [9/147], loss=7.2006
	step [10/147], loss=7.3341
	step [11/147], loss=7.7346
	step [12/147], loss=8.1769
	step [13/147], loss=8.1292
	step [14/147], loss=8.6714
	step [15/147], loss=7.3233
	step [16/147], loss=8.3931
	step [17/147], loss=7.9781
	step [18/147], loss=6.9749
	step [19/147], loss=7.3127
	step [20/147], loss=7.9886
	step [21/147], loss=7.3634
	step [22/147], loss=8.0541
	step [23/147], loss=7.6028
	step [24/147], loss=8.3198
	step [25/147], loss=6.7784
	step [26/147], loss=8.1234
	step [27/147], loss=7.5704
	step [28/147], loss=7.8108
	step [29/147], loss=8.6041
	step [30/147], loss=7.9648
	step [31/147], loss=8.2147
	step [32/147], loss=7.3096
	step [33/147], loss=9.4275
	step [34/147], loss=7.6653
	step [35/147], loss=8.4421
	step [36/147], loss=6.9327
	step [37/147], loss=8.4899
	step [38/147], loss=7.6008
	step [39/147], loss=7.9185
	step [40/147], loss=9.2696
	step [41/147], loss=7.7542
	step [42/147], loss=6.9910
	step [43/147], loss=8.5156
	step [44/147], loss=8.2918
	step [45/147], loss=9.1706
	step [46/147], loss=7.6001
	step [47/147], loss=7.9528
	step [48/147], loss=7.8931
	step [49/147], loss=7.9570
	step [50/147], loss=6.9094
	step [51/147], loss=7.4878
	step [52/147], loss=7.9397
	step [53/147], loss=8.0041
	step [54/147], loss=8.0819
	step [55/147], loss=8.9653
	step [56/147], loss=9.4418
	step [57/147], loss=7.9264
	step [58/147], loss=6.8092
	step [59/147], loss=7.9102
	step [60/147], loss=7.1214
	step [61/147], loss=7.8762
	step [62/147], loss=8.0012
	step [63/147], loss=8.1893
	step [64/147], loss=7.9340
	step [65/147], loss=8.3955
	step [66/147], loss=7.6184
	step [67/147], loss=6.9177
	step [68/147], loss=6.6702
	step [69/147], loss=8.2537
	step [70/147], loss=7.5433
	step [71/147], loss=7.4380
	step [72/147], loss=7.3497
	step [73/147], loss=8.5869
	step [74/147], loss=8.2151
	step [75/147], loss=7.9079
	step [76/147], loss=8.6098
	step [77/147], loss=8.3850
	step [78/147], loss=8.0646
	step [79/147], loss=8.0986
	step [80/147], loss=7.2568
	step [81/147], loss=7.5578
	step [82/147], loss=7.8783
	step [83/147], loss=7.4775
	step [84/147], loss=7.2926
	step [85/147], loss=7.1191
	step [86/147], loss=8.5163
	step [87/147], loss=8.2202
	step [88/147], loss=6.3730
	step [89/147], loss=7.8885
	step [90/147], loss=8.3745
	step [91/147], loss=7.3889
	step [92/147], loss=7.1393
	step [93/147], loss=7.9906
	step [94/147], loss=7.7873
	step [95/147], loss=7.2052
	step [96/147], loss=7.4757
	step [97/147], loss=8.5342
	step [98/147], loss=8.2984
	step [99/147], loss=8.5360
	step [100/147], loss=8.5457
	step [101/147], loss=8.1971
	step [102/147], loss=7.6807
	step [103/147], loss=8.8072
	step [104/147], loss=7.7208
	step [105/147], loss=8.6169
	step [106/147], loss=8.5448
	step [107/147], loss=7.1353
	step [108/147], loss=7.9290
	step [109/147], loss=8.9921
	step [110/147], loss=7.1725
	step [111/147], loss=6.6934
	step [112/147], loss=9.4993
	step [113/147], loss=7.6376
	step [114/147], loss=6.2613
	step [115/147], loss=8.4609
	step [116/147], loss=8.1556
	step [117/147], loss=7.3296
	step [118/147], loss=7.7726
	step [119/147], loss=7.8534
	step [120/147], loss=8.4247
	step [121/147], loss=7.1751
	step [122/147], loss=7.9828
	step [123/147], loss=8.8891
	step [124/147], loss=7.6137
	step [125/147], loss=8.1174
	step [126/147], loss=8.1736
	step [127/147], loss=8.3658
	step [128/147], loss=8.4848
	step [129/147], loss=7.2075
	step [130/147], loss=7.8256
	step [131/147], loss=8.1367
	step [132/147], loss=8.0178
	step [133/147], loss=8.2849
	step [134/147], loss=7.7860
	step [135/147], loss=8.1450
	step [136/147], loss=7.2589
	step [137/147], loss=8.6429
	step [138/147], loss=6.9738
	step [139/147], loss=8.4010
	step [140/147], loss=6.5999
	step [141/147], loss=8.0426
	step [142/147], loss=6.5519
	step [143/147], loss=7.6624
	step [144/147], loss=8.0745
	step [145/147], loss=7.2786
	step [146/147], loss=7.2976
	step [147/147], loss=3.4138
	Evaluating
	loss=0.0239, precision=0.2541, recall=0.9908, f1=0.4044
Training epoch 32
	step [1/147], loss=7.7913
	step [2/147], loss=8.6525
	step [3/147], loss=8.2560
	step [4/147], loss=7.8312
	step [5/147], loss=7.0358
	step [6/147], loss=7.0416
	step [7/147], loss=7.9859
	step [8/147], loss=6.7271
	step [9/147], loss=7.0309
	step [10/147], loss=7.6212
	step [11/147], loss=7.8305
	step [12/147], loss=7.1779
	step [13/147], loss=8.3949
	step [14/147], loss=7.6498
	step [15/147], loss=7.9156
	step [16/147], loss=7.3691
	step [17/147], loss=8.5017
	step [18/147], loss=7.2877
	step [19/147], loss=7.8369
	step [20/147], loss=7.7327
	step [21/147], loss=6.9516
	step [22/147], loss=6.0617
	step [23/147], loss=8.7698
	step [24/147], loss=8.7044
	step [25/147], loss=8.7918
	step [26/147], loss=6.6559
	step [27/147], loss=8.1364
	step [28/147], loss=7.3488
	step [29/147], loss=6.7733
	step [30/147], loss=7.7434
	step [31/147], loss=7.5134
	step [32/147], loss=7.1618
	step [33/147], loss=8.8568
	step [34/147], loss=6.7076
	step [35/147], loss=8.1047
	step [36/147], loss=8.7876
	step [37/147], loss=8.5794
	step [38/147], loss=8.1086
	step [39/147], loss=7.8837
	step [40/147], loss=7.5277
	step [41/147], loss=7.9073
	step [42/147], loss=7.3812
	step [43/147], loss=9.1141
	step [44/147], loss=8.5954
	step [45/147], loss=7.1184
	step [46/147], loss=7.4843
	step [47/147], loss=8.7199
	step [48/147], loss=7.3229
	step [49/147], loss=8.2224
	step [50/147], loss=9.3650
	step [51/147], loss=7.3144
	step [52/147], loss=7.9843
	step [53/147], loss=8.3186
	step [54/147], loss=7.0315
	step [55/147], loss=7.5365
	step [56/147], loss=7.2746
	step [57/147], loss=6.6432
	step [58/147], loss=7.6869
	step [59/147], loss=7.4483
	step [60/147], loss=6.7827
	step [61/147], loss=8.1170
	step [62/147], loss=7.9804
	step [63/147], loss=7.7595
	step [64/147], loss=7.7670
	step [65/147], loss=8.6326
	step [66/147], loss=7.2435
	step [67/147], loss=7.7548
	step [68/147], loss=8.3135
	step [69/147], loss=7.7154
	step [70/147], loss=8.6622
	step [71/147], loss=8.3724
	step [72/147], loss=7.7705
	step [73/147], loss=7.8201
	step [74/147], loss=6.9093
	step [75/147], loss=8.2186
	step [76/147], loss=8.1978
	step [77/147], loss=6.9028
	step [78/147], loss=7.9194
	step [79/147], loss=7.8940
	step [80/147], loss=7.1245
	step [81/147], loss=7.7287
	step [82/147], loss=7.8945
	step [83/147], loss=8.2080
	step [84/147], loss=7.5015
	step [85/147], loss=7.2921
	step [86/147], loss=8.0558
	step [87/147], loss=7.1019
	step [88/147], loss=8.1620
	step [89/147], loss=8.3438
	step [90/147], loss=8.1039
	step [91/147], loss=8.0488
	step [92/147], loss=7.2894
	step [93/147], loss=7.9155
	step [94/147], loss=8.6033
	step [95/147], loss=7.1911
	step [96/147], loss=6.7140
	step [97/147], loss=8.5263
	step [98/147], loss=7.0384
	step [99/147], loss=6.8427
	step [100/147], loss=7.6893
	step [101/147], loss=7.2395
	step [102/147], loss=8.2134
	step [103/147], loss=7.3239
	step [104/147], loss=8.2697
	step [105/147], loss=6.8904
	step [106/147], loss=7.3338
	step [107/147], loss=8.8792
	step [108/147], loss=8.5547
	step [109/147], loss=8.0662
	step [110/147], loss=8.3417
	step [111/147], loss=7.8514
	step [112/147], loss=7.0515
	step [113/147], loss=7.5919
	step [114/147], loss=7.6327
	step [115/147], loss=8.4554
	step [116/147], loss=6.6891
	step [117/147], loss=7.5369
	step [118/147], loss=7.3777
	step [119/147], loss=6.5408
	step [120/147], loss=7.2727
	step [121/147], loss=7.4335
	step [122/147], loss=7.1855
	step [123/147], loss=8.3069
	step [124/147], loss=7.3622
	step [125/147], loss=7.4566
	step [126/147], loss=7.4326
	step [127/147], loss=8.7561
	step [128/147], loss=8.0982
	step [129/147], loss=8.0523
	step [130/147], loss=8.2090
	step [131/147], loss=8.2746
	step [132/147], loss=9.0528
	step [133/147], loss=8.2071
	step [134/147], loss=7.8043
	step [135/147], loss=7.4663
	step [136/147], loss=7.2656
	step [137/147], loss=7.9799
	step [138/147], loss=8.9403
	step [139/147], loss=7.2545
	step [140/147], loss=7.4039
	step [141/147], loss=7.5919
	step [142/147], loss=7.3296
	step [143/147], loss=8.6059
	step [144/147], loss=7.4947
	step [145/147], loss=8.0268
	step [146/147], loss=9.2338
	step [147/147], loss=3.8452
	Evaluating
	loss=0.0265, precision=0.2330, recall=0.9929, f1=0.3775
Training epoch 33
	step [1/147], loss=7.5264
	step [2/147], loss=7.8043
	step [3/147], loss=8.3019
	step [4/147], loss=7.0381
	step [5/147], loss=6.7004
	step [6/147], loss=8.0476
	step [7/147], loss=7.5780
	step [8/147], loss=8.5444
	step [9/147], loss=7.5317
	step [10/147], loss=7.5299
	step [11/147], loss=8.0739
	step [12/147], loss=7.2026
	step [13/147], loss=8.9228
	step [14/147], loss=7.6090
	step [15/147], loss=6.5153
	step [16/147], loss=6.5548
	step [17/147], loss=7.6204
	step [18/147], loss=7.7272
	step [19/147], loss=7.3796
	step [20/147], loss=7.5700
	step [21/147], loss=7.1017
	step [22/147], loss=6.7500
	step [23/147], loss=7.7522
	step [24/147], loss=8.0864
	step [25/147], loss=7.7850
	step [26/147], loss=7.9705
	step [27/147], loss=6.9654
	step [28/147], loss=6.6691
	step [29/147], loss=8.5294
	step [30/147], loss=7.2861
	step [31/147], loss=8.0774
	step [32/147], loss=6.9649
	step [33/147], loss=7.1427
	step [34/147], loss=7.7727
	step [35/147], loss=7.7747
	step [36/147], loss=7.7703
	step [37/147], loss=7.3631
	step [38/147], loss=8.1968
	step [39/147], loss=7.0747
	step [40/147], loss=8.2196
	step [41/147], loss=7.9089
	step [42/147], loss=7.3883
	step [43/147], loss=6.2188
	step [44/147], loss=7.5535
	step [45/147], loss=8.0425
	step [46/147], loss=6.9295
	step [47/147], loss=6.9467
	step [48/147], loss=8.5842
	step [49/147], loss=7.0678
	step [50/147], loss=6.4888
	step [51/147], loss=6.5917
	step [52/147], loss=7.9805
	step [53/147], loss=7.9828
	step [54/147], loss=7.6769
	step [55/147], loss=7.5202
	step [56/147], loss=7.5760
	step [57/147], loss=7.7687
	step [58/147], loss=7.2897
	step [59/147], loss=6.6030
	step [60/147], loss=8.0597
	step [61/147], loss=7.3101
	step [62/147], loss=7.8454
	step [63/147], loss=6.8639
	step [64/147], loss=6.6885
	step [65/147], loss=7.4522
	step [66/147], loss=7.0108
	step [67/147], loss=7.3216
	step [68/147], loss=7.9623
	step [69/147], loss=8.1649
	step [70/147], loss=8.0496
	step [71/147], loss=7.1169
	step [72/147], loss=8.1133
	step [73/147], loss=7.4703
	step [74/147], loss=7.8888
	step [75/147], loss=7.0342
	step [76/147], loss=7.4382
	step [77/147], loss=6.9534
	step [78/147], loss=7.9210
	step [79/147], loss=7.4345
	step [80/147], loss=7.5332
	step [81/147], loss=6.4678
	step [82/147], loss=7.4030
	step [83/147], loss=7.6600
	step [84/147], loss=8.1014
	step [85/147], loss=7.2761
	step [86/147], loss=7.8030
	step [87/147], loss=7.8368
	step [88/147], loss=7.0145
	step [89/147], loss=7.1197
	step [90/147], loss=8.4598
	step [91/147], loss=7.1089
	step [92/147], loss=6.8497
	step [93/147], loss=6.7773
	step [94/147], loss=8.2737
	step [95/147], loss=7.4544
	step [96/147], loss=7.1555
	step [97/147], loss=6.5251
	step [98/147], loss=8.3308
	step [99/147], loss=7.4002
	step [100/147], loss=7.0385
	step [101/147], loss=8.2176
	step [102/147], loss=6.8663
	step [103/147], loss=7.2617
	step [104/147], loss=6.9316
	step [105/147], loss=7.3432
	step [106/147], loss=7.4965
	step [107/147], loss=6.7532
	step [108/147], loss=6.8749
	step [109/147], loss=7.4399
	step [110/147], loss=5.7704
	step [111/147], loss=6.2878
	step [112/147], loss=7.9028
	step [113/147], loss=7.3085
	step [114/147], loss=7.0626
	step [115/147], loss=7.3845
	step [116/147], loss=8.1872
	step [117/147], loss=7.5611
	step [118/147], loss=7.0149
	step [119/147], loss=8.1944
	step [120/147], loss=7.1179
	step [121/147], loss=8.0286
	step [122/147], loss=6.9037
	step [123/147], loss=7.4383
	step [124/147], loss=9.0778
	step [125/147], loss=7.6344
	step [126/147], loss=7.7930
	step [127/147], loss=6.9883
	step [128/147], loss=7.1568
	step [129/147], loss=7.5286
	step [130/147], loss=7.4251
	step [131/147], loss=7.2485
	step [132/147], loss=8.2875
	step [133/147], loss=6.2710
	step [134/147], loss=7.4044
	step [135/147], loss=7.2252
	step [136/147], loss=7.1975
	step [137/147], loss=6.1692
	step [138/147], loss=6.4167
	step [139/147], loss=7.3744
	step [140/147], loss=7.4623
	step [141/147], loss=7.6532
	step [142/147], loss=7.9163
	step [143/147], loss=8.2227
	step [144/147], loss=6.5376
	step [145/147], loss=6.9783
	step [146/147], loss=7.9956
	step [147/147], loss=3.2889
	Evaluating
	loss=0.0232, precision=0.2495, recall=0.9896, f1=0.3985
Training finished
best_f1: 0.4196647657642918
directing: Y rim_enhanced: False test_id 0
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15956 # image files with weight 15917
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4127 # image files with weight 4113
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15917
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/249], loss=347.1791
	step [2/249], loss=215.1923
	step [3/249], loss=166.3817
	step [4/249], loss=148.7597
	step [5/249], loss=135.7775
	step [6/249], loss=129.6748
	step [7/249], loss=126.7855
	step [8/249], loss=128.0890
	step [9/249], loss=123.1312
	step [10/249], loss=123.7311
	step [11/249], loss=122.2011
	step [12/249], loss=121.8031
	step [13/249], loss=118.9227
	step [14/249], loss=117.5365
	step [15/249], loss=117.5331
	step [16/249], loss=115.3993
	step [17/249], loss=113.5950
	step [18/249], loss=114.4844
	step [19/249], loss=114.1299
	step [20/249], loss=110.6669
	step [21/249], loss=110.9005
	step [22/249], loss=110.7122
	step [23/249], loss=110.5691
	step [24/249], loss=109.4178
	step [25/249], loss=108.8575
	step [26/249], loss=107.5376
	step [27/249], loss=107.9254
	step [28/249], loss=106.8636
	step [29/249], loss=104.6004
	step [30/249], loss=105.8260
	step [31/249], loss=107.3972
	step [32/249], loss=104.9953
	step [33/249], loss=105.1767
	step [34/249], loss=102.8773
	step [35/249], loss=102.2222
	step [36/249], loss=101.3790
	step [37/249], loss=102.1221
	step [38/249], loss=99.9963
	step [39/249], loss=98.8411
	step [40/249], loss=97.7503
	step [41/249], loss=97.8917
	step [42/249], loss=97.0163
	step [43/249], loss=97.2868
	step [44/249], loss=97.8495
	step [45/249], loss=99.3376
	step [46/249], loss=95.5832
	step [47/249], loss=97.8335
	step [48/249], loss=95.6166
	step [49/249], loss=95.6591
	step [50/249], loss=96.5579
	step [51/249], loss=93.2866
	step [52/249], loss=93.8859
	step [53/249], loss=94.6721
	step [54/249], loss=93.8635
	step [55/249], loss=93.4304
	step [56/249], loss=90.2640
	step [57/249], loss=91.3527
	step [58/249], loss=91.9866
	step [59/249], loss=92.0150
	step [60/249], loss=91.0821
	step [61/249], loss=92.3651
	step [62/249], loss=89.4527
	step [63/249], loss=91.2734
	step [64/249], loss=89.2384
	step [65/249], loss=89.4724
	step [66/249], loss=88.2155
	step [67/249], loss=89.3542
	step [68/249], loss=87.6543
	step [69/249], loss=88.7859
	step [70/249], loss=86.8023
	step [71/249], loss=86.4513
	step [72/249], loss=86.7164
	step [73/249], loss=86.9699
	step [74/249], loss=86.7103
	step [75/249], loss=84.7610
	step [76/249], loss=83.9508
	step [77/249], loss=86.8129
	step [78/249], loss=84.8656
	step [79/249], loss=82.6113
	step [80/249], loss=84.4539
	step [81/249], loss=81.6439
	step [82/249], loss=83.3967
	step [83/249], loss=85.3724
	step [84/249], loss=82.2491
	step [85/249], loss=80.8820
	step [86/249], loss=80.2744
	step [87/249], loss=82.3847
	step [88/249], loss=83.1196
	step [89/249], loss=82.4914
	step [90/249], loss=80.7881
	step [91/249], loss=80.5416
	step [92/249], loss=80.7840
	step [93/249], loss=82.2410
	step [94/249], loss=81.0930
	step [95/249], loss=80.5004
	step [96/249], loss=77.6884
	step [97/249], loss=78.3211
	step [98/249], loss=79.5068
	step [99/249], loss=80.4906
	step [100/249], loss=79.5118
	step [101/249], loss=78.9485
	step [102/249], loss=78.8498
	step [103/249], loss=76.2463
	step [104/249], loss=77.9690
	step [105/249], loss=77.3525
	step [106/249], loss=77.9017
	step [107/249], loss=77.5837
	step [108/249], loss=78.1169
	step [109/249], loss=77.2121
	step [110/249], loss=77.4532
	step [111/249], loss=75.4373
	step [112/249], loss=75.6929
	step [113/249], loss=76.0743
	step [114/249], loss=76.8738
	step [115/249], loss=74.8370
	step [116/249], loss=75.2778
	step [117/249], loss=77.7372
	step [118/249], loss=75.5961
	step [119/249], loss=76.1314
	step [120/249], loss=76.3037
	step [121/249], loss=76.0629
	step [122/249], loss=74.5811
	step [123/249], loss=72.7863
	step [124/249], loss=72.0210
	step [125/249], loss=74.3979
	step [126/249], loss=73.2205
	step [127/249], loss=73.2255
	step [128/249], loss=74.9415
	step [129/249], loss=74.5175
	step [130/249], loss=73.4461
	step [131/249], loss=72.7866
	step [132/249], loss=73.1655
	step [133/249], loss=72.9462
	step [134/249], loss=72.1245
	step [135/249], loss=74.9025
	step [136/249], loss=72.5605
	step [137/249], loss=69.7241
	step [138/249], loss=71.1309
	step [139/249], loss=71.5789
	step [140/249], loss=71.8525
	step [141/249], loss=71.5085
	step [142/249], loss=72.0440
	step [143/249], loss=71.0138
	step [144/249], loss=71.5765
	step [145/249], loss=71.1000
	step [146/249], loss=69.6481
	step [147/249], loss=71.0448
	step [148/249], loss=69.3621
	step [149/249], loss=69.9494
	step [150/249], loss=70.2695
	step [151/249], loss=70.3660
	step [152/249], loss=70.9172
	step [153/249], loss=68.7554
	step [154/249], loss=71.3710
	step [155/249], loss=70.8952
	step [156/249], loss=70.5452
	step [157/249], loss=71.2150
	step [158/249], loss=70.6005
	step [159/249], loss=66.5245
	step [160/249], loss=68.8013
	step [161/249], loss=71.3490
	step [162/249], loss=68.2828
	step [163/249], loss=67.7490
	step [164/249], loss=68.3582
	step [165/249], loss=66.7550
	step [166/249], loss=67.6975
	step [167/249], loss=65.2351
	step [168/249], loss=66.8129
	step [169/249], loss=69.4501
	step [170/249], loss=68.3688
	step [171/249], loss=68.0398
	step [172/249], loss=66.9067
	step [173/249], loss=66.0289
	step [174/249], loss=66.0652
	step [175/249], loss=66.9924
	step [176/249], loss=67.8727
	step [177/249], loss=67.9833
	step [178/249], loss=67.7405
	step [179/249], loss=64.9048
	step [180/249], loss=65.4020
	step [181/249], loss=65.8850
	step [182/249], loss=64.4772
	step [183/249], loss=64.8121
	step [184/249], loss=65.1522
	step [185/249], loss=68.7262
	step [186/249], loss=65.8906
	step [187/249], loss=63.9362
	step [188/249], loss=63.8896
	step [189/249], loss=66.5464
	step [190/249], loss=64.0820
	step [191/249], loss=63.7418
	step [192/249], loss=65.1236
	step [193/249], loss=62.7385
	step [194/249], loss=65.2290
	step [195/249], loss=65.1579
	step [196/249], loss=63.4729
	step [197/249], loss=65.6435
	step [198/249], loss=62.9514
	step [199/249], loss=65.6846
	step [200/249], loss=63.1173
	step [201/249], loss=62.3686
	step [202/249], loss=64.7003
	step [203/249], loss=63.0035
	step [204/249], loss=62.6875
	step [205/249], loss=63.8319
	step [206/249], loss=63.1401
	step [207/249], loss=62.6831
	step [208/249], loss=61.8302
	step [209/249], loss=61.1402
	step [210/249], loss=63.2794
	step [211/249], loss=61.9919
	step [212/249], loss=60.8954
	step [213/249], loss=62.5104
	step [214/249], loss=59.7040
	step [215/249], loss=60.5180
	step [216/249], loss=61.5798
	step [217/249], loss=61.7572
	step [218/249], loss=60.9006
	step [219/249], loss=58.7427
	step [220/249], loss=63.0913
	step [221/249], loss=60.2696
	step [222/249], loss=61.9927
	step [223/249], loss=62.9226
	step [224/249], loss=62.4259
	step [225/249], loss=58.4574
	step [226/249], loss=60.0604
	step [227/249], loss=61.1241
	step [228/249], loss=62.0916
	step [229/249], loss=60.1648
	step [230/249], loss=59.4465
	step [231/249], loss=61.8429
	step [232/249], loss=60.0525
	step [233/249], loss=60.0060
	step [234/249], loss=59.8366
	step [235/249], loss=62.7120
	step [236/249], loss=56.5349
	step [237/249], loss=59.0343
	step [238/249], loss=58.0232
	step [239/249], loss=58.9443
	step [240/249], loss=58.4093
	step [241/249], loss=58.1045
	step [242/249], loss=59.1353
	step [243/249], loss=56.0254
	step [244/249], loss=58.6150
	step [245/249], loss=57.9273
	step [246/249], loss=58.4569
	step [247/249], loss=58.8770
	step [248/249], loss=58.6745
	step [249/249], loss=40.2818
	Evaluating
	loss=0.2121, precision=0.1367, recall=0.9964, f1=0.2405
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/249], loss=55.8066
	step [2/249], loss=57.8790
	step [3/249], loss=56.6392
	step [4/249], loss=55.2365
	step [5/249], loss=58.0855
	step [6/249], loss=58.8484
	step [7/249], loss=58.1053
	step [8/249], loss=58.8723
	step [9/249], loss=58.1683
	step [10/249], loss=58.2786
	step [11/249], loss=57.0022
	step [12/249], loss=56.8976
	step [13/249], loss=56.4479
	step [14/249], loss=56.6256
	step [15/249], loss=57.8549
	step [16/249], loss=55.9108
	step [17/249], loss=56.1613
	step [18/249], loss=57.2526
	step [19/249], loss=56.8853
	step [20/249], loss=55.3952
	step [21/249], loss=53.5719
	step [22/249], loss=54.7012
	step [23/249], loss=55.7092
	step [24/249], loss=55.3391
	step [25/249], loss=57.5013
	step [26/249], loss=59.8041
	step [27/249], loss=55.0742
	step [28/249], loss=55.1910
	step [29/249], loss=56.5275
	step [30/249], loss=55.8730
	step [31/249], loss=55.4460
	step [32/249], loss=55.1070
	step [33/249], loss=54.5519
	step [34/249], loss=52.8052
	step [35/249], loss=55.0069
	step [36/249], loss=55.5719
	step [37/249], loss=53.9732
	step [38/249], loss=53.8092
	step [39/249], loss=53.3240
	step [40/249], loss=56.5161
	step [41/249], loss=54.4030
	step [42/249], loss=53.1181
	step [43/249], loss=52.2433
	step [44/249], loss=54.0180
	step [45/249], loss=55.0686
	step [46/249], loss=54.0398
	step [47/249], loss=53.6635
	step [48/249], loss=50.8590
	step [49/249], loss=52.7109
	step [50/249], loss=52.3213
	step [51/249], loss=52.4355
	step [52/249], loss=51.5918
	step [53/249], loss=53.5130
	step [54/249], loss=54.1914
	step [55/249], loss=52.0598
	step [56/249], loss=52.8644
	step [57/249], loss=52.8361
	step [58/249], loss=52.2072
	step [59/249], loss=51.3658
	step [60/249], loss=53.9325
	step [61/249], loss=52.5336
	step [62/249], loss=53.8558
	step [63/249], loss=51.5076
	step [64/249], loss=52.5231
	step [65/249], loss=52.6209
	step [66/249], loss=50.6699
	step [67/249], loss=51.1069
	step [68/249], loss=51.3088
	step [69/249], loss=52.0250
	step [70/249], loss=53.7825
	step [71/249], loss=50.1012
	step [72/249], loss=53.1135
	step [73/249], loss=52.3360
	step [74/249], loss=51.3797
	step [75/249], loss=51.6533
	step [76/249], loss=50.6962
	step [77/249], loss=49.5578
	step [78/249], loss=49.4208
	step [79/249], loss=51.3085
	step [80/249], loss=50.2906
	step [81/249], loss=50.4627
	step [82/249], loss=49.7426
	step [83/249], loss=48.5542
	step [84/249], loss=50.0914
	step [85/249], loss=49.8002
	step [86/249], loss=49.1971
	step [87/249], loss=49.4039
	step [88/249], loss=47.0267
	step [89/249], loss=51.0469
	step [90/249], loss=50.8669
	step [91/249], loss=49.5596
	step [92/249], loss=50.5535
	step [93/249], loss=50.3092
	step [94/249], loss=47.7649
	step [95/249], loss=49.6359
	step [96/249], loss=48.9516
	step [97/249], loss=48.6464
	step [98/249], loss=51.6515
	step [99/249], loss=49.8917
	step [100/249], loss=49.4549
	step [101/249], loss=48.1338
	step [102/249], loss=49.6586
	step [103/249], loss=49.7033
	step [104/249], loss=49.0416
	step [105/249], loss=48.2378
	step [106/249], loss=49.0511
	step [107/249], loss=49.9251
	step [108/249], loss=46.1103
	step [109/249], loss=47.8230
	step [110/249], loss=49.6903
	step [111/249], loss=49.1056
	step [112/249], loss=47.2181
	step [113/249], loss=47.9493
	step [114/249], loss=47.6915
	step [115/249], loss=46.3305
	step [116/249], loss=48.1555
	step [117/249], loss=46.5436
	step [118/249], loss=46.5349
	step [119/249], loss=47.2791
	step [120/249], loss=47.3958
	step [121/249], loss=48.7858
	step [122/249], loss=46.3992
	step [123/249], loss=46.5291
	step [124/249], loss=45.7388
	step [125/249], loss=45.9072
	step [126/249], loss=46.7548
	step [127/249], loss=44.2373
	step [128/249], loss=46.4044
	step [129/249], loss=46.9223
	step [130/249], loss=44.7258
	step [131/249], loss=45.4912
	step [132/249], loss=48.4401
	step [133/249], loss=44.5747
	step [134/249], loss=47.0096
	step [135/249], loss=45.6722
	step [136/249], loss=46.8214
	step [137/249], loss=45.4727
	step [138/249], loss=45.8577
	step [139/249], loss=46.0411
	step [140/249], loss=44.9859
	step [141/249], loss=51.0529
	step [142/249], loss=45.2803
	step [143/249], loss=44.8762
	step [144/249], loss=43.5415
	step [145/249], loss=45.6416
	step [146/249], loss=47.7435
	step [147/249], loss=44.5200
	step [148/249], loss=45.5559
	step [149/249], loss=44.3068
	step [150/249], loss=46.3012
	step [151/249], loss=44.7465
	step [152/249], loss=45.9149
	step [153/249], loss=43.4978
	step [154/249], loss=44.9028
	step [155/249], loss=44.8905
	step [156/249], loss=45.1180
	step [157/249], loss=44.6765
	step [158/249], loss=42.1366
	step [159/249], loss=45.1855
	step [160/249], loss=43.2946
	step [161/249], loss=44.3412
	step [162/249], loss=46.5291
	step [163/249], loss=43.2451
	step [164/249], loss=44.1920
	step [165/249], loss=42.9401
	step [166/249], loss=44.6416
	step [167/249], loss=45.3091
	step [168/249], loss=42.8002
	step [169/249], loss=43.5602
	step [170/249], loss=42.1658
	step [171/249], loss=45.2470
	step [172/249], loss=43.7349
	step [173/249], loss=44.1581
	step [174/249], loss=40.7260
	step [175/249], loss=43.0736
	step [176/249], loss=42.2421
	step [177/249], loss=44.0741
	step [178/249], loss=42.5397
	step [179/249], loss=41.6874
	step [180/249], loss=43.7884
	step [181/249], loss=44.0874
	step [182/249], loss=44.5917
	step [183/249], loss=44.2480
	step [184/249], loss=43.3245
	step [185/249], loss=41.9326
	step [186/249], loss=44.4727
	step [187/249], loss=42.7318
	step [188/249], loss=41.7410
	step [189/249], loss=42.5099
	step [190/249], loss=42.8479
	step [191/249], loss=40.2487
	step [192/249], loss=42.8582
	step [193/249], loss=42.3057
	step [194/249], loss=43.4647
	step [195/249], loss=42.3677
	step [196/249], loss=41.5411
	step [197/249], loss=41.4543
	step [198/249], loss=42.6751
	step [199/249], loss=41.4706
	step [200/249], loss=42.0138
	step [201/249], loss=43.4763
	step [202/249], loss=41.0110
	step [203/249], loss=40.7271
	step [204/249], loss=42.2151
	step [205/249], loss=40.8649
	step [206/249], loss=41.0602
	step [207/249], loss=40.8789
	step [208/249], loss=40.5926
	step [209/249], loss=43.4262
	step [210/249], loss=39.3196
	step [211/249], loss=40.8695
	step [212/249], loss=42.0535
	step [213/249], loss=41.5794
	step [214/249], loss=42.8557
	step [215/249], loss=40.6451
	step [216/249], loss=39.5726
	step [217/249], loss=41.1840
	step [218/249], loss=41.1057
	step [219/249], loss=40.0984
	step [220/249], loss=40.5284
	step [221/249], loss=42.7957
	step [222/249], loss=38.5776
	step [223/249], loss=41.2841
	step [224/249], loss=40.4661
	step [225/249], loss=39.0013
	step [226/249], loss=37.9948
	step [227/249], loss=38.6263
	step [228/249], loss=41.7253
	step [229/249], loss=41.0533
	step [230/249], loss=39.7234
	step [231/249], loss=37.9877
	step [232/249], loss=40.1633
	step [233/249], loss=40.3691
	step [234/249], loss=39.2633
	step [235/249], loss=38.7476
	step [236/249], loss=39.1574
	step [237/249], loss=40.0361
	step [238/249], loss=40.0939
	step [239/249], loss=39.7988
	step [240/249], loss=39.5032
	step [241/249], loss=39.1708
	step [242/249], loss=40.4581
	step [243/249], loss=40.5026
	step [244/249], loss=38.5323
	step [245/249], loss=40.1062
	step [246/249], loss=39.0457
	step [247/249], loss=40.9849
	step [248/249], loss=41.5294
	step [249/249], loss=27.1578
	Evaluating
	loss=0.1430, precision=0.1366, recall=0.9966, f1=0.2403
Training epoch 3
	step [1/249], loss=39.2377
	step [2/249], loss=38.5909
	step [3/249], loss=38.3021
	step [4/249], loss=38.2572
	step [5/249], loss=39.5025
	step [6/249], loss=37.7207
	step [7/249], loss=39.7047
	step [8/249], loss=40.3417
	step [9/249], loss=38.1308
	step [10/249], loss=38.6567
	step [11/249], loss=38.2436
	step [12/249], loss=38.9296
	step [13/249], loss=37.7035
	step [14/249], loss=37.1406
	step [15/249], loss=37.2856
	step [16/249], loss=37.5651
	step [17/249], loss=38.5804
	step [18/249], loss=36.6294
	step [19/249], loss=38.7824
	step [20/249], loss=37.3373
	step [21/249], loss=37.8322
	step [22/249], loss=39.3845
	step [23/249], loss=37.7177
	step [24/249], loss=37.3783
	step [25/249], loss=39.0669
	step [26/249], loss=37.3480
	step [27/249], loss=37.7109
	step [28/249], loss=37.1077
	step [29/249], loss=35.9180
	step [30/249], loss=36.5066
	step [31/249], loss=36.9927
	step [32/249], loss=37.6467
	step [33/249], loss=35.8883
	step [34/249], loss=36.2443
	step [35/249], loss=37.3858
	step [36/249], loss=36.9689
	step [37/249], loss=37.4552
	step [38/249], loss=37.1226
	step [39/249], loss=37.5173
	step [40/249], loss=38.6903
	step [41/249], loss=35.6345
	step [42/249], loss=35.5587
	step [43/249], loss=38.2821
	step [44/249], loss=38.1068
	step [45/249], loss=37.8190
	step [46/249], loss=38.0801
	step [47/249], loss=35.5091
	step [48/249], loss=37.1757
	step [49/249], loss=35.3089
	step [50/249], loss=37.4801
	step [51/249], loss=34.9988
	step [52/249], loss=37.1415
	step [53/249], loss=35.4682
	step [54/249], loss=35.9277
	step [55/249], loss=36.7164
	step [56/249], loss=36.9745
	step [57/249], loss=36.5358
	step [58/249], loss=34.9881
	step [59/249], loss=37.6537
	step [60/249], loss=35.9218
	step [61/249], loss=35.7228
	step [62/249], loss=36.7026
	step [63/249], loss=34.1851
	step [64/249], loss=35.6153
	step [65/249], loss=36.4076
	step [66/249], loss=36.5127
	step [67/249], loss=34.7397
	step [68/249], loss=36.7769
	step [69/249], loss=36.2701
	step [70/249], loss=35.7018
	step [71/249], loss=35.2431
	step [72/249], loss=36.7594
	step [73/249], loss=34.8935
	step [74/249], loss=33.0396
	step [75/249], loss=35.0481
	step [76/249], loss=36.1410
	step [77/249], loss=34.1884
	step [78/249], loss=35.2770
	step [79/249], loss=35.3294
	step [80/249], loss=34.9602
	step [81/249], loss=34.8379
	step [82/249], loss=34.3517
	step [83/249], loss=33.1667
	step [84/249], loss=33.0149
	step [85/249], loss=33.4424
	step [86/249], loss=36.1252
	step [87/249], loss=34.7282
	step [88/249], loss=35.5025
	step [89/249], loss=33.9767
	step [90/249], loss=34.3030
	step [91/249], loss=34.5991
	step [92/249], loss=35.3182
	step [93/249], loss=34.5990
	step [94/249], loss=33.5980
	step [95/249], loss=34.4873
	step [96/249], loss=33.4706
	step [97/249], loss=34.7890
	step [98/249], loss=34.0818
	step [99/249], loss=33.5879
	step [100/249], loss=35.0835
	step [101/249], loss=34.3137
	step [102/249], loss=33.2245
	step [103/249], loss=31.7644
	step [104/249], loss=35.2624
	step [105/249], loss=33.7628
	step [106/249], loss=33.7119
	step [107/249], loss=33.3189
	step [108/249], loss=37.1854
	step [109/249], loss=33.4097
	step [110/249], loss=34.1302
	step [111/249], loss=33.8125
	step [112/249], loss=33.8102
	step [113/249], loss=33.8143
	step [114/249], loss=32.5412
	step [115/249], loss=32.3161
	step [116/249], loss=33.0656
	step [117/249], loss=34.6698
	step [118/249], loss=33.6051
	step [119/249], loss=33.3851
	step [120/249], loss=31.4212
	step [121/249], loss=31.5073
	step [122/249], loss=33.6708
	step [123/249], loss=33.4431
	step [124/249], loss=32.7017
	step [125/249], loss=35.0616
	step [126/249], loss=33.3439
	step [127/249], loss=34.0133
	step [128/249], loss=31.9320
	step [129/249], loss=31.6646
	step [130/249], loss=32.0911
	step [131/249], loss=33.0728
	step [132/249], loss=32.2807
	step [133/249], loss=32.1022
	step [134/249], loss=31.6949
	step [135/249], loss=34.4986
	step [136/249], loss=31.4930
	step [137/249], loss=33.7696
	step [138/249], loss=32.0644
	step [139/249], loss=31.8698
	step [140/249], loss=34.3502
	step [141/249], loss=31.6597
	step [142/249], loss=31.7265
	step [143/249], loss=32.8347
	step [144/249], loss=32.2109
	step [145/249], loss=31.6729
	step [146/249], loss=32.7410
	step [147/249], loss=32.1420
	step [148/249], loss=32.4044
	step [149/249], loss=31.9408
	step [150/249], loss=32.8442
	step [151/249], loss=32.0602
	step [152/249], loss=30.9156
	step [153/249], loss=32.1712
	step [154/249], loss=31.2344
	step [155/249], loss=32.1175
	step [156/249], loss=31.8357
	step [157/249], loss=31.0129
	step [158/249], loss=30.3982
	step [159/249], loss=31.8115
	step [160/249], loss=34.5542
	step [161/249], loss=31.5545
	step [162/249], loss=31.4923
	step [163/249], loss=31.6960
	step [164/249], loss=31.2935
	step [165/249], loss=31.5286
	step [166/249], loss=31.0669
	step [167/249], loss=29.5855
	step [168/249], loss=31.8631
	step [169/249], loss=29.8746
	step [170/249], loss=29.9329
	step [171/249], loss=31.6021
	step [172/249], loss=30.2606
	step [173/249], loss=30.7406
	step [174/249], loss=32.5910
	step [175/249], loss=30.9691
	step [176/249], loss=30.5575
	step [177/249], loss=30.2989
	step [178/249], loss=29.9633
	step [179/249], loss=31.5914
	step [180/249], loss=28.7662
	step [181/249], loss=31.4377
	step [182/249], loss=31.6410
	step [183/249], loss=30.6916
	step [184/249], loss=29.7041
	step [185/249], loss=30.9345
	step [186/249], loss=30.6966
	step [187/249], loss=29.8509
	step [188/249], loss=30.3923
	step [189/249], loss=30.4684
	step [190/249], loss=29.8306
	step [191/249], loss=29.1260
	step [192/249], loss=31.4434
	step [193/249], loss=31.7148
	step [194/249], loss=32.0467
	step [195/249], loss=30.7015
	step [196/249], loss=30.3780
	step [197/249], loss=32.0485
	step [198/249], loss=31.8317
	step [199/249], loss=28.6430
	step [200/249], loss=30.2945
	step [201/249], loss=30.8241
	step [202/249], loss=30.6200
	step [203/249], loss=32.1220
	step [204/249], loss=29.0305
	step [205/249], loss=30.9841
	step [206/249], loss=29.5510
	step [207/249], loss=30.1179
	step [208/249], loss=30.1468
	step [209/249], loss=31.9085
	step [210/249], loss=29.1501
	step [211/249], loss=31.4587
	step [212/249], loss=28.1294
	step [213/249], loss=29.2329
	step [214/249], loss=28.4662
	step [215/249], loss=27.1927
	step [216/249], loss=30.7552
	step [217/249], loss=30.2758
	step [218/249], loss=30.3100
	step [219/249], loss=29.3505
	step [220/249], loss=31.6587
	step [221/249], loss=28.2619
	step [222/249], loss=29.1405
	step [223/249], loss=29.2224
	step [224/249], loss=29.7126
	step [225/249], loss=29.1621
	step [226/249], loss=28.3585
	step [227/249], loss=29.3021
	step [228/249], loss=29.6290
	step [229/249], loss=28.8601
	step [230/249], loss=30.3633
	step [231/249], loss=27.3200
	step [232/249], loss=29.8417
	step [233/249], loss=27.7664
	step [234/249], loss=29.2291
	step [235/249], loss=27.7837
	step [236/249], loss=28.0466
	step [237/249], loss=33.3277
	step [238/249], loss=27.4220
	step [239/249], loss=29.1548
	step [240/249], loss=28.4764
	step [241/249], loss=28.8045
	step [242/249], loss=32.0502
	step [243/249], loss=30.2768
	step [244/249], loss=31.0743
	step [245/249], loss=28.3195
	step [246/249], loss=29.4799
	step [247/249], loss=28.2708
	step [248/249], loss=27.2485
	step [249/249], loss=19.7569
	Evaluating
	loss=0.0968, precision=0.1661, recall=0.9952, f1=0.2848
saving model as: 0_saved_model.pth
Training epoch 4
	step [1/249], loss=29.7429
	step [2/249], loss=31.0283
	step [3/249], loss=27.2616
	step [4/249], loss=31.4805
	step [5/249], loss=27.4622
	step [6/249], loss=28.5421
	step [7/249], loss=27.3346
	step [8/249], loss=29.3470
	step [9/249], loss=30.7861
	step [10/249], loss=27.6795
	step [11/249], loss=26.3707
	step [12/249], loss=27.9019
	step [13/249], loss=28.2903
	step [14/249], loss=28.2786
	step [15/249], loss=27.9654
	step [16/249], loss=26.8201
	step [17/249], loss=27.5355
	step [18/249], loss=27.3030
	step [19/249], loss=27.4373
	step [20/249], loss=28.5516
	step [21/249], loss=26.4431
	step [22/249], loss=28.4543
	step [23/249], loss=29.0413
	step [24/249], loss=26.9363
	step [25/249], loss=26.9647
	step [26/249], loss=27.9138
	step [27/249], loss=27.7237
	step [28/249], loss=26.3130
	step [29/249], loss=28.4690
	step [30/249], loss=27.0303
	step [31/249], loss=28.5272
	step [32/249], loss=26.0847
	step [33/249], loss=25.7476
	step [34/249], loss=27.2770
	step [35/249], loss=26.7654
	step [36/249], loss=28.5768
	step [37/249], loss=28.3629
	step [38/249], loss=28.7927
	step [39/249], loss=28.0704
	step [40/249], loss=26.7990
	step [41/249], loss=26.8068
	step [42/249], loss=29.9024
	step [43/249], loss=27.6176
	step [44/249], loss=26.7021
	step [45/249], loss=28.5448
	step [46/249], loss=26.5682
	step [47/249], loss=27.0723
	step [48/249], loss=26.5471
	step [49/249], loss=28.2619
	step [50/249], loss=25.8452
	step [51/249], loss=25.8603
	step [52/249], loss=25.4268
	step [53/249], loss=26.1550
	step [54/249], loss=25.6224
	step [55/249], loss=26.5674
	step [56/249], loss=28.0968
	step [57/249], loss=26.7668
	step [58/249], loss=26.7796
	step [59/249], loss=25.5588
	step [60/249], loss=25.5256
	step [61/249], loss=26.0151
	step [62/249], loss=26.4142
	step [63/249], loss=25.9727
	step [64/249], loss=25.5114
	step [65/249], loss=26.2100
	step [66/249], loss=27.0372
	step [67/249], loss=28.2390
	step [68/249], loss=25.8010
	step [69/249], loss=25.4699
	step [70/249], loss=27.2136
	step [71/249], loss=26.5983
	step [72/249], loss=27.7854
	step [73/249], loss=27.1796
	step [74/249], loss=26.2112
	step [75/249], loss=26.8739
	step [76/249], loss=25.4544
	step [77/249], loss=27.1566
	step [78/249], loss=24.7823
	step [79/249], loss=26.0912
	step [80/249], loss=25.6143
	step [81/249], loss=24.7195
	step [82/249], loss=26.1852
	step [83/249], loss=26.6711
	step [84/249], loss=27.1588
	step [85/249], loss=25.1899
	step [86/249], loss=26.2501
	step [87/249], loss=27.5269
	step [88/249], loss=26.8524
	step [89/249], loss=26.4638
	step [90/249], loss=26.3232
	step [91/249], loss=25.2121
	step [92/249], loss=26.9897
	step [93/249], loss=25.8639
	step [94/249], loss=25.1146
	step [95/249], loss=26.6067
	step [96/249], loss=24.4334
	step [97/249], loss=25.2903
	step [98/249], loss=26.3895
	step [99/249], loss=30.1040
	step [100/249], loss=25.8623
	step [101/249], loss=26.0178
	step [102/249], loss=25.9124
	step [103/249], loss=24.0978
	step [104/249], loss=24.5885
	step [105/249], loss=25.6252
	step [106/249], loss=26.4070
	step [107/249], loss=24.0251
	step [108/249], loss=26.4258
	step [109/249], loss=25.9231
	step [110/249], loss=24.9579
	step [111/249], loss=25.4290
	step [112/249], loss=24.8219
	step [113/249], loss=24.4450
	step [114/249], loss=23.7150
	step [115/249], loss=23.4218
	step [116/249], loss=26.8735
	step [117/249], loss=25.0252
	step [118/249], loss=25.1130
	step [119/249], loss=23.8544
	step [120/249], loss=25.1554
	step [121/249], loss=24.5646
	step [122/249], loss=25.2475
	step [123/249], loss=25.7371
	step [124/249], loss=23.2091
	step [125/249], loss=24.3171
	step [126/249], loss=24.7094
	step [127/249], loss=26.1130
	step [128/249], loss=25.4015
	step [129/249], loss=25.0902
	step [130/249], loss=24.1551
	step [131/249], loss=23.8484
	step [132/249], loss=26.3925
	step [133/249], loss=25.9604
	step [134/249], loss=23.7241
	step [135/249], loss=24.8088
	step [136/249], loss=24.9491
	step [137/249], loss=22.8800
	step [138/249], loss=24.9818
	step [139/249], loss=25.4258
	step [140/249], loss=24.0301
	step [141/249], loss=24.7174
	step [142/249], loss=24.0211
	step [143/249], loss=24.3079
	step [144/249], loss=26.6490
	step [145/249], loss=25.3084
	step [146/249], loss=23.6644
	step [147/249], loss=24.4326
	step [148/249], loss=23.5927
	step [149/249], loss=23.8435
	step [150/249], loss=24.5959
	step [151/249], loss=24.2323
	step [152/249], loss=23.0557
	step [153/249], loss=23.4529
	step [154/249], loss=28.2946
	step [155/249], loss=27.0639
	step [156/249], loss=22.4609
	step [157/249], loss=23.4908
	step [158/249], loss=23.8450
	step [159/249], loss=23.4156
	step [160/249], loss=22.8277
	step [161/249], loss=24.6609
	step [162/249], loss=23.8593
	step [163/249], loss=27.0592
	step [164/249], loss=23.6439
	step [165/249], loss=22.3723
	step [166/249], loss=24.9219
	step [167/249], loss=23.4946
	step [168/249], loss=23.3563
	step [169/249], loss=22.4086
	step [170/249], loss=23.2806
	step [171/249], loss=22.4589
	step [172/249], loss=23.4149
	step [173/249], loss=22.6724
	step [174/249], loss=25.7559
	step [175/249], loss=23.2470
	step [176/249], loss=23.7551
	step [177/249], loss=24.4501
	step [178/249], loss=24.4484
	step [179/249], loss=26.9812
	step [180/249], loss=21.9985
	step [181/249], loss=22.3272
	step [182/249], loss=23.0604
	step [183/249], loss=23.0752
	step [184/249], loss=23.2979
	step [185/249], loss=24.9183
	step [186/249], loss=21.7950
	step [187/249], loss=24.2577
	step [188/249], loss=22.8601
	step [189/249], loss=21.0803
	step [190/249], loss=22.6436
	step [191/249], loss=23.5732
	step [192/249], loss=22.1816
	step [193/249], loss=21.9348
	step [194/249], loss=22.5804
	step [195/249], loss=22.3022
	step [196/249], loss=22.6391
	step [197/249], loss=23.3262
	step [198/249], loss=23.6410
	step [199/249], loss=22.1595
	step [200/249], loss=21.9579
	step [201/249], loss=24.1506
	step [202/249], loss=22.8033
	step [203/249], loss=23.0676
	step [204/249], loss=21.5815
	step [205/249], loss=24.6073
	step [206/249], loss=23.3038
	step [207/249], loss=22.5466
	step [208/249], loss=22.0238
	step [209/249], loss=22.2192
	step [210/249], loss=23.5009
	step [211/249], loss=22.0550
	step [212/249], loss=23.1983
	step [213/249], loss=26.1710
	step [214/249], loss=22.3857
	step [215/249], loss=21.0949
	step [216/249], loss=21.6369
	step [217/249], loss=22.9436
	step [218/249], loss=21.9842
	step [219/249], loss=23.4983
	step [220/249], loss=21.2094
	step [221/249], loss=23.3479
	step [222/249], loss=22.6071
	step [223/249], loss=23.9982
	step [224/249], loss=21.9165
	step [225/249], loss=22.9786
	step [226/249], loss=22.7186
	step [227/249], loss=22.4137
	step [228/249], loss=22.0344
	step [229/249], loss=21.9087
	step [230/249], loss=22.8316
	step [231/249], loss=21.8471
	step [232/249], loss=22.3586
	step [233/249], loss=21.6114
	step [234/249], loss=23.4458
	step [235/249], loss=23.3739
	step [236/249], loss=20.8969
	step [237/249], loss=23.2562
	step [238/249], loss=22.5160
	step [239/249], loss=23.0482
	step [240/249], loss=23.4813
	step [241/249], loss=22.0397
	step [242/249], loss=23.7778
	step [243/249], loss=21.9165
	step [244/249], loss=22.6628
	step [245/249], loss=24.0631
	step [246/249], loss=20.5494
	step [247/249], loss=21.2552
	step [248/249], loss=20.7672
	step [249/249], loss=16.2622
	Evaluating
	loss=0.0751, precision=0.1598, recall=0.9959, f1=0.2754
Training epoch 5
	step [1/249], loss=23.6104
	step [2/249], loss=20.5393
	step [3/249], loss=21.2559
	step [4/249], loss=20.4270
	step [5/249], loss=24.4827
	step [6/249], loss=20.5730
	step [7/249], loss=20.5188
	step [8/249], loss=21.8003
	step [9/249], loss=20.8759
	step [10/249], loss=20.2801
	step [11/249], loss=20.7704
	step [12/249], loss=23.6304
	step [13/249], loss=23.2711
	step [14/249], loss=21.6717
	step [15/249], loss=23.2195
	step [16/249], loss=21.7158
	step [17/249], loss=21.1753
	step [18/249], loss=21.1495
	step [19/249], loss=23.4626
	step [20/249], loss=21.5045
	step [21/249], loss=19.7128
	step [22/249], loss=21.2004
	step [23/249], loss=20.3004
	step [24/249], loss=22.6831
	step [25/249], loss=20.6576
	step [26/249], loss=21.8748
	step [27/249], loss=23.4178
	step [28/249], loss=22.3977
	step [29/249], loss=20.3195
	step [30/249], loss=21.6469
	step [31/249], loss=22.0207
	step [32/249], loss=21.8960
	step [33/249], loss=19.8850
	step [34/249], loss=20.6864
	step [35/249], loss=21.4046
	step [36/249], loss=21.1645
	step [37/249], loss=19.3418
	step [38/249], loss=19.8151
	step [39/249], loss=21.3201
	step [40/249], loss=19.5013
	step [41/249], loss=21.8339
	step [42/249], loss=21.6654
	step [43/249], loss=20.5147
	step [44/249], loss=21.4040
	step [45/249], loss=20.0570
	step [46/249], loss=22.9669
	step [47/249], loss=21.1786
	step [48/249], loss=19.8325
	step [49/249], loss=21.5765
	step [50/249], loss=20.2129
	step [51/249], loss=20.5235
	step [52/249], loss=22.5759
	step [53/249], loss=20.9381
	step [54/249], loss=22.0408
	step [55/249], loss=20.0407
	step [56/249], loss=19.9502
	step [57/249], loss=21.2483
	step [58/249], loss=20.7211
	step [59/249], loss=24.1165
	step [60/249], loss=21.1190
	step [61/249], loss=20.7267
	step [62/249], loss=20.6122
	step [63/249], loss=21.1002
	step [64/249], loss=21.4696
	step [65/249], loss=21.8289
	step [66/249], loss=22.2896
	step [67/249], loss=20.7446
	step [68/249], loss=21.8377
	step [69/249], loss=21.2448
	step [70/249], loss=20.5524
	step [71/249], loss=22.5832
	step [72/249], loss=20.6850
	step [73/249], loss=20.6464
	step [74/249], loss=19.8136
	step [75/249], loss=20.6538
	step [76/249], loss=20.3154
	step [77/249], loss=19.3596
	step [78/249], loss=22.0803
	step [79/249], loss=21.4710
	step [80/249], loss=19.6777
	step [81/249], loss=19.9064
	step [82/249], loss=23.6672
	step [83/249], loss=21.7931
	step [84/249], loss=19.1709
	step [85/249], loss=23.2025
	step [86/249], loss=21.5569
	step [87/249], loss=20.9234
	step [88/249], loss=21.3887
	step [89/249], loss=20.2928
	step [90/249], loss=21.6837
	step [91/249], loss=21.1415
	step [92/249], loss=20.8567
	step [93/249], loss=21.8483
	step [94/249], loss=21.3774
	step [95/249], loss=19.9703
	step [96/249], loss=19.9937
	step [97/249], loss=20.1957
	step [98/249], loss=20.4891
	step [99/249], loss=18.5827
	step [100/249], loss=19.8342
	step [101/249], loss=19.4151
	step [102/249], loss=19.8131
	step [103/249], loss=18.2217
	step [104/249], loss=19.2305
	step [105/249], loss=21.5444
	step [106/249], loss=21.4185
	step [107/249], loss=18.2972
	step [108/249], loss=18.7098
	step [109/249], loss=19.9069
	step [110/249], loss=21.2668
	step [111/249], loss=17.8594
	step [112/249], loss=19.9367
	step [113/249], loss=17.9831
	step [114/249], loss=20.0937
	step [115/249], loss=19.3757
	step [116/249], loss=21.8218
	step [117/249], loss=19.0995
	step [118/249], loss=19.3716
	step [119/249], loss=19.1189
	step [120/249], loss=18.5351
	step [121/249], loss=20.5505
	step [122/249], loss=20.3012
	step [123/249], loss=18.9084
	step [124/249], loss=19.2608
	step [125/249], loss=19.4901
	step [126/249], loss=17.8371
	step [127/249], loss=21.8627
	step [128/249], loss=21.5633
	step [129/249], loss=18.5854
	step [130/249], loss=18.7799
	step [131/249], loss=19.6005
	step [132/249], loss=20.7751
	step [133/249], loss=17.8266
	step [134/249], loss=20.3791
	step [135/249], loss=19.3665
	step [136/249], loss=19.2132
	step [137/249], loss=20.7448
	step [138/249], loss=19.3296
	step [139/249], loss=19.5075
	step [140/249], loss=19.8360
	step [141/249], loss=19.9614
	step [142/249], loss=19.3350
	step [143/249], loss=20.3704
	step [144/249], loss=20.3072
	step [145/249], loss=18.8247
	step [146/249], loss=19.4048
	step [147/249], loss=21.6287
	step [148/249], loss=21.3563
	step [149/249], loss=18.5563
	step [150/249], loss=22.1162
	step [151/249], loss=20.4631
	step [152/249], loss=17.8717
	step [153/249], loss=19.0971
	step [154/249], loss=19.2608
	step [155/249], loss=19.0658
	step [156/249], loss=19.0518
	step [157/249], loss=20.0506
	step [158/249], loss=16.6869
	step [159/249], loss=19.6014
	step [160/249], loss=19.3732
	step [161/249], loss=20.6672
	step [162/249], loss=19.5570
	step [163/249], loss=18.8862
	step [164/249], loss=19.6272
	step [165/249], loss=19.8230
	step [166/249], loss=20.2252
	step [167/249], loss=17.8620
	step [168/249], loss=17.4293
	step [169/249], loss=19.1676
	step [170/249], loss=18.3204
	step [171/249], loss=19.9123
	step [172/249], loss=18.6475
	step [173/249], loss=18.6566
	step [174/249], loss=17.9007
	step [175/249], loss=18.8178
	step [176/249], loss=22.5645
	step [177/249], loss=19.6503
	step [178/249], loss=19.8427
	step [179/249], loss=19.7594
	step [180/249], loss=20.3832
	step [181/249], loss=16.7424
	step [182/249], loss=20.5965
	step [183/249], loss=17.8915
	step [184/249], loss=19.5664
	step [185/249], loss=19.0942
	step [186/249], loss=17.5900
	step [187/249], loss=19.9855
	step [188/249], loss=19.0612
	step [189/249], loss=18.2167
	step [190/249], loss=19.3697
	step [191/249], loss=19.2530
	step [192/249], loss=18.2624
	step [193/249], loss=17.0623
	step [194/249], loss=20.0507
	step [195/249], loss=19.8485
	step [196/249], loss=18.4630
	step [197/249], loss=17.9466
	step [198/249], loss=18.1421
	step [199/249], loss=16.1705
	step [200/249], loss=20.1200
	step [201/249], loss=17.0012
	step [202/249], loss=19.1548
	step [203/249], loss=19.4464
	step [204/249], loss=17.0601
	step [205/249], loss=18.6407
	step [206/249], loss=19.1646
	step [207/249], loss=18.9539
	step [208/249], loss=18.8341
	step [209/249], loss=19.6396
	step [210/249], loss=19.7466
	step [211/249], loss=18.1715
	step [212/249], loss=18.5741
	step [213/249], loss=18.0961
	step [214/249], loss=20.6034
	step [215/249], loss=20.6104
	step [216/249], loss=17.2957
	step [217/249], loss=17.6288
	step [218/249], loss=18.0649
	step [219/249], loss=17.1488
	step [220/249], loss=19.0242
	step [221/249], loss=20.9065
	step [222/249], loss=19.4050
	step [223/249], loss=17.5892
	step [224/249], loss=18.8645
	step [225/249], loss=18.1109
	step [226/249], loss=18.1494
	step [227/249], loss=17.3989
	step [228/249], loss=19.7246
	step [229/249], loss=20.3483
	step [230/249], loss=17.1506
	step [231/249], loss=19.5513
	step [232/249], loss=18.2904
	step [233/249], loss=18.9720
	step [234/249], loss=17.9513
	step [235/249], loss=17.8110
	step [236/249], loss=16.6049
	step [237/249], loss=19.0480
	step [238/249], loss=18.3363
	step [239/249], loss=20.4268
	step [240/249], loss=19.8526
	step [241/249], loss=18.6564
	step [242/249], loss=17.7603
	step [243/249], loss=16.2099
	step [244/249], loss=18.9464
	step [245/249], loss=19.1260
	step [246/249], loss=17.4985
	step [247/249], loss=18.6120
	step [248/249], loss=17.6005
	step [249/249], loss=12.0295
	Evaluating
	loss=0.0605, precision=0.1432, recall=0.9967, f1=0.2503
Training epoch 6
	step [1/249], loss=19.0241
	step [2/249], loss=16.9981
	step [3/249], loss=17.7599
	step [4/249], loss=18.6579
	step [5/249], loss=16.6247
	step [6/249], loss=15.5936
	step [7/249], loss=17.0038
	step [8/249], loss=18.2179
	step [9/249], loss=17.5747
	step [10/249], loss=17.8302
	step [11/249], loss=19.1621
	step [12/249], loss=16.8852
	step [13/249], loss=18.9634
	step [14/249], loss=16.8753
	step [15/249], loss=18.1252
	step [16/249], loss=17.5085
	step [17/249], loss=18.3405
	step [18/249], loss=16.2948
	step [19/249], loss=18.6254
	step [20/249], loss=17.5855
	step [21/249], loss=17.1783
	step [22/249], loss=20.2120
	step [23/249], loss=17.4388
	step [24/249], loss=17.7765
	step [25/249], loss=18.1958
	step [26/249], loss=16.3708
	step [27/249], loss=18.4510
	step [28/249], loss=18.3534
	step [29/249], loss=18.5920
	step [30/249], loss=19.0806
	step [31/249], loss=17.0967
	step [32/249], loss=18.0860
	step [33/249], loss=18.6500
	step [34/249], loss=17.7291
	step [35/249], loss=16.9312
	step [36/249], loss=17.7483
	step [37/249], loss=17.5781
	step [38/249], loss=17.1825
	step [39/249], loss=18.7808
	step [40/249], loss=16.8983
	step [41/249], loss=15.9821
	step [42/249], loss=16.5661
	step [43/249], loss=18.8524
	step [44/249], loss=17.5567
	step [45/249], loss=17.0548
	step [46/249], loss=20.1721
	step [47/249], loss=18.0618
	step [48/249], loss=15.9737
	step [49/249], loss=15.7966
	step [50/249], loss=18.1766
	step [51/249], loss=18.0552
	step [52/249], loss=18.5999
	step [53/249], loss=16.4687
	step [54/249], loss=17.3416
	step [55/249], loss=17.4649
	step [56/249], loss=16.1073
	step [57/249], loss=15.9304
	step [58/249], loss=18.1962
	step [59/249], loss=17.7473
	step [60/249], loss=18.5227
	step [61/249], loss=16.6414
	step [62/249], loss=16.9337
	step [63/249], loss=19.4323
	step [64/249], loss=16.8092
	step [65/249], loss=16.6857
	step [66/249], loss=16.9982
	step [67/249], loss=19.2484
	step [68/249], loss=17.5040
	step [69/249], loss=17.1200
	step [70/249], loss=16.4558
	step [71/249], loss=16.3922
	step [72/249], loss=15.6126
	step [73/249], loss=16.9860
	step [74/249], loss=18.4718
	step [75/249], loss=16.0568
	step [76/249], loss=17.5620
	step [77/249], loss=17.2712
	step [78/249], loss=16.7200
	step [79/249], loss=17.5719
	step [80/249], loss=16.5748
	step [81/249], loss=17.4080
	step [82/249], loss=17.4651
	step [83/249], loss=19.3377
	step [84/249], loss=15.6488
	step [85/249], loss=17.3425
	step [86/249], loss=17.5654
	step [87/249], loss=15.2478
	step [88/249], loss=17.9881
	step [89/249], loss=17.5691
	step [90/249], loss=17.9856
	step [91/249], loss=17.5954
	step [92/249], loss=17.5232
	step [93/249], loss=17.7464
	step [94/249], loss=15.8781
	step [95/249], loss=16.0293
	step [96/249], loss=16.5063
	step [97/249], loss=16.7867
	step [98/249], loss=17.0085
	step [99/249], loss=17.2554
	step [100/249], loss=15.9425
	step [101/249], loss=18.4548
	step [102/249], loss=17.2969
	step [103/249], loss=18.0959
	step [104/249], loss=16.5580
	step [105/249], loss=17.0608
	step [106/249], loss=16.4435
	step [107/249], loss=16.8759
	step [108/249], loss=17.9438
	step [109/249], loss=19.3047
	step [110/249], loss=16.1859
	step [111/249], loss=16.1533
	step [112/249], loss=15.3232
	step [113/249], loss=15.1348
	step [114/249], loss=15.0102
	step [115/249], loss=15.1917
	step [116/249], loss=16.3495
	step [117/249], loss=17.3126
	step [118/249], loss=16.2085
	step [119/249], loss=15.4136
	step [120/249], loss=15.3102
	step [121/249], loss=15.7869
	step [122/249], loss=16.4610
	step [123/249], loss=16.4325
	step [124/249], loss=15.9324
	step [125/249], loss=16.3870
	step [126/249], loss=19.8782
	step [127/249], loss=17.7460
	step [128/249], loss=16.5924
	step [129/249], loss=16.4072
	step [130/249], loss=16.8246
	step [131/249], loss=17.1165
	step [132/249], loss=18.3110
	step [133/249], loss=17.0768
	step [134/249], loss=18.7000
	step [135/249], loss=16.4994
	step [136/249], loss=15.7952
	step [137/249], loss=16.8966
	step [138/249], loss=17.1857
	step [139/249], loss=16.3275
	step [140/249], loss=17.4588
	step [141/249], loss=15.2329
	step [142/249], loss=16.3341
	step [143/249], loss=16.3714
	step [144/249], loss=14.6906
	step [145/249], loss=15.9899
	step [146/249], loss=15.0482
	step [147/249], loss=16.8483
	step [148/249], loss=15.5903
	step [149/249], loss=15.8387
	step [150/249], loss=16.9706
	step [151/249], loss=16.6757
	step [152/249], loss=14.9956
	step [153/249], loss=16.7588
	step [154/249], loss=14.6819
	step [155/249], loss=17.4996
	step [156/249], loss=16.0340
	step [157/249], loss=16.4357
	step [158/249], loss=15.7039
	step [159/249], loss=16.5900
	step [160/249], loss=17.0905
	step [161/249], loss=16.1926
	step [162/249], loss=15.8046
	step [163/249], loss=17.5117
	step [164/249], loss=17.2721
	step [165/249], loss=15.6566
	step [166/249], loss=14.1101
	step [167/249], loss=14.7711
	step [168/249], loss=14.6415
	step [169/249], loss=16.2445
	step [170/249], loss=15.3587
	step [171/249], loss=14.5026
	step [172/249], loss=15.4572
	step [173/249], loss=15.8760
	step [174/249], loss=16.7262
	step [175/249], loss=18.3452
	step [176/249], loss=15.7636
	step [177/249], loss=17.6509
	step [178/249], loss=15.6755
	step [179/249], loss=16.6279
	step [180/249], loss=17.3806
	step [181/249], loss=15.6841
	step [182/249], loss=17.7835
	step [183/249], loss=16.7057
	step [184/249], loss=17.0671
	step [185/249], loss=14.5176
	step [186/249], loss=16.4395
	step [187/249], loss=16.1109
	step [188/249], loss=15.8982
	step [189/249], loss=14.8979
	step [190/249], loss=15.7934
	step [191/249], loss=14.6338
	step [192/249], loss=16.5748
	step [193/249], loss=14.2732
	step [194/249], loss=17.8222
	step [195/249], loss=17.2750
	step [196/249], loss=18.7845
	step [197/249], loss=16.9832
	step [198/249], loss=15.1112
	step [199/249], loss=15.4135
	step [200/249], loss=15.3628
	step [201/249], loss=15.0454
	step [202/249], loss=13.8366
	step [203/249], loss=15.2729
	step [204/249], loss=15.3155
	step [205/249], loss=16.3111
	step [206/249], loss=16.2908
	step [207/249], loss=14.4620
	step [208/249], loss=16.1020
	step [209/249], loss=17.0536
	step [210/249], loss=17.7611
	step [211/249], loss=16.0208
	step [212/249], loss=15.4741
	step [213/249], loss=15.9867
	step [214/249], loss=15.7641
	step [215/249], loss=14.6118
	step [216/249], loss=17.3980
	step [217/249], loss=17.7171
	step [218/249], loss=13.9198
	step [219/249], loss=14.2619
	step [220/249], loss=13.9202
	step [221/249], loss=16.1344
	step [222/249], loss=16.0496
	step [223/249], loss=16.4567
	step [224/249], loss=15.7438
	step [225/249], loss=15.6238
	step [226/249], loss=16.4031
	step [227/249], loss=16.2000
	step [228/249], loss=14.5087
	step [229/249], loss=14.3497
	step [230/249], loss=16.1055
	step [231/249], loss=14.6428
	step [232/249], loss=15.4063
	step [233/249], loss=15.7864
	step [234/249], loss=16.1550
	step [235/249], loss=13.1107
	step [236/249], loss=17.7035
	step [237/249], loss=14.8722
	step [238/249], loss=15.2827
	step [239/249], loss=16.8202
	step [240/249], loss=13.5106
	step [241/249], loss=16.3227
	step [242/249], loss=17.1819
	step [243/249], loss=13.8644
	step [244/249], loss=14.8086
	step [245/249], loss=14.9648
	step [246/249], loss=15.4313
	step [247/249], loss=14.7793
	step [248/249], loss=14.9193
	step [249/249], loss=12.1515
	Evaluating
	loss=0.0497, precision=0.1497, recall=0.9966, f1=0.2603
Training epoch 7
	step [1/249], loss=13.9169
	step [2/249], loss=16.1344
	step [3/249], loss=14.6164
	step [4/249], loss=14.9466
	step [5/249], loss=15.4919
	step [6/249], loss=14.7756
	step [7/249], loss=14.9794
	step [8/249], loss=15.1188
	step [9/249], loss=15.0073
	step [10/249], loss=14.2156
	step [11/249], loss=18.2811
	step [12/249], loss=13.9574
	step [13/249], loss=14.4722
	step [14/249], loss=15.9308
	step [15/249], loss=13.6469
	step [16/249], loss=14.6271
	step [17/249], loss=15.4229
	step [18/249], loss=15.3401
	step [19/249], loss=13.7915
	step [20/249], loss=15.0843
	step [21/249], loss=15.2013
	step [22/249], loss=12.9244
	step [23/249], loss=14.2833
	step [24/249], loss=18.2294
	step [25/249], loss=15.3593
	step [26/249], loss=15.6270
	step [27/249], loss=13.9575
	step [28/249], loss=14.7268
	step [29/249], loss=17.1251
	step [30/249], loss=15.8909
	step [31/249], loss=15.7940
	step [32/249], loss=14.4250
	step [33/249], loss=13.3549
	step [34/249], loss=12.5201
	step [35/249], loss=15.0641
	step [36/249], loss=15.0472
	step [37/249], loss=12.6239
	step [38/249], loss=14.5260
	step [39/249], loss=13.1566
	step [40/249], loss=17.1548
	step [41/249], loss=14.2395
	step [42/249], loss=13.9195
	step [43/249], loss=16.4974
	step [44/249], loss=15.1459
	step [45/249], loss=13.0954
	step [46/249], loss=15.3427
	step [47/249], loss=14.1721
	step [48/249], loss=14.7285
	step [49/249], loss=13.8880
	step [50/249], loss=13.4261
	step [51/249], loss=15.9197
	step [52/249], loss=15.2007
	step [53/249], loss=16.6569
	step [54/249], loss=16.0440
	step [55/249], loss=16.8863
	step [56/249], loss=15.1945
	step [57/249], loss=13.0697
	step [58/249], loss=15.7127
	step [59/249], loss=14.5589
	step [60/249], loss=13.6563
	step [61/249], loss=14.2023
	step [62/249], loss=13.9060
	step [63/249], loss=15.5257
	step [64/249], loss=15.7966
	step [65/249], loss=13.2660
	step [66/249], loss=14.6356
	step [67/249], loss=15.3252
	step [68/249], loss=15.8976
	step [69/249], loss=14.6305
	step [70/249], loss=15.8573
	step [71/249], loss=14.2422
	step [72/249], loss=14.5449
	step [73/249], loss=14.3688
	step [74/249], loss=12.8710
	step [75/249], loss=14.9374
	step [76/249], loss=14.2180
	step [77/249], loss=15.7269
	step [78/249], loss=13.1815
	step [79/249], loss=14.0003
	step [80/249], loss=14.3682
	step [81/249], loss=14.8663
	step [82/249], loss=13.9275
	step [83/249], loss=13.6408
	step [84/249], loss=15.3072
	step [85/249], loss=14.0491
	step [86/249], loss=13.9831
	step [87/249], loss=16.3386
	step [88/249], loss=17.9140
	step [89/249], loss=14.5255
	step [90/249], loss=13.4883
	step [91/249], loss=14.9029
	step [92/249], loss=14.3360
	step [93/249], loss=15.9117
	step [94/249], loss=14.6106
	step [95/249], loss=16.1402
	step [96/249], loss=14.1929
	step [97/249], loss=13.6334
	step [98/249], loss=14.1194
	step [99/249], loss=13.3155
	step [100/249], loss=15.1876
	step [101/249], loss=15.8233
	step [102/249], loss=14.6610
	step [103/249], loss=14.2625
	step [104/249], loss=14.1532
	step [105/249], loss=16.3694
	step [106/249], loss=12.0183
	step [107/249], loss=13.9205
	step [108/249], loss=14.3398
	step [109/249], loss=15.9947
	step [110/249], loss=14.5542
	step [111/249], loss=14.1689
	step [112/249], loss=15.6185
	step [113/249], loss=14.4836
	step [114/249], loss=17.0402
	step [115/249], loss=14.8705
	step [116/249], loss=14.5384
	step [117/249], loss=14.0589
	step [118/249], loss=14.0247
	step [119/249], loss=13.6278
	step [120/249], loss=14.4587
	step [121/249], loss=15.1802
	step [122/249], loss=14.4146
	step [123/249], loss=13.2401
	step [124/249], loss=14.5289
	step [125/249], loss=14.4378
	step [126/249], loss=13.3886
	step [127/249], loss=12.8287
	step [128/249], loss=16.2790
	step [129/249], loss=14.7660
	step [130/249], loss=15.7311
	step [131/249], loss=13.2542
	step [132/249], loss=15.3519
	step [133/249], loss=14.1863
	step [134/249], loss=16.0891
	step [135/249], loss=17.6165
	step [136/249], loss=14.8501
	step [137/249], loss=14.0730
	step [138/249], loss=14.6563
	step [139/249], loss=15.5246
	step [140/249], loss=14.4108
	step [141/249], loss=14.5619
	step [142/249], loss=15.4552
	step [143/249], loss=15.8094
	step [144/249], loss=14.6331
	step [145/249], loss=15.3009
	step [146/249], loss=14.6481
	step [147/249], loss=15.8259
	step [148/249], loss=16.5382
	step [149/249], loss=14.1865
	step [150/249], loss=14.2679
	step [151/249], loss=15.3867
	step [152/249], loss=13.4784
	step [153/249], loss=13.7045
	step [154/249], loss=13.5737
	step [155/249], loss=13.2894
	step [156/249], loss=13.5409
	step [157/249], loss=15.8028
	step [158/249], loss=13.6786
	step [159/249], loss=13.4866
	step [160/249], loss=13.7777
	step [161/249], loss=13.6027
	step [162/249], loss=13.7516
	step [163/249], loss=12.9492
	step [164/249], loss=13.7398
	step [165/249], loss=12.7879
	step [166/249], loss=14.9164
	step [167/249], loss=14.8565
	step [168/249], loss=13.4445
	step [169/249], loss=14.0289
	step [170/249], loss=13.1635
	step [171/249], loss=14.3588
	step [172/249], loss=13.6389
	step [173/249], loss=15.7642
	step [174/249], loss=13.9202
	step [175/249], loss=14.7695
	step [176/249], loss=13.3948
	step [177/249], loss=14.0851
	step [178/249], loss=13.4325
	step [179/249], loss=12.1802
	step [180/249], loss=12.5138
	step [181/249], loss=13.4116
	step [182/249], loss=13.2910
	step [183/249], loss=14.8665
	step [184/249], loss=13.4742
	step [185/249], loss=13.4711
	step [186/249], loss=14.2011
	step [187/249], loss=13.0473
	step [188/249], loss=13.2506
	step [189/249], loss=14.4808
	step [190/249], loss=13.0546
	step [191/249], loss=13.4024
	step [192/249], loss=11.5169
	step [193/249], loss=14.4984
	step [194/249], loss=16.2279
	step [195/249], loss=12.5991
	step [196/249], loss=14.2835
	step [197/249], loss=13.0223
	step [198/249], loss=14.0508
	step [199/249], loss=13.9315
	step [200/249], loss=14.6439
	step [201/249], loss=14.6545
	step [202/249], loss=13.4490
	step [203/249], loss=15.1177
	step [204/249], loss=12.0708
	step [205/249], loss=12.9862
	step [206/249], loss=13.0875
	step [207/249], loss=15.0277
	step [208/249], loss=12.9074
	step [209/249], loss=13.4787
	step [210/249], loss=12.8961
	step [211/249], loss=13.4751
	step [212/249], loss=16.3408
	step [213/249], loss=14.5010
	step [214/249], loss=15.1306
	step [215/249], loss=14.6779
	step [216/249], loss=13.9352
	step [217/249], loss=13.6375
	step [218/249], loss=13.3187
	step [219/249], loss=14.2237
	step [220/249], loss=14.5193
	step [221/249], loss=13.7682
	step [222/249], loss=14.1648
	step [223/249], loss=14.3732
	step [224/249], loss=11.8732
	step [225/249], loss=12.5294
	step [226/249], loss=12.7053
	step [227/249], loss=12.3803
	step [228/249], loss=13.3880
	step [229/249], loss=14.3300
	step [230/249], loss=13.0238
	step [231/249], loss=15.5776
	step [232/249], loss=12.8612
	step [233/249], loss=12.7177
	step [234/249], loss=13.8874
	step [235/249], loss=12.6259
	step [236/249], loss=12.9973
	step [237/249], loss=15.7870
	step [238/249], loss=13.3043
	step [239/249], loss=13.0044
	step [240/249], loss=14.0321
	step [241/249], loss=13.9339
	step [242/249], loss=14.2439
	step [243/249], loss=14.7979
	step [244/249], loss=13.8786
	step [245/249], loss=12.6598
	step [246/249], loss=15.9035
	step [247/249], loss=13.2967
	step [248/249], loss=13.4935
	step [249/249], loss=9.4524
	Evaluating
	loss=0.0463, precision=0.1222, recall=0.9962, f1=0.2177
Training epoch 8
	step [1/249], loss=13.8288
	step [2/249], loss=13.7422
	step [3/249], loss=14.5263
	step [4/249], loss=12.8646
	step [5/249], loss=13.2754
	step [6/249], loss=13.5328
	step [7/249], loss=13.3932
	step [8/249], loss=11.9036
	step [9/249], loss=14.2227
	step [10/249], loss=11.5679
	step [11/249], loss=14.6232
	step [12/249], loss=15.2475
	step [13/249], loss=14.1564
	step [14/249], loss=13.0044
	step [15/249], loss=12.9688
	step [16/249], loss=13.9856
	step [17/249], loss=13.2529
	step [18/249], loss=12.5351
	step [19/249], loss=12.5730
	step [20/249], loss=14.5047
	step [21/249], loss=13.5887
	step [22/249], loss=13.8240
	step [23/249], loss=14.5646
	step [24/249], loss=12.3850
	step [25/249], loss=14.1640
	step [26/249], loss=13.0039
	step [27/249], loss=12.1919
	step [28/249], loss=14.2116
	step [29/249], loss=12.7762
	step [30/249], loss=12.9728
	step [31/249], loss=13.2687
	step [32/249], loss=11.6706
	step [33/249], loss=12.6107
	step [34/249], loss=12.1985
	step [35/249], loss=13.8357
	step [36/249], loss=15.1662
	step [37/249], loss=12.1118
	step [38/249], loss=12.8512
	step [39/249], loss=11.8355
	step [40/249], loss=14.4257
	step [41/249], loss=14.0318
	step [42/249], loss=12.7151
	step [43/249], loss=13.1187
	step [44/249], loss=12.7924
	step [45/249], loss=14.6020
	step [46/249], loss=12.8387
	step [47/249], loss=11.6940
	step [48/249], loss=11.2430
	step [49/249], loss=15.1694
	step [50/249], loss=11.5564
	step [51/249], loss=14.1185
	step [52/249], loss=11.8896
	step [53/249], loss=12.9916
	step [54/249], loss=12.3256
	step [55/249], loss=11.7686
	step [56/249], loss=14.0991
	step [57/249], loss=12.0183
	step [58/249], loss=12.8390
	step [59/249], loss=14.7171
	step [60/249], loss=13.9377
	step [61/249], loss=16.8029
	step [62/249], loss=13.8578
	step [63/249], loss=11.9281
	step [64/249], loss=13.4487
	step [65/249], loss=12.8920
	step [66/249], loss=12.2056
	step [67/249], loss=11.7565
	step [68/249], loss=11.6833
	step [69/249], loss=11.7757
	step [70/249], loss=14.8734
	step [71/249], loss=14.9810
	step [72/249], loss=13.3773
	step [73/249], loss=12.5428
	step [74/249], loss=12.5870
	step [75/249], loss=15.7975
	step [76/249], loss=11.7373
	step [77/249], loss=14.1360
	step [78/249], loss=13.6362
	step [79/249], loss=12.4697
	step [80/249], loss=13.4773
	step [81/249], loss=13.3198
	step [82/249], loss=13.9827
	step [83/249], loss=14.3465
	step [84/249], loss=13.6369
	step [85/249], loss=11.1247
	step [86/249], loss=11.4199
	step [87/249], loss=11.3657
	step [88/249], loss=13.1877
	step [89/249], loss=12.5745
	step [90/249], loss=15.2819
	step [91/249], loss=13.9665
	step [92/249], loss=12.5605
	step [93/249], loss=12.6855
	step [94/249], loss=13.4700
	step [95/249], loss=13.2788
	step [96/249], loss=13.7385
	step [97/249], loss=12.6378
	step [98/249], loss=12.1800
	step [99/249], loss=14.3990
	step [100/249], loss=13.3599
	step [101/249], loss=11.8336
	step [102/249], loss=13.8117
	step [103/249], loss=11.7260
	step [104/249], loss=12.1195
	step [105/249], loss=12.5768
	step [106/249], loss=10.1191
	step [107/249], loss=12.9797
	step [108/249], loss=12.5050
	step [109/249], loss=13.3715
	step [110/249], loss=14.1132
	step [111/249], loss=12.4657
	step [112/249], loss=11.5434
	step [113/249], loss=14.8955
	step [114/249], loss=11.0961
	step [115/249], loss=12.4080
	step [116/249], loss=10.5304
	step [117/249], loss=12.5891
	step [118/249], loss=13.0325
	step [119/249], loss=14.0581
	step [120/249], loss=13.2598
	step [121/249], loss=11.6952
	step [122/249], loss=11.3574
	step [123/249], loss=14.6874
	step [124/249], loss=13.1570
	step [125/249], loss=13.2277
	step [126/249], loss=12.9919
	step [127/249], loss=13.4232
	step [128/249], loss=12.9287
	step [129/249], loss=12.0722
	step [130/249], loss=12.1406
	step [131/249], loss=14.2864
	step [132/249], loss=12.6439
	step [133/249], loss=11.5485
	step [134/249], loss=11.2039
	step [135/249], loss=15.3625
	step [136/249], loss=12.2932
	step [137/249], loss=12.0454
	step [138/249], loss=12.8829
	step [139/249], loss=13.1209
	step [140/249], loss=12.2112
	step [141/249], loss=12.6981
	step [142/249], loss=12.2280
	step [143/249], loss=13.7844
	step [144/249], loss=12.9371
	step [145/249], loss=12.8600
	step [146/249], loss=13.4917
	step [147/249], loss=12.7622
	step [148/249], loss=15.8574
	step [149/249], loss=10.7014
	step [150/249], loss=14.4126
	step [151/249], loss=12.6574
	step [152/249], loss=11.1586
	step [153/249], loss=13.5688
	step [154/249], loss=12.5518
	step [155/249], loss=13.6412
	step [156/249], loss=12.7651
	step [157/249], loss=11.6546
	step [158/249], loss=13.5671
	step [159/249], loss=13.2291
	step [160/249], loss=12.2418
	step [161/249], loss=12.2605
	step [162/249], loss=13.4745
	step [163/249], loss=10.7744
	step [164/249], loss=12.1868
	step [165/249], loss=14.0417
	step [166/249], loss=12.4781
	step [167/249], loss=12.5901
	step [168/249], loss=11.1820
	step [169/249], loss=13.2309
	step [170/249], loss=10.6997
	step [171/249], loss=10.5834
	step [172/249], loss=11.9420
	step [173/249], loss=12.4849
	step [174/249], loss=12.2722
	step [175/249], loss=10.8342
	step [176/249], loss=12.1421
	step [177/249], loss=13.8397
	step [178/249], loss=12.3576
	step [179/249], loss=10.9530
	step [180/249], loss=12.9481
	step [181/249], loss=11.9830
	step [182/249], loss=12.8317
	step [183/249], loss=12.9784
	step [184/249], loss=12.7072
	step [185/249], loss=13.3939
	step [186/249], loss=13.5107
	step [187/249], loss=12.7695
	step [188/249], loss=14.1158
	step [189/249], loss=11.9989
	step [190/249], loss=13.7617
	step [191/249], loss=12.1388
	step [192/249], loss=12.9554
	step [193/249], loss=11.3632
	step [194/249], loss=13.0297
	step [195/249], loss=12.6855
	step [196/249], loss=12.7514
	step [197/249], loss=11.0408
	step [198/249], loss=12.6068
	step [199/249], loss=12.2884
	step [200/249], loss=13.0431
	step [201/249], loss=12.7195
	step [202/249], loss=13.3425
	step [203/249], loss=11.8514
	step [204/249], loss=11.6270
	step [205/249], loss=12.6867
	step [206/249], loss=10.9374
	step [207/249], loss=11.8973
	step [208/249], loss=11.2260
	step [209/249], loss=15.7069
	step [210/249], loss=13.3568
	step [211/249], loss=13.3027
	step [212/249], loss=12.0821
	step [213/249], loss=13.7604
	step [214/249], loss=12.4963
	step [215/249], loss=13.4232
	step [216/249], loss=13.1276
	step [217/249], loss=10.7858
	step [218/249], loss=12.0663
	step [219/249], loss=12.9141
	step [220/249], loss=12.1045
	step [221/249], loss=12.8979
	step [222/249], loss=11.7483
	step [223/249], loss=11.3718
	step [224/249], loss=11.7059
	step [225/249], loss=11.7556
	step [226/249], loss=12.3655
	step [227/249], loss=11.3709
	step [228/249], loss=12.0281
	step [229/249], loss=14.3192
	step [230/249], loss=11.9474
	step [231/249], loss=12.4984
	step [232/249], loss=11.6839
	step [233/249], loss=13.5749
	step [234/249], loss=13.0722
	step [235/249], loss=11.8901
	step [236/249], loss=12.4625
	step [237/249], loss=10.5607
	step [238/249], loss=11.6384
	step [239/249], loss=12.8009
	step [240/249], loss=10.9108
	step [241/249], loss=11.2756
	step [242/249], loss=11.8193
	step [243/249], loss=12.8224
	step [244/249], loss=10.7169
	step [245/249], loss=10.6664
	step [246/249], loss=11.0347
	step [247/249], loss=14.0066
	step [248/249], loss=12.3064
	step [249/249], loss=8.2390
	Evaluating
	loss=0.0362, precision=0.1630, recall=0.9963, f1=0.2802
Training epoch 9
	step [1/249], loss=13.1961
	step [2/249], loss=11.0636
	step [3/249], loss=10.8400
	step [4/249], loss=11.9904
	step [5/249], loss=12.4467
	step [6/249], loss=11.5596
	step [7/249], loss=12.3695
	step [8/249], loss=10.4880
	step [9/249], loss=10.8650
	step [10/249], loss=10.9615
	step [11/249], loss=12.6671
	step [12/249], loss=11.2665
	step [13/249], loss=11.5678
	step [14/249], loss=12.2691
	step [15/249], loss=12.1552
	step [16/249], loss=12.6493
	step [17/249], loss=12.4358
	step [18/249], loss=10.2526
	step [19/249], loss=12.8982
	step [20/249], loss=11.1278
	step [21/249], loss=10.6132
	step [22/249], loss=11.5625
	step [23/249], loss=10.8505
	step [24/249], loss=12.8251
	step [25/249], loss=12.0387
	step [26/249], loss=12.5188
	step [27/249], loss=12.5969
	step [28/249], loss=11.2519
	step [29/249], loss=11.8590
	step [30/249], loss=13.8706
	step [31/249], loss=10.9222
	step [32/249], loss=11.9357
	step [33/249], loss=14.6471
	step [34/249], loss=11.0461
	step [35/249], loss=11.0666
	step [36/249], loss=11.9853
	step [37/249], loss=11.4491
	step [38/249], loss=11.1304
	step [39/249], loss=10.1606
	step [40/249], loss=9.7426
	step [41/249], loss=11.4245
	step [42/249], loss=12.4535
	step [43/249], loss=10.9153
	step [44/249], loss=12.9335
	step [45/249], loss=10.3253
	step [46/249], loss=10.6403
	step [47/249], loss=12.4271
	step [48/249], loss=12.1382
	step [49/249], loss=12.0126
	step [50/249], loss=13.5723
	step [51/249], loss=10.1945
	step [52/249], loss=10.6341
	step [53/249], loss=10.6775
	step [54/249], loss=15.6130
	step [55/249], loss=10.4302
	step [56/249], loss=11.4502
	step [57/249], loss=12.5506
	step [58/249], loss=12.4028
	step [59/249], loss=11.0731
	step [60/249], loss=10.0260
	step [61/249], loss=11.7320
	step [62/249], loss=11.3573
	step [63/249], loss=13.2379
	step [64/249], loss=10.6994
	step [65/249], loss=11.6221
	step [66/249], loss=11.6892
	step [67/249], loss=11.9123
	step [68/249], loss=11.0658
	step [69/249], loss=10.6140
	step [70/249], loss=15.1740
	step [71/249], loss=11.0285
	step [72/249], loss=11.4475
	step [73/249], loss=14.6927
	step [74/249], loss=13.3464
	step [75/249], loss=11.8172
	step [76/249], loss=12.1964
	step [77/249], loss=12.8234
	step [78/249], loss=12.2649
	step [79/249], loss=12.1212
	step [80/249], loss=13.0806
	step [81/249], loss=11.9743
	step [82/249], loss=11.6541
	step [83/249], loss=13.9278
	step [84/249], loss=11.7063
	step [85/249], loss=11.7062
	step [86/249], loss=13.8354
	step [87/249], loss=10.8269
	step [88/249], loss=11.5485
	step [89/249], loss=12.1941
	step [90/249], loss=10.7166
	step [91/249], loss=10.2943
	step [92/249], loss=11.6488
	step [93/249], loss=9.5707
	step [94/249], loss=11.1483
	step [95/249], loss=11.0707
	step [96/249], loss=10.8979
	step [97/249], loss=10.5516
	step [98/249], loss=12.9084
	step [99/249], loss=11.0870
	step [100/249], loss=11.8845
	step [101/249], loss=13.1141
	step [102/249], loss=10.5482
	step [103/249], loss=12.8120
	step [104/249], loss=10.2276
	step [105/249], loss=12.7862
	step [106/249], loss=11.5508
	step [107/249], loss=11.8111
	step [108/249], loss=12.1729
	step [109/249], loss=14.8117
	step [110/249], loss=12.4930
	step [111/249], loss=11.6885
	step [112/249], loss=11.6651
	step [113/249], loss=11.8412
	step [114/249], loss=10.4439
	step [115/249], loss=13.9184
	step [116/249], loss=11.6271
	step [117/249], loss=11.0626
	step [118/249], loss=10.8485
	step [119/249], loss=10.7501
	step [120/249], loss=10.9172
	step [121/249], loss=12.2209
	step [122/249], loss=12.1110
	step [123/249], loss=10.2276
	step [124/249], loss=12.6106
	step [125/249], loss=11.8271
	step [126/249], loss=10.8396
	step [127/249], loss=11.6027
	step [128/249], loss=10.2101
	step [129/249], loss=13.6123
	step [130/249], loss=12.1217
	step [131/249], loss=11.3018
	step [132/249], loss=12.1369
	step [133/249], loss=10.3122
	step [134/249], loss=11.6268
	step [135/249], loss=11.8673
	step [136/249], loss=9.5558
	step [137/249], loss=11.2829
	step [138/249], loss=11.3809
	step [139/249], loss=11.4487
	step [140/249], loss=10.3327
	step [141/249], loss=13.1733
	step [142/249], loss=10.8012
	step [143/249], loss=12.1561
	step [144/249], loss=10.9130
	step [145/249], loss=11.6121
	step [146/249], loss=13.1874
	step [147/249], loss=10.6309
	step [148/249], loss=11.3077
	step [149/249], loss=10.6090
	step [150/249], loss=12.0262
	step [151/249], loss=11.8116
	step [152/249], loss=13.8725
	step [153/249], loss=11.6269
	step [154/249], loss=11.8426
	step [155/249], loss=12.9431
	step [156/249], loss=12.0278
	step [157/249], loss=10.9438
	step [158/249], loss=11.5341
	step [159/249], loss=11.9954
	step [160/249], loss=10.5502
	step [161/249], loss=13.2709
	step [162/249], loss=9.7354
	step [163/249], loss=12.9335
	step [164/249], loss=11.1072
	step [165/249], loss=10.8074
	step [166/249], loss=10.7501
	step [167/249], loss=10.4448
	step [168/249], loss=11.2040
	step [169/249], loss=12.2596
	step [170/249], loss=11.6768
	step [171/249], loss=10.1378
	step [172/249], loss=10.7336
	step [173/249], loss=11.1339
	step [174/249], loss=13.0463
	step [175/249], loss=11.9825
	step [176/249], loss=9.3606
	step [177/249], loss=11.5661
	step [178/249], loss=11.3692
	step [179/249], loss=11.8480
	step [180/249], loss=11.8729
	step [181/249], loss=10.4854
	step [182/249], loss=11.5087
	step [183/249], loss=11.8372
	step [184/249], loss=13.2910
	step [185/249], loss=11.1390
	step [186/249], loss=11.5527
	step [187/249], loss=9.6919
	step [188/249], loss=12.4682
	step [189/249], loss=12.6311
	step [190/249], loss=13.4246
	step [191/249], loss=11.6144
	step [192/249], loss=11.4751
	step [193/249], loss=10.5968
	step [194/249], loss=9.8944
	step [195/249], loss=13.1022
	step [196/249], loss=12.0847
	step [197/249], loss=10.6021
	step [198/249], loss=9.2000
	step [199/249], loss=12.7964
	step [200/249], loss=12.5168
	step [201/249], loss=13.1819
	step [202/249], loss=10.7542
	step [203/249], loss=12.6936
	step [204/249], loss=12.8107
	step [205/249], loss=11.2602
	step [206/249], loss=12.1013
	step [207/249], loss=11.9743
	step [208/249], loss=10.6061
	step [209/249], loss=10.2603
	step [210/249], loss=12.3742
	step [211/249], loss=11.9503
	step [212/249], loss=10.3616
	step [213/249], loss=12.3383
	step [214/249], loss=11.4809
	step [215/249], loss=10.8406
	step [216/249], loss=10.9745
	step [217/249], loss=11.9776
	step [218/249], loss=10.9570
	step [219/249], loss=11.3457
	step [220/249], loss=10.8208
	step [221/249], loss=10.7603
	step [222/249], loss=11.7709
	step [223/249], loss=10.0685
	step [224/249], loss=12.9027
	step [225/249], loss=10.0467
	step [226/249], loss=10.3969
	step [227/249], loss=11.4234
	step [228/249], loss=11.4603
	step [229/249], loss=11.6785
	step [230/249], loss=13.2826
	step [231/249], loss=9.9490
	step [232/249], loss=11.6573
	step [233/249], loss=11.4235
	step [234/249], loss=11.0358
	step [235/249], loss=11.5066
	step [236/249], loss=11.3745
	step [237/249], loss=14.5120
	step [238/249], loss=10.0587
	step [239/249], loss=13.9443
	step [240/249], loss=9.6285
	step [241/249], loss=10.5720
	step [242/249], loss=11.5781
	step [243/249], loss=10.9931
	step [244/249], loss=9.6082
	step [245/249], loss=11.8238
	step [246/249], loss=9.8509
	step [247/249], loss=13.4834
	step [248/249], loss=10.7511
	step [249/249], loss=7.5164
	Evaluating
	loss=0.0368, precision=0.1340, recall=0.9968, f1=0.2363
Training epoch 10
	step [1/249], loss=9.7691
	step [2/249], loss=10.8316
	step [3/249], loss=10.6043
	step [4/249], loss=11.2758
	step [5/249], loss=10.6767
	step [6/249], loss=11.6677
	step [7/249], loss=10.7588
	step [8/249], loss=11.8634
	step [9/249], loss=12.9184
	step [10/249], loss=12.6955
	step [11/249], loss=9.5768
	step [12/249], loss=13.2129
	step [13/249], loss=11.2272
	step [14/249], loss=12.4642
	step [15/249], loss=11.0487
	step [16/249], loss=11.4474
	step [17/249], loss=11.1614
	step [18/249], loss=13.1836
	step [19/249], loss=13.1704
	step [20/249], loss=10.6611
	step [21/249], loss=9.8666
	step [22/249], loss=11.5451
	step [23/249], loss=9.9351
	step [24/249], loss=11.7196
	step [25/249], loss=13.4887
	step [26/249], loss=10.3700
	step [27/249], loss=10.7148
	step [28/249], loss=11.9384
	step [29/249], loss=11.2866
	step [30/249], loss=9.7559
	step [31/249], loss=12.5396
	step [32/249], loss=10.7832
	step [33/249], loss=12.2339
	step [34/249], loss=9.1736
	step [35/249], loss=10.2925
	step [36/249], loss=11.4479
	step [37/249], loss=9.4714
	step [38/249], loss=12.2194
	step [39/249], loss=9.2816
	step [40/249], loss=12.6961
	step [41/249], loss=11.4097
	step [42/249], loss=11.5257
	step [43/249], loss=10.8221
	step [44/249], loss=8.8961
	step [45/249], loss=10.0558
	step [46/249], loss=9.9851
	step [47/249], loss=11.8814
	step [48/249], loss=11.7835
	step [49/249], loss=9.5993
	step [50/249], loss=11.7637
	step [51/249], loss=12.0232
	step [52/249], loss=11.5485
	step [53/249], loss=11.1550
	step [54/249], loss=10.5866
	step [55/249], loss=10.4187
	step [56/249], loss=10.9751
	step [57/249], loss=9.4695
	step [58/249], loss=10.5991
	step [59/249], loss=11.7051
	step [60/249], loss=11.0166
	step [61/249], loss=11.6963
	step [62/249], loss=10.5459
	step [63/249], loss=10.0514
	step [64/249], loss=10.2670
	step [65/249], loss=10.8286
	step [66/249], loss=11.2324
	step [67/249], loss=11.8528
	step [68/249], loss=10.0199
	step [69/249], loss=9.3596
	step [70/249], loss=12.3952
	step [71/249], loss=11.1039
	step [72/249], loss=13.9775
	step [73/249], loss=10.6616
	step [74/249], loss=10.4018
	step [75/249], loss=9.9973
	step [76/249], loss=10.0258
	step [77/249], loss=10.4212
	step [78/249], loss=11.0477
	step [79/249], loss=9.6203
	step [80/249], loss=11.0209
	step [81/249], loss=10.2151
	step [82/249], loss=10.8456
	step [83/249], loss=9.7584
	step [84/249], loss=10.7625
	step [85/249], loss=10.5693
	step [86/249], loss=10.8397
	step [87/249], loss=11.5874
	step [88/249], loss=11.5818
	step [89/249], loss=9.5229
	step [90/249], loss=12.5976
	step [91/249], loss=11.5737
	step [92/249], loss=9.7794
	step [93/249], loss=12.1766
	step [94/249], loss=9.8277
	step [95/249], loss=9.5452
	step [96/249], loss=12.3065
	step [97/249], loss=8.2207
	step [98/249], loss=12.1395
	step [99/249], loss=10.2004
	step [100/249], loss=11.8246
	step [101/249], loss=11.2280
	step [102/249], loss=12.8525
	step [103/249], loss=11.0672
	step [104/249], loss=10.5527
	step [105/249], loss=10.9668
	step [106/249], loss=11.6968
	step [107/249], loss=10.9878
	step [108/249], loss=10.0544
	step [109/249], loss=10.8460
	step [110/249], loss=10.2800
	step [111/249], loss=10.8292
	step [112/249], loss=10.8014
	step [113/249], loss=11.3481
	step [114/249], loss=11.6461
	step [115/249], loss=10.9066
	step [116/249], loss=10.3375
	step [117/249], loss=12.6386
	step [118/249], loss=10.6724
	step [119/249], loss=10.9464
	step [120/249], loss=11.2313
	step [121/249], loss=10.7353
	step [122/249], loss=12.8466
	step [123/249], loss=9.9794
	step [124/249], loss=10.7023
	step [125/249], loss=9.3487
	step [126/249], loss=11.1314
	step [127/249], loss=11.5472
	step [128/249], loss=9.6670
	step [129/249], loss=10.8689
	step [130/249], loss=9.6932
	step [131/249], loss=10.7438
	step [132/249], loss=13.4926
	step [133/249], loss=10.8414
	step [134/249], loss=12.1864
	step [135/249], loss=10.6813
	step [136/249], loss=10.6151
	step [137/249], loss=10.9356
	step [138/249], loss=10.4446
	step [139/249], loss=12.8830
	step [140/249], loss=9.8808
	step [141/249], loss=9.7507
	step [142/249], loss=11.3725
	step [143/249], loss=10.6710
	step [144/249], loss=11.3790
	step [145/249], loss=9.8411
	step [146/249], loss=10.5752
	step [147/249], loss=10.8053
	step [148/249], loss=12.9183
	step [149/249], loss=9.8945
	step [150/249], loss=10.5009
	step [151/249], loss=10.7503
	step [152/249], loss=11.9271
	step [153/249], loss=11.6082
	step [154/249], loss=9.6469
	step [155/249], loss=12.5257
	step [156/249], loss=10.6425
	step [157/249], loss=10.6838
	step [158/249], loss=9.9121
	step [159/249], loss=9.5323
	step [160/249], loss=11.4127
	step [161/249], loss=10.3087
	step [162/249], loss=10.6737
	step [163/249], loss=10.1014
	step [164/249], loss=10.3884
	step [165/249], loss=9.1939
	step [166/249], loss=10.9796
	step [167/249], loss=11.3203
	step [168/249], loss=10.2853
	step [169/249], loss=9.9410
	step [170/249], loss=10.6138
	step [171/249], loss=11.6480
	step [172/249], loss=11.7586
	step [173/249], loss=10.4396
	step [174/249], loss=11.8255
	step [175/249], loss=10.4306
	step [176/249], loss=9.6968
	step [177/249], loss=11.5264
	step [178/249], loss=11.3399
	step [179/249], loss=9.8439
	step [180/249], loss=9.5142
	step [181/249], loss=10.0309
	step [182/249], loss=11.0028
	step [183/249], loss=10.1850
	step [184/249], loss=10.6286
	step [185/249], loss=11.8233
	step [186/249], loss=10.3361
	step [187/249], loss=9.4339
	step [188/249], loss=11.2951
	step [189/249], loss=10.3072
	step [190/249], loss=10.5991
	step [191/249], loss=12.1715
	step [192/249], loss=10.6570
	step [193/249], loss=9.0935
	step [194/249], loss=11.7741
	step [195/249], loss=10.5363
	step [196/249], loss=12.0910
	step [197/249], loss=9.5802
	step [198/249], loss=9.6038
	step [199/249], loss=8.7688
	step [200/249], loss=10.0892
	step [201/249], loss=11.0550
	step [202/249], loss=10.2758
	step [203/249], loss=12.0757
	step [204/249], loss=8.5178
	step [205/249], loss=10.2211
	step [206/249], loss=12.2443
	step [207/249], loss=9.8538
	step [208/249], loss=10.7913
	step [209/249], loss=10.3426
	step [210/249], loss=10.5909
	step [211/249], loss=9.7761
	step [212/249], loss=9.6361
	step [213/249], loss=10.3173
	step [214/249], loss=10.4856
	step [215/249], loss=9.4532
	step [216/249], loss=9.4708
	step [217/249], loss=9.5001
	step [218/249], loss=10.6183
	step [219/249], loss=10.3524
	step [220/249], loss=11.5627
	step [221/249], loss=10.6724
	step [222/249], loss=9.5356
	step [223/249], loss=9.4579
	step [224/249], loss=10.5990
	step [225/249], loss=9.4782
	step [226/249], loss=11.1859
	step [227/249], loss=8.9208
	step [228/249], loss=11.0495
	step [229/249], loss=8.9207
	step [230/249], loss=9.2910
	step [231/249], loss=10.2308
	step [232/249], loss=10.0498
	step [233/249], loss=9.5832
	step [234/249], loss=13.0087
	step [235/249], loss=9.4788
	step [236/249], loss=8.6865
	step [237/249], loss=8.7708
	step [238/249], loss=11.2041
	step [239/249], loss=10.1299
	step [240/249], loss=9.3426
	step [241/249], loss=11.0415
	step [242/249], loss=10.3157
	step [243/249], loss=10.0732
	step [244/249], loss=10.1375
	step [245/249], loss=10.9634
	step [246/249], loss=9.3424
	step [247/249], loss=10.9914
	step [248/249], loss=10.1793
	step [249/249], loss=6.9044
	Evaluating
	loss=0.0334, precision=0.1368, recall=0.9967, f1=0.2406
Training epoch 11
	step [1/249], loss=10.3430
	step [2/249], loss=8.7423
	step [3/249], loss=10.3794
	step [4/249], loss=9.5468
	step [5/249], loss=10.0731
	step [6/249], loss=10.6204
	step [7/249], loss=8.5540
	step [8/249], loss=8.3237
	step [9/249], loss=11.9869
	step [10/249], loss=11.1049
	step [11/249], loss=8.9434
	step [12/249], loss=10.6774
	step [13/249], loss=8.9927
	step [14/249], loss=9.6567
	step [15/249], loss=10.3482
	step [16/249], loss=11.4935
	step [17/249], loss=9.5098
	step [18/249], loss=11.6200
	step [19/249], loss=10.7315
	step [20/249], loss=9.8040
	step [21/249], loss=10.9664
	step [22/249], loss=10.2454
	step [23/249], loss=9.0827
	step [24/249], loss=9.8875
	step [25/249], loss=10.4915
	step [26/249], loss=10.3220
	step [27/249], loss=10.7021
	step [28/249], loss=10.6164
	step [29/249], loss=10.1429
	step [30/249], loss=10.3884
	step [31/249], loss=10.8807
	step [32/249], loss=10.1047
	step [33/249], loss=11.7511
	step [34/249], loss=9.9069
	step [35/249], loss=9.2341
	step [36/249], loss=10.1307
	step [37/249], loss=11.4385
	step [38/249], loss=9.7501
	step [39/249], loss=11.2516
	step [40/249], loss=11.3341
	step [41/249], loss=10.8538
	step [42/249], loss=10.8085
	step [43/249], loss=10.2365
	step [44/249], loss=10.5462
	step [45/249], loss=10.0071
	step [46/249], loss=11.6198
	step [47/249], loss=9.6846
	step [48/249], loss=9.9846
	step [49/249], loss=9.5357
	step [50/249], loss=10.0697
	step [51/249], loss=10.3117
	step [52/249], loss=11.4463
	step [53/249], loss=9.5334
	step [54/249], loss=8.9028
	step [55/249], loss=12.1323
	step [56/249], loss=11.3057
	step [57/249], loss=9.8412
	step [58/249], loss=11.6820
	step [59/249], loss=9.3225
	step [60/249], loss=8.8769
	step [61/249], loss=9.0792
	step [62/249], loss=8.8711
	step [63/249], loss=10.5402
	step [64/249], loss=9.7692
	step [65/249], loss=10.8575
	step [66/249], loss=7.8527
	step [67/249], loss=11.9752
	step [68/249], loss=9.6790
	step [69/249], loss=10.5751
	step [70/249], loss=9.2397
	step [71/249], loss=11.3366
	step [72/249], loss=9.1956
	step [73/249], loss=9.1026
	step [74/249], loss=10.1468
	step [75/249], loss=11.7604
	step [76/249], loss=9.3943
	step [77/249], loss=9.6335
	step [78/249], loss=9.8640
	step [79/249], loss=10.1699
	step [80/249], loss=11.4422
	step [81/249], loss=10.6713
	step [82/249], loss=9.5939
	step [83/249], loss=11.0759
	step [84/249], loss=8.6517
	step [85/249], loss=10.6564
	step [86/249], loss=10.1422
	step [87/249], loss=10.0574
	step [88/249], loss=11.6921
	step [89/249], loss=10.4184
	step [90/249], loss=10.3319
	step [91/249], loss=10.6321
	step [92/249], loss=10.2047
	step [93/249], loss=11.0072
	step [94/249], loss=10.5173
	step [95/249], loss=10.9260
	step [96/249], loss=8.7404
	step [97/249], loss=8.7980
	step [98/249], loss=9.9456
	step [99/249], loss=9.5357
	step [100/249], loss=9.6031
	step [101/249], loss=12.7489
	step [102/249], loss=10.4331
	step [103/249], loss=8.5816
	step [104/249], loss=12.8840
	step [105/249], loss=9.2499
	step [106/249], loss=10.0619
	step [107/249], loss=11.6875
	step [108/249], loss=9.1088
	step [109/249], loss=11.3585
	step [110/249], loss=9.4916
	step [111/249], loss=8.7951
	step [112/249], loss=8.9990
	step [113/249], loss=9.1541
	step [114/249], loss=10.3846
	step [115/249], loss=9.4833
	step [116/249], loss=10.4038
	step [117/249], loss=9.1902
	step [118/249], loss=11.7829
	step [119/249], loss=9.4823
	step [120/249], loss=11.5971
	step [121/249], loss=9.0229
	step [122/249], loss=9.8690
	step [123/249], loss=10.2426
	step [124/249], loss=9.8801
	step [125/249], loss=9.3375
	step [126/249], loss=10.5280
	step [127/249], loss=10.2956
	step [128/249], loss=8.3993
	step [129/249], loss=8.5280
	step [130/249], loss=9.7298
	step [131/249], loss=9.3283
	step [132/249], loss=11.8683
	step [133/249], loss=10.0469
	step [134/249], loss=10.8317
	step [135/249], loss=9.4655
	step [136/249], loss=9.9080
	step [137/249], loss=8.9358
	step [138/249], loss=11.2345
	step [139/249], loss=9.6911
	step [140/249], loss=10.9714
	step [141/249], loss=11.6023
	step [142/249], loss=10.1561
	step [143/249], loss=9.7441
	step [144/249], loss=10.6569
	step [145/249], loss=10.7958
	step [146/249], loss=9.8346
	step [147/249], loss=8.6236
	step [148/249], loss=10.2780
	step [149/249], loss=10.7038
	step [150/249], loss=9.0393
	step [151/249], loss=8.7276
	step [152/249], loss=9.6398
	step [153/249], loss=9.0921
	step [154/249], loss=9.0291
	step [155/249], loss=10.1112
	step [156/249], loss=12.8231
	step [157/249], loss=8.8773
	step [158/249], loss=10.5077
	step [159/249], loss=9.2785
	step [160/249], loss=10.1731
	step [161/249], loss=10.8511
	step [162/249], loss=10.2363
	step [163/249], loss=9.4495
	step [164/249], loss=9.1213
	step [165/249], loss=10.0266
	step [166/249], loss=7.5999
	step [167/249], loss=10.6228
	step [168/249], loss=9.7484
	step [169/249], loss=8.9961
	step [170/249], loss=12.4408
	step [171/249], loss=10.9803
	step [172/249], loss=10.4586
	step [173/249], loss=9.2720
	step [174/249], loss=9.1524
	step [175/249], loss=11.5870
	step [176/249], loss=8.9721
	step [177/249], loss=10.7756
	step [178/249], loss=9.5089
	step [179/249], loss=10.2076
	step [180/249], loss=9.9789
	step [181/249], loss=11.5739
	step [182/249], loss=10.8213
	step [183/249], loss=9.7987
	step [184/249], loss=9.2041
	step [185/249], loss=10.0648
	step [186/249], loss=9.6172
	step [187/249], loss=8.7583
	step [188/249], loss=8.9696
	step [189/249], loss=7.6474
	step [190/249], loss=10.7707
	step [191/249], loss=9.3153
	step [192/249], loss=9.0614
	step [193/249], loss=9.7114
	step [194/249], loss=10.1387
	step [195/249], loss=9.8052
	step [196/249], loss=8.9117
	step [197/249], loss=10.7234
	step [198/249], loss=9.7045
	step [199/249], loss=11.2673
	step [200/249], loss=11.2915
	step [201/249], loss=8.3064
	step [202/249], loss=9.1562
	step [203/249], loss=10.9769
	step [204/249], loss=8.8262
	step [205/249], loss=9.8895
	step [206/249], loss=12.6329
	step [207/249], loss=10.1382
	step [208/249], loss=10.6324
	step [209/249], loss=8.4897
	step [210/249], loss=7.6660
	step [211/249], loss=10.5350
	step [212/249], loss=9.6463
	step [213/249], loss=8.2479
	step [214/249], loss=9.6053
	step [215/249], loss=9.9834
	step [216/249], loss=8.9319
	step [217/249], loss=8.6743
	step [218/249], loss=10.6764
	step [219/249], loss=9.3168
	step [220/249], loss=10.7807
	step [221/249], loss=9.5057
	step [222/249], loss=10.1963
	step [223/249], loss=8.5374
	step [224/249], loss=8.8937
	step [225/249], loss=9.5439
	step [226/249], loss=9.2300
	step [227/249], loss=8.5499
	step [228/249], loss=10.5140
	step [229/249], loss=11.3529
	step [230/249], loss=8.5995
	step [231/249], loss=8.8389
	step [232/249], loss=8.2324
	step [233/249], loss=10.4454
	step [234/249], loss=11.1418
	step [235/249], loss=11.5916
	step [236/249], loss=9.2276
	step [237/249], loss=9.6096
	step [238/249], loss=11.1056
	step [239/249], loss=10.1767
	step [240/249], loss=9.0196
	step [241/249], loss=9.1942
	step [242/249], loss=10.0251
	step [243/249], loss=10.0884
	step [244/249], loss=9.6805
	step [245/249], loss=9.4664
	step [246/249], loss=9.0627
	step [247/249], loss=9.6647
	step [248/249], loss=8.6814
	step [249/249], loss=7.0069
	Evaluating
	loss=0.0298, precision=0.1438, recall=0.9966, f1=0.2513
Training epoch 12
	step [1/249], loss=11.3019
	step [2/249], loss=10.3728
	step [3/249], loss=8.0825
	step [4/249], loss=9.7008
	step [5/249], loss=10.4058
	step [6/249], loss=8.8643
	step [7/249], loss=9.2381
	step [8/249], loss=10.2195
	step [9/249], loss=9.8121
	step [10/249], loss=9.8274
	step [11/249], loss=10.9757
	step [12/249], loss=9.0784
	step [13/249], loss=8.8783
	step [14/249], loss=10.7087
	step [15/249], loss=9.6492
	step [16/249], loss=9.4301
	step [17/249], loss=8.9707
	step [18/249], loss=9.5408
	step [19/249], loss=10.8543
	step [20/249], loss=9.3460
	step [21/249], loss=9.1340
	step [22/249], loss=11.1289
	step [23/249], loss=10.1475
	step [24/249], loss=10.9273
	step [25/249], loss=11.4277
	step [26/249], loss=9.2752
	step [27/249], loss=9.6034
	step [28/249], loss=9.2260
	step [29/249], loss=8.7738
	step [30/249], loss=9.6219
	step [31/249], loss=8.9863
	step [32/249], loss=8.9065
	step [33/249], loss=9.2091
	step [34/249], loss=8.9208
	step [35/249], loss=9.4304
	step [36/249], loss=9.0568
	step [37/249], loss=10.1181
	step [38/249], loss=10.2730
	step [39/249], loss=9.1562
	step [40/249], loss=10.0712
	step [41/249], loss=9.5904
	step [42/249], loss=7.2514
	step [43/249], loss=9.5274
	step [44/249], loss=9.7913
	step [45/249], loss=10.6393
	step [46/249], loss=8.3201
	step [47/249], loss=8.7318
	step [48/249], loss=7.9273
	step [49/249], loss=8.7492
	step [50/249], loss=11.0123
	step [51/249], loss=10.4315
	step [52/249], loss=8.0556
	step [53/249], loss=9.5716
	step [54/249], loss=8.1229
	step [55/249], loss=9.3901
	step [56/249], loss=9.8621
	step [57/249], loss=9.7876
	step [58/249], loss=9.4457
	step [59/249], loss=9.3863
	step [60/249], loss=9.0319
	step [61/249], loss=10.8847
	step [62/249], loss=8.7675
	step [63/249], loss=9.1221
	step [64/249], loss=8.5642
	step [65/249], loss=8.8646
	step [66/249], loss=8.1953
	step [67/249], loss=8.9661
	step [68/249], loss=9.6988
	step [69/249], loss=10.8177
	step [70/249], loss=9.2558
	step [71/249], loss=10.4140
	step [72/249], loss=9.2151
	step [73/249], loss=10.0221
	step [74/249], loss=7.7979
	step [75/249], loss=10.7281
	step [76/249], loss=8.8328
	step [77/249], loss=8.0543
	step [78/249], loss=9.6462
	step [79/249], loss=10.4999
	step [80/249], loss=8.8931
	step [81/249], loss=8.7203
	step [82/249], loss=8.7183
	step [83/249], loss=10.2446
	step [84/249], loss=8.8751
	step [85/249], loss=9.3827
	step [86/249], loss=8.9151
	step [87/249], loss=9.2774
	step [88/249], loss=11.8334
	step [89/249], loss=10.5790
	step [90/249], loss=8.9848
	step [91/249], loss=11.7185
	step [92/249], loss=9.3612
	step [93/249], loss=9.0385
	step [94/249], loss=8.7044
	step [95/249], loss=9.8252
	step [96/249], loss=9.3630
	step [97/249], loss=8.9978
	step [98/249], loss=7.6750
	step [99/249], loss=11.4049
	step [100/249], loss=8.4295
	step [101/249], loss=8.5076
	step [102/249], loss=9.6538
	step [103/249], loss=9.1029
	step [104/249], loss=9.3418
	step [105/249], loss=10.5977
	step [106/249], loss=10.4996
	step [107/249], loss=9.4652
	step [108/249], loss=9.6049
	step [109/249], loss=10.9414
	step [110/249], loss=8.3689
	step [111/249], loss=7.4407
	step [112/249], loss=9.0831
	step [113/249], loss=10.2503
	step [114/249], loss=10.2623
	step [115/249], loss=11.2843
	step [116/249], loss=8.4029
	step [117/249], loss=9.5105
	step [118/249], loss=8.4287
	step [119/249], loss=9.2134
	step [120/249], loss=9.3279
	step [121/249], loss=8.8428
	step [122/249], loss=8.6462
	step [123/249], loss=8.0103
	step [124/249], loss=11.1786
	step [125/249], loss=10.1671
	step [126/249], loss=10.6961
	step [127/249], loss=9.3010
	step [128/249], loss=10.2904
	step [129/249], loss=9.4385
	step [130/249], loss=10.0320
	step [131/249], loss=8.6107
	step [132/249], loss=9.9133
	step [133/249], loss=10.7736
	step [134/249], loss=9.8372
	step [135/249], loss=11.0271
	step [136/249], loss=11.1899
	step [137/249], loss=8.7222
	step [138/249], loss=7.7441
	step [139/249], loss=10.0201
	step [140/249], loss=8.3194
	step [141/249], loss=9.7089
	step [142/249], loss=12.7370
	step [143/249], loss=9.9722
	step [144/249], loss=9.0818
	step [145/249], loss=9.2294
	step [146/249], loss=9.1972
	step [147/249], loss=10.1670
	step [148/249], loss=8.6504
	step [149/249], loss=8.9896
	step [150/249], loss=10.4255
	step [151/249], loss=9.7913
	step [152/249], loss=8.2421
	step [153/249], loss=9.3126
	step [154/249], loss=7.6966
	step [155/249], loss=10.1611
	step [156/249], loss=8.6169
	step [157/249], loss=9.1233
	step [158/249], loss=10.5919
	step [159/249], loss=10.5913
	step [160/249], loss=10.0292
	step [161/249], loss=9.9063
	step [162/249], loss=9.1704
	step [163/249], loss=9.3042
	step [164/249], loss=9.4043
	step [165/249], loss=10.8040
	step [166/249], loss=8.8024
	step [167/249], loss=9.1110
	step [168/249], loss=9.2652
	step [169/249], loss=11.3615
	step [170/249], loss=8.7359
	step [171/249], loss=8.4974
	step [172/249], loss=8.4413
	step [173/249], loss=9.1811
	step [174/249], loss=8.8132
	step [175/249], loss=9.0061
	step [176/249], loss=9.7807
	step [177/249], loss=9.0106
	step [178/249], loss=8.8273
	step [179/249], loss=8.8989
	step [180/249], loss=10.3908
	step [181/249], loss=11.6319
	step [182/249], loss=9.7766
	step [183/249], loss=9.2107
	step [184/249], loss=10.3784
	step [185/249], loss=9.1686
	step [186/249], loss=8.6778
	step [187/249], loss=8.9314
	step [188/249], loss=9.2103
	step [189/249], loss=9.4745
	step [190/249], loss=9.0746
	step [191/249], loss=9.6134
	step [192/249], loss=10.0915
	step [193/249], loss=9.3392
	step [194/249], loss=8.6527
	step [195/249], loss=7.8427
	step [196/249], loss=11.2466
	step [197/249], loss=7.9298
	step [198/249], loss=9.3721
	step [199/249], loss=9.7319
	step [200/249], loss=9.1872
	step [201/249], loss=8.5994
	step [202/249], loss=9.1704
	step [203/249], loss=10.3825
	step [204/249], loss=7.5503
	step [205/249], loss=10.4000
	step [206/249], loss=8.8796
	step [207/249], loss=8.9953
	step [208/249], loss=9.5083
	step [209/249], loss=8.7350
	step [210/249], loss=9.1867
	step [211/249], loss=8.4970
	step [212/249], loss=10.3074
	step [213/249], loss=9.1116
	step [214/249], loss=9.2689
	step [215/249], loss=8.6333
	step [216/249], loss=9.1533
	step [217/249], loss=10.5428
	step [218/249], loss=7.7367
	step [219/249], loss=10.2079
	step [220/249], loss=8.9001
	step [221/249], loss=8.7551
	step [222/249], loss=8.9532
	step [223/249], loss=7.2004
	step [224/249], loss=9.1086
	step [225/249], loss=7.7798
	step [226/249], loss=11.6434
	step [227/249], loss=9.1972
	step [228/249], loss=9.0351
	step [229/249], loss=7.8067
	step [230/249], loss=10.6185
	step [231/249], loss=8.7640
	step [232/249], loss=9.1077
	step [233/249], loss=9.4616
	step [234/249], loss=10.2455
	step [235/249], loss=8.6514
	step [236/249], loss=9.2447
	step [237/249], loss=8.3749
	step [238/249], loss=9.0162
	step [239/249], loss=9.0969
	step [240/249], loss=7.9633
	step [241/249], loss=9.3161
	step [242/249], loss=10.3614
	step [243/249], loss=8.2155
	step [244/249], loss=9.1559
	step [245/249], loss=9.8554
	step [246/249], loss=10.1723
	step [247/249], loss=10.0385
	step [248/249], loss=8.8838
	step [249/249], loss=5.6077
	Evaluating
	loss=0.0255, precision=0.1665, recall=0.9950, f1=0.2853
saving model as: 0_saved_model.pth
Training epoch 13
	step [1/249], loss=10.0619
	step [2/249], loss=8.8372
	step [3/249], loss=6.6701
	step [4/249], loss=11.1838
	step [5/249], loss=8.9525
	step [6/249], loss=8.0453
	step [7/249], loss=8.0554
	step [8/249], loss=9.6114
	step [9/249], loss=7.6941
	step [10/249], loss=9.8880
	step [11/249], loss=8.4800
	step [12/249], loss=8.4566
	step [13/249], loss=9.7771
	step [14/249], loss=8.6864
	step [15/249], loss=8.2709
	step [16/249], loss=9.9483
	step [17/249], loss=8.0140
	step [18/249], loss=8.0873
	step [19/249], loss=9.6593
	step [20/249], loss=9.4120
	step [21/249], loss=7.9897
	step [22/249], loss=9.3456
	step [23/249], loss=8.6686
	step [24/249], loss=9.5076
	step [25/249], loss=12.0624
	step [26/249], loss=10.0330
	step [27/249], loss=8.5762
	step [28/249], loss=8.2821
	step [29/249], loss=8.3244
	step [30/249], loss=8.6468
	step [31/249], loss=10.3795
	step [32/249], loss=9.5890
	step [33/249], loss=9.6048
	step [34/249], loss=8.6246
	step [35/249], loss=7.2038
	step [36/249], loss=7.6567
	step [37/249], loss=8.6031
	step [38/249], loss=9.0250
	step [39/249], loss=9.8079
	step [40/249], loss=8.1180
	step [41/249], loss=9.1706
	step [42/249], loss=9.8109
	step [43/249], loss=8.5937
	step [44/249], loss=10.1011
	step [45/249], loss=8.4728
	step [46/249], loss=8.3345
	step [47/249], loss=11.4278
	step [48/249], loss=8.6253
	step [49/249], loss=10.5785
	step [50/249], loss=10.8629
	step [51/249], loss=9.5328
	step [52/249], loss=9.7606
	step [53/249], loss=8.9837
	step [54/249], loss=8.6900
	step [55/249], loss=7.5420
	step [56/249], loss=8.6663
	step [57/249], loss=9.0687
	step [58/249], loss=7.2683
	step [59/249], loss=9.5961
	step [60/249], loss=10.0415
	step [61/249], loss=7.8662
	step [62/249], loss=10.0605
	step [63/249], loss=11.2194
	step [64/249], loss=7.4961
	step [65/249], loss=8.0817
	step [66/249], loss=8.2921
	step [67/249], loss=8.0302
	step [68/249], loss=7.3477
	step [69/249], loss=7.7964
	step [70/249], loss=10.5949
	step [71/249], loss=9.3628
	step [72/249], loss=7.9778
	step [73/249], loss=8.5884
	step [74/249], loss=8.0312
	step [75/249], loss=7.8085
	step [76/249], loss=8.9651
	step [77/249], loss=9.6410
	step [78/249], loss=8.6353
	step [79/249], loss=8.0994
	step [80/249], loss=8.7641
	step [81/249], loss=8.3897
	step [82/249], loss=9.3758
	step [83/249], loss=7.9469
	step [84/249], loss=8.7079
	step [85/249], loss=10.2608
	step [86/249], loss=9.4336
	step [87/249], loss=8.8827
	step [88/249], loss=8.9083
	step [89/249], loss=10.0601
	step [90/249], loss=11.4218
	step [91/249], loss=9.9724
	step [92/249], loss=9.0160
	step [93/249], loss=8.6506
	step [94/249], loss=6.3185
	step [95/249], loss=9.3008
	step [96/249], loss=8.2749
	step [97/249], loss=9.6215
	step [98/249], loss=9.6126
	step [99/249], loss=9.1835
	step [100/249], loss=9.0413
	step [101/249], loss=7.9855
	step [102/249], loss=10.1303
	step [103/249], loss=8.5162
	step [104/249], loss=7.4925
	step [105/249], loss=9.6728
	step [106/249], loss=7.5368
	step [107/249], loss=9.7412
	step [108/249], loss=7.8899
	step [109/249], loss=7.7731
	step [110/249], loss=8.9757
	step [111/249], loss=8.4351
	step [112/249], loss=8.4037
	step [113/249], loss=7.4798
	step [114/249], loss=10.5286
	step [115/249], loss=9.0269
	step [116/249], loss=9.0335
	step [117/249], loss=9.6299
	step [118/249], loss=9.5540
	step [119/249], loss=9.9570
	step [120/249], loss=8.0707
	step [121/249], loss=8.8630
	step [122/249], loss=9.2747
	step [123/249], loss=9.0097
	step [124/249], loss=10.8052
	step [125/249], loss=9.7208
	step [126/249], loss=10.4111
	step [127/249], loss=8.0729
	step [128/249], loss=8.3431
	step [129/249], loss=10.8636
	step [130/249], loss=8.8313
	step [131/249], loss=9.2704
	step [132/249], loss=8.8543
	step [133/249], loss=10.6259
	step [134/249], loss=10.3290
	step [135/249], loss=8.5402
	step [136/249], loss=7.8002
	step [137/249], loss=8.0020
	step [138/249], loss=8.4556
	step [139/249], loss=8.3422
	step [140/249], loss=8.6779
	step [141/249], loss=9.3153
	step [142/249], loss=8.3889
	step [143/249], loss=8.2938
	step [144/249], loss=9.2629
	step [145/249], loss=8.0174
	step [146/249], loss=9.6961
	step [147/249], loss=8.1701
	step [148/249], loss=7.9275
	step [149/249], loss=8.4411
	step [150/249], loss=9.4256
	step [151/249], loss=10.5890
	step [152/249], loss=8.2033
	step [153/249], loss=6.9295
	step [154/249], loss=7.8543
	step [155/249], loss=9.9717
	step [156/249], loss=7.9843
	step [157/249], loss=7.8412
	step [158/249], loss=7.8894
	step [159/249], loss=8.4635
	step [160/249], loss=9.5842
	step [161/249], loss=10.8900
	step [162/249], loss=9.1557
	step [163/249], loss=9.1747
	step [164/249], loss=9.5835
	step [165/249], loss=8.1278
	step [166/249], loss=9.1523
	step [167/249], loss=7.5800
	step [168/249], loss=9.8817
	step [169/249], loss=7.7973
	step [170/249], loss=8.7205
	step [171/249], loss=9.3691
	step [172/249], loss=8.5374
	step [173/249], loss=8.4104
	step [174/249], loss=9.4117
	step [175/249], loss=8.1815
	step [176/249], loss=9.0367
	step [177/249], loss=7.2925
	step [178/249], loss=9.4736
	step [179/249], loss=9.0040
	step [180/249], loss=8.7258
	step [181/249], loss=8.6821
	step [182/249], loss=9.2685
	step [183/249], loss=9.4510
	step [184/249], loss=8.8287
	step [185/249], loss=10.5731
	step [186/249], loss=8.0131
	step [187/249], loss=7.8631
	step [188/249], loss=8.5777
	step [189/249], loss=8.5209
	step [190/249], loss=7.1939
	step [191/249], loss=8.5626
	step [192/249], loss=9.9182
	step [193/249], loss=8.4788
	step [194/249], loss=8.3414
	step [195/249], loss=7.1912
	step [196/249], loss=10.8510
	step [197/249], loss=8.9740
	step [198/249], loss=7.9629
	step [199/249], loss=7.5671
	step [200/249], loss=10.0400
	step [201/249], loss=9.2309
	step [202/249], loss=7.4220
	step [203/249], loss=7.5140
	step [204/249], loss=7.8834
	step [205/249], loss=10.1537
	step [206/249], loss=10.1947
	step [207/249], loss=7.2606
	step [208/249], loss=8.0676
	step [209/249], loss=8.4276
	step [210/249], loss=8.4648
	step [211/249], loss=8.2745
	step [212/249], loss=8.7915
	step [213/249], loss=8.5412
	step [214/249], loss=7.8223
	step [215/249], loss=8.6890
	step [216/249], loss=10.2818
	step [217/249], loss=9.8499
	step [218/249], loss=8.9468
	step [219/249], loss=7.8893
	step [220/249], loss=7.7053
	step [221/249], loss=7.9477
	step [222/249], loss=10.5057
	step [223/249], loss=7.7679
	step [224/249], loss=7.0498
	step [225/249], loss=9.7928
	step [226/249], loss=9.3335
	step [227/249], loss=8.3493
	step [228/249], loss=7.0218
	step [229/249], loss=8.9826
	step [230/249], loss=9.3629
	step [231/249], loss=8.7480
	step [232/249], loss=7.7832
	step [233/249], loss=8.5469
	step [234/249], loss=8.7140
	step [235/249], loss=10.6638
	step [236/249], loss=9.0726
	step [237/249], loss=8.3893
	step [238/249], loss=8.8405
	step [239/249], loss=9.1707
	step [240/249], loss=10.2978
	step [241/249], loss=9.0562
	step [242/249], loss=7.9874
	step [243/249], loss=9.3509
	step [244/249], loss=9.0860
	step [245/249], loss=8.6476
	step [246/249], loss=9.8816
	step [247/249], loss=8.1811
	step [248/249], loss=7.8374
	step [249/249], loss=5.9923
	Evaluating
	loss=0.0267, precision=0.1511, recall=0.9963, f1=0.2625
Training epoch 14
	step [1/249], loss=8.3906
	step [2/249], loss=8.5093
	step [3/249], loss=7.6240
	step [4/249], loss=9.1672
	step [5/249], loss=8.1769
	step [6/249], loss=8.4605
	step [7/249], loss=8.5923
	step [8/249], loss=7.7853
	step [9/249], loss=8.7918
	step [10/249], loss=9.2052
	step [11/249], loss=8.5287
	step [12/249], loss=9.5021
	step [13/249], loss=7.8463
	step [14/249], loss=10.7181
	step [15/249], loss=7.7269
	step [16/249], loss=9.5054
	step [17/249], loss=8.2864
	step [18/249], loss=7.6678
	step [19/249], loss=9.2834
	step [20/249], loss=7.9769
	step [21/249], loss=7.4031
	step [22/249], loss=8.2149
	step [23/249], loss=8.2208
	step [24/249], loss=7.4195
	step [25/249], loss=8.2275
	step [26/249], loss=7.4261
	step [27/249], loss=9.3792
	step [28/249], loss=8.7829
	step [29/249], loss=9.0082
	step [30/249], loss=8.4188
	step [31/249], loss=9.3564
	step [32/249], loss=8.0817
	step [33/249], loss=10.0149
	step [34/249], loss=9.0425
	step [35/249], loss=9.0383
	step [36/249], loss=12.0416
	step [37/249], loss=8.8595
	step [38/249], loss=9.2189
	step [39/249], loss=8.4291
	step [40/249], loss=8.7931
	step [41/249], loss=9.1669
	step [42/249], loss=9.1663
	step [43/249], loss=7.6281
	step [44/249], loss=8.1995
	step [45/249], loss=7.9825
	step [46/249], loss=7.1546
	step [47/249], loss=6.7517
	step [48/249], loss=10.1436
	step [49/249], loss=8.5723
	step [50/249], loss=9.0071
	step [51/249], loss=7.3467
	step [52/249], loss=9.0673
	step [53/249], loss=9.1094
	step [54/249], loss=7.6994
	step [55/249], loss=9.0805
	step [56/249], loss=9.8729
	step [57/249], loss=8.6146
	step [58/249], loss=9.6899
	step [59/249], loss=8.6395
	step [60/249], loss=8.0564
	step [61/249], loss=7.7820
	step [62/249], loss=10.2780
	step [63/249], loss=7.4802
	step [64/249], loss=7.9685
	step [65/249], loss=7.9204
	step [66/249], loss=8.1404
	step [67/249], loss=9.2890
	step [68/249], loss=7.1465
	step [69/249], loss=6.7867
	step [70/249], loss=8.8586
	step [71/249], loss=8.9300
	step [72/249], loss=9.6209
	step [73/249], loss=8.8111
	step [74/249], loss=7.7561
	step [75/249], loss=7.1209
	step [76/249], loss=7.1000
	step [77/249], loss=9.5841
	step [78/249], loss=8.0655
	step [79/249], loss=8.6351
	step [80/249], loss=7.7849
	step [81/249], loss=8.0926
	step [82/249], loss=7.9111
	step [83/249], loss=8.0913
	step [84/249], loss=8.4254
	step [85/249], loss=9.3917
	step [86/249], loss=8.4043
	step [87/249], loss=6.3764
	step [88/249], loss=8.5481
	step [89/249], loss=8.2543
	step [90/249], loss=9.5652
	step [91/249], loss=7.6421
	step [92/249], loss=9.0945
	step [93/249], loss=7.7860
	step [94/249], loss=7.8745
	step [95/249], loss=6.7149
	step [96/249], loss=7.9760
	step [97/249], loss=6.9367
	step [98/249], loss=7.7188
	step [99/249], loss=8.2712
	step [100/249], loss=8.6507
	step [101/249], loss=8.0266
	step [102/249], loss=7.9181
	step [103/249], loss=8.1452
	step [104/249], loss=7.4425
	step [105/249], loss=7.1096
	step [106/249], loss=11.3098
	step [107/249], loss=7.9326
	step [108/249], loss=7.6563
	step [109/249], loss=7.8839
	step [110/249], loss=9.3299
	step [111/249], loss=10.8766
	step [112/249], loss=8.0612
	step [113/249], loss=8.1789
	step [114/249], loss=7.9301
	step [115/249], loss=9.3072
	step [116/249], loss=7.6339
	step [117/249], loss=6.9387
	step [118/249], loss=8.0750
	step [119/249], loss=6.9736
	step [120/249], loss=8.0909
	step [121/249], loss=7.7990
	step [122/249], loss=7.9863
	step [123/249], loss=8.8866
	step [124/249], loss=9.5836
	step [125/249], loss=6.3782
	step [126/249], loss=8.3251
	step [127/249], loss=9.6125
	step [128/249], loss=8.7704
	step [129/249], loss=7.6379
	step [130/249], loss=8.5859
	step [131/249], loss=9.9443
	step [132/249], loss=8.6180
	step [133/249], loss=7.9034
	step [134/249], loss=8.7545
	step [135/249], loss=7.8114
	step [136/249], loss=8.8655
	step [137/249], loss=9.7409
	step [138/249], loss=9.4098
	step [139/249], loss=9.4504
	step [140/249], loss=7.9320
	step [141/249], loss=8.1686
	step [142/249], loss=8.9190
	step [143/249], loss=7.1211
	step [144/249], loss=7.7787
	step [145/249], loss=8.2753
	step [146/249], loss=9.8023
	step [147/249], loss=10.0241
	step [148/249], loss=7.3592
	step [149/249], loss=8.1387
	step [150/249], loss=8.9911
	step [151/249], loss=9.2101
	step [152/249], loss=9.3208
	step [153/249], loss=9.1414
	step [154/249], loss=9.7304
	step [155/249], loss=8.8040
	step [156/249], loss=7.8221
	step [157/249], loss=8.9403
	step [158/249], loss=8.4368
	step [159/249], loss=8.8830
	step [160/249], loss=8.6894
	step [161/249], loss=8.2295
	step [162/249], loss=7.3339
	step [163/249], loss=8.0083
	step [164/249], loss=8.8893
	step [165/249], loss=8.4038
	step [166/249], loss=8.3550
	step [167/249], loss=7.9021
	step [168/249], loss=7.4864
	step [169/249], loss=7.3770
	step [170/249], loss=7.8925
	step [171/249], loss=8.4100
	step [172/249], loss=7.4356
	step [173/249], loss=8.5244
	step [174/249], loss=10.7762
	step [175/249], loss=9.4319
	step [176/249], loss=8.2919
	step [177/249], loss=8.4590
	step [178/249], loss=10.3834
	step [179/249], loss=9.2643
	step [180/249], loss=7.6279
	step [181/249], loss=8.8347
	step [182/249], loss=7.8446
	step [183/249], loss=8.3874
	step [184/249], loss=8.6406
	step [185/249], loss=6.4592
	step [186/249], loss=9.1763
	step [187/249], loss=8.8217
	step [188/249], loss=7.9030
	step [189/249], loss=9.2794
	step [190/249], loss=8.1021
	step [191/249], loss=9.4808
	step [192/249], loss=8.5108
	step [193/249], loss=8.3940
	step [194/249], loss=8.7681
	step [195/249], loss=8.4691
	step [196/249], loss=8.2232
	step [197/249], loss=7.4661
	step [198/249], loss=8.8022
	step [199/249], loss=9.4573
	step [200/249], loss=7.2502
	step [201/249], loss=7.6665
	step [202/249], loss=8.8708
	step [203/249], loss=6.9224
	step [204/249], loss=6.6143
	step [205/249], loss=8.1283
	step [206/249], loss=9.4233
	step [207/249], loss=8.4767
	step [208/249], loss=7.2552
	step [209/249], loss=9.2436
	step [210/249], loss=8.2321
	step [211/249], loss=7.8348
	step [212/249], loss=8.3370
	step [213/249], loss=6.7306
	step [214/249], loss=7.2298
	step [215/249], loss=7.4249
	step [216/249], loss=9.0570
	step [217/249], loss=8.0328
	step [218/249], loss=8.6997
	step [219/249], loss=9.5339
	step [220/249], loss=9.7245
	step [221/249], loss=6.8378
	step [222/249], loss=8.9003
	step [223/249], loss=8.2684
	step [224/249], loss=7.4032
	step [225/249], loss=8.6102
	step [226/249], loss=7.9976
	step [227/249], loss=9.0921
	step [228/249], loss=8.7992
	step [229/249], loss=8.8216
	step [230/249], loss=8.0329
	step [231/249], loss=7.6410
	step [232/249], loss=7.2999
	step [233/249], loss=8.6253
	step [234/249], loss=9.9137
	step [235/249], loss=8.6835
	step [236/249], loss=8.5538
	step [237/249], loss=7.7495
	step [238/249], loss=7.7793
	step [239/249], loss=8.6288
	step [240/249], loss=9.5454
	step [241/249], loss=7.3345
	step [242/249], loss=7.5728
	step [243/249], loss=9.2063
	step [244/249], loss=7.6648
	step [245/249], loss=7.8967
	step [246/249], loss=7.1883
	step [247/249], loss=8.2314
	step [248/249], loss=6.2277
	step [249/249], loss=4.8932
	Evaluating
	loss=0.0252, precision=0.1531, recall=0.9954, f1=0.2653
Training epoch 15
	step [1/249], loss=8.5694
	step [2/249], loss=8.7643
	step [3/249], loss=6.6824
	step [4/249], loss=9.3186
	step [5/249], loss=9.3076
	step [6/249], loss=6.6346
	step [7/249], loss=8.2237
	step [8/249], loss=7.6848
	step [9/249], loss=7.9877
	step [10/249], loss=8.2730
	step [11/249], loss=8.8572
	step [12/249], loss=7.5769
	step [13/249], loss=7.7024
	step [14/249], loss=6.6891
	step [15/249], loss=8.1663
	step [16/249], loss=8.7728
	step [17/249], loss=8.5032
	step [18/249], loss=7.1800
	step [19/249], loss=9.9915
	step [20/249], loss=7.6258
	step [21/249], loss=8.3940
	step [22/249], loss=7.6953
	step [23/249], loss=8.9431
	step [24/249], loss=7.5101
	step [25/249], loss=8.3111
	step [26/249], loss=8.1683
	step [27/249], loss=8.2028
	step [28/249], loss=7.7124
	step [29/249], loss=8.6248
	step [30/249], loss=8.5211
	step [31/249], loss=9.5246
	step [32/249], loss=8.7038
	step [33/249], loss=8.6782
	step [34/249], loss=8.6753
	step [35/249], loss=7.3429
	step [36/249], loss=8.0528
	step [37/249], loss=7.2720
	step [38/249], loss=8.0321
	step [39/249], loss=8.2668
	step [40/249], loss=8.1692
	step [41/249], loss=7.4734
	step [42/249], loss=9.2523
	step [43/249], loss=8.5705
	step [44/249], loss=9.1178
	step [45/249], loss=7.2728
	step [46/249], loss=8.2719
	step [47/249], loss=8.8920
	step [48/249], loss=7.2189
	step [49/249], loss=7.7228
	step [50/249], loss=10.2027
	step [51/249], loss=9.8142
	step [52/249], loss=9.3483
	step [53/249], loss=7.5899
	step [54/249], loss=8.7592
	step [55/249], loss=7.6702
	step [56/249], loss=7.9341
	step [57/249], loss=7.3893
	step [58/249], loss=7.5700
	step [59/249], loss=8.5698
	step [60/249], loss=8.5595
	step [61/249], loss=7.2973
	step [62/249], loss=9.0050
	step [63/249], loss=8.8493
	step [64/249], loss=7.9035
	step [65/249], loss=7.4444
	step [66/249], loss=8.9543
	step [67/249], loss=7.4342
	step [68/249], loss=7.2376
	step [69/249], loss=7.5126
	step [70/249], loss=8.1416
	step [71/249], loss=7.5544
	step [72/249], loss=6.5350
	step [73/249], loss=7.2605
	step [74/249], loss=8.0930
	step [75/249], loss=7.4145
	step [76/249], loss=7.8649
	step [77/249], loss=8.3642
	step [78/249], loss=8.2707
	step [79/249], loss=8.0096
	step [80/249], loss=8.5780
	step [81/249], loss=7.0307
	step [82/249], loss=7.5574
	step [83/249], loss=7.9904
	step [84/249], loss=8.7957
	step [85/249], loss=7.2672
	step [86/249], loss=7.8567
	step [87/249], loss=8.2718
	step [88/249], loss=7.7328
	step [89/249], loss=7.0527
	step [90/249], loss=8.6757
	step [91/249], loss=8.1528
	step [92/249], loss=8.5289
	step [93/249], loss=8.2722
	step [94/249], loss=8.0816
	step [95/249], loss=8.4567
	step [96/249], loss=8.1905
	step [97/249], loss=8.5421
	step [98/249], loss=7.1264
	step [99/249], loss=7.6077
	step [100/249], loss=7.7885
	step [101/249], loss=8.0646
	step [102/249], loss=8.5935
	step [103/249], loss=7.2188
	step [104/249], loss=8.6563
	step [105/249], loss=10.0978
	step [106/249], loss=7.9323
	step [107/249], loss=6.5485
	step [108/249], loss=8.9558
	step [109/249], loss=7.1221
	step [110/249], loss=7.2383
	step [111/249], loss=7.9013
	step [112/249], loss=7.8279
	step [113/249], loss=7.7014
	step [114/249], loss=8.0939
	step [115/249], loss=7.0369
	step [116/249], loss=7.4118
	step [117/249], loss=6.9495
	step [118/249], loss=7.5262
	step [119/249], loss=7.5640
	step [120/249], loss=8.1592
	step [121/249], loss=9.3697
	step [122/249], loss=7.2444
	step [123/249], loss=6.7792
	step [124/249], loss=8.1757
	step [125/249], loss=7.5299
	step [126/249], loss=8.6598
	step [127/249], loss=9.8919
	step [128/249], loss=7.2892
	step [129/249], loss=7.9175
	step [130/249], loss=7.6561
	step [131/249], loss=8.9324
	step [132/249], loss=6.5945
	step [133/249], loss=7.2095
	step [134/249], loss=7.4160
	step [135/249], loss=8.8735
	step [136/249], loss=8.0571
	step [137/249], loss=7.7932
	step [138/249], loss=7.8083
	step [139/249], loss=8.0536
	step [140/249], loss=7.0726
	step [141/249], loss=8.3022
	step [142/249], loss=7.0314
	step [143/249], loss=6.7834
	step [144/249], loss=8.2332
	step [145/249], loss=7.3386
	step [146/249], loss=7.5246
	step [147/249], loss=8.7437
	step [148/249], loss=8.9262
	step [149/249], loss=7.5501
	step [150/249], loss=7.4620
	step [151/249], loss=8.9025
	step [152/249], loss=7.5289
	step [153/249], loss=7.7422
	step [154/249], loss=8.6176
	step [155/249], loss=7.8701
	step [156/249], loss=7.7806
	step [157/249], loss=8.1727
	step [158/249], loss=8.0349
	step [159/249], loss=8.6281
	step [160/249], loss=8.3911
	step [161/249], loss=8.4207
	step [162/249], loss=8.3212
	step [163/249], loss=7.3612
	step [164/249], loss=7.7152
	step [165/249], loss=7.8890
	step [166/249], loss=7.4122
	step [167/249], loss=9.0426
	step [168/249], loss=8.1630
	step [169/249], loss=6.9971
	step [170/249], loss=7.8800
	step [171/249], loss=8.5179
	step [172/249], loss=6.7194
	step [173/249], loss=7.7820
	step [174/249], loss=8.2118
	step [175/249], loss=9.3650
	step [176/249], loss=7.6359
	step [177/249], loss=8.1153
	step [178/249], loss=5.9590
	step [179/249], loss=10.2126
	step [180/249], loss=6.7595
	step [181/249], loss=10.4626
	step [182/249], loss=8.4517
	step [183/249], loss=7.7283
	step [184/249], loss=6.1709
	step [185/249], loss=6.7318
	step [186/249], loss=8.9890
	step [187/249], loss=9.6762
	step [188/249], loss=6.8183
	step [189/249], loss=7.4660
	step [190/249], loss=9.3544
	step [191/249], loss=7.2935
	step [192/249], loss=8.2128
	step [193/249], loss=6.2556
	step [194/249], loss=9.1012
	step [195/249], loss=10.2567
	step [196/249], loss=6.8627
	step [197/249], loss=9.8192
	step [198/249], loss=6.0229
	step [199/249], loss=7.8997
	step [200/249], loss=7.8906
	step [201/249], loss=8.4818
	step [202/249], loss=9.4788
	step [203/249], loss=8.1984
	step [204/249], loss=7.7168
	step [205/249], loss=7.1786
	step [206/249], loss=7.3468
	step [207/249], loss=8.6747
	step [208/249], loss=8.8484
	step [209/249], loss=6.3517
	step [210/249], loss=9.8033
	step [211/249], loss=8.6623
	step [212/249], loss=7.9549
	step [213/249], loss=8.0297
	step [214/249], loss=7.8348
	step [215/249], loss=7.2811
	step [216/249], loss=7.5205
	step [217/249], loss=7.2438
	step [218/249], loss=8.5256
	step [219/249], loss=7.2263
	step [220/249], loss=7.1837
	step [221/249], loss=8.0313
	step [222/249], loss=7.3351
	step [223/249], loss=6.7566
	step [224/249], loss=7.5572
	step [225/249], loss=8.7251
	step [226/249], loss=8.1751
	step [227/249], loss=9.3281
	step [228/249], loss=9.0837
	step [229/249], loss=7.6568
	step [230/249], loss=9.0110
	step [231/249], loss=6.9313
	step [232/249], loss=6.5889
	step [233/249], loss=7.7435
	step [234/249], loss=7.6771
	step [235/249], loss=7.9446
	step [236/249], loss=7.5901
	step [237/249], loss=7.6479
	step [238/249], loss=8.2039
	step [239/249], loss=7.5387
	step [240/249], loss=10.5972
	step [241/249], loss=6.9679
	step [242/249], loss=7.8326
	step [243/249], loss=8.7964
	step [244/249], loss=6.6250
	step [245/249], loss=8.2146
	step [246/249], loss=7.3888
	step [247/249], loss=9.6330
	step [248/249], loss=8.6986
	step [249/249], loss=5.4487
	Evaluating
	loss=0.0205, precision=0.1892, recall=0.9946, f1=0.3179
saving model as: 0_saved_model.pth
Training epoch 16
	step [1/249], loss=8.5914
	step [2/249], loss=6.4520
	step [3/249], loss=8.3072
	step [4/249], loss=7.9666
	step [5/249], loss=7.1611
	step [6/249], loss=7.0355
	step [7/249], loss=6.9246
	step [8/249], loss=8.7775
	step [9/249], loss=7.2701
	step [10/249], loss=8.5284
	step [11/249], loss=7.4896
	step [12/249], loss=7.1988
	step [13/249], loss=7.7238
	step [14/249], loss=7.3436
	step [15/249], loss=8.6733
	step [16/249], loss=7.5802
	step [17/249], loss=6.8559
	step [18/249], loss=9.5430
	step [19/249], loss=7.2324
	step [20/249], loss=8.1203
	step [21/249], loss=9.0173
	step [22/249], loss=8.9207
	step [23/249], loss=7.7116
	step [24/249], loss=8.7547
	step [25/249], loss=6.9340
	step [26/249], loss=8.0771
	step [27/249], loss=7.0264
	step [28/249], loss=7.2878
	step [29/249], loss=6.7119
	step [30/249], loss=7.6732
	step [31/249], loss=6.9717
	step [32/249], loss=7.2614
	step [33/249], loss=7.1860
	step [34/249], loss=8.0326
	step [35/249], loss=7.6556
	step [36/249], loss=7.0019
	step [37/249], loss=8.0463
	step [38/249], loss=6.5717
	step [39/249], loss=8.0360
	step [40/249], loss=7.8640
	step [41/249], loss=7.4633
	step [42/249], loss=8.4507
	step [43/249], loss=7.1241
	step [44/249], loss=7.0498
	step [45/249], loss=6.0719
	step [46/249], loss=7.8341
	step [47/249], loss=7.1727
	step [48/249], loss=6.9464
	step [49/249], loss=6.9984
	step [50/249], loss=7.5361
	step [51/249], loss=7.3461
	step [52/249], loss=7.4918
	step [53/249], loss=7.9534
	step [54/249], loss=7.0991
	step [55/249], loss=6.9017
	step [56/249], loss=7.6603
	step [57/249], loss=8.0541
	step [58/249], loss=9.3575
	step [59/249], loss=8.7961
	step [60/249], loss=7.4230
	step [61/249], loss=6.9809
	step [62/249], loss=7.5278
	step [63/249], loss=7.0288
	step [64/249], loss=8.4568
	step [65/249], loss=6.6404
	step [66/249], loss=6.2205
	step [67/249], loss=7.6836
	step [68/249], loss=7.9385
	step [69/249], loss=6.5276
	step [70/249], loss=8.4210
	step [71/249], loss=7.3412
	step [72/249], loss=6.9860
	step [73/249], loss=6.3010
	step [74/249], loss=7.8067
	step [75/249], loss=8.1261
	step [76/249], loss=9.0992
	step [77/249], loss=7.0831
	step [78/249], loss=9.0428
	step [79/249], loss=8.4516
	step [80/249], loss=8.0128
	step [81/249], loss=8.4891
	step [82/249], loss=7.1870
	step [83/249], loss=7.3735
	step [84/249], loss=7.9499
	step [85/249], loss=6.4373
	step [86/249], loss=7.4821
	step [87/249], loss=7.0386
	step [88/249], loss=7.1059
	step [89/249], loss=7.8247
	step [90/249], loss=8.9350
	step [91/249], loss=6.9065
	step [92/249], loss=7.7394
	step [93/249], loss=6.4493
	step [94/249], loss=8.0535
	step [95/249], loss=7.5252
	step [96/249], loss=8.0530
	step [97/249], loss=8.8345
	step [98/249], loss=6.3376
	step [99/249], loss=9.1312
	step [100/249], loss=7.3933
	step [101/249], loss=7.4255
	step [102/249], loss=7.6673
	step [103/249], loss=6.7769
	step [104/249], loss=8.0002
	step [105/249], loss=8.9785
	step [106/249], loss=8.2644
	step [107/249], loss=7.5810
	step [108/249], loss=8.5947
	step [109/249], loss=7.7461
	step [110/249], loss=7.3695
	step [111/249], loss=8.8921
	step [112/249], loss=8.2401
	step [113/249], loss=8.4320
	step [114/249], loss=7.6897
	step [115/249], loss=7.3308
	step [116/249], loss=8.0052
	step [117/249], loss=6.4078
	step [118/249], loss=8.3042
	step [119/249], loss=8.2141
	step [120/249], loss=9.1919
	step [121/249], loss=6.8292
	step [122/249], loss=7.2909
	step [123/249], loss=7.3480
	step [124/249], loss=8.2672
	step [125/249], loss=8.5868
	step [126/249], loss=8.0760
	step [127/249], loss=7.6086
	step [128/249], loss=8.8475
	step [129/249], loss=7.0318
	step [130/249], loss=7.2559
	step [131/249], loss=9.1808
	step [132/249], loss=7.4933
	step [133/249], loss=7.3407
	step [134/249], loss=6.9091
	step [135/249], loss=7.2016
	step [136/249], loss=6.6273
	step [137/249], loss=6.5476
	step [138/249], loss=7.9402
	step [139/249], loss=7.6059
	step [140/249], loss=7.6976
	step [141/249], loss=7.5213
	step [142/249], loss=6.9622
	step [143/249], loss=8.2176
	step [144/249], loss=6.2504
	step [145/249], loss=8.3822
	step [146/249], loss=6.9859
	step [147/249], loss=6.6819
	step [148/249], loss=7.3186
	step [149/249], loss=7.5218
	step [150/249], loss=7.5774
	step [151/249], loss=7.4763
	step [152/249], loss=8.6043
	step [153/249], loss=8.4074
	step [154/249], loss=7.7938
	step [155/249], loss=8.3344
	step [156/249], loss=6.8413
	step [157/249], loss=7.4681
	step [158/249], loss=8.9435
	step [159/249], loss=7.5371
	step [160/249], loss=7.2673
	step [161/249], loss=7.3954
	step [162/249], loss=7.6769
	step [163/249], loss=6.7073
	step [164/249], loss=8.9444
	step [165/249], loss=8.4434
	step [166/249], loss=6.4310
	step [167/249], loss=6.9083
	step [168/249], loss=6.5589
	step [169/249], loss=8.5308
	step [170/249], loss=7.5857
	step [171/249], loss=8.2930
	step [172/249], loss=8.1845
	step [173/249], loss=6.7627
	step [174/249], loss=8.3974
	step [175/249], loss=7.5607
	step [176/249], loss=8.8751
	step [177/249], loss=8.1877
	step [178/249], loss=6.6656
	step [179/249], loss=7.6589
	step [180/249], loss=8.2668
	step [181/249], loss=8.1649
	step [182/249], loss=6.5270
	step [183/249], loss=7.4019
	step [184/249], loss=6.8702
	step [185/249], loss=6.5607
	step [186/249], loss=8.3502
	step [187/249], loss=7.3017
	step [188/249], loss=7.6553
	step [189/249], loss=9.3344
	step [190/249], loss=6.7571
	step [191/249], loss=7.0138
	step [192/249], loss=8.4503
	step [193/249], loss=8.4129
	step [194/249], loss=6.7389
	step [195/249], loss=7.0241
	step [196/249], loss=8.0976
	step [197/249], loss=7.7390
	step [198/249], loss=7.8635
	step [199/249], loss=7.1632
	step [200/249], loss=8.4028
	step [201/249], loss=6.7691
	step [202/249], loss=8.0571
	step [203/249], loss=7.1780
	step [204/249], loss=6.9935
	step [205/249], loss=6.9102
	step [206/249], loss=7.3386
	step [207/249], loss=7.8474
	step [208/249], loss=8.6229
	step [209/249], loss=7.4336
	step [210/249], loss=7.1985
	step [211/249], loss=7.9274
	step [212/249], loss=6.3741
	step [213/249], loss=6.8775
	step [214/249], loss=7.4394
	step [215/249], loss=7.8465
	step [216/249], loss=5.9168
	step [217/249], loss=7.4577
	step [218/249], loss=6.2524
	step [219/249], loss=6.1127
	step [220/249], loss=7.9616
	step [221/249], loss=7.0163
	step [222/249], loss=7.5362
	step [223/249], loss=7.5763
	step [224/249], loss=7.6521
	step [225/249], loss=8.1728
	step [226/249], loss=7.4512
	step [227/249], loss=7.0876
	step [228/249], loss=7.8343
	step [229/249], loss=7.4369
	step [230/249], loss=7.7066
	step [231/249], loss=7.0533
	step [232/249], loss=7.9011
	step [233/249], loss=8.0912
	step [234/249], loss=6.6038
	step [235/249], loss=6.9455
	step [236/249], loss=7.2628
	step [237/249], loss=7.6310
	step [238/249], loss=6.8270
	step [239/249], loss=6.7501
	step [240/249], loss=8.1699
	step [241/249], loss=6.9831
	step [242/249], loss=8.6825
	step [243/249], loss=7.0355
	step [244/249], loss=7.9262
	step [245/249], loss=8.5468
	step [246/249], loss=5.7087
	step [247/249], loss=7.0286
	step [248/249], loss=6.3168
	step [249/249], loss=5.0335
	Evaluating
	loss=0.0212, precision=0.1726, recall=0.9956, f1=0.2942
Training epoch 17
	step [1/249], loss=6.6080
	step [2/249], loss=8.0536
	step [3/249], loss=7.4880
	step [4/249], loss=7.1932
	step [5/249], loss=9.9981
	step [6/249], loss=7.4349
	step [7/249], loss=6.7254
	step [8/249], loss=7.9053
	step [9/249], loss=6.8593
	step [10/249], loss=7.9893
	step [11/249], loss=7.1699
	step [12/249], loss=8.0434
	step [13/249], loss=6.7407
	step [14/249], loss=6.5368
	step [15/249], loss=6.6103
	step [16/249], loss=5.7956
	step [17/249], loss=8.8603
	step [18/249], loss=6.0281
	step [19/249], loss=7.2624
	step [20/249], loss=8.2754
	step [21/249], loss=8.5804
	step [22/249], loss=7.2803
	step [23/249], loss=6.5705
	step [24/249], loss=7.5080
	step [25/249], loss=7.1043
	step [26/249], loss=8.0717
	step [27/249], loss=8.6516
	step [28/249], loss=7.2149
	step [29/249], loss=7.0938
	step [30/249], loss=9.1297
	step [31/249], loss=7.5375
	step [32/249], loss=7.1405
	step [33/249], loss=7.7130
	step [34/249], loss=6.7070
	step [35/249], loss=7.1092
	step [36/249], loss=7.2571
	step [37/249], loss=7.8389
	step [38/249], loss=7.7568
	step [39/249], loss=6.6911
	step [40/249], loss=7.1349
	step [41/249], loss=7.7770
	step [42/249], loss=7.5336
	step [43/249], loss=7.9853
	step [44/249], loss=7.4206
	step [45/249], loss=6.4588
	step [46/249], loss=6.6873
	step [47/249], loss=6.4896
	step [48/249], loss=6.4293
	step [49/249], loss=7.0156
	step [50/249], loss=7.8118
	step [51/249], loss=6.4517
	step [52/249], loss=8.2520
	step [53/249], loss=9.1889
	step [54/249], loss=8.5944
	step [55/249], loss=6.8740
	step [56/249], loss=8.4947
	step [57/249], loss=8.0425
	step [58/249], loss=8.0352
	step [59/249], loss=8.5617
	step [60/249], loss=7.1446
	step [61/249], loss=6.9659
	step [62/249], loss=7.1240
	step [63/249], loss=7.8990
	step [64/249], loss=7.7881
	step [65/249], loss=7.0365
	step [66/249], loss=6.5206
	step [67/249], loss=7.4161
	step [68/249], loss=7.6807
	step [69/249], loss=7.9127
	step [70/249], loss=9.6127
	step [71/249], loss=7.8131
	step [72/249], loss=8.9244
	step [73/249], loss=7.2545
	step [74/249], loss=6.8039
	step [75/249], loss=7.1276
	step [76/249], loss=6.9275
	step [77/249], loss=7.7653
	step [78/249], loss=6.3712
	step [79/249], loss=7.5529
	step [80/249], loss=7.2324
	step [81/249], loss=6.8131
	step [82/249], loss=5.7835
	step [83/249], loss=6.6533
	step [84/249], loss=7.0025
	step [85/249], loss=7.0477
	step [86/249], loss=7.0615
	step [87/249], loss=7.5409
	step [88/249], loss=7.5446
	step [89/249], loss=8.5849
	step [90/249], loss=7.2674
	step [91/249], loss=7.7526
	step [92/249], loss=8.7159
	step [93/249], loss=7.1569
	step [94/249], loss=7.1759
	step [95/249], loss=7.5328
	step [96/249], loss=8.0642
	step [97/249], loss=7.4974
	step [98/249], loss=7.9129
	step [99/249], loss=6.8645
	step [100/249], loss=7.2045
	step [101/249], loss=7.4797
	step [102/249], loss=8.0944
	step [103/249], loss=8.3139
	step [104/249], loss=8.4203
	step [105/249], loss=6.8965
	step [106/249], loss=6.2897
	step [107/249], loss=7.2769
	step [108/249], loss=6.9855
	step [109/249], loss=7.1290
	step [110/249], loss=7.1946
	step [111/249], loss=7.8134
	step [112/249], loss=6.3145
	step [113/249], loss=8.0002
	step [114/249], loss=7.6618
	step [115/249], loss=7.0740
	step [116/249], loss=7.9758
	step [117/249], loss=7.1952
	step [118/249], loss=5.7824
	step [119/249], loss=8.3468
	step [120/249], loss=6.1745
	step [121/249], loss=6.3913
	step [122/249], loss=6.5538
	step [123/249], loss=6.5167
	step [124/249], loss=6.9285
	step [125/249], loss=8.4179
	step [126/249], loss=7.1653
	step [127/249], loss=5.3450
	step [128/249], loss=8.1800
	step [129/249], loss=8.5968
	step [130/249], loss=7.6579
	step [131/249], loss=5.8197
	step [132/249], loss=6.9022
	step [133/249], loss=5.7055
	step [134/249], loss=7.1097
	step [135/249], loss=6.9569
	step [136/249], loss=5.8570
	step [137/249], loss=7.7984
	step [138/249], loss=7.8860
	step [139/249], loss=7.2750
	step [140/249], loss=5.9791
	step [141/249], loss=6.3530
	step [142/249], loss=6.8140
	step [143/249], loss=6.0604
	step [144/249], loss=5.9245
	step [145/249], loss=7.3325
	step [146/249], loss=8.0256
	step [147/249], loss=7.5249
	step [148/249], loss=8.4027
	step [149/249], loss=7.1308
	step [150/249], loss=8.3018
	step [151/249], loss=8.1315
	step [152/249], loss=6.5942
	step [153/249], loss=6.7004
	step [154/249], loss=7.4683
	step [155/249], loss=9.1149
	step [156/249], loss=6.6479
	step [157/249], loss=6.5567
	step [158/249], loss=7.1726
	step [159/249], loss=7.4021
	step [160/249], loss=7.0249
	step [161/249], loss=8.7899
	step [162/249], loss=6.1858
	step [163/249], loss=6.7643
	step [164/249], loss=9.3118
	step [165/249], loss=7.7434
	step [166/249], loss=6.7295
	step [167/249], loss=6.5456
	step [168/249], loss=6.7523
	step [169/249], loss=7.5583
	step [170/249], loss=6.5891
	step [171/249], loss=8.6196
	step [172/249], loss=7.2878
	step [173/249], loss=6.0028
	step [174/249], loss=7.5067
	step [175/249], loss=8.1527
	step [176/249], loss=8.6539
	step [177/249], loss=5.4748
	step [178/249], loss=6.4326
	step [179/249], loss=8.4551
	step [180/249], loss=7.1272
	step [181/249], loss=7.9547
	step [182/249], loss=6.7956
	step [183/249], loss=6.0437
	step [184/249], loss=5.8158
	step [185/249], loss=5.7237
	step [186/249], loss=7.8001
	step [187/249], loss=8.0245
	step [188/249], loss=7.1134
	step [189/249], loss=8.0599
	step [190/249], loss=7.0315
	step [191/249], loss=7.8744
	step [192/249], loss=7.3211
	step [193/249], loss=6.5098
	step [194/249], loss=5.5745
	step [195/249], loss=6.5560
	step [196/249], loss=7.0579
	step [197/249], loss=7.2685
	step [198/249], loss=7.6769
	step [199/249], loss=6.8995
	step [200/249], loss=7.6358
	step [201/249], loss=6.4359
	step [202/249], loss=7.4609
	step [203/249], loss=6.2603
	step [204/249], loss=7.5274
	step [205/249], loss=7.3061
	step [206/249], loss=7.1999
	step [207/249], loss=7.4496
	step [208/249], loss=7.8980
	step [209/249], loss=7.8490
	step [210/249], loss=6.4170
	step [211/249], loss=10.1037
	step [212/249], loss=6.9198
	step [213/249], loss=7.9976
	step [214/249], loss=8.0017
	step [215/249], loss=7.9332
	step [216/249], loss=8.5083
	step [217/249], loss=8.8973
	step [218/249], loss=6.7489
	step [219/249], loss=6.0293
	step [220/249], loss=6.5776
	step [221/249], loss=7.4082
	step [222/249], loss=8.3957
	step [223/249], loss=8.1029
	step [224/249], loss=9.6118
	step [225/249], loss=7.4210
	step [226/249], loss=7.4240
	step [227/249], loss=6.7400
	step [228/249], loss=7.2289
	step [229/249], loss=7.1028
	step [230/249], loss=7.3645
	step [231/249], loss=6.6585
	step [232/249], loss=7.7499
	step [233/249], loss=7.1647
	step [234/249], loss=7.5656
	step [235/249], loss=7.6839
	step [236/249], loss=6.2556
	step [237/249], loss=7.2426
	step [238/249], loss=8.0873
	step [239/249], loss=6.6268
	step [240/249], loss=5.7304
	step [241/249], loss=5.6251
	step [242/249], loss=6.5979
	step [243/249], loss=7.9482
	step [244/249], loss=6.7702
	step [245/249], loss=6.6487
	step [246/249], loss=8.1371
	step [247/249], loss=7.8184
	step [248/249], loss=7.9643
	step [249/249], loss=4.8734
	Evaluating
	loss=0.0217, precision=0.1731, recall=0.9951, f1=0.2949
Training epoch 18
	step [1/249], loss=7.0829
	step [2/249], loss=9.0312
	step [3/249], loss=8.0142
	step [4/249], loss=6.1378
	step [5/249], loss=7.8798
	step [6/249], loss=6.6195
	step [7/249], loss=7.7167
	step [8/249], loss=6.8992
	step [9/249], loss=7.3125
	step [10/249], loss=7.0204
	step [11/249], loss=6.4527
	step [12/249], loss=9.4650
	step [13/249], loss=7.3041
	step [14/249], loss=6.1174
	step [15/249], loss=6.5183
	step [16/249], loss=6.7022
	step [17/249], loss=6.9368
	step [18/249], loss=6.3779
	step [19/249], loss=6.4222
	step [20/249], loss=8.0540
	step [21/249], loss=6.9367
	step [22/249], loss=8.0318
	step [23/249], loss=6.7865
	step [24/249], loss=8.0058
	step [25/249], loss=7.8163
	step [26/249], loss=6.9751
	step [27/249], loss=6.3640
	step [28/249], loss=6.3888
	step [29/249], loss=7.7030
	step [30/249], loss=8.1972
	step [31/249], loss=7.8268
	step [32/249], loss=5.6903
	step [33/249], loss=6.6851
	step [34/249], loss=5.4680
	step [35/249], loss=6.3077
	step [36/249], loss=7.1425
	step [37/249], loss=6.9803
	step [38/249], loss=7.0445
	step [39/249], loss=8.1247
	step [40/249], loss=5.7828
	step [41/249], loss=5.9127
	step [42/249], loss=8.4761
	step [43/249], loss=7.3988
	step [44/249], loss=6.4029
	step [45/249], loss=7.2271
	step [46/249], loss=7.9605
	step [47/249], loss=7.1020
	step [48/249], loss=6.9926
	step [49/249], loss=8.1676
	step [50/249], loss=7.1700
	step [51/249], loss=7.4760
	step [52/249], loss=7.4635
	step [53/249], loss=8.1602
	step [54/249], loss=6.7194
	step [55/249], loss=7.6288
	step [56/249], loss=7.4085
	step [57/249], loss=5.9441
	step [58/249], loss=5.6284
	step [59/249], loss=7.6698
	step [60/249], loss=6.4489
	step [61/249], loss=6.4420
	step [62/249], loss=7.6149
	step [63/249], loss=6.6874
	step [64/249], loss=7.4881
	step [65/249], loss=6.3577
	step [66/249], loss=6.5248
	step [67/249], loss=6.8075
	step [68/249], loss=6.7443
	step [69/249], loss=7.4595
	step [70/249], loss=7.3132
	step [71/249], loss=7.8936
	step [72/249], loss=7.7415
	step [73/249], loss=6.8824
	step [74/249], loss=6.8703
	step [75/249], loss=7.7147
	step [76/249], loss=5.9385
	step [77/249], loss=5.4965
	step [78/249], loss=7.2607
	step [79/249], loss=7.4101
	step [80/249], loss=5.9850
	step [81/249], loss=6.7386
	step [82/249], loss=7.9101
	step [83/249], loss=7.0272
	step [84/249], loss=7.4805
	step [85/249], loss=7.3861
	step [86/249], loss=7.9702
	step [87/249], loss=8.0464
	step [88/249], loss=7.2336
	step [89/249], loss=6.5297
	step [90/249], loss=7.0019
	step [91/249], loss=6.2288
	step [92/249], loss=7.1591
	step [93/249], loss=7.1699
	step [94/249], loss=7.3299
	step [95/249], loss=6.0609
	step [96/249], loss=6.9288
	step [97/249], loss=8.5799
	step [98/249], loss=6.7268
	step [99/249], loss=6.7672
	step [100/249], loss=6.8907
	step [101/249], loss=7.5098
	step [102/249], loss=5.7490
	step [103/249], loss=7.0017
	step [104/249], loss=6.7989
	step [105/249], loss=6.4257
	step [106/249], loss=8.5440
	step [107/249], loss=7.8755
	step [108/249], loss=6.8291
	step [109/249], loss=5.9879
	step [110/249], loss=7.7517
	step [111/249], loss=6.3642
	step [112/249], loss=6.5452
	step [113/249], loss=7.4964
	step [114/249], loss=6.5781
	step [115/249], loss=8.5192
	step [116/249], loss=6.9471
	step [117/249], loss=7.7763
	step [118/249], loss=7.5553
	step [119/249], loss=7.4713
	step [120/249], loss=7.5770
	step [121/249], loss=7.3230
	step [122/249], loss=6.4771
	step [123/249], loss=7.0344
	step [124/249], loss=6.8762
	step [125/249], loss=8.2805
	step [126/249], loss=7.5900
	step [127/249], loss=7.4320
	step [128/249], loss=7.0340
	step [129/249], loss=7.3830
	step [130/249], loss=5.5242
	step [131/249], loss=7.0251
	step [132/249], loss=6.2763
	step [133/249], loss=7.9485
	step [134/249], loss=6.6781
	step [135/249], loss=5.9240
	step [136/249], loss=7.5619
	step [137/249], loss=6.2314
	step [138/249], loss=6.4148
	step [139/249], loss=7.4767
	step [140/249], loss=6.3890
	step [141/249], loss=5.5149
	step [142/249], loss=6.7394
	step [143/249], loss=6.4831
	step [144/249], loss=7.2695
	step [145/249], loss=6.7403
	step [146/249], loss=5.9908
	step [147/249], loss=8.6507
	step [148/249], loss=7.5549
	step [149/249], loss=7.3330
	step [150/249], loss=6.1922
	step [151/249], loss=7.5224
	step [152/249], loss=6.9582
	step [153/249], loss=5.8966
	step [154/249], loss=5.7469
	step [155/249], loss=6.7945
	step [156/249], loss=7.7520
	step [157/249], loss=6.5922
	step [158/249], loss=6.6255
	step [159/249], loss=5.9553
	step [160/249], loss=6.3639
	step [161/249], loss=6.8120
	step [162/249], loss=6.6232
	step [163/249], loss=7.4652
	step [164/249], loss=6.8141
	step [165/249], loss=7.3680
	step [166/249], loss=5.9174
	step [167/249], loss=7.0983
	step [168/249], loss=7.6334
	step [169/249], loss=6.8633
	step [170/249], loss=6.5865
	step [171/249], loss=7.4328
	step [172/249], loss=9.4188
	step [173/249], loss=7.0821
	step [174/249], loss=6.6357
	step [175/249], loss=7.1054
	step [176/249], loss=6.8272
	step [177/249], loss=5.6761
	step [178/249], loss=7.1546
	step [179/249], loss=6.6232
	step [180/249], loss=6.0006
	step [181/249], loss=7.7532
	step [182/249], loss=6.3054
	step [183/249], loss=10.3579
	step [184/249], loss=6.4320
	step [185/249], loss=6.0444
	step [186/249], loss=7.9944
	step [187/249], loss=6.0283
	step [188/249], loss=6.6262
	step [189/249], loss=6.7135
	step [190/249], loss=7.7309
	step [191/249], loss=6.8270
	step [192/249], loss=7.4895
	step [193/249], loss=6.0178
	step [194/249], loss=6.8512
	step [195/249], loss=6.4610
	step [196/249], loss=6.6694
	step [197/249], loss=6.0071
	step [198/249], loss=5.8629
	step [199/249], loss=5.7252
	step [200/249], loss=6.2386
	step [201/249], loss=6.2566
	step [202/249], loss=6.4765
	step [203/249], loss=7.6907
	step [204/249], loss=6.1832
	step [205/249], loss=7.3380
	step [206/249], loss=6.2274
	step [207/249], loss=8.1024
	step [208/249], loss=6.3547
	step [209/249], loss=6.8396
	step [210/249], loss=6.9579
	step [211/249], loss=6.4856
	step [212/249], loss=7.4794
	step [213/249], loss=5.9587
	step [214/249], loss=7.0928
	step [215/249], loss=7.3010
	step [216/249], loss=5.6127
	step [217/249], loss=7.7379
	step [218/249], loss=8.8682
	step [219/249], loss=6.2250
	step [220/249], loss=7.5586
	step [221/249], loss=6.7807
	step [222/249], loss=6.4657
	step [223/249], loss=5.9760
	step [224/249], loss=7.0651
	step [225/249], loss=5.9681
	step [226/249], loss=7.1214
	step [227/249], loss=6.6393
	step [228/249], loss=7.8757
	step [229/249], loss=8.1667
	step [230/249], loss=7.0509
	step [231/249], loss=6.3354
	step [232/249], loss=7.7349
	step [233/249], loss=7.4272
	step [234/249], loss=7.7213
	step [235/249], loss=7.3242
	step [236/249], loss=6.8922
	step [237/249], loss=6.7695
	step [238/249], loss=6.1972
	step [239/249], loss=7.6039
	step [240/249], loss=5.8801
	step [241/249], loss=8.0333
	step [242/249], loss=5.9120
	step [243/249], loss=8.1901
	step [244/249], loss=6.4594
	step [245/249], loss=6.6205
	step [246/249], loss=7.0399
	step [247/249], loss=7.0359
	step [248/249], loss=6.8999
	step [249/249], loss=3.4346
	Evaluating
	loss=0.0200, precision=0.1830, recall=0.9954, f1=0.3091
Training epoch 19
	step [1/249], loss=6.6268
	step [2/249], loss=8.0266
	step [3/249], loss=7.1288
	step [4/249], loss=7.2212
	step [5/249], loss=7.6327
	step [6/249], loss=7.2524
	step [7/249], loss=8.1602
	step [8/249], loss=7.1686
	step [9/249], loss=8.2486
	step [10/249], loss=7.0103
	step [11/249], loss=6.7946
	step [12/249], loss=8.0765
	step [13/249], loss=5.6549
	step [14/249], loss=6.5257
	step [15/249], loss=6.3656
	step [16/249], loss=6.4118
	step [17/249], loss=6.4016
	step [18/249], loss=6.5113
	step [19/249], loss=7.9981
	step [20/249], loss=6.3721
	step [21/249], loss=5.2829
	step [22/249], loss=7.2900
	step [23/249], loss=5.7524
	step [24/249], loss=5.8449
	step [25/249], loss=6.6998
	step [26/249], loss=7.3593
	step [27/249], loss=5.5193
	step [28/249], loss=6.4987
	step [29/249], loss=6.7283
	step [30/249], loss=9.1704
	step [31/249], loss=5.9387
	step [32/249], loss=7.1973
	step [33/249], loss=6.3326
	step [34/249], loss=6.4385
	step [35/249], loss=7.1175
	step [36/249], loss=5.6879
	step [37/249], loss=7.5028
	step [38/249], loss=7.6124
	step [39/249], loss=5.3665
	step [40/249], loss=6.9650
	step [41/249], loss=5.8103
	step [42/249], loss=6.3701
	step [43/249], loss=6.3801
	step [44/249], loss=6.3471
	step [45/249], loss=6.2680
	step [46/249], loss=8.2241
	step [47/249], loss=7.5823
	step [48/249], loss=8.5220
	step [49/249], loss=6.4500
	step [50/249], loss=5.4331
	step [51/249], loss=5.8959
	step [52/249], loss=5.9435
	step [53/249], loss=6.5008
	step [54/249], loss=6.0491
	step [55/249], loss=6.8484
	step [56/249], loss=6.5066
	step [57/249], loss=6.7394
	step [58/249], loss=5.8653
	step [59/249], loss=6.4169
	step [60/249], loss=7.7018
	step [61/249], loss=6.0943
	step [62/249], loss=7.1213
	step [63/249], loss=6.7801
	step [64/249], loss=6.3837
	step [65/249], loss=7.9141
	step [66/249], loss=6.8360
	step [67/249], loss=5.9478
	step [68/249], loss=5.8404
	step [69/249], loss=7.2831
	step [70/249], loss=7.2844
	step [71/249], loss=7.0097
	step [72/249], loss=5.8325
	step [73/249], loss=8.7444
	step [74/249], loss=6.1839
	step [75/249], loss=8.0173
	step [76/249], loss=6.0879
	step [77/249], loss=6.5287
	step [78/249], loss=5.8486
	step [79/249], loss=6.9185
	step [80/249], loss=7.7143
	step [81/249], loss=7.6396
	step [82/249], loss=8.9449
	step [83/249], loss=6.3948
	step [84/249], loss=6.5586
	step [85/249], loss=6.0711
	step [86/249], loss=6.2434
	step [87/249], loss=8.3902
	step [88/249], loss=9.1911
	step [89/249], loss=6.7337
	step [90/249], loss=6.8525
	step [91/249], loss=6.6464
	step [92/249], loss=6.1078
	step [93/249], loss=6.9344
	step [94/249], loss=6.2372
	step [95/249], loss=6.0215
	step [96/249], loss=5.8341
	step [97/249], loss=6.1001
	step [98/249], loss=6.5588
	step [99/249], loss=5.9754
	step [100/249], loss=6.5131
	step [101/249], loss=7.6694
	step [102/249], loss=7.2742
	step [103/249], loss=6.5161
	step [104/249], loss=6.6114
	step [105/249], loss=8.0323
	step [106/249], loss=6.7903
	step [107/249], loss=5.9357
	step [108/249], loss=7.4762
	step [109/249], loss=6.3747
	step [110/249], loss=5.8654
	step [111/249], loss=7.0244
	step [112/249], loss=7.1698
	step [113/249], loss=6.6385
	step [114/249], loss=6.2305
	step [115/249], loss=6.9657
	step [116/249], loss=7.6896
	step [117/249], loss=7.3695
	step [118/249], loss=6.9618
	step [119/249], loss=6.3957
	step [120/249], loss=6.4036
	step [121/249], loss=6.4928
	step [122/249], loss=7.5214
	step [123/249], loss=7.3273
	step [124/249], loss=5.8821
	step [125/249], loss=7.5494
	step [126/249], loss=9.3879
	step [127/249], loss=7.8437
	step [128/249], loss=6.7647
	step [129/249], loss=6.4799
	step [130/249], loss=7.0505
	step [131/249], loss=7.1622
	step [132/249], loss=6.2374
	step [133/249], loss=6.5242
	step [134/249], loss=5.8931
	step [135/249], loss=6.0995
	step [136/249], loss=7.8439
	step [137/249], loss=6.6860
	step [138/249], loss=6.6502
	step [139/249], loss=7.5126
	step [140/249], loss=5.8013
	step [141/249], loss=7.4905
	step [142/249], loss=6.5483
	step [143/249], loss=5.0637
	step [144/249], loss=7.5140
	step [145/249], loss=5.9778
	step [146/249], loss=6.6005
	step [147/249], loss=7.2524
	step [148/249], loss=7.1774
	step [149/249], loss=5.7826
	step [150/249], loss=7.4415
	step [151/249], loss=7.2817
	step [152/249], loss=6.8656
	step [153/249], loss=7.0869
	step [154/249], loss=5.7080
	step [155/249], loss=5.5467
	step [156/249], loss=9.1074
	step [157/249], loss=6.5730
	step [158/249], loss=6.6375
	step [159/249], loss=6.1868
	step [160/249], loss=5.9512
	step [161/249], loss=6.5385
	step [162/249], loss=7.4071
	step [163/249], loss=7.2713
	step [164/249], loss=7.1448
	step [165/249], loss=7.3379
	step [166/249], loss=7.3098
	step [167/249], loss=6.2054
	step [168/249], loss=7.1402
	step [169/249], loss=5.8579
	step [170/249], loss=6.6804
	step [171/249], loss=5.5563
	step [172/249], loss=6.9029
	step [173/249], loss=5.7380
	step [174/249], loss=8.9976
	step [175/249], loss=7.2244
	step [176/249], loss=6.3301
	step [177/249], loss=5.3356
	step [178/249], loss=8.4185
	step [179/249], loss=6.9653
	step [180/249], loss=6.6522
	step [181/249], loss=6.9381
	step [182/249], loss=6.6665
	step [183/249], loss=7.0762
	step [184/249], loss=6.1560
	step [185/249], loss=6.3642
	step [186/249], loss=7.2177
	step [187/249], loss=6.8128
	step [188/249], loss=5.6121
	step [189/249], loss=6.5063
	step [190/249], loss=6.5603
	step [191/249], loss=5.8305
	step [192/249], loss=7.8367
	step [193/249], loss=5.6891
	step [194/249], loss=7.0398
	step [195/249], loss=8.6154
	step [196/249], loss=6.7339
	step [197/249], loss=7.1765
	step [198/249], loss=6.1747
	step [199/249], loss=5.6397
	step [200/249], loss=7.0636
	step [201/249], loss=7.1923
	step [202/249], loss=5.7247
	step [203/249], loss=5.7382
	step [204/249], loss=7.2702
	step [205/249], loss=7.5044
	step [206/249], loss=7.2232
	step [207/249], loss=6.2539
	step [208/249], loss=6.5005
	step [209/249], loss=6.2265
	step [210/249], loss=6.4009
	step [211/249], loss=7.1757
	step [212/249], loss=6.4168
	step [213/249], loss=7.1174
	step [214/249], loss=7.8366
	step [215/249], loss=6.0390
	step [216/249], loss=6.6186
	step [217/249], loss=6.2026
	step [218/249], loss=6.9956
	step [219/249], loss=6.8603
	step [220/249], loss=6.7399
	step [221/249], loss=6.2807
	step [222/249], loss=6.8244
	step [223/249], loss=6.2461
	step [224/249], loss=6.4391
	step [225/249], loss=7.4200
	step [226/249], loss=6.9881
	step [227/249], loss=6.6888
	step [228/249], loss=5.1332
	step [229/249], loss=8.4156
	step [230/249], loss=6.8796
	step [231/249], loss=6.9658
	step [232/249], loss=7.1643
	step [233/249], loss=6.2858
	step [234/249], loss=6.4054
	step [235/249], loss=5.8146
	step [236/249], loss=7.1808
	step [237/249], loss=6.4780
	step [238/249], loss=7.4612
	step [239/249], loss=7.1231
	step [240/249], loss=5.9303
	step [241/249], loss=6.5876
	step [242/249], loss=5.5534
	step [243/249], loss=5.6372
	step [244/249], loss=7.4353
	step [245/249], loss=5.6766
	step [246/249], loss=6.2220
	step [247/249], loss=6.7536
	step [248/249], loss=6.1732
	step [249/249], loss=4.9311
	Evaluating
	loss=0.0211, precision=0.1771, recall=0.9942, f1=0.3006
Training epoch 20
	step [1/249], loss=6.0388
	step [2/249], loss=6.3512
	step [3/249], loss=6.7847
	step [4/249], loss=6.0442
	step [5/249], loss=6.8612
	step [6/249], loss=5.5859
	step [7/249], loss=5.9821
	step [8/249], loss=7.5930
	step [9/249], loss=5.9748
	step [10/249], loss=8.2209
	step [11/249], loss=5.9266
	step [12/249], loss=5.7666
	step [13/249], loss=5.9831
	step [14/249], loss=6.8354
	step [15/249], loss=6.2573
	step [16/249], loss=8.4003
	step [17/249], loss=5.6481
	step [18/249], loss=6.5137
	step [19/249], loss=6.9513
	step [20/249], loss=5.2500
	step [21/249], loss=7.4477
	step [22/249], loss=6.0117
	step [23/249], loss=6.8301
	step [24/249], loss=6.5218
	step [25/249], loss=6.4192
	step [26/249], loss=6.7934
	step [27/249], loss=6.7116
	step [28/249], loss=5.8659
	step [29/249], loss=6.0565
	step [30/249], loss=6.7697
	step [31/249], loss=7.7014
	step [32/249], loss=6.8206
	step [33/249], loss=6.9657
	step [34/249], loss=6.4302
	step [35/249], loss=5.6112
	step [36/249], loss=6.8792
	step [37/249], loss=7.3488
	step [38/249], loss=6.7325
	step [39/249], loss=5.8341
	step [40/249], loss=6.4386
	step [41/249], loss=5.0208
	step [42/249], loss=6.7496
	step [43/249], loss=6.3261
	step [44/249], loss=6.8274
	step [45/249], loss=7.1001
	step [46/249], loss=7.6972
	step [47/249], loss=6.5406
	step [48/249], loss=6.4450
	step [49/249], loss=6.6274
	step [50/249], loss=7.1887
	step [51/249], loss=8.1635
	step [52/249], loss=6.1840
	step [53/249], loss=8.7786
	step [54/249], loss=5.9135
	step [55/249], loss=7.1801
	step [56/249], loss=6.1892
	step [57/249], loss=5.4736
	step [58/249], loss=5.9071
	step [59/249], loss=5.6435
	step [60/249], loss=5.8314
	step [61/249], loss=5.8499
	step [62/249], loss=6.0776
	step [63/249], loss=5.8031
	step [64/249], loss=6.6708
	step [65/249], loss=5.6655
	step [66/249], loss=5.4977
	step [67/249], loss=7.3734
	step [68/249], loss=5.1011
	step [69/249], loss=6.2846
	step [70/249], loss=7.6209
	step [71/249], loss=6.4273
	step [72/249], loss=6.8736
	step [73/249], loss=7.0316
	step [74/249], loss=6.4665
	step [75/249], loss=5.9902
	step [76/249], loss=5.6098
	step [77/249], loss=5.5515
	step [78/249], loss=7.2528
	step [79/249], loss=6.1916
	step [80/249], loss=6.1193
	step [81/249], loss=6.6587
	step [82/249], loss=6.6078
	step [83/249], loss=6.3710
	step [84/249], loss=7.2581
	step [85/249], loss=7.4276
	step [86/249], loss=5.7459
	step [87/249], loss=6.2917
	step [88/249], loss=6.2385
	step [89/249], loss=5.6604
	step [90/249], loss=6.3636
	step [91/249], loss=7.6822
	step [92/249], loss=7.2465
	step [93/249], loss=8.2928
	step [94/249], loss=6.5424
	step [95/249], loss=6.0293
	step [96/249], loss=6.8194
	step [97/249], loss=6.1682
	step [98/249], loss=6.8260
	step [99/249], loss=6.7814
	step [100/249], loss=6.7002
	step [101/249], loss=7.0791
	step [102/249], loss=6.7605
	step [103/249], loss=7.3284
	step [104/249], loss=7.3215
	step [105/249], loss=6.2603
	step [106/249], loss=7.6728
	step [107/249], loss=6.6692
	step [108/249], loss=6.9249
	step [109/249], loss=5.4040
	step [110/249], loss=5.2475
	step [111/249], loss=7.0385
	step [112/249], loss=7.7899
	step [113/249], loss=7.1465
	step [114/249], loss=6.2383
	step [115/249], loss=6.1586
	step [116/249], loss=5.9772
	step [117/249], loss=7.2057
	step [118/249], loss=6.3519
	step [119/249], loss=7.8032
	step [120/249], loss=5.7432
	step [121/249], loss=5.7640
	step [122/249], loss=6.6131
	step [123/249], loss=5.3210
	step [124/249], loss=7.3237
	step [125/249], loss=5.8513
	step [126/249], loss=7.0502
	step [127/249], loss=5.7270
	step [128/249], loss=5.9741
	step [129/249], loss=6.0887
	step [130/249], loss=5.8771
	step [131/249], loss=6.3630
	step [132/249], loss=5.8913
	step [133/249], loss=5.3904
	step [134/249], loss=5.6869
	step [135/249], loss=7.5889
	step [136/249], loss=6.8025
	step [137/249], loss=6.8016
	step [138/249], loss=5.4208
	step [139/249], loss=6.2722
	step [140/249], loss=6.6428
	step [141/249], loss=6.2429
	step [142/249], loss=7.3900
	step [143/249], loss=7.1015
	step [144/249], loss=7.2782
	step [145/249], loss=7.1478
	step [146/249], loss=7.2321
	step [147/249], loss=6.2564
	step [148/249], loss=6.4172
	step [149/249], loss=6.3087
	step [150/249], loss=6.4609
	step [151/249], loss=6.6810
	step [152/249], loss=7.0801
	step [153/249], loss=6.8125
	step [154/249], loss=7.0142
	step [155/249], loss=5.9622
	step [156/249], loss=5.4343
	step [157/249], loss=5.9414
	step [158/249], loss=7.2289
	step [159/249], loss=7.2023
	step [160/249], loss=6.4610
	step [161/249], loss=7.5048
	step [162/249], loss=5.9467
	step [163/249], loss=5.9845
	step [164/249], loss=6.8294
	step [165/249], loss=6.4382
	step [166/249], loss=6.7037
	step [167/249], loss=6.8047
	step [168/249], loss=6.2417
	step [169/249], loss=6.4489
	step [170/249], loss=5.9769
	step [171/249], loss=5.4442
	step [172/249], loss=5.9008
	step [173/249], loss=6.1096
	step [174/249], loss=5.5791
	step [175/249], loss=6.8088
	step [176/249], loss=6.7151
	step [177/249], loss=5.0329
	step [178/249], loss=7.3528
	step [179/249], loss=6.2883
	step [180/249], loss=6.1310
	step [181/249], loss=5.8005
	step [182/249], loss=6.8599
	step [183/249], loss=5.5169
	step [184/249], loss=8.1061
	step [185/249], loss=7.0528
	step [186/249], loss=5.6400
	step [187/249], loss=8.1598
	step [188/249], loss=6.3073
	step [189/249], loss=6.2178
	step [190/249], loss=7.1121
	step [191/249], loss=6.5311
	step [192/249], loss=6.1297
	step [193/249], loss=5.8506
	step [194/249], loss=6.3189
	step [195/249], loss=6.6224
	step [196/249], loss=6.0962
	step [197/249], loss=7.4690
	step [198/249], loss=6.5750
	step [199/249], loss=7.2481
	step [200/249], loss=6.1461
	step [201/249], loss=5.7418
	step [202/249], loss=6.9728
	step [203/249], loss=7.6111
	step [204/249], loss=6.6811
	step [205/249], loss=7.7593
	step [206/249], loss=6.8382
	step [207/249], loss=6.0381
	step [208/249], loss=5.8844
	step [209/249], loss=5.7917
	step [210/249], loss=5.6747
	step [211/249], loss=6.1853
	step [212/249], loss=6.4135
	step [213/249], loss=6.5810
	step [214/249], loss=7.6320
	step [215/249], loss=6.0798
	step [216/249], loss=6.7967
	step [217/249], loss=6.3332
	step [218/249], loss=6.1443
	step [219/249], loss=6.2295
	step [220/249], loss=6.7038
	step [221/249], loss=6.6679
	step [222/249], loss=6.3289
	step [223/249], loss=5.8088
	step [224/249], loss=6.3113
	step [225/249], loss=6.2957
	step [226/249], loss=5.4630
	step [227/249], loss=7.6613
	step [228/249], loss=6.4977
	step [229/249], loss=5.4044
	step [230/249], loss=6.2123
	step [231/249], loss=6.8291
	step [232/249], loss=6.5866
	step [233/249], loss=6.4060
	step [234/249], loss=6.0388
	step [235/249], loss=6.1366
	step [236/249], loss=6.2090
	step [237/249], loss=5.6574
	step [238/249], loss=5.6174
	step [239/249], loss=6.7894
	step [240/249], loss=6.5258
	step [241/249], loss=6.8775
	step [242/249], loss=6.7234
	step [243/249], loss=6.6303
	step [244/249], loss=6.1795
	step [245/249], loss=7.9432
	step [246/249], loss=7.0681
	step [247/249], loss=5.7221
	step [248/249], loss=6.4556
	step [249/249], loss=4.0714
	Evaluating
	loss=0.0228, precision=0.1569, recall=0.9948, f1=0.2710
Training epoch 21
	step [1/249], loss=5.2676
	step [2/249], loss=4.7250
	step [3/249], loss=6.0392
	step [4/249], loss=6.7287
	step [5/249], loss=5.9895
	step [6/249], loss=5.3937
	step [7/249], loss=6.6996
	step [8/249], loss=8.3201
	step [9/249], loss=5.5437
	step [10/249], loss=6.3820
	step [11/249], loss=6.5634
	step [12/249], loss=5.6775
	step [13/249], loss=6.0816
	step [14/249], loss=5.5606
	step [15/249], loss=5.7458
	step [16/249], loss=5.4990
	step [17/249], loss=6.3810
	step [18/249], loss=5.9694
	step [19/249], loss=6.2946
	step [20/249], loss=6.7575
	step [21/249], loss=7.0026
	step [22/249], loss=6.1433
	step [23/249], loss=5.9930
	step [24/249], loss=6.5441
	step [25/249], loss=5.7937
	step [26/249], loss=6.9104
	step [27/249], loss=5.3391
	step [28/249], loss=7.0199
	step [29/249], loss=6.8360
	step [30/249], loss=7.3928
	step [31/249], loss=6.7043
	step [32/249], loss=6.6308
	step [33/249], loss=5.3357
	step [34/249], loss=6.0204
	step [35/249], loss=6.0463
	step [36/249], loss=5.5383
	step [37/249], loss=5.0420
	step [38/249], loss=5.7290
	step [39/249], loss=6.1853
	step [40/249], loss=6.5130
	step [41/249], loss=8.1298
	step [42/249], loss=5.9846
	step [43/249], loss=7.0911
	step [44/249], loss=6.2468
	step [45/249], loss=6.4162
	step [46/249], loss=6.4444
	step [47/249], loss=6.7871
	step [48/249], loss=7.6772
	step [49/249], loss=7.3984
	step [50/249], loss=6.1579
	step [51/249], loss=7.0894
	step [52/249], loss=6.4039
	step [53/249], loss=5.6429
	step [54/249], loss=7.9339
	step [55/249], loss=5.1466
	step [56/249], loss=7.4577
	step [57/249], loss=5.5660
	step [58/249], loss=5.7693
	step [59/249], loss=7.6338
	step [60/249], loss=7.2725
	step [61/249], loss=6.0707
	step [62/249], loss=5.9597
	step [63/249], loss=5.6998
	step [64/249], loss=6.1254
	step [65/249], loss=5.3539
	step [66/249], loss=6.7522
	step [67/249], loss=6.5599
	step [68/249], loss=7.1194
	step [69/249], loss=6.2170
	step [70/249], loss=6.4315
	step [71/249], loss=6.6960
	step [72/249], loss=8.3799
	step [73/249], loss=7.2227
	step [74/249], loss=6.6457
	step [75/249], loss=6.7230
	step [76/249], loss=6.1054
	step [77/249], loss=6.9617
	step [78/249], loss=6.3479
	step [79/249], loss=5.0786
	step [80/249], loss=6.3669
	step [81/249], loss=6.4209
	step [82/249], loss=6.0235
	step [83/249], loss=7.0259
	step [84/249], loss=5.5105
	step [85/249], loss=6.0366
	step [86/249], loss=6.0333
	step [87/249], loss=6.4263
	step [88/249], loss=7.4986
	step [89/249], loss=6.6634
	step [90/249], loss=5.5251
	step [91/249], loss=5.5749
	step [92/249], loss=5.7298
	step [93/249], loss=6.1600
	step [94/249], loss=6.3397
	step [95/249], loss=7.7198
	step [96/249], loss=6.7899
	step [97/249], loss=6.3150
	step [98/249], loss=7.2185
	step [99/249], loss=6.2428
	step [100/249], loss=6.1233
	step [101/249], loss=6.8123
	step [102/249], loss=5.3543
	step [103/249], loss=6.7670
	step [104/249], loss=6.3820
	step [105/249], loss=5.4492
	step [106/249], loss=6.3986
	step [107/249], loss=6.3890
	step [108/249], loss=7.5774
	step [109/249], loss=7.0061
	step [110/249], loss=6.0354
	step [111/249], loss=6.5132
	step [112/249], loss=6.6842
	step [113/249], loss=6.4563
	step [114/249], loss=5.3661
	step [115/249], loss=7.4249
	step [116/249], loss=6.3738
	step [117/249], loss=7.3727
	step [118/249], loss=6.6589
	step [119/249], loss=7.1516
	step [120/249], loss=7.2487
	step [121/249], loss=7.0887
	step [122/249], loss=5.0016
	step [123/249], loss=6.7040
	step [124/249], loss=6.4571
	step [125/249], loss=5.7172
	step [126/249], loss=5.3086
	step [127/249], loss=5.9643
	step [128/249], loss=6.7706
	step [129/249], loss=5.0398
	step [130/249], loss=5.6740
	step [131/249], loss=5.4425
	step [132/249], loss=5.7590
	step [133/249], loss=6.6629
	step [134/249], loss=7.1719
	step [135/249], loss=5.5837
	step [136/249], loss=5.7589
	step [137/249], loss=5.5940
	step [138/249], loss=5.7477
	step [139/249], loss=6.2257
	step [140/249], loss=5.3894
	step [141/249], loss=6.3367
	step [142/249], loss=7.5277
	step [143/249], loss=5.9090
	step [144/249], loss=6.9609
	step [145/249], loss=6.8689
	step [146/249], loss=6.2746
	step [147/249], loss=6.3866
	step [148/249], loss=6.6616
	step [149/249], loss=6.0804
	step [150/249], loss=6.3223
	step [151/249], loss=7.2336
	step [152/249], loss=8.4178
	step [153/249], loss=7.0774
	step [154/249], loss=6.5622
	step [155/249], loss=6.3188
	step [156/249], loss=5.2990
	step [157/249], loss=5.3922
	step [158/249], loss=6.8245
	step [159/249], loss=6.2384
	step [160/249], loss=5.8950
	step [161/249], loss=6.5377
	step [162/249], loss=6.2331
	step [163/249], loss=7.7384
	step [164/249], loss=6.1586
	step [165/249], loss=5.2101
	step [166/249], loss=5.6119
	step [167/249], loss=5.7080
	step [168/249], loss=6.2484
	step [169/249], loss=5.6179
	step [170/249], loss=6.1644
	step [171/249], loss=6.1185
	step [172/249], loss=6.0465
	step [173/249], loss=5.8729
	step [174/249], loss=6.3418
	step [175/249], loss=6.2169
	step [176/249], loss=7.1744
	step [177/249], loss=6.1908
	step [178/249], loss=7.1332
	step [179/249], loss=6.7572
	step [180/249], loss=7.3304
	step [181/249], loss=5.9789
	step [182/249], loss=6.3247
	step [183/249], loss=5.5276
	step [184/249], loss=7.6158
	step [185/249], loss=5.4047
	step [186/249], loss=7.0080
	step [187/249], loss=5.6176
	step [188/249], loss=5.4561
	step [189/249], loss=6.1339
	step [190/249], loss=5.8548
	step [191/249], loss=5.1113
	step [192/249], loss=5.3642
	step [193/249], loss=5.8930
	step [194/249], loss=6.3136
	step [195/249], loss=7.1002
	step [196/249], loss=6.9913
	step [197/249], loss=7.0122
	step [198/249], loss=5.0045
	step [199/249], loss=5.9884
	step [200/249], loss=5.2637
	step [201/249], loss=5.5677
	step [202/249], loss=6.6590
	step [203/249], loss=7.4166
	step [204/249], loss=7.3616
	step [205/249], loss=6.0543
	step [206/249], loss=5.7278
	step [207/249], loss=6.7547
	step [208/249], loss=6.1189
	step [209/249], loss=7.4288
	step [210/249], loss=6.2928
	step [211/249], loss=6.4848
	step [212/249], loss=6.2662
	step [213/249], loss=7.1712
	step [214/249], loss=7.3873
	step [215/249], loss=5.5778
	step [216/249], loss=7.0332
	step [217/249], loss=7.8575
	step [218/249], loss=5.5793
	step [219/249], loss=6.1256
	step [220/249], loss=6.7105
	step [221/249], loss=5.6059
	step [222/249], loss=5.9639
	step [223/249], loss=5.7801
	step [224/249], loss=6.0697
	step [225/249], loss=7.9884
	step [226/249], loss=6.3332
	step [227/249], loss=5.4439
	step [228/249], loss=6.4320
	step [229/249], loss=7.3517
	step [230/249], loss=6.9427
	step [231/249], loss=5.1462
	step [232/249], loss=5.9770
	step [233/249], loss=6.5097
	step [234/249], loss=6.3665
	step [235/249], loss=6.0384
	step [236/249], loss=6.4678
	step [237/249], loss=7.7463
	step [238/249], loss=6.4099
	step [239/249], loss=5.8076
	step [240/249], loss=6.4817
	step [241/249], loss=5.8425
	step [242/249], loss=5.9223
	step [243/249], loss=6.1538
	step [244/249], loss=6.2675
	step [245/249], loss=5.8688
	step [246/249], loss=5.3279
	step [247/249], loss=5.8924
	step [248/249], loss=5.6723
	step [249/249], loss=2.7236
	Evaluating
	loss=0.0149, precision=0.2217, recall=0.9940, f1=0.3625
saving model as: 0_saved_model.pth
Training epoch 22
	step [1/249], loss=4.0999
	step [2/249], loss=6.3157
	step [3/249], loss=7.2932
	step [4/249], loss=5.6024
	step [5/249], loss=6.1593
	step [6/249], loss=6.8760
	step [7/249], loss=6.6304
	step [8/249], loss=6.6287
	step [9/249], loss=5.4015
	step [10/249], loss=4.9036
	step [11/249], loss=5.3460
	step [12/249], loss=5.2041
	step [13/249], loss=6.7542
	step [14/249], loss=5.7398
	step [15/249], loss=7.3254
	step [16/249], loss=5.8136
	step [17/249], loss=5.9749
	step [18/249], loss=5.8352
	step [19/249], loss=8.0031
	step [20/249], loss=5.9140
	step [21/249], loss=6.0924
	step [22/249], loss=7.1727
	step [23/249], loss=6.1486
	step [24/249], loss=5.1081
	step [25/249], loss=7.1556
	step [26/249], loss=5.6157
	step [27/249], loss=6.1299
	step [28/249], loss=6.7688
	step [29/249], loss=6.4152
	step [30/249], loss=5.5658
	step [31/249], loss=7.1629
	step [32/249], loss=5.6914
	step [33/249], loss=6.0859
	step [34/249], loss=6.6303
	step [35/249], loss=6.5467
	step [36/249], loss=7.7738
	step [37/249], loss=6.1896
	step [38/249], loss=5.8904
	step [39/249], loss=6.6841
	step [40/249], loss=5.6112
	step [41/249], loss=5.6431
	step [42/249], loss=6.3991
	step [43/249], loss=6.7797
	step [44/249], loss=5.7400
	step [45/249], loss=5.7850
	step [46/249], loss=7.0333
	step [47/249], loss=6.7153
	step [48/249], loss=6.1936
	step [49/249], loss=6.5353
	step [50/249], loss=5.1876
	step [51/249], loss=5.5432
	step [52/249], loss=5.5359
	step [53/249], loss=5.1654
	step [54/249], loss=7.7809
	step [55/249], loss=8.6946
	step [56/249], loss=5.0445
	step [57/249], loss=6.4718
	step [58/249], loss=4.9905
	step [59/249], loss=7.1347
	step [60/249], loss=7.3626
	step [61/249], loss=5.5635
	step [62/249], loss=6.5179
	step [63/249], loss=5.4125
	step [64/249], loss=6.2320
	step [65/249], loss=5.4354
	step [66/249], loss=6.3226
	step [67/249], loss=7.3623
	step [68/249], loss=5.8063
	step [69/249], loss=5.7796
	step [70/249], loss=6.0189
	step [71/249], loss=5.6013
	step [72/249], loss=5.3377
	step [73/249], loss=4.8661
	step [74/249], loss=6.0588
	step [75/249], loss=5.6591
	step [76/249], loss=6.1492
	step [77/249], loss=5.5983
	step [78/249], loss=5.0464
	step [79/249], loss=7.4028
	step [80/249], loss=7.0869
	step [81/249], loss=6.5428
	step [82/249], loss=6.2401
	step [83/249], loss=7.3221
	step [84/249], loss=7.3816
	step [85/249], loss=6.1780
	step [86/249], loss=7.2851
	step [87/249], loss=7.0053
	step [88/249], loss=6.2681
	step [89/249], loss=7.8157
	step [90/249], loss=7.1044
	step [91/249], loss=5.6974
	step [92/249], loss=6.1602
	step [93/249], loss=5.9442
	step [94/249], loss=5.9928
	step [95/249], loss=6.5191
	step [96/249], loss=5.8063
	step [97/249], loss=5.7494
	step [98/249], loss=5.5982
	step [99/249], loss=5.2722
	step [100/249], loss=5.5081
	step [101/249], loss=7.6071
	step [102/249], loss=6.1292
	step [103/249], loss=6.2802
	step [104/249], loss=8.0410
	step [105/249], loss=6.3345
	step [106/249], loss=6.7699
	step [107/249], loss=8.0431
	step [108/249], loss=5.9513
	step [109/249], loss=5.5036
	step [110/249], loss=5.5522
	step [111/249], loss=7.3259
	step [112/249], loss=5.1134
	step [113/249], loss=6.2977
	step [114/249], loss=5.7138
	step [115/249], loss=5.6010
	step [116/249], loss=6.8029
	step [117/249], loss=4.9568
	step [118/249], loss=6.5348
	step [119/249], loss=6.4446
	step [120/249], loss=5.0855
	step [121/249], loss=6.0259
	step [122/249], loss=7.8962
	step [123/249], loss=5.8645
	step [124/249], loss=5.5205
	step [125/249], loss=5.7556
	step [126/249], loss=6.0303
	step [127/249], loss=6.9956
	step [128/249], loss=7.2051
	step [129/249], loss=6.1088
	step [130/249], loss=5.8497
	step [131/249], loss=4.6705
	step [132/249], loss=6.4574
	step [133/249], loss=5.8605
	step [134/249], loss=6.5150
	step [135/249], loss=6.2302
	step [136/249], loss=5.9714
	step [137/249], loss=4.9806
	step [138/249], loss=5.8956
	step [139/249], loss=5.7796
	step [140/249], loss=7.0627
	step [141/249], loss=6.1017
	step [142/249], loss=5.2879
	step [143/249], loss=6.4413
	step [144/249], loss=6.0622
	step [145/249], loss=5.0432
	step [146/249], loss=5.1829
	step [147/249], loss=5.9472
	step [148/249], loss=6.2508
	step [149/249], loss=5.0707
	step [150/249], loss=6.8928
	step [151/249], loss=5.3405
	step [152/249], loss=6.0198
	step [153/249], loss=5.3416
	step [154/249], loss=5.7120
	step [155/249], loss=4.7307
	step [156/249], loss=6.4660
	step [157/249], loss=6.6224
	step [158/249], loss=6.5088
	step [159/249], loss=6.8969
	step [160/249], loss=6.3548
	step [161/249], loss=4.5654
	step [162/249], loss=5.3870
	step [163/249], loss=6.3485
	step [164/249], loss=4.9950
	step [165/249], loss=5.8364
	step [166/249], loss=6.2243
	step [167/249], loss=6.3999
	step [168/249], loss=5.6567
	step [169/249], loss=6.2730
	step [170/249], loss=6.3239
	step [171/249], loss=5.3595
	step [172/249], loss=6.2345
	step [173/249], loss=6.2944
	step [174/249], loss=5.7375
	step [175/249], loss=5.6490
	step [176/249], loss=8.3641
	step [177/249], loss=5.5678
	step [178/249], loss=5.3431
	step [179/249], loss=6.8588
	step [180/249], loss=6.4061
	step [181/249], loss=5.3373
	step [182/249], loss=6.9205
	step [183/249], loss=6.8401
	step [184/249], loss=6.0189
	step [185/249], loss=5.3941
	step [186/249], loss=6.6134
	step [187/249], loss=5.7677
	step [188/249], loss=6.0034
	step [189/249], loss=5.9635
	step [190/249], loss=6.1990
	step [191/249], loss=5.7996
	step [192/249], loss=6.9179
	step [193/249], loss=7.7659
	step [194/249], loss=6.7912
	step [195/249], loss=5.7177
	step [196/249], loss=5.9486
	step [197/249], loss=5.8733
	step [198/249], loss=7.2545
	step [199/249], loss=6.0629
	step [200/249], loss=6.4377
	step [201/249], loss=6.5900
	step [202/249], loss=6.1113
	step [203/249], loss=6.1129
	step [204/249], loss=5.6333
	step [205/249], loss=5.6097
	step [206/249], loss=6.7080
	step [207/249], loss=7.2496
	step [208/249], loss=5.5044
	step [209/249], loss=5.3387
	step [210/249], loss=5.6363
	step [211/249], loss=6.1211
	step [212/249], loss=4.6645
	step [213/249], loss=6.3160
	step [214/249], loss=6.3323
	step [215/249], loss=6.3220
	step [216/249], loss=5.8832
	step [217/249], loss=5.7030
	step [218/249], loss=5.1475
	step [219/249], loss=5.4013
	step [220/249], loss=6.7121
	step [221/249], loss=6.6426
	step [222/249], loss=6.1008
	step [223/249], loss=6.2701
	step [224/249], loss=5.8693
	step [225/249], loss=6.2406
	step [226/249], loss=5.8417
	step [227/249], loss=5.3941
	step [228/249], loss=6.3825
	step [229/249], loss=8.9946
	step [230/249], loss=5.5191
	step [231/249], loss=5.0577
	step [232/249], loss=6.3034
	step [233/249], loss=6.4615
	step [234/249], loss=7.3300
	step [235/249], loss=5.9786
	step [236/249], loss=6.7228
	step [237/249], loss=5.5603
	step [238/249], loss=6.9103
	step [239/249], loss=7.5352
	step [240/249], loss=6.5767
	step [241/249], loss=6.9893
	step [242/249], loss=6.5082
	step [243/249], loss=5.9892
	step [244/249], loss=5.5127
	step [245/249], loss=6.2004
	step [246/249], loss=6.1964
	step [247/249], loss=5.3581
	step [248/249], loss=5.8427
	step [249/249], loss=4.4341
	Evaluating
	loss=0.0138, precision=0.2413, recall=0.9927, f1=0.3882
saving model as: 0_saved_model.pth
Training epoch 23
	step [1/249], loss=6.1061
	step [2/249], loss=6.6169
	step [3/249], loss=6.1000
	step [4/249], loss=5.5025
	step [5/249], loss=5.2105
	step [6/249], loss=5.7116
	step [7/249], loss=5.6055
	step [8/249], loss=6.6539
	step [9/249], loss=5.5527
	step [10/249], loss=5.3787
	step [11/249], loss=5.7800
	step [12/249], loss=6.8670
	step [13/249], loss=6.2060
	step [14/249], loss=6.2591
	step [15/249], loss=6.0319
	step [16/249], loss=6.0458
	step [17/249], loss=5.6472
	step [18/249], loss=5.2182
	step [19/249], loss=6.3452
	step [20/249], loss=5.4134
	step [21/249], loss=6.4382
	step [22/249], loss=5.7862
	step [23/249], loss=7.7296
	step [24/249], loss=7.1641
	step [25/249], loss=5.9844
	step [26/249], loss=5.9284
	step [27/249], loss=6.3334
	step [28/249], loss=5.8648
	step [29/249], loss=4.4278
	step [30/249], loss=5.4688
	step [31/249], loss=5.2624
	step [32/249], loss=7.2554
	step [33/249], loss=6.9580
	step [34/249], loss=5.1753
	step [35/249], loss=6.2314
	step [36/249], loss=5.6639
	step [37/249], loss=5.7959
	step [38/249], loss=5.9539
	step [39/249], loss=6.4243
	step [40/249], loss=5.5993
	step [41/249], loss=5.2937
	step [42/249], loss=6.2999
	step [43/249], loss=6.8664
	step [44/249], loss=6.3209
	step [45/249], loss=5.3992
	step [46/249], loss=7.3190
	step [47/249], loss=6.0294
	step [48/249], loss=6.1775
	step [49/249], loss=7.0401
	step [50/249], loss=5.2434
	step [51/249], loss=6.0991
	step [52/249], loss=6.9161
	step [53/249], loss=5.0994
	step [54/249], loss=5.6554
	step [55/249], loss=4.9446
	step [56/249], loss=6.1633
	step [57/249], loss=6.7659
	step [58/249], loss=5.6993
	step [59/249], loss=5.8610
	step [60/249], loss=5.9049
	step [61/249], loss=5.6795
	step [62/249], loss=5.3615
	step [63/249], loss=5.7111
	step [64/249], loss=4.5639
	step [65/249], loss=5.3900
	step [66/249], loss=6.9629
	step [67/249], loss=5.1835
	step [68/249], loss=5.7874
	step [69/249], loss=4.7377
	step [70/249], loss=5.4573
	step [71/249], loss=5.2117
	step [72/249], loss=6.7694
	step [73/249], loss=5.3178
	step [74/249], loss=5.4559
	step [75/249], loss=6.2348
	step [76/249], loss=6.3702
	step [77/249], loss=5.5743
	step [78/249], loss=5.8237
	step [79/249], loss=7.5325
	step [80/249], loss=6.1425
	step [81/249], loss=4.8602
	step [82/249], loss=6.6290
	step [83/249], loss=5.9000
	step [84/249], loss=5.8441
	step [85/249], loss=6.1912
	step [86/249], loss=5.2760
	step [87/249], loss=5.3994
	step [88/249], loss=5.7713
	step [89/249], loss=4.6616
	step [90/249], loss=5.9599
	step [91/249], loss=5.3708
	step [92/249], loss=6.3332
	step [93/249], loss=5.9767
	step [94/249], loss=5.7924
	step [95/249], loss=6.3012
	step [96/249], loss=5.5475
	step [97/249], loss=5.7217
	step [98/249], loss=6.1933
	step [99/249], loss=6.0661
	step [100/249], loss=6.4589
	step [101/249], loss=6.4059
	step [102/249], loss=6.2302
	step [103/249], loss=6.2771
	step [104/249], loss=6.5913
	step [105/249], loss=6.0960
	step [106/249], loss=5.9633
	step [107/249], loss=5.3609
	step [108/249], loss=5.4978
	step [109/249], loss=4.8665
	step [110/249], loss=5.1416
	step [111/249], loss=5.6700
	step [112/249], loss=5.9999
	step [113/249], loss=6.8724
	step [114/249], loss=5.5668
	step [115/249], loss=6.8091
	step [116/249], loss=5.9166
	step [117/249], loss=6.8577
	step [118/249], loss=6.9381
	step [119/249], loss=5.6690
	step [120/249], loss=5.8004
	step [121/249], loss=5.6004
	step [122/249], loss=4.7836
	step [123/249], loss=6.0743
	step [124/249], loss=5.7440
	step [125/249], loss=6.3615
	step [126/249], loss=7.6989
	step [127/249], loss=6.6155
	step [128/249], loss=5.6296
	step [129/249], loss=5.4116
	step [130/249], loss=6.4056
	step [131/249], loss=5.5889
	step [132/249], loss=6.8974
	step [133/249], loss=6.4953
	step [134/249], loss=6.2614
	step [135/249], loss=5.3004
	step [136/249], loss=4.9219
	step [137/249], loss=5.6869
	step [138/249], loss=6.9810
	step [139/249], loss=5.3998
	step [140/249], loss=5.5375
	step [141/249], loss=6.3064
	step [142/249], loss=5.9869
	step [143/249], loss=5.9054
	step [144/249], loss=5.7641
	step [145/249], loss=5.5874
	step [146/249], loss=6.4942
	step [147/249], loss=6.0659
	step [148/249], loss=6.3105
	step [149/249], loss=6.6004
	step [150/249], loss=5.3485
	step [151/249], loss=5.7066
	step [152/249], loss=5.3071
	step [153/249], loss=5.9545
	step [154/249], loss=5.7800
	step [155/249], loss=6.5665
	step [156/249], loss=6.1485
	step [157/249], loss=5.6328
	step [158/249], loss=6.3696
	step [159/249], loss=6.3101
	step [160/249], loss=6.4888
	step [161/249], loss=5.7437
	step [162/249], loss=6.3055
	step [163/249], loss=4.7424
	step [164/249], loss=6.2472
	step [165/249], loss=6.0741
	step [166/249], loss=5.5507
	step [167/249], loss=6.0422
	step [168/249], loss=5.9138
	step [169/249], loss=6.3401
	step [170/249], loss=5.6471
	step [171/249], loss=5.8295
	step [172/249], loss=5.8739
	step [173/249], loss=5.4945
	step [174/249], loss=6.2813
	step [175/249], loss=4.5914
	step [176/249], loss=5.9225
	step [177/249], loss=4.9631
	step [178/249], loss=5.5036
	step [179/249], loss=5.7878
	step [180/249], loss=6.4706
	step [181/249], loss=4.6299
	step [182/249], loss=4.6559
	step [183/249], loss=5.4043
	step [184/249], loss=5.7437
	step [185/249], loss=5.1046
	step [186/249], loss=6.4807
	step [187/249], loss=5.6616
	step [188/249], loss=5.8722
	step [189/249], loss=5.3957
	step [190/249], loss=6.2782
	step [191/249], loss=6.0294
	step [192/249], loss=5.5727
	step [193/249], loss=5.6919
	step [194/249], loss=5.0072
	step [195/249], loss=5.5358
	step [196/249], loss=5.5930
	step [197/249], loss=7.6616
	step [198/249], loss=5.6920
	step [199/249], loss=5.4666
	step [200/249], loss=7.0531
	step [201/249], loss=6.8887
	step [202/249], loss=6.0564
	step [203/249], loss=5.7111
	step [204/249], loss=7.6683
	step [205/249], loss=6.0004
	step [206/249], loss=5.8784
	step [207/249], loss=6.2126
	step [208/249], loss=6.1644
	step [209/249], loss=5.7371
	step [210/249], loss=6.2428
	step [211/249], loss=5.7600
	step [212/249], loss=6.2218
	step [213/249], loss=5.6434
	step [214/249], loss=5.7703
	step [215/249], loss=6.4399
	step [216/249], loss=5.9435
	step [217/249], loss=5.4732
	step [218/249], loss=5.3847
	step [219/249], loss=5.9750
	step [220/249], loss=6.5193
	step [221/249], loss=5.8064
	step [222/249], loss=6.7447
	step [223/249], loss=5.6906
	step [224/249], loss=6.3748
	step [225/249], loss=5.0497
	step [226/249], loss=5.7551
	step [227/249], loss=5.8603
	step [228/249], loss=3.8881
	step [229/249], loss=5.3890
	step [230/249], loss=6.1431
	step [231/249], loss=6.5834
	step [232/249], loss=5.9426
	step [233/249], loss=6.2927
	step [234/249], loss=6.9176
	step [235/249], loss=5.9008
	step [236/249], loss=5.4091
	step [237/249], loss=5.6361
	step [238/249], loss=6.3383
	step [239/249], loss=5.3000
	step [240/249], loss=6.4716
	step [241/249], loss=6.3246
	step [242/249], loss=5.7355
	step [243/249], loss=5.6448
	step [244/249], loss=4.9485
	step [245/249], loss=6.0808
	step [246/249], loss=5.5655
	step [247/249], loss=5.4752
	step [248/249], loss=6.3276
	step [249/249], loss=4.3573
	Evaluating
	loss=0.0190, precision=0.1962, recall=0.9934, f1=0.3277
Training epoch 24
	step [1/249], loss=5.1703
	step [2/249], loss=5.4231
	step [3/249], loss=5.8942
	step [4/249], loss=4.6430
	step [5/249], loss=4.9567
	step [6/249], loss=5.2491
	step [7/249], loss=6.9114
	step [8/249], loss=6.0708
	step [9/249], loss=6.2545
	step [10/249], loss=5.5583
	step [11/249], loss=5.7965
	step [12/249], loss=5.3909
	step [13/249], loss=5.5479
	step [14/249], loss=5.5756
	step [15/249], loss=5.2545
	step [16/249], loss=5.3502
	step [17/249], loss=5.2728
	step [18/249], loss=5.4094
	step [19/249], loss=6.0376
	step [20/249], loss=5.7158
	step [21/249], loss=6.0349
	step [22/249], loss=5.4312
	step [23/249], loss=6.2085
	step [24/249], loss=5.0537
	step [25/249], loss=7.4336
	step [26/249], loss=5.9253
	step [27/249], loss=5.9286
	step [28/249], loss=5.6106
	step [29/249], loss=5.4112
	step [30/249], loss=6.9735
	step [31/249], loss=6.1320
	step [32/249], loss=6.3559
	step [33/249], loss=7.1491
	step [34/249], loss=5.3068
	step [35/249], loss=6.4734
	step [36/249], loss=6.0784
	step [37/249], loss=5.3937
	step [38/249], loss=5.8299
	step [39/249], loss=5.1771
	step [40/249], loss=6.4644
	step [41/249], loss=5.6530
	step [42/249], loss=4.9653
	step [43/249], loss=5.9722
	step [44/249], loss=6.6649
	step [45/249], loss=6.0954
	step [46/249], loss=6.5603
	step [47/249], loss=4.9253
	step [48/249], loss=5.2333
	step [49/249], loss=5.2817
	step [50/249], loss=7.0388
	step [51/249], loss=6.2058
	step [52/249], loss=5.5963
	step [53/249], loss=6.3412
	step [54/249], loss=5.1270
	step [55/249], loss=4.7856
	step [56/249], loss=6.1200
	step [57/249], loss=5.4744
	step [58/249], loss=6.0890
	step [59/249], loss=5.8136
	step [60/249], loss=5.5829
	step [61/249], loss=6.2662
	step [62/249], loss=6.4816
	step [63/249], loss=6.2162
	step [64/249], loss=7.0024
	step [65/249], loss=6.4071
	step [66/249], loss=5.6284
	step [67/249], loss=5.4543
	step [68/249], loss=7.0558
	step [69/249], loss=4.6097
	step [70/249], loss=8.0719
	step [71/249], loss=5.8426
	step [72/249], loss=5.3376
	step [73/249], loss=5.4662
	step [74/249], loss=6.0486
	step [75/249], loss=4.8125
	step [76/249], loss=5.5122
	step [77/249], loss=5.6507
	step [78/249], loss=5.3881
	step [79/249], loss=5.4565
	step [80/249], loss=6.2351
	step [81/249], loss=4.8048
	step [82/249], loss=4.6841
	step [83/249], loss=6.2315
	step [84/249], loss=5.4327
	step [85/249], loss=6.1683
	step [86/249], loss=5.7278
	step [87/249], loss=5.6988
	step [88/249], loss=6.2545
	step [89/249], loss=5.4383
	step [90/249], loss=5.0689
	step [91/249], loss=4.8138
	step [92/249], loss=5.9384
	step [93/249], loss=5.1518
	step [94/249], loss=5.4980
	step [95/249], loss=6.4351
	step [96/249], loss=4.8137
	step [97/249], loss=5.7528
	step [98/249], loss=5.1327
	step [99/249], loss=6.0288
	step [100/249], loss=6.0197
	step [101/249], loss=5.5104
	step [102/249], loss=5.4398
	step [103/249], loss=6.1944
	step [104/249], loss=5.9180
	step [105/249], loss=5.5519
	step [106/249], loss=5.0204
	step [107/249], loss=5.5786
	step [108/249], loss=6.7243
	step [109/249], loss=5.6486
	step [110/249], loss=5.6948
	step [111/249], loss=5.5093
	step [112/249], loss=4.9716
	step [113/249], loss=5.6654
	step [114/249], loss=4.8543
	step [115/249], loss=4.7053
	step [116/249], loss=5.3988
	step [117/249], loss=4.6710
	step [118/249], loss=6.1725
	step [119/249], loss=6.2419
	step [120/249], loss=5.3021
	step [121/249], loss=6.3803
	step [122/249], loss=5.4144
	step [123/249], loss=6.7652
	step [124/249], loss=5.9035
	step [125/249], loss=5.5751
	step [126/249], loss=4.9819
	step [127/249], loss=6.2755
	step [128/249], loss=6.1259
	step [129/249], loss=6.5441
	step [130/249], loss=6.4048
	step [131/249], loss=6.6605
	step [132/249], loss=5.6968
	step [133/249], loss=5.8139
	step [134/249], loss=6.6758
	step [135/249], loss=5.8449
	step [136/249], loss=6.1145
	step [137/249], loss=5.3352
	step [138/249], loss=6.5839
	step [139/249], loss=6.2211
	step [140/249], loss=6.4659
	step [141/249], loss=5.7038
	step [142/249], loss=5.8086
	step [143/249], loss=5.1243
	step [144/249], loss=7.0755
	step [145/249], loss=4.6333
	step [146/249], loss=5.6956
	step [147/249], loss=5.4217
	step [148/249], loss=5.7745
	step [149/249], loss=5.4546
	step [150/249], loss=5.4708
	step [151/249], loss=5.0134
	step [152/249], loss=5.4983
	step [153/249], loss=5.7388
	step [154/249], loss=4.7426
	step [155/249], loss=5.3321
	step [156/249], loss=5.3338
	step [157/249], loss=8.0218
	step [158/249], loss=6.4347
	step [159/249], loss=6.1107
	step [160/249], loss=6.0876
	step [161/249], loss=4.9363
	step [162/249], loss=5.7768
	step [163/249], loss=5.6938
	step [164/249], loss=5.7651
	step [165/249], loss=6.1359
	step [166/249], loss=4.6825
	step [167/249], loss=6.0632
	step [168/249], loss=6.8886
	step [169/249], loss=5.7742
	step [170/249], loss=6.1227
	step [171/249], loss=5.5179
	step [172/249], loss=5.3146
	step [173/249], loss=4.6696
	step [174/249], loss=5.5069
	step [175/249], loss=5.9913
	step [176/249], loss=6.2905
	step [177/249], loss=5.6619
	step [178/249], loss=6.1145
	step [179/249], loss=6.0412
	step [180/249], loss=5.4162
	step [181/249], loss=5.3442
	step [182/249], loss=5.8958
	step [183/249], loss=5.0228
	step [184/249], loss=5.9738
	step [185/249], loss=5.4614
	step [186/249], loss=5.7372
	step [187/249], loss=5.6000
	step [188/249], loss=5.2164
	step [189/249], loss=4.8992
	step [190/249], loss=7.4007
	step [191/249], loss=6.0811
	step [192/249], loss=5.9702
	step [193/249], loss=5.7194
	step [194/249], loss=6.1666
	step [195/249], loss=7.1001
	step [196/249], loss=5.3358
	step [197/249], loss=6.8582
	step [198/249], loss=5.0357
	step [199/249], loss=5.7896
	step [200/249], loss=4.9635
	step [201/249], loss=6.1624
	step [202/249], loss=5.5944
	step [203/249], loss=5.5268
	step [204/249], loss=5.8724
	step [205/249], loss=5.6125
	step [206/249], loss=5.8963
	step [207/249], loss=6.2668
	step [208/249], loss=5.5340
	step [209/249], loss=4.0813
	step [210/249], loss=4.8765
	step [211/249], loss=4.9474
	step [212/249], loss=6.6607
	step [213/249], loss=5.9742
	step [214/249], loss=4.5367
	step [215/249], loss=5.0391
	step [216/249], loss=6.2799
	step [217/249], loss=5.8886
	step [218/249], loss=5.9821
	step [219/249], loss=6.1423
	step [220/249], loss=7.0652
	step [221/249], loss=5.2070
	step [222/249], loss=5.7778
	step [223/249], loss=6.5356
	step [224/249], loss=6.6296
	step [225/249], loss=6.2360
	step [226/249], loss=5.1550
	step [227/249], loss=5.7974
	step [228/249], loss=7.3878
	step [229/249], loss=5.0646
	step [230/249], loss=5.9394
	step [231/249], loss=5.8614
	step [232/249], loss=6.5701
	step [233/249], loss=6.3639
	step [234/249], loss=5.3748
	step [235/249], loss=6.8277
	step [236/249], loss=5.9333
	step [237/249], loss=5.6286
	step [238/249], loss=5.2510
	step [239/249], loss=5.8766
	step [240/249], loss=7.1422
	step [241/249], loss=5.9076
	step [242/249], loss=5.8610
	step [243/249], loss=4.9851
	step [244/249], loss=5.9557
	step [245/249], loss=5.6860
	step [246/249], loss=6.0498
	step [247/249], loss=6.4223
	step [248/249], loss=5.5346
	step [249/249], loss=5.5089
	Evaluating
	loss=0.0186, precision=0.1972, recall=0.9944, f1=0.3292
Training epoch 25
	step [1/249], loss=5.0756
	step [2/249], loss=5.6175
	step [3/249], loss=5.3506
	step [4/249], loss=4.5080
	step [5/249], loss=6.7292
	step [6/249], loss=4.8243
	step [7/249], loss=5.0769
	step [8/249], loss=6.3061
	step [9/249], loss=5.8856
	step [10/249], loss=5.4225
	step [11/249], loss=4.6487
	step [12/249], loss=5.5583
	step [13/249], loss=6.8864
	step [14/249], loss=5.6863
	step [15/249], loss=4.8791
	step [16/249], loss=5.6013
	step [17/249], loss=4.9682
	step [18/249], loss=5.7451
	step [19/249], loss=5.0927
	step [20/249], loss=4.1346
	step [21/249], loss=5.5148
	step [22/249], loss=5.1769
	step [23/249], loss=5.8568
	step [24/249], loss=4.9128
	step [25/249], loss=5.7511
	step [26/249], loss=5.8806
	step [27/249], loss=5.1514
	step [28/249], loss=5.9836
	step [29/249], loss=5.5518
	step [30/249], loss=5.9604
	step [31/249], loss=6.9923
	step [32/249], loss=4.0466
	step [33/249], loss=5.6106
	step [34/249], loss=4.6067
	step [35/249], loss=6.2688
	step [36/249], loss=4.8880
	step [37/249], loss=5.9140
	step [38/249], loss=5.3045
	step [39/249], loss=5.8687
	step [40/249], loss=6.1198
	step [41/249], loss=5.3465
	step [42/249], loss=5.6625
	step [43/249], loss=4.7292
	step [44/249], loss=6.1415
	step [45/249], loss=6.0452
	step [46/249], loss=5.9306
	step [47/249], loss=5.8967
	step [48/249], loss=5.7026
	step [49/249], loss=5.5075
	step [50/249], loss=4.8280
	step [51/249], loss=4.9727
	step [52/249], loss=5.2608
	step [53/249], loss=4.7180
	step [54/249], loss=4.2151
	step [55/249], loss=5.1180
	step [56/249], loss=4.8199
	step [57/249], loss=5.9992
	step [58/249], loss=4.8577
	step [59/249], loss=6.4157
	step [60/249], loss=5.0159
	step [61/249], loss=5.8419
	step [62/249], loss=5.3608
	step [63/249], loss=5.6915
	step [64/249], loss=5.1658
	step [65/249], loss=5.5592
	step [66/249], loss=6.6831
	step [67/249], loss=6.2362
	step [68/249], loss=6.0068
	step [69/249], loss=5.6295
	step [70/249], loss=6.0662
	step [71/249], loss=5.4810
	step [72/249], loss=5.1929
	step [73/249], loss=5.1961
	step [74/249], loss=6.1825
	step [75/249], loss=5.5116
	step [76/249], loss=5.5349
	step [77/249], loss=4.3372
	step [78/249], loss=5.0534
	step [79/249], loss=5.1644
	step [80/249], loss=5.3641
	step [81/249], loss=6.0105
	step [82/249], loss=5.7582
	step [83/249], loss=5.2207
	step [84/249], loss=5.7699
	step [85/249], loss=6.4131
	step [86/249], loss=5.6530
	step [87/249], loss=6.2412
	step [88/249], loss=5.9345
	step [89/249], loss=5.0355
	step [90/249], loss=6.1642
	step [91/249], loss=5.0338
	step [92/249], loss=5.8896
	step [93/249], loss=5.9283
	step [94/249], loss=5.2654
	step [95/249], loss=4.9203
	step [96/249], loss=5.0001
	step [97/249], loss=5.9530
	step [98/249], loss=6.3879
	step [99/249], loss=5.8228
	step [100/249], loss=5.5397
	step [101/249], loss=5.8883
	step [102/249], loss=5.8321
	step [103/249], loss=4.4128
	step [104/249], loss=7.0805
	step [105/249], loss=5.6173
	step [106/249], loss=5.2982
	step [107/249], loss=5.2831
	step [108/249], loss=4.8169
	step [109/249], loss=5.4965
	step [110/249], loss=7.1245
	step [111/249], loss=5.3821
	step [112/249], loss=5.4537
	step [113/249], loss=5.4801
	step [114/249], loss=6.1173
	step [115/249], loss=6.0537
	step [116/249], loss=4.7314
	step [117/249], loss=6.7321
	step [118/249], loss=4.4479
	step [119/249], loss=4.9348
	step [120/249], loss=5.6223
	step [121/249], loss=5.1162
	step [122/249], loss=6.0543
	step [123/249], loss=5.5390
	step [124/249], loss=5.4655
	step [125/249], loss=6.0226
	step [126/249], loss=5.6727
	step [127/249], loss=5.1864
	step [128/249], loss=4.8211
	step [129/249], loss=6.9449
	step [130/249], loss=5.8812
	step [131/249], loss=4.9017
	step [132/249], loss=5.2805
	step [133/249], loss=4.7879
	step [134/249], loss=6.0581
	step [135/249], loss=5.5528
	step [136/249], loss=5.9236
	step [137/249], loss=4.7641
	step [138/249], loss=5.5506
	step [139/249], loss=6.3875
	step [140/249], loss=5.7216
	step [141/249], loss=6.5136
	step [142/249], loss=5.2697
	step [143/249], loss=5.3751
	step [144/249], loss=5.6607
	step [145/249], loss=6.1882
	step [146/249], loss=5.3484
	step [147/249], loss=6.4308
	step [148/249], loss=6.5130
	step [149/249], loss=5.3461
	step [150/249], loss=5.8482
	step [151/249], loss=5.9112
	step [152/249], loss=5.6240
	step [153/249], loss=5.6096
	step [154/249], loss=5.7494
	step [155/249], loss=6.1731
	step [156/249], loss=5.8532
	step [157/249], loss=6.0443
	step [158/249], loss=5.7251
	step [159/249], loss=5.5892
	step [160/249], loss=5.3802
	step [161/249], loss=6.6961
	step [162/249], loss=6.0876
	step [163/249], loss=5.6233
	step [164/249], loss=5.5424
	step [165/249], loss=6.4877
	step [166/249], loss=5.6198
	step [167/249], loss=5.0709
	step [168/249], loss=5.4476
	step [169/249], loss=5.3071
	step [170/249], loss=5.3390
	step [171/249], loss=7.1838
	step [172/249], loss=6.7135
	step [173/249], loss=4.4196
	step [174/249], loss=5.2064
	step [175/249], loss=6.2505
	step [176/249], loss=4.8319
	step [177/249], loss=7.2651
	step [178/249], loss=5.9042
	step [179/249], loss=5.6246
	step [180/249], loss=6.3462
	step [181/249], loss=6.6539
	step [182/249], loss=5.3507
	step [183/249], loss=5.4475
	step [184/249], loss=5.9390
	step [185/249], loss=6.9498
	step [186/249], loss=5.2651
	step [187/249], loss=5.4306
	step [188/249], loss=5.4188
	step [189/249], loss=5.4914
	step [190/249], loss=5.2211
	step [191/249], loss=5.4706
	step [192/249], loss=6.0149
	step [193/249], loss=6.1291
	step [194/249], loss=7.3135
	step [195/249], loss=4.6396
	step [196/249], loss=5.5095
	step [197/249], loss=6.2537
	step [198/249], loss=5.6017
	step [199/249], loss=5.1939
	step [200/249], loss=5.2863
	step [201/249], loss=7.0615
	step [202/249], loss=5.0069
	step [203/249], loss=5.1252
	step [204/249], loss=6.8435
	step [205/249], loss=5.2882
	step [206/249], loss=5.9904
	step [207/249], loss=5.1613
	step [208/249], loss=5.2666
	step [209/249], loss=6.1904
	step [210/249], loss=5.8987
	step [211/249], loss=6.0774
	step [212/249], loss=5.5811
	step [213/249], loss=5.6012
	step [214/249], loss=5.5798
	step [215/249], loss=6.7952
	step [216/249], loss=5.6301
	step [217/249], loss=5.0615
	step [218/249], loss=5.9617
	step [219/249], loss=6.0848
	step [220/249], loss=5.8415
	step [221/249], loss=7.0235
	step [222/249], loss=6.3969
	step [223/249], loss=5.3865
	step [224/249], loss=5.2674
	step [225/249], loss=6.4849
	step [226/249], loss=6.2724
	step [227/249], loss=4.8314
	step [228/249], loss=5.0306
	step [229/249], loss=4.6734
	step [230/249], loss=5.3069
	step [231/249], loss=5.0451
	step [232/249], loss=5.9003
	step [233/249], loss=5.8727
	step [234/249], loss=5.9131
	step [235/249], loss=6.2742
	step [236/249], loss=4.7057
	step [237/249], loss=6.0313
	step [238/249], loss=4.9773
	step [239/249], loss=4.7813
	step [240/249], loss=5.5221
	step [241/249], loss=5.2729
	step [242/249], loss=6.9201
	step [243/249], loss=6.1679
	step [244/249], loss=6.1554
	step [245/249], loss=5.0218
	step [246/249], loss=5.3054
	step [247/249], loss=5.9452
	step [248/249], loss=4.9368
	step [249/249], loss=4.1414
	Evaluating
	loss=0.0149, precision=0.2111, recall=0.9938, f1=0.3482
Training epoch 26
	step [1/249], loss=5.6488
	step [2/249], loss=5.9124
	step [3/249], loss=5.5252
	step [4/249], loss=4.2357
	step [5/249], loss=4.9775
	step [6/249], loss=5.4216
	step [7/249], loss=4.7197
	step [8/249], loss=5.0045
	step [9/249], loss=6.2433
	step [10/249], loss=5.5032
	step [11/249], loss=5.0455
	step [12/249], loss=4.8314
	step [13/249], loss=7.4213
	step [14/249], loss=4.7057
	step [15/249], loss=4.8707
	step [16/249], loss=5.4445
	step [17/249], loss=5.1633
	step [18/249], loss=7.2608
	step [19/249], loss=4.8070
	step [20/249], loss=5.9762
	step [21/249], loss=5.9770
	step [22/249], loss=5.3549
	step [23/249], loss=5.5608
	step [24/249], loss=5.3008
	step [25/249], loss=5.8534
	step [26/249], loss=4.5073
	step [27/249], loss=7.3282
	step [28/249], loss=5.4556
	step [29/249], loss=5.9160
	step [30/249], loss=4.8530
	step [31/249], loss=4.6862
	step [32/249], loss=4.7245
	step [33/249], loss=5.3241
	step [34/249], loss=5.3047
	step [35/249], loss=7.5855
	step [36/249], loss=5.5039
	step [37/249], loss=5.1369
	step [38/249], loss=5.0008
	step [39/249], loss=6.2574
	step [40/249], loss=6.2804
	step [41/249], loss=6.0611
	step [42/249], loss=5.1536
	step [43/249], loss=4.8064
	step [44/249], loss=5.7858
	step [45/249], loss=6.0466
	step [46/249], loss=5.4221
	step [47/249], loss=5.1577
	step [48/249], loss=6.1682
	step [49/249], loss=5.3485
	step [50/249], loss=5.7581
	step [51/249], loss=5.9158
	step [52/249], loss=4.5831
	step [53/249], loss=5.0513
	step [54/249], loss=7.3495
	step [55/249], loss=5.1691
	step [56/249], loss=4.7432
	step [57/249], loss=5.4599
	step [58/249], loss=5.5635
	step [59/249], loss=4.9087
	step [60/249], loss=5.0558
	step [61/249], loss=5.3995
	step [62/249], loss=5.7405
	step [63/249], loss=5.5443
	step [64/249], loss=4.8929
	step [65/249], loss=5.1416
	step [66/249], loss=4.4205
	step [67/249], loss=4.7894
	step [68/249], loss=5.1615
	step [69/249], loss=7.4312
	step [70/249], loss=5.7351
	step [71/249], loss=4.8588
	step [72/249], loss=5.4007
	step [73/249], loss=5.1703
	step [74/249], loss=5.3728
	step [75/249], loss=4.7990
	step [76/249], loss=4.2304
	step [77/249], loss=4.6107
	step [78/249], loss=4.9309
	step [79/249], loss=5.4265
	step [80/249], loss=5.3992
	step [81/249], loss=5.4169
	step [82/249], loss=4.7220
	step [83/249], loss=5.0511
	step [84/249], loss=5.3061
	step [85/249], loss=6.7927
	step [86/249], loss=6.8698
	step [87/249], loss=5.8516
	step [88/249], loss=4.7700
	step [89/249], loss=4.6805
	step [90/249], loss=6.0865
	step [91/249], loss=4.6075
	step [92/249], loss=5.2423
	step [93/249], loss=4.5712
	step [94/249], loss=5.0567
	step [95/249], loss=5.5131
	step [96/249], loss=4.9064
	step [97/249], loss=6.8676
	step [98/249], loss=4.3366
	step [99/249], loss=5.3095
	step [100/249], loss=5.6029
	step [101/249], loss=5.4444
	step [102/249], loss=6.2035
	step [103/249], loss=5.0197
	step [104/249], loss=6.3769
	step [105/249], loss=5.2322
	step [106/249], loss=5.9115
	step [107/249], loss=5.2458
	step [108/249], loss=5.8788
	step [109/249], loss=5.9158
	step [110/249], loss=5.7119
	step [111/249], loss=5.4709
	step [112/249], loss=5.4460
	step [113/249], loss=5.7643
	step [114/249], loss=6.8343
	step [115/249], loss=6.0421
	step [116/249], loss=5.3957
	step [117/249], loss=5.6604
	step [118/249], loss=5.0114
	step [119/249], loss=5.2203
	step [120/249], loss=4.6468
	step [121/249], loss=5.6026
	step [122/249], loss=5.6728
	step [123/249], loss=5.3528
	step [124/249], loss=6.1784
	step [125/249], loss=5.5681
	step [126/249], loss=5.2405
	step [127/249], loss=5.7168
	step [128/249], loss=6.3387
	step [129/249], loss=6.5968
	step [130/249], loss=4.5307
	step [131/249], loss=5.5394
	step [132/249], loss=6.3952
	step [133/249], loss=5.8216
	step [134/249], loss=5.3290
	step [135/249], loss=7.4359
	step [136/249], loss=7.0946
	step [137/249], loss=6.5421
	step [138/249], loss=4.8606
	step [139/249], loss=6.1830
	step [140/249], loss=5.1582
	step [141/249], loss=5.2651
	step [142/249], loss=5.2858
	step [143/249], loss=5.9813
	step [144/249], loss=6.5118
	step [145/249], loss=5.2769
	step [146/249], loss=4.2847
	step [147/249], loss=5.8471
	step [148/249], loss=4.8401
	step [149/249], loss=5.0631
	step [150/249], loss=5.0091
	step [151/249], loss=4.8991
	step [152/249], loss=5.9848
	step [153/249], loss=4.4402
	step [154/249], loss=5.6581
	step [155/249], loss=5.7759
	step [156/249], loss=5.1367
	step [157/249], loss=6.3231
	step [158/249], loss=5.6576
	step [159/249], loss=5.5683
	step [160/249], loss=5.9848
	step [161/249], loss=5.1841
	step [162/249], loss=5.1244
	step [163/249], loss=5.1283
	step [164/249], loss=5.6748
	step [165/249], loss=6.2465
	step [166/249], loss=5.0776
	step [167/249], loss=4.4952
	step [168/249], loss=5.7639
	step [169/249], loss=6.7510
	step [170/249], loss=5.2327
	step [171/249], loss=5.1533
	step [172/249], loss=6.1164
	step [173/249], loss=5.6009
	step [174/249], loss=5.7379
	step [175/249], loss=5.5819
	step [176/249], loss=4.1020
	step [177/249], loss=5.1468
	step [178/249], loss=6.6927
	step [179/249], loss=5.7806
	step [180/249], loss=4.9886
	step [181/249], loss=5.6179
	step [182/249], loss=5.5867
	step [183/249], loss=5.5911
	step [184/249], loss=5.6549
	step [185/249], loss=4.9281
	step [186/249], loss=5.5732
	step [187/249], loss=5.1752
	step [188/249], loss=6.1991
	step [189/249], loss=5.2518
	step [190/249], loss=4.7985
	step [191/249], loss=5.0427
	step [192/249], loss=6.3728
	step [193/249], loss=5.1382
	step [194/249], loss=5.9218
	step [195/249], loss=5.9708
	step [196/249], loss=5.3579
	step [197/249], loss=6.0386
	step [198/249], loss=4.9343
	step [199/249], loss=5.5619
	step [200/249], loss=6.6288
	step [201/249], loss=5.3870
	step [202/249], loss=5.0210
	step [203/249], loss=5.0239
	step [204/249], loss=4.3117
	step [205/249], loss=5.4954
	step [206/249], loss=4.8937
	step [207/249], loss=5.0835
	step [208/249], loss=4.4187
	step [209/249], loss=5.5768
	step [210/249], loss=5.9570
	step [211/249], loss=5.3395
	step [212/249], loss=5.2994
	step [213/249], loss=6.5215
	step [214/249], loss=5.0388
	step [215/249], loss=5.4769
	step [216/249], loss=5.5247
	step [217/249], loss=5.3183
	step [218/249], loss=4.5410
	step [219/249], loss=6.7852
	step [220/249], loss=5.6583
	step [221/249], loss=5.6336
	step [222/249], loss=5.5192
	step [223/249], loss=5.2327
	step [224/249], loss=4.8017
	step [225/249], loss=5.3780
	step [226/249], loss=4.7922
	step [227/249], loss=5.6386
	step [228/249], loss=6.2131
	step [229/249], loss=4.8391
	step [230/249], loss=5.7830
	step [231/249], loss=4.8858
	step [232/249], loss=6.1015
	step [233/249], loss=4.4011
	step [234/249], loss=7.6964
	step [235/249], loss=4.4426
	step [236/249], loss=5.7452
	step [237/249], loss=5.4084
	step [238/249], loss=5.2667
	step [239/249], loss=5.9404
	step [240/249], loss=4.8175
	step [241/249], loss=5.4163
	step [242/249], loss=4.6423
	step [243/249], loss=4.8566
	step [244/249], loss=5.7632
	step [245/249], loss=6.3175
	step [246/249], loss=4.9839
	step [247/249], loss=5.6883
	step [248/249], loss=4.9167
	step [249/249], loss=4.0104
	Evaluating
	loss=0.0155, precision=0.2183, recall=0.9937, f1=0.3580
Training epoch 27
	step [1/249], loss=5.7385
	step [2/249], loss=4.3523
	step [3/249], loss=5.0678
	step [4/249], loss=4.7072
	step [5/249], loss=5.0593
	step [6/249], loss=7.5924
	step [7/249], loss=4.8163
	step [8/249], loss=6.2749
	step [9/249], loss=4.9993
	step [10/249], loss=5.8559
	step [11/249], loss=5.4171
	step [12/249], loss=5.1860
	step [13/249], loss=5.3261
	step [14/249], loss=5.0288
	step [15/249], loss=5.7150
	step [16/249], loss=5.6756
	step [17/249], loss=4.7422
	step [18/249], loss=5.8099
	step [19/249], loss=5.3574
	step [20/249], loss=4.7746
	step [21/249], loss=5.7813
	step [22/249], loss=4.8339
	step [23/249], loss=4.7249
	step [24/249], loss=5.8554
	step [25/249], loss=5.0757
	step [26/249], loss=4.5986
	step [27/249], loss=5.3512
	step [28/249], loss=5.2006
	step [29/249], loss=4.7695
	step [30/249], loss=4.7640
	step [31/249], loss=5.6999
	step [32/249], loss=5.6199
	step [33/249], loss=5.7367
	step [34/249], loss=5.4962
	step [35/249], loss=5.3245
	step [36/249], loss=6.5778
	step [37/249], loss=6.2366
	step [38/249], loss=6.9533
	step [39/249], loss=5.9431
	step [40/249], loss=4.7200
	step [41/249], loss=5.9674
	step [42/249], loss=4.8307
	step [43/249], loss=5.8433
	step [44/249], loss=4.5884
	step [45/249], loss=5.9840
	step [46/249], loss=5.9119
	step [47/249], loss=5.0472
	step [48/249], loss=4.9963
	step [49/249], loss=4.8069
	step [50/249], loss=5.4652
	step [51/249], loss=5.1631
	step [52/249], loss=6.5998
	step [53/249], loss=5.0537
	step [54/249], loss=5.0039
	step [55/249], loss=5.7945
	step [56/249], loss=6.3906
	step [57/249], loss=5.4985
	step [58/249], loss=4.5854
	step [59/249], loss=5.1069
	step [60/249], loss=5.9022
	step [61/249], loss=5.0920
	step [62/249], loss=6.2856
	step [63/249], loss=4.8905
	step [64/249], loss=5.4339
	step [65/249], loss=5.7650
	step [66/249], loss=5.1600
	step [67/249], loss=6.0620
	step [68/249], loss=5.6809
	step [69/249], loss=4.9711
	step [70/249], loss=5.3652
	step [71/249], loss=6.2985
	step [72/249], loss=6.0342
	step [73/249], loss=4.5693
	step [74/249], loss=4.6210
	step [75/249], loss=6.1336
	step [76/249], loss=4.6941
	step [77/249], loss=6.1016
	step [78/249], loss=4.7579
	step [79/249], loss=5.5780
	step [80/249], loss=5.4908
	step [81/249], loss=5.2317
	step [82/249], loss=5.0870
	step [83/249], loss=5.5282
	step [84/249], loss=5.3283
	step [85/249], loss=6.0635
	step [86/249], loss=5.2935
	step [87/249], loss=5.3554
	step [88/249], loss=5.2503
	step [89/249], loss=4.8957
	step [90/249], loss=5.2909
	step [91/249], loss=7.1789
	step [92/249], loss=4.7209
	step [93/249], loss=4.7860
	step [94/249], loss=5.9285
	step [95/249], loss=4.6832
	step [96/249], loss=4.2087
	step [97/249], loss=5.4554
	step [98/249], loss=5.0212
	step [99/249], loss=4.8979
	step [100/249], loss=5.4976
	step [101/249], loss=5.0540
	step [102/249], loss=4.8279
	step [103/249], loss=4.5099
	step [104/249], loss=5.7155
	step [105/249], loss=4.0473
	step [106/249], loss=5.3443
	step [107/249], loss=5.4172
	step [108/249], loss=4.9863
	step [109/249], loss=6.7206
	step [110/249], loss=5.0129
	step [111/249], loss=5.5083
	step [112/249], loss=5.1806
	step [113/249], loss=5.6484
	step [114/249], loss=5.0190
	step [115/249], loss=6.5184
	step [116/249], loss=5.1053
	step [117/249], loss=5.5489
	step [118/249], loss=5.2581
	step [119/249], loss=6.6204
	step [120/249], loss=4.9518
	step [121/249], loss=6.3281
	step [122/249], loss=4.5862
	step [123/249], loss=5.4018
	step [124/249], loss=5.0080
	step [125/249], loss=4.4334
	step [126/249], loss=4.8754
	step [127/249], loss=6.9089
	step [128/249], loss=4.3617
	step [129/249], loss=5.2938
	step [130/249], loss=5.9843
	step [131/249], loss=5.3616
	step [132/249], loss=4.7408
	step [133/249], loss=5.4588
	step [134/249], loss=5.9885
	step [135/249], loss=5.5648
	step [136/249], loss=6.3024
	step [137/249], loss=5.4624
	step [138/249], loss=5.5166
	step [139/249], loss=5.7380
	step [140/249], loss=6.4827
	step [141/249], loss=7.0683
	step [142/249], loss=5.3764
	step [143/249], loss=5.2571
	step [144/249], loss=4.5444
	step [145/249], loss=6.2294
	step [146/249], loss=5.3665
	step [147/249], loss=5.3144
	step [148/249], loss=5.0809
	step [149/249], loss=6.8740
	step [150/249], loss=5.4089
	step [151/249], loss=6.3691
	step [152/249], loss=6.7044
	step [153/249], loss=5.9600
	step [154/249], loss=5.3943
	step [155/249], loss=5.2450
	step [156/249], loss=6.8875
	step [157/249], loss=5.2448
	step [158/249], loss=5.6606
	step [159/249], loss=4.7098
	step [160/249], loss=4.8652
	step [161/249], loss=5.2723
	step [162/249], loss=5.4495
	step [163/249], loss=5.9748
	step [164/249], loss=5.1573
	step [165/249], loss=5.3919
	step [166/249], loss=5.3918
	step [167/249], loss=5.4224
	step [168/249], loss=6.2510
	step [169/249], loss=5.2365
	step [170/249], loss=5.4188
	step [171/249], loss=5.0699
	step [172/249], loss=6.0408
	step [173/249], loss=5.3543
	step [174/249], loss=5.6983
	step [175/249], loss=5.2983
	step [176/249], loss=4.7190
	step [177/249], loss=6.0195
	step [178/249], loss=6.1573
	step [179/249], loss=5.4377
	step [180/249], loss=5.7963
	step [181/249], loss=5.0186
	step [182/249], loss=5.1315
	step [183/249], loss=7.1563
	step [184/249], loss=4.5304
	step [185/249], loss=5.4510
	step [186/249], loss=4.9268
	step [187/249], loss=4.9747
	step [188/249], loss=5.1005
	step [189/249], loss=5.1728
	step [190/249], loss=4.7432
	step [191/249], loss=5.1093
	step [192/249], loss=5.4178
	step [193/249], loss=5.1822
	step [194/249], loss=5.5372
	step [195/249], loss=5.6481
	step [196/249], loss=4.6844
	step [197/249], loss=4.6280
	step [198/249], loss=4.6917
	step [199/249], loss=4.3962
	step [200/249], loss=4.8841
	step [201/249], loss=5.3886
	step [202/249], loss=5.5198
	step [203/249], loss=5.0497
	step [204/249], loss=4.6441
	step [205/249], loss=5.5259
	step [206/249], loss=5.3421
	step [207/249], loss=5.4040
	step [208/249], loss=5.0726
	step [209/249], loss=5.0484
	step [210/249], loss=6.1200
	step [211/249], loss=5.3691
	step [212/249], loss=6.0897
	step [213/249], loss=5.2518
	step [214/249], loss=5.9457
	step [215/249], loss=4.3657
	step [216/249], loss=5.2511
	step [217/249], loss=4.1854
	step [218/249], loss=5.1685
	step [219/249], loss=5.1743
	step [220/249], loss=5.9818
	step [221/249], loss=5.5087
	step [222/249], loss=4.1686
	step [223/249], loss=4.9643
	step [224/249], loss=5.2882
	step [225/249], loss=6.3591
	step [226/249], loss=5.6134
	step [227/249], loss=5.2769
	step [228/249], loss=4.5232
	step [229/249], loss=5.3707
	step [230/249], loss=4.4112
	step [231/249], loss=5.8373
	step [232/249], loss=5.0417
	step [233/249], loss=5.5563
	step [234/249], loss=4.7561
	step [235/249], loss=4.2444
	step [236/249], loss=4.6617
	step [237/249], loss=6.0560
	step [238/249], loss=4.9002
	step [239/249], loss=5.2071
	step [240/249], loss=5.4986
	step [241/249], loss=5.1872
	step [242/249], loss=5.4496
	step [243/249], loss=5.4168
	step [244/249], loss=4.7932
	step [245/249], loss=4.6971
	step [246/249], loss=4.9855
	step [247/249], loss=5.1484
	step [248/249], loss=6.3879
	step [249/249], loss=3.3906
	Evaluating
	loss=0.0139, precision=0.2405, recall=0.9922, f1=0.3872
Training epoch 28
	step [1/249], loss=4.6844
	step [2/249], loss=5.9227
	step [3/249], loss=4.7038
	step [4/249], loss=6.0438
	step [5/249], loss=5.0941
	step [6/249], loss=5.0785
	step [7/249], loss=4.1010
	step [8/249], loss=5.0378
	step [9/249], loss=5.5156
	step [10/249], loss=5.7371
	step [11/249], loss=4.9487
	step [12/249], loss=4.6878
	step [13/249], loss=5.6531
	step [14/249], loss=5.8301
	step [15/249], loss=5.4774
	step [16/249], loss=6.4619
	step [17/249], loss=5.9013
	step [18/249], loss=6.0894
	step [19/249], loss=4.6121
	step [20/249], loss=5.5331
	step [21/249], loss=5.5588
	step [22/249], loss=6.0313
	step [23/249], loss=5.0946
	step [24/249], loss=5.2099
	step [25/249], loss=4.4111
	step [26/249], loss=4.9042
	step [27/249], loss=4.8766
	step [28/249], loss=5.9121
	step [29/249], loss=5.3661
	step [30/249], loss=5.0084
	step [31/249], loss=4.4498
	step [32/249], loss=4.8075
	step [33/249], loss=6.1272
	step [34/249], loss=5.1517
	step [35/249], loss=5.0217
	step [36/249], loss=4.7369
	step [37/249], loss=5.7250
	step [38/249], loss=5.3729
	step [39/249], loss=5.6633
	step [40/249], loss=5.8147
	step [41/249], loss=4.9611
	step [42/249], loss=5.4460
	step [43/249], loss=5.1742
	step [44/249], loss=3.8616
	step [45/249], loss=5.1925
	step [46/249], loss=5.8967
	step [47/249], loss=4.8340
	step [48/249], loss=4.7106
	step [49/249], loss=5.1493
	step [50/249], loss=4.8533
	step [51/249], loss=5.1098
	step [52/249], loss=4.9283
	step [53/249], loss=4.4470
	step [54/249], loss=4.8278
	step [55/249], loss=4.7918
	step [56/249], loss=4.5933
	step [57/249], loss=5.2075
	step [58/249], loss=5.8539
	step [59/249], loss=5.3408
	step [60/249], loss=5.7357
	step [61/249], loss=4.9225
	step [62/249], loss=5.0002
	step [63/249], loss=5.7717
	step [64/249], loss=6.8101
	step [65/249], loss=5.5902
	step [66/249], loss=4.1251
	step [67/249], loss=5.3391
	step [68/249], loss=5.5662
	step [69/249], loss=6.1588
	step [70/249], loss=4.8981
	step [71/249], loss=5.1737
	step [72/249], loss=4.2005
	step [73/249], loss=5.2658
	step [74/249], loss=4.4998
	step [75/249], loss=5.1979
	step [76/249], loss=4.3353
	step [77/249], loss=4.4293
	step [78/249], loss=4.5244
	step [79/249], loss=5.0682
	step [80/249], loss=5.4563
	step [81/249], loss=3.8636
	step [82/249], loss=5.4499
	step [83/249], loss=5.3179
	step [84/249], loss=5.2733
	step [85/249], loss=5.9329
	step [86/249], loss=5.1155
	step [87/249], loss=4.0870
	step [88/249], loss=4.9525
	step [89/249], loss=4.4280
	step [90/249], loss=6.3480
	step [91/249], loss=4.2146
	step [92/249], loss=5.2341
	step [93/249], loss=5.4893
	step [94/249], loss=5.3319
	step [95/249], loss=4.9650
	step [96/249], loss=5.3241
	step [97/249], loss=5.4888
	step [98/249], loss=5.6861
	step [99/249], loss=3.9777
	step [100/249], loss=5.0133
	step [101/249], loss=6.1216
	step [102/249], loss=5.5384
	step [103/249], loss=5.4447
	step [104/249], loss=6.0514
	step [105/249], loss=4.1583
	step [106/249], loss=6.0589
	step [107/249], loss=4.8681
	step [108/249], loss=5.5160
	step [109/249], loss=4.8676
	step [110/249], loss=4.7435
	step [111/249], loss=5.4309
	step [112/249], loss=5.0872
	step [113/249], loss=4.9934
	step [114/249], loss=5.5348
	step [115/249], loss=5.8668
	step [116/249], loss=5.3863
	step [117/249], loss=4.8803
	step [118/249], loss=6.1584
	step [119/249], loss=4.7035
	step [120/249], loss=4.1210
	step [121/249], loss=4.3253
	step [122/249], loss=6.0441
	step [123/249], loss=5.4613
	step [124/249], loss=5.2726
	step [125/249], loss=5.5998
	step [126/249], loss=5.0236
	step [127/249], loss=5.8818
	step [128/249], loss=5.5795
	step [129/249], loss=6.2435
	step [130/249], loss=5.5290
	step [131/249], loss=5.3003
	step [132/249], loss=5.6423
	step [133/249], loss=5.3404
	step [134/249], loss=4.9258
	step [135/249], loss=4.9467
	step [136/249], loss=6.1790
	step [137/249], loss=5.2344
	step [138/249], loss=5.8965
	step [139/249], loss=4.8465
	step [140/249], loss=4.9362
	step [141/249], loss=5.0505
	step [142/249], loss=5.3658
	step [143/249], loss=6.0343
	step [144/249], loss=5.4269
	step [145/249], loss=4.9844
	step [146/249], loss=4.8818
	step [147/249], loss=4.7404
	step [148/249], loss=5.2547
	step [149/249], loss=5.6918
	step [150/249], loss=6.3106
	step [151/249], loss=5.4318
	step [152/249], loss=4.7991
	step [153/249], loss=4.9537
	step [154/249], loss=4.8441
	step [155/249], loss=4.7238
	step [156/249], loss=5.0097
	step [157/249], loss=6.4953
	step [158/249], loss=5.6458
	step [159/249], loss=5.8976
	step [160/249], loss=5.1550
	step [161/249], loss=4.9489
	step [162/249], loss=4.9695
	step [163/249], loss=5.4859
	step [164/249], loss=6.1621
	step [165/249], loss=5.2220
	step [166/249], loss=4.8750
	step [167/249], loss=5.9935
	step [168/249], loss=4.4769
	step [169/249], loss=6.1261
	step [170/249], loss=5.7386
	step [171/249], loss=4.5666
	step [172/249], loss=4.4199
	step [173/249], loss=6.0078
	step [174/249], loss=4.8072
	step [175/249], loss=5.1957
	step [176/249], loss=5.2786
	step [177/249], loss=5.8043
	step [178/249], loss=5.8877
	step [179/249], loss=5.7935
	step [180/249], loss=5.7574
	step [181/249], loss=5.3939
	step [182/249], loss=5.5313
	step [183/249], loss=5.0645
	step [184/249], loss=5.4088
	step [185/249], loss=6.0849
	step [186/249], loss=5.9661
	step [187/249], loss=5.6911
	step [188/249], loss=5.7334
	step [189/249], loss=6.2285
	step [190/249], loss=5.3788
	step [191/249], loss=5.0678
	step [192/249], loss=6.1824
	step [193/249], loss=5.2252
	step [194/249], loss=4.7016
	step [195/249], loss=4.8786
	step [196/249], loss=5.8042
	step [197/249], loss=5.1803
	step [198/249], loss=4.3708
	step [199/249], loss=6.2726
	step [200/249], loss=5.4857
	step [201/249], loss=5.2778
	step [202/249], loss=5.4400
	step [203/249], loss=4.9083
	step [204/249], loss=5.0021
	step [205/249], loss=5.6289
	step [206/249], loss=5.0836
	step [207/249], loss=6.2229
	step [208/249], loss=5.6820
	step [209/249], loss=5.0569
	step [210/249], loss=5.0498
	step [211/249], loss=5.2484
	step [212/249], loss=5.1045
	step [213/249], loss=4.6961
	step [214/249], loss=5.2713
	step [215/249], loss=5.8559
	step [216/249], loss=5.0556
	step [217/249], loss=6.2385
	step [218/249], loss=5.8222
	step [219/249], loss=4.4008
	step [220/249], loss=5.1780
	step [221/249], loss=6.0409
	step [222/249], loss=6.0212
	step [223/249], loss=4.4956
	step [224/249], loss=5.2162
	step [225/249], loss=6.2720
	step [226/249], loss=5.3258
	step [227/249], loss=4.3084
	step [228/249], loss=5.4771
	step [229/249], loss=5.0077
	step [230/249], loss=5.9788
	step [231/249], loss=4.9840
	step [232/249], loss=5.2057
	step [233/249], loss=5.8841
	step [234/249], loss=5.5012
	step [235/249], loss=4.2520
	step [236/249], loss=6.1246
	step [237/249], loss=5.4772
	step [238/249], loss=6.0536
	step [239/249], loss=5.0380
	step [240/249], loss=5.6167
	step [241/249], loss=4.8844
	step [242/249], loss=5.1659
	step [243/249], loss=5.7569
	step [244/249], loss=5.1558
	step [245/249], loss=4.2423
	step [246/249], loss=5.6677
	step [247/249], loss=4.1953
	step [248/249], loss=5.1763
	step [249/249], loss=3.9258
	Evaluating
	loss=0.0127, precision=0.2561, recall=0.9916, f1=0.4071
saving model as: 0_saved_model.pth
Training epoch 29
	step [1/249], loss=4.5453
	step [2/249], loss=5.4681
	step [3/249], loss=5.9337
	step [4/249], loss=4.4502
	step [5/249], loss=4.5670
	step [6/249], loss=4.5346
	step [7/249], loss=4.1976
	step [8/249], loss=5.3295
	step [9/249], loss=4.9072
	step [10/249], loss=4.4736
	step [11/249], loss=5.5100
	step [12/249], loss=5.0811
	step [13/249], loss=5.2563
	step [14/249], loss=4.8083
	step [15/249], loss=5.3829
	step [16/249], loss=5.3262
	step [17/249], loss=4.6745
	step [18/249], loss=4.9111
	step [19/249], loss=5.6352
	step [20/249], loss=5.6471
	step [21/249], loss=3.7062
	step [22/249], loss=4.7841
	step [23/249], loss=5.9193
	step [24/249], loss=5.5288
	step [25/249], loss=5.8036
	step [26/249], loss=5.0913
	step [27/249], loss=4.8949
	step [28/249], loss=4.4989
	step [29/249], loss=5.9436
	step [30/249], loss=4.3400
	step [31/249], loss=4.9920
	step [32/249], loss=6.0430
	step [33/249], loss=5.5646
	step [34/249], loss=5.3830
	step [35/249], loss=3.8831
	step [36/249], loss=5.5968
	step [37/249], loss=4.6073
	step [38/249], loss=5.5691
	step [39/249], loss=5.4581
	step [40/249], loss=4.6482
	step [41/249], loss=5.1304
	step [42/249], loss=5.5162
	step [43/249], loss=4.7893
	step [44/249], loss=5.2995
	step [45/249], loss=5.3431
	step [46/249], loss=4.9140
	step [47/249], loss=4.9973
	step [48/249], loss=4.7589
	step [49/249], loss=4.4654
	step [50/249], loss=6.3473
	step [51/249], loss=5.6471
	step [52/249], loss=5.1391
	step [53/249], loss=5.4877
	step [54/249], loss=5.3543
	step [55/249], loss=6.7067
	step [56/249], loss=4.3635
	step [57/249], loss=5.5387
	step [58/249], loss=5.1343
	step [59/249], loss=4.8699
	step [60/249], loss=4.4426
	step [61/249], loss=5.3205
	step [62/249], loss=4.9608
	step [63/249], loss=5.4016
	step [64/249], loss=5.7393
	step [65/249], loss=5.2130
	step [66/249], loss=4.8043
	step [67/249], loss=4.6271
	step [68/249], loss=4.9985
	step [69/249], loss=7.2797
	step [70/249], loss=5.1475
	step [71/249], loss=6.3747
	step [72/249], loss=4.1873
	step [73/249], loss=6.7353
	step [74/249], loss=4.8602
	step [75/249], loss=5.2496
	step [76/249], loss=5.7366
	step [77/249], loss=5.5335
	step [78/249], loss=5.7095
	step [79/249], loss=5.7579
	step [80/249], loss=4.2100
	step [81/249], loss=5.0876
	step [82/249], loss=5.2638
	step [83/249], loss=4.6766
	step [84/249], loss=5.0093
	step [85/249], loss=5.4380
	step [86/249], loss=5.4210
	step [87/249], loss=5.1695
	step [88/249], loss=5.5868
	step [89/249], loss=5.5890
	step [90/249], loss=4.7225
	step [91/249], loss=4.2891
	step [92/249], loss=4.5939
	step [93/249], loss=4.6256
	step [94/249], loss=4.4764
	step [95/249], loss=5.7578
	step [96/249], loss=5.3548
	step [97/249], loss=4.7877
	step [98/249], loss=4.8883
	step [99/249], loss=4.4273
	step [100/249], loss=5.1581
	step [101/249], loss=4.8167
	step [102/249], loss=4.0734
	step [103/249], loss=4.7911
	step [104/249], loss=6.8150
	step [105/249], loss=6.1548
	step [106/249], loss=5.4724
	step [107/249], loss=4.7801
	step [108/249], loss=5.1092
	step [109/249], loss=4.8658
	step [110/249], loss=5.5311
	step [111/249], loss=4.6853
	step [112/249], loss=5.4085
	step [113/249], loss=4.3259
	step [114/249], loss=4.9034
	step [115/249], loss=6.3609
	step [116/249], loss=4.7467
	step [117/249], loss=5.4718
	step [118/249], loss=5.4773
	step [119/249], loss=4.9492
	step [120/249], loss=5.4176
	step [121/249], loss=5.1464
	step [122/249], loss=4.4393
	step [123/249], loss=5.1323
	step [124/249], loss=4.8366
	step [125/249], loss=5.3526
	step [126/249], loss=5.3783
	step [127/249], loss=4.5837
	step [128/249], loss=4.9152
	step [129/249], loss=5.1107
	step [130/249], loss=4.8381
	step [131/249], loss=4.5289
	step [132/249], loss=5.6610
	step [133/249], loss=5.3251
	step [134/249], loss=7.8834
	step [135/249], loss=5.6623
	step [136/249], loss=5.1126
	step [137/249], loss=5.1134
	step [138/249], loss=5.6008
	step [139/249], loss=5.1451
	step [140/249], loss=4.5933
	step [141/249], loss=5.1470
	step [142/249], loss=5.7258
	step [143/249], loss=4.3120
	step [144/249], loss=4.7254
	step [145/249], loss=5.3101
	step [146/249], loss=5.2645
	step [147/249], loss=5.6992
	step [148/249], loss=5.3176
	step [149/249], loss=4.3700
	step [150/249], loss=5.6677
	step [151/249], loss=5.3940
	step [152/249], loss=4.7777
	step [153/249], loss=5.1968
	step [154/249], loss=4.7836
	step [155/249], loss=5.4976
	step [156/249], loss=4.5174
	step [157/249], loss=4.3614
	step [158/249], loss=5.0686
	step [159/249], loss=6.1182
	step [160/249], loss=4.9946
	step [161/249], loss=4.6594
	step [162/249], loss=4.3342
	step [163/249], loss=5.4221
	step [164/249], loss=4.9812
	step [165/249], loss=5.7612
	step [166/249], loss=5.4039
	step [167/249], loss=5.0903
	step [168/249], loss=6.2208
	step [169/249], loss=4.6557
	step [170/249], loss=6.1769
	step [171/249], loss=5.6618
	step [172/249], loss=5.2758
	step [173/249], loss=5.2323
	step [174/249], loss=4.1110
	step [175/249], loss=5.5586
	step [176/249], loss=6.1102
	step [177/249], loss=5.0655
	step [178/249], loss=4.7508
	step [179/249], loss=4.4141
	step [180/249], loss=5.3156
	step [181/249], loss=4.5044
	step [182/249], loss=5.0910
	step [183/249], loss=4.3110
	step [184/249], loss=4.7805
	step [185/249], loss=5.4969
	step [186/249], loss=5.0004
	step [187/249], loss=5.9590
	step [188/249], loss=5.3146
	step [189/249], loss=5.5966
	step [190/249], loss=4.4418
	step [191/249], loss=4.7648
	step [192/249], loss=4.6088
	step [193/249], loss=4.7939
	step [194/249], loss=4.5898
	step [195/249], loss=6.7648
	step [196/249], loss=4.4234
	step [197/249], loss=5.1768
	step [198/249], loss=5.0224
	step [199/249], loss=5.3323
	step [200/249], loss=5.1724
	step [201/249], loss=4.6495
	step [202/249], loss=5.3946
	step [203/249], loss=5.1889
	step [204/249], loss=4.9011
	step [205/249], loss=5.5852
	step [206/249], loss=4.5331
	step [207/249], loss=4.1897
	step [208/249], loss=5.2009
	step [209/249], loss=5.9074
	step [210/249], loss=5.3469
	step [211/249], loss=6.3793
	step [212/249], loss=5.1940
	step [213/249], loss=6.2512
	step [214/249], loss=4.6545
	step [215/249], loss=4.5757
	step [216/249], loss=5.0440
	step [217/249], loss=6.6805
	step [218/249], loss=5.7781
	step [219/249], loss=5.0980
	step [220/249], loss=5.4217
	step [221/249], loss=5.1202
	step [222/249], loss=5.1658
	step [223/249], loss=4.6159
	step [224/249], loss=5.0017
	step [225/249], loss=4.8302
	step [226/249], loss=5.8547
	step [227/249], loss=6.7736
	step [228/249], loss=6.4012
	step [229/249], loss=4.8230
	step [230/249], loss=5.4973
	step [231/249], loss=5.3379
	step [232/249], loss=5.5809
	step [233/249], loss=4.8526
	step [234/249], loss=5.8930
	step [235/249], loss=5.2927
	step [236/249], loss=4.8258
	step [237/249], loss=5.5357
	step [238/249], loss=5.2263
	step [239/249], loss=6.0701
	step [240/249], loss=5.5355
	step [241/249], loss=5.4002
	step [242/249], loss=5.7079
	step [243/249], loss=5.4734
	step [244/249], loss=5.1421
	step [245/249], loss=5.6974
	step [246/249], loss=5.2048
	step [247/249], loss=4.9861
	step [248/249], loss=5.4003
	step [249/249], loss=2.8893
	Evaluating
	loss=0.0137, precision=0.2349, recall=0.9924, f1=0.3798
Training epoch 30
	step [1/249], loss=5.2754
	step [2/249], loss=4.3125
	step [3/249], loss=5.3900
	step [4/249], loss=4.9213
	step [5/249], loss=4.9230
	step [6/249], loss=4.7751
	step [7/249], loss=4.8837
	step [8/249], loss=5.6253
	step [9/249], loss=5.9828
	step [10/249], loss=4.7621
	step [11/249], loss=4.9717
	step [12/249], loss=4.8754
	step [13/249], loss=4.5842
	step [14/249], loss=5.5738
	step [15/249], loss=5.6555
	step [16/249], loss=4.0130
	step [17/249], loss=5.4415
	step [18/249], loss=5.5052
	step [19/249], loss=4.6476
	step [20/249], loss=4.6684
	step [21/249], loss=4.9628
	step [22/249], loss=4.3019
	step [23/249], loss=5.0989
	step [24/249], loss=4.8734
	step [25/249], loss=5.3090
	step [26/249], loss=4.0412
	step [27/249], loss=4.3306
	step [28/249], loss=4.6464
	step [29/249], loss=5.9442
	step [30/249], loss=4.3647
	step [31/249], loss=5.2302
	step [32/249], loss=4.0011
	step [33/249], loss=5.6296
	step [34/249], loss=3.6539
	step [35/249], loss=4.7446
	step [36/249], loss=5.2770
	step [37/249], loss=5.9194
	step [38/249], loss=5.0971
	step [39/249], loss=4.7053
	step [40/249], loss=5.7337
	step [41/249], loss=4.5955
	step [42/249], loss=4.7590
	step [43/249], loss=4.9629
	step [44/249], loss=5.6938
	step [45/249], loss=5.1365
	step [46/249], loss=5.1989
	step [47/249], loss=5.1154
	step [48/249], loss=5.4057
	step [49/249], loss=5.6477
	step [50/249], loss=4.1740
	step [51/249], loss=5.2284
	step [52/249], loss=4.4445
	step [53/249], loss=5.1503
	step [54/249], loss=5.1826
	step [55/249], loss=5.3742
	step [56/249], loss=4.5460
	step [57/249], loss=4.3986
	step [58/249], loss=6.5101
	step [59/249], loss=4.8982
	step [60/249], loss=4.2004
	step [61/249], loss=5.4807
	step [62/249], loss=4.6654
	step [63/249], loss=4.2604
	step [64/249], loss=4.9516
	step [65/249], loss=5.7012
	step [66/249], loss=5.4654
	step [67/249], loss=4.5991
	step [68/249], loss=5.0877
	step [69/249], loss=5.4601
	step [70/249], loss=4.6759
	step [71/249], loss=6.1113
	step [72/249], loss=5.2114
	step [73/249], loss=5.3262
	step [74/249], loss=4.1840
	step [75/249], loss=4.8094
	step [76/249], loss=5.2408
	step [77/249], loss=4.2178
	step [78/249], loss=5.0597
	step [79/249], loss=5.5403
	step [80/249], loss=4.9632
	step [81/249], loss=4.2051
	step [82/249], loss=4.5537
	step [83/249], loss=4.8280
	step [84/249], loss=6.8320
	step [85/249], loss=4.6141
	step [86/249], loss=4.3922
	step [87/249], loss=4.6033
	step [88/249], loss=5.0065
	step [89/249], loss=4.6338
	step [90/249], loss=5.3127
	step [91/249], loss=4.8708
	step [92/249], loss=4.9949
	step [93/249], loss=4.9496
	step [94/249], loss=5.2692
	step [95/249], loss=5.0045
	step [96/249], loss=5.2065
	step [97/249], loss=5.2534
	step [98/249], loss=5.6157
	step [99/249], loss=4.6759
	step [100/249], loss=6.6389
	step [101/249], loss=5.4176
	step [102/249], loss=5.8423
	step [103/249], loss=5.2984
	step [104/249], loss=4.3439
	step [105/249], loss=4.0387
	step [106/249], loss=5.0139
	step [107/249], loss=5.0889
	step [108/249], loss=5.1703
	step [109/249], loss=4.6471
	step [110/249], loss=5.3194
	step [111/249], loss=5.3285
	step [112/249], loss=5.1454
	step [113/249], loss=5.8734
	step [114/249], loss=5.1434
	step [115/249], loss=6.3353
	step [116/249], loss=4.1967
	step [117/249], loss=5.7545
	step [118/249], loss=5.6454
	step [119/249], loss=4.9518
	step [120/249], loss=4.6257
	step [121/249], loss=4.8176
	step [122/249], loss=4.4750
	step [123/249], loss=5.2860
	step [124/249], loss=6.4075
	step [125/249], loss=5.5370
	step [126/249], loss=5.0619
	step [127/249], loss=5.7354
	step [128/249], loss=4.5090
	step [129/249], loss=4.7656
	step [130/249], loss=5.5142
	step [131/249], loss=4.2822
	step [132/249], loss=4.5551
	step [133/249], loss=5.3664
	step [134/249], loss=4.9564
	step [135/249], loss=5.0868
	step [136/249], loss=5.5511
	step [137/249], loss=4.1856
	step [138/249], loss=5.9790
	step [139/249], loss=4.3040
	step [140/249], loss=6.0681
	step [141/249], loss=4.6594
	step [142/249], loss=5.0312
	step [143/249], loss=5.1011
	step [144/249], loss=4.6473
	step [145/249], loss=5.1560
	step [146/249], loss=6.2184
	step [147/249], loss=4.4707
	step [148/249], loss=4.8526
	step [149/249], loss=4.4011
	step [150/249], loss=5.0697
	step [151/249], loss=5.8112
	step [152/249], loss=4.0456
	step [153/249], loss=5.1099
	step [154/249], loss=5.2118
	step [155/249], loss=4.9007
	step [156/249], loss=4.5545
	step [157/249], loss=4.5995
	step [158/249], loss=4.9193
	step [159/249], loss=4.8882
	step [160/249], loss=4.2727
	step [161/249], loss=5.3087
	step [162/249], loss=4.1173
	step [163/249], loss=4.9805
	step [164/249], loss=4.5351
	step [165/249], loss=5.2267
	step [166/249], loss=4.4420
	step [167/249], loss=4.9381
	step [168/249], loss=5.4702
	step [169/249], loss=6.4383
	step [170/249], loss=4.6945
	step [171/249], loss=5.5855
	step [172/249], loss=6.0489
	step [173/249], loss=4.7862
	step [174/249], loss=5.1580
	step [175/249], loss=3.9014
	step [176/249], loss=5.1884
	step [177/249], loss=4.9375
	step [178/249], loss=5.8097
	step [179/249], loss=5.8340
	step [180/249], loss=4.1959
	step [181/249], loss=4.9943
	step [182/249], loss=4.4502
	step [183/249], loss=5.3714
	step [184/249], loss=4.4952
	step [185/249], loss=4.8923
	step [186/249], loss=3.7345
	step [187/249], loss=5.5546
	step [188/249], loss=5.8452
	step [189/249], loss=4.6487
	step [190/249], loss=4.6175
	step [191/249], loss=4.8858
	step [192/249], loss=5.0906
	step [193/249], loss=4.8474
	step [194/249], loss=4.7150
	step [195/249], loss=4.8953
	step [196/249], loss=5.1853
	step [197/249], loss=4.9069
	step [198/249], loss=5.0051
	step [199/249], loss=4.7282
	step [200/249], loss=5.1579
	step [201/249], loss=5.2671
	step [202/249], loss=4.8753
	step [203/249], loss=6.5270
	step [204/249], loss=4.8250
	step [205/249], loss=5.4192
	step [206/249], loss=5.7621
	step [207/249], loss=4.1198
	step [208/249], loss=5.9455
	step [209/249], loss=4.8968
	step [210/249], loss=4.8471
	step [211/249], loss=4.6757
	step [212/249], loss=5.3932
	step [213/249], loss=5.3090
	step [214/249], loss=5.0048
	step [215/249], loss=5.5964
	step [216/249], loss=4.7432
	step [217/249], loss=5.4716
	step [218/249], loss=4.5019
	step [219/249], loss=4.9315
	step [220/249], loss=4.8035
	step [221/249], loss=4.5657
	step [222/249], loss=5.7811
	step [223/249], loss=4.6555
	step [224/249], loss=4.8451
	step [225/249], loss=5.1509
	step [226/249], loss=5.1366
	step [227/249], loss=5.0735
	step [228/249], loss=5.1634
	step [229/249], loss=4.9319
	step [230/249], loss=4.3656
	step [231/249], loss=5.4894
	step [232/249], loss=4.4764
	step [233/249], loss=5.5054
	step [234/249], loss=4.1124
	step [235/249], loss=4.7773
	step [236/249], loss=5.2257
	step [237/249], loss=4.9632
	step [238/249], loss=5.0802
	step [239/249], loss=4.8101
	step [240/249], loss=5.5394
	step [241/249], loss=5.3307
	step [242/249], loss=4.9064
	step [243/249], loss=5.0369
	step [244/249], loss=5.0339
	step [245/249], loss=4.5512
	step [246/249], loss=4.8206
	step [247/249], loss=4.9400
	step [248/249], loss=6.7991
	step [249/249], loss=3.1912
	Evaluating
	loss=0.0167, precision=0.2138, recall=0.9942, f1=0.3519
Training finished
best_f1: 0.40707687884283567
directing: Z rim_enhanced: False test_id 0
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12263 # image files with weight 12232
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3256 # image files with weight 3252
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Z 12232
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/192], loss=245.2497
	step [2/192], loss=222.0682
	step [3/192], loss=211.3184
	step [4/192], loss=207.4951
	step [5/192], loss=203.9915
	step [6/192], loss=199.0640
	step [7/192], loss=193.3695
	step [8/192], loss=192.8357
	step [9/192], loss=188.4004
	step [10/192], loss=185.9541
	step [11/192], loss=182.0952
	step [12/192], loss=179.0551
	step [13/192], loss=178.5240
	step [14/192], loss=172.9909
	step [15/192], loss=171.6267
	step [16/192], loss=172.2143
	step [17/192], loss=170.8752
	step [18/192], loss=165.8960
	step [19/192], loss=162.4828
	step [20/192], loss=163.3139
	step [21/192], loss=157.1384
	step [22/192], loss=157.8915
	step [23/192], loss=158.4813
	step [24/192], loss=155.1691
	step [25/192], loss=154.5010
	step [26/192], loss=152.7038
	step [27/192], loss=150.3227
	step [28/192], loss=146.8676
	step [29/192], loss=147.5086
	step [30/192], loss=145.7068
	step [31/192], loss=145.9941
	step [32/192], loss=141.7182
	step [33/192], loss=142.2783
	step [34/192], loss=139.3760
	step [35/192], loss=139.3998
	step [36/192], loss=135.2747
	step [37/192], loss=138.0157
	step [38/192], loss=140.1821
	step [39/192], loss=135.8666
	step [40/192], loss=134.5308
	step [41/192], loss=134.5207
	step [42/192], loss=130.3578
	step [43/192], loss=132.5649
	step [44/192], loss=131.6168
	step [45/192], loss=130.4854
	step [46/192], loss=128.3167
	step [47/192], loss=126.4524
	step [48/192], loss=126.9180
	step [49/192], loss=129.3406
	step [50/192], loss=124.1951
	step [51/192], loss=128.6994
	step [52/192], loss=125.4982
	step [53/192], loss=124.4010
	step [54/192], loss=124.3927
	step [55/192], loss=124.7090
	step [56/192], loss=123.8844
	step [57/192], loss=121.0921
	step [58/192], loss=123.0913
	step [59/192], loss=121.6259
	step [60/192], loss=120.6620
	step [61/192], loss=121.2251
	step [62/192], loss=119.2408
	step [63/192], loss=118.8270
	step [64/192], loss=117.7647
	step [65/192], loss=120.4909
	step [66/192], loss=117.0983
	step [67/192], loss=117.0762
	step [68/192], loss=118.6695
	step [69/192], loss=116.8516
	step [70/192], loss=116.7248
	step [71/192], loss=114.8549
	step [72/192], loss=112.3780
	step [73/192], loss=115.4282
	step [74/192], loss=111.5052
	step [75/192], loss=113.6716
	step [76/192], loss=115.5984
	step [77/192], loss=116.6507
	step [78/192], loss=112.9188
	step [79/192], loss=112.7695
	step [80/192], loss=112.5070
	step [81/192], loss=109.8928
	step [82/192], loss=110.5033
	step [83/192], loss=114.3854
	step [84/192], loss=111.1856
	step [85/192], loss=112.0453
	step [86/192], loss=112.9251
	step [87/192], loss=111.8171
	step [88/192], loss=107.9604
	step [89/192], loss=111.4992
	step [90/192], loss=106.6848
	step [91/192], loss=107.6348
	step [92/192], loss=108.9350
	step [93/192], loss=108.7410
	step [94/192], loss=108.3648
	step [95/192], loss=106.8233
	step [96/192], loss=108.1557
	step [97/192], loss=108.1824
	step [98/192], loss=106.0392
	step [99/192], loss=106.1852
	step [100/192], loss=105.0331
	step [101/192], loss=105.1156
	step [102/192], loss=105.7861
	step [103/192], loss=105.8004
	step [104/192], loss=105.0990
	step [105/192], loss=104.5149
	step [106/192], loss=104.4294
	step [107/192], loss=104.0376
	step [108/192], loss=103.8375
	step [109/192], loss=105.3520
	step [110/192], loss=103.1128
	step [111/192], loss=101.2051
	step [112/192], loss=103.6141
	step [113/192], loss=102.5274
	step [114/192], loss=102.9298
	step [115/192], loss=101.3556
	step [116/192], loss=102.0475
	step [117/192], loss=102.3994
	step [118/192], loss=102.2308
	step [119/192], loss=102.7197
	step [120/192], loss=100.8642
	step [121/192], loss=100.9233
	step [122/192], loss=99.6426
	step [123/192], loss=102.7472
	step [124/192], loss=101.4038
	step [125/192], loss=99.7186
	step [126/192], loss=99.5242
	step [127/192], loss=100.5852
	step [128/192], loss=97.4439
	step [129/192], loss=99.0375
	step [130/192], loss=98.7709
	step [131/192], loss=99.8974
	step [132/192], loss=99.4336
	step [133/192], loss=99.2915
	step [134/192], loss=99.4966
	step [135/192], loss=97.9317
	step [136/192], loss=98.8953
	step [137/192], loss=97.8035
	step [138/192], loss=100.2230
	step [139/192], loss=98.6319
	step [140/192], loss=97.7811
	step [141/192], loss=97.9878
	step [142/192], loss=96.2576
	step [143/192], loss=97.3323
	step [144/192], loss=96.5751
	step [145/192], loss=95.6731
	step [146/192], loss=96.3416
	step [147/192], loss=97.6348
	step [148/192], loss=96.2564
	step [149/192], loss=97.8557
	step [150/192], loss=97.9271
	step [151/192], loss=95.2371
	step [152/192], loss=96.4629
	step [153/192], loss=96.9480
	step [154/192], loss=94.9516
	step [155/192], loss=99.1034
	step [156/192], loss=97.0971
	step [157/192], loss=94.8364
	step [158/192], loss=93.7819
	step [159/192], loss=95.7096
	step [160/192], loss=94.5987
	step [161/192], loss=94.6969
	step [162/192], loss=94.0189
	step [163/192], loss=93.4266
	step [164/192], loss=93.0844
	step [165/192], loss=93.7845
	step [166/192], loss=93.8370
	step [167/192], loss=95.6868
	step [168/192], loss=94.2446
	step [169/192], loss=93.7104
	step [170/192], loss=93.9244
	step [171/192], loss=93.2820
	step [172/192], loss=92.9447
	step [173/192], loss=93.4047
	step [174/192], loss=92.8438
	step [175/192], loss=95.6974
	step [176/192], loss=91.2709
	step [177/192], loss=91.5130
	step [178/192], loss=90.6429
	step [179/192], loss=92.2888
	step [180/192], loss=93.3856
	step [181/192], loss=91.7703
	step [182/192], loss=93.7681
	step [183/192], loss=90.6475
	step [184/192], loss=90.7179
	step [185/192], loss=92.3027
	step [186/192], loss=89.3782
	step [187/192], loss=91.6708
	step [188/192], loss=90.0077
	step [189/192], loss=92.7255
	step [190/192], loss=90.0842
	step [191/192], loss=89.9556
	step [192/192], loss=12.5344
	Evaluating
	loss=0.3404, precision=0.1132, recall=0.9970, f1=0.2034
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/192], loss=91.4487
	step [2/192], loss=89.9644
	step [3/192], loss=89.9222
	step [4/192], loss=89.4480
	step [5/192], loss=88.4370
	step [6/192], loss=89.8941
	step [7/192], loss=90.4130
	step [8/192], loss=89.4834
	step [9/192], loss=88.2563
	step [10/192], loss=87.8312
	step [11/192], loss=88.4952
	step [12/192], loss=88.5132
	step [13/192], loss=89.1292
	step [14/192], loss=88.9078
	step [15/192], loss=89.3452
	step [16/192], loss=89.9029
	step [17/192], loss=88.5787
	step [18/192], loss=89.0493
	step [19/192], loss=88.8250
	step [20/192], loss=86.1099
	step [21/192], loss=87.2991
	step [22/192], loss=87.5077
	step [23/192], loss=86.8533
	step [24/192], loss=88.0213
	step [25/192], loss=85.9986
	step [26/192], loss=84.9284
	step [27/192], loss=85.7718
	step [28/192], loss=86.2694
	step [29/192], loss=88.3719
	step [30/192], loss=86.9838
	step [31/192], loss=83.8266
	step [32/192], loss=85.9825
	step [33/192], loss=86.4045
	step [34/192], loss=85.8527
	step [35/192], loss=87.7933
	step [36/192], loss=86.5615
	step [37/192], loss=85.6322
	step [38/192], loss=84.9469
	step [39/192], loss=84.3352
	step [40/192], loss=85.4373
	step [41/192], loss=84.6952
	step [42/192], loss=83.7442
	step [43/192], loss=84.7261
	step [44/192], loss=84.5749
	step [45/192], loss=83.7931
	step [46/192], loss=84.7939
	step [47/192], loss=83.0639
	step [48/192], loss=82.9087
	step [49/192], loss=85.5308
	step [50/192], loss=83.3901
	step [51/192], loss=84.0668
	step [52/192], loss=83.9745
	step [53/192], loss=83.6806
	step [54/192], loss=83.4897
	step [55/192], loss=81.2307
	step [56/192], loss=83.8712
	step [57/192], loss=82.4585
	step [58/192], loss=84.8511
	step [59/192], loss=83.8117
	step [60/192], loss=81.5784
	step [61/192], loss=82.7483
	step [62/192], loss=83.9645
	step [63/192], loss=83.8122
	step [64/192], loss=85.9106
	step [65/192], loss=82.0710
	step [66/192], loss=83.4941
	step [67/192], loss=84.6146
	step [68/192], loss=83.8189
	step [69/192], loss=81.4415
	step [70/192], loss=81.7177
	step [71/192], loss=81.5323
	step [72/192], loss=79.7695
	step [73/192], loss=83.3597
	step [74/192], loss=80.6508
	step [75/192], loss=83.6208
	step [76/192], loss=80.4645
	step [77/192], loss=79.1666
	step [78/192], loss=81.7595
	step [79/192], loss=80.7054
	step [80/192], loss=81.3268
	step [81/192], loss=79.9074
	step [82/192], loss=79.1775
	step [83/192], loss=80.1285
	step [84/192], loss=80.2453
	step [85/192], loss=81.9595
	step [86/192], loss=80.4429
	step [87/192], loss=78.7236
	step [88/192], loss=79.4288
	step [89/192], loss=79.8705
	step [90/192], loss=79.4128
	step [91/192], loss=81.3524
	step [92/192], loss=79.5580
	step [93/192], loss=80.7018
	step [94/192], loss=80.1398
	step [95/192], loss=79.7392
	step [96/192], loss=79.9466
	step [97/192], loss=79.0464
	step [98/192], loss=79.2787
	step [99/192], loss=77.0994
	step [100/192], loss=76.9711
	step [101/192], loss=79.8248
	step [102/192], loss=79.4510
	step [103/192], loss=77.8010
	step [104/192], loss=81.3744
	step [105/192], loss=78.6423
	step [106/192], loss=78.3718
	step [107/192], loss=76.6528
	step [108/192], loss=77.4927
	step [109/192], loss=75.6494
	step [110/192], loss=79.3846
	step [111/192], loss=79.0177
	step [112/192], loss=78.7653
	step [113/192], loss=77.5129
	step [114/192], loss=76.0980
	step [115/192], loss=77.2758
	step [116/192], loss=77.3816
	step [117/192], loss=77.0303
	step [118/192], loss=75.8519
	step [119/192], loss=76.1110
	step [120/192], loss=76.1431
	step [121/192], loss=77.3453
	step [122/192], loss=76.9906
	step [123/192], loss=75.4301
	step [124/192], loss=78.0894
	step [125/192], loss=77.2686
	step [126/192], loss=76.2324
	step [127/192], loss=74.5732
	step [128/192], loss=74.9848
	step [129/192], loss=78.6657
	step [130/192], loss=75.5919
	step [131/192], loss=75.1545
	step [132/192], loss=74.6166
	step [133/192], loss=74.9800
	step [134/192], loss=75.6005
	step [135/192], loss=76.4268
	step [136/192], loss=74.2731
	step [137/192], loss=75.6883
	step [138/192], loss=75.2673
	step [139/192], loss=75.3885
	step [140/192], loss=73.9111
	step [141/192], loss=74.9733
	step [142/192], loss=76.2483
	step [143/192], loss=74.1113
	step [144/192], loss=75.1354
	step [145/192], loss=75.2274
	step [146/192], loss=74.9017
	step [147/192], loss=75.0068
	step [148/192], loss=72.4197
	step [149/192], loss=72.5760
	step [150/192], loss=74.0784
	step [151/192], loss=74.1580
	step [152/192], loss=74.5850
	step [153/192], loss=75.6722
	step [154/192], loss=72.5003
	step [155/192], loss=71.9959
	step [156/192], loss=72.4494
	step [157/192], loss=73.5131
	step [158/192], loss=72.2244
	step [159/192], loss=73.5555
	step [160/192], loss=74.7337
	step [161/192], loss=73.1014
	step [162/192], loss=73.5540
	step [163/192], loss=74.0920
	step [164/192], loss=70.7372
	step [165/192], loss=73.9264
	step [166/192], loss=72.0407
	step [167/192], loss=73.1503
	step [168/192], loss=72.1481
	step [169/192], loss=72.9857
	step [170/192], loss=71.2508
	step [171/192], loss=71.9878
	step [172/192], loss=72.7093
	step [173/192], loss=70.0537
	step [174/192], loss=72.0310
	step [175/192], loss=70.8201
	step [176/192], loss=69.2368
	step [177/192], loss=71.5298
	step [178/192], loss=69.5556
	step [179/192], loss=73.0164
	step [180/192], loss=70.5817
	step [181/192], loss=70.7991
	step [182/192], loss=70.3663
	step [183/192], loss=72.1670
	step [184/192], loss=69.8912
	step [185/192], loss=71.1587
	step [186/192], loss=68.9980
	step [187/192], loss=68.6965
	step [188/192], loss=69.9917
	step [189/192], loss=68.2298
	step [190/192], loss=68.8714
	step [191/192], loss=69.6704
	step [192/192], loss=10.8678
	Evaluating
	loss=0.2949, precision=0.1505, recall=0.9960, f1=0.2615
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/192], loss=69.2763
	step [2/192], loss=69.7411
	step [3/192], loss=70.8125
	step [4/192], loss=67.5296
	step [5/192], loss=70.1013
	step [6/192], loss=69.4445
	step [7/192], loss=69.8796
	step [8/192], loss=69.6611
	step [9/192], loss=68.7010
	step [10/192], loss=67.9529
	step [11/192], loss=67.6031
	step [12/192], loss=70.5027
	step [13/192], loss=69.6455
	step [14/192], loss=70.7505
	step [15/192], loss=69.0071
	step [16/192], loss=69.2120
	step [17/192], loss=67.9595
	step [18/192], loss=68.6631
	step [19/192], loss=70.2591
	step [20/192], loss=68.6332
	step [21/192], loss=69.8309
	step [22/192], loss=69.9901
	step [23/192], loss=69.4607
	step [24/192], loss=68.2036
	step [25/192], loss=67.5069
	step [26/192], loss=68.4414
	step [27/192], loss=67.0172
	step [28/192], loss=68.7490
	step [29/192], loss=68.5704
	step [30/192], loss=65.1827
	step [31/192], loss=68.3877
	step [32/192], loss=65.8164
	step [33/192], loss=68.2630
	step [34/192], loss=66.3761
	step [35/192], loss=65.4777
	step [36/192], loss=66.8851
	step [37/192], loss=65.7844
	step [38/192], loss=66.8116
	step [39/192], loss=65.0822
	step [40/192], loss=67.6287
	step [41/192], loss=66.5084
	step [42/192], loss=66.3916
	step [43/192], loss=64.8610
	step [44/192], loss=65.8392
	step [45/192], loss=68.2128
	step [46/192], loss=65.6058
	step [47/192], loss=66.0529
	step [48/192], loss=65.1505
	step [49/192], loss=64.8547
	step [50/192], loss=64.3050
	step [51/192], loss=65.7977
	step [52/192], loss=64.6507
	step [53/192], loss=63.1468
	step [54/192], loss=67.0010
	step [55/192], loss=65.4608
	step [56/192], loss=66.1113
	step [57/192], loss=63.4163
	step [58/192], loss=64.0413
	step [59/192], loss=67.4233
	step [60/192], loss=63.6819
	step [61/192], loss=65.4276
	step [62/192], loss=64.4044
	step [63/192], loss=66.4956
	step [64/192], loss=64.4172
	step [65/192], loss=64.0560
	step [66/192], loss=63.7483
	step [67/192], loss=63.5025
	step [68/192], loss=65.1443
	step [69/192], loss=64.4350
	step [70/192], loss=64.6631
	step [71/192], loss=62.6680
	step [72/192], loss=64.8124
	step [73/192], loss=63.5610
	step [74/192], loss=61.6109
	step [75/192], loss=63.5530
	step [76/192], loss=63.4042
	step [77/192], loss=62.8670
	step [78/192], loss=63.4852
	step [79/192], loss=63.7273
	step [80/192], loss=63.0955
	step [81/192], loss=62.5458
	step [82/192], loss=63.4616
	step [83/192], loss=62.2340
	step [84/192], loss=61.7017
	step [85/192], loss=61.9823
	step [86/192], loss=61.9804
	step [87/192], loss=62.0590
	step [88/192], loss=62.7036
	step [89/192], loss=62.4192
	step [90/192], loss=63.9183
	step [91/192], loss=63.7013
	step [92/192], loss=62.2561
	step [93/192], loss=62.1027
	step [94/192], loss=62.8006
	step [95/192], loss=62.0302
	step [96/192], loss=61.4514
	step [97/192], loss=61.3359
	step [98/192], loss=62.9009
	step [99/192], loss=60.4270
	step [100/192], loss=60.2130
	step [101/192], loss=64.8385
	step [102/192], loss=62.1594
	step [103/192], loss=61.0398
	step [104/192], loss=60.5212
	step [105/192], loss=62.0749
	step [106/192], loss=61.9742
	step [107/192], loss=60.9700
	step [108/192], loss=60.8448
	step [109/192], loss=62.1093
	step [110/192], loss=61.5896
	step [111/192], loss=60.0141
	step [112/192], loss=60.9518
	step [113/192], loss=60.5678
	step [114/192], loss=61.2900
	step [115/192], loss=59.7723
	step [116/192], loss=61.5789
	step [117/192], loss=63.1550
	step [118/192], loss=59.6346
	step [119/192], loss=61.0123
	step [120/192], loss=61.2703
	step [121/192], loss=61.1505
	step [122/192], loss=60.1137
	step [123/192], loss=58.7789
	step [124/192], loss=59.1349
	step [125/192], loss=59.3314
	step [126/192], loss=58.9645
	step [127/192], loss=60.0037
	step [128/192], loss=57.3310
	step [129/192], loss=57.3494
	step [130/192], loss=58.4341
	step [131/192], loss=58.6429
	step [132/192], loss=58.3394
	step [133/192], loss=57.1543
	step [134/192], loss=57.2682
	step [135/192], loss=58.6628
	step [136/192], loss=57.5965
	step [137/192], loss=58.5075
	step [138/192], loss=58.7928
	step [139/192], loss=58.4364
	step [140/192], loss=60.7373
	step [141/192], loss=58.8164
	step [142/192], loss=57.3459
	step [143/192], loss=59.2307
	step [144/192], loss=58.6514
	step [145/192], loss=55.7777
	step [146/192], loss=58.6189
	step [147/192], loss=60.5160
	step [148/192], loss=58.0090
	step [149/192], loss=57.1501
	step [150/192], loss=55.9469
	step [151/192], loss=57.8106
	step [152/192], loss=58.5569
	step [153/192], loss=57.1220
	step [154/192], loss=56.6048
	step [155/192], loss=59.0682
	step [156/192], loss=57.5024
	step [157/192], loss=57.2341
	step [158/192], loss=55.9669
	step [159/192], loss=55.9470
	step [160/192], loss=58.4794
	step [161/192], loss=56.7622
	step [162/192], loss=56.7035
	step [163/192], loss=55.2198
	step [164/192], loss=56.9056
	step [165/192], loss=56.2016
	step [166/192], loss=55.7861
	step [167/192], loss=54.3044
	step [168/192], loss=53.9494
	step [169/192], loss=54.3981
	step [170/192], loss=56.5064
	step [171/192], loss=54.8355
	step [172/192], loss=56.3229
	step [173/192], loss=54.0088
	step [174/192], loss=56.4181
	step [175/192], loss=54.3432
	step [176/192], loss=55.8205
	step [177/192], loss=55.7818
	step [178/192], loss=54.4007
	step [179/192], loss=56.4041
	step [180/192], loss=53.6900
	step [181/192], loss=53.4346
	step [182/192], loss=54.0798
	step [183/192], loss=54.2648
	step [184/192], loss=54.7889
	step [185/192], loss=54.6670
	step [186/192], loss=57.2649
	step [187/192], loss=52.5893
	step [188/192], loss=54.5019
	step [189/192], loss=52.2073
	step [190/192], loss=55.3018
	step [191/192], loss=55.5335
	step [192/192], loss=6.5515
	Evaluating
	loss=0.4935, precision=0.0757, recall=0.9964, f1=0.1407
Training epoch 4
	step [1/192], loss=55.3112
	step [2/192], loss=57.0757
	step [3/192], loss=54.6216
	step [4/192], loss=55.7647
	step [5/192], loss=52.6910
	step [6/192], loss=53.6592
	step [7/192], loss=55.1094
	step [8/192], loss=53.5078
	step [9/192], loss=53.0995
	step [10/192], loss=51.6616
	step [11/192], loss=54.1959
	step [12/192], loss=52.0958
	step [13/192], loss=54.3501
	step [14/192], loss=51.8266
	step [15/192], loss=53.9119
	step [16/192], loss=52.7411
	step [17/192], loss=52.0512
	step [18/192], loss=51.9846
	step [19/192], loss=52.1158
	step [20/192], loss=52.1246
	step [21/192], loss=53.6142
	step [22/192], loss=52.7268
	step [23/192], loss=53.1392
	step [24/192], loss=51.8095
	step [25/192], loss=51.7577
	step [26/192], loss=51.1848
	step [27/192], loss=53.9460
	step [28/192], loss=53.7626
	step [29/192], loss=51.5074
	step [30/192], loss=50.8862
	step [31/192], loss=51.3918
	step [32/192], loss=50.9385
	step [33/192], loss=51.3679
	step [34/192], loss=52.7240
	step [35/192], loss=50.6682
	step [36/192], loss=51.5564
	step [37/192], loss=52.0531
	step [38/192], loss=51.6998
	step [39/192], loss=51.1296
	step [40/192], loss=50.1757
	step [41/192], loss=51.7966
	step [42/192], loss=50.6655
	step [43/192], loss=50.7147
	step [44/192], loss=51.0550
	step [45/192], loss=49.5912
	step [46/192], loss=50.6496
	step [47/192], loss=49.5985
	step [48/192], loss=49.5065
	step [49/192], loss=50.9230
	step [50/192], loss=50.6050
	step [51/192], loss=49.8951
	step [52/192], loss=51.7596
	step [53/192], loss=50.5440
	step [54/192], loss=50.5828
	step [55/192], loss=50.0730
	step [56/192], loss=49.4047
	step [57/192], loss=49.8413
	step [58/192], loss=48.6933
	step [59/192], loss=49.2286
	step [60/192], loss=50.1063
	step [61/192], loss=51.4709
	step [62/192], loss=51.3770
	step [63/192], loss=50.2681
	step [64/192], loss=50.3212
	step [65/192], loss=49.7454
	step [66/192], loss=50.3713
	step [67/192], loss=48.1533
	step [68/192], loss=46.4686
	step [69/192], loss=48.5846
	step [70/192], loss=47.2670
	step [71/192], loss=49.5511
	step [72/192], loss=47.8672
	step [73/192], loss=48.0648
	step [74/192], loss=46.8318
	step [75/192], loss=48.8694
	step [76/192], loss=47.9112
	step [77/192], loss=48.5040
	step [78/192], loss=48.0894
	step [79/192], loss=46.6985
	step [80/192], loss=47.1320
	step [81/192], loss=48.0029
	step [82/192], loss=48.7293
	step [83/192], loss=46.5829
	step [84/192], loss=49.9663
	step [85/192], loss=47.7097
	step [86/192], loss=47.1106
	step [87/192], loss=48.9126
	step [88/192], loss=47.3497
	step [89/192], loss=46.6562
	step [90/192], loss=47.5073
	step [91/192], loss=47.0834
	step [92/192], loss=46.3826
	step [93/192], loss=46.8819
	step [94/192], loss=46.4794
	step [95/192], loss=48.4033
	step [96/192], loss=47.5075
	step [97/192], loss=47.0831
	step [98/192], loss=46.3643
	step [99/192], loss=46.1199
	step [100/192], loss=48.6191
	step [101/192], loss=45.8963
	step [102/192], loss=46.3308
	step [103/192], loss=47.2499
	step [104/192], loss=47.6395
	step [105/192], loss=47.0058
	step [106/192], loss=47.0296
	step [107/192], loss=46.1233
	step [108/192], loss=48.0462
	step [109/192], loss=46.5357
	step [110/192], loss=45.9182
	step [111/192], loss=47.7859
	step [112/192], loss=45.7166
	step [113/192], loss=45.5780
	step [114/192], loss=46.9600
	step [115/192], loss=45.1865
	step [116/192], loss=45.3757
	step [117/192], loss=48.5607
	step [118/192], loss=45.4614
	step [119/192], loss=45.6417
	step [120/192], loss=45.4338
	step [121/192], loss=47.9744
	step [122/192], loss=45.0860
	step [123/192], loss=45.4700
	step [124/192], loss=47.6536
	step [125/192], loss=47.0429
	step [126/192], loss=45.0363
	step [127/192], loss=45.9074
	step [128/192], loss=44.5665
	step [129/192], loss=43.9898
	step [130/192], loss=45.4912
	step [131/192], loss=45.9160
	step [132/192], loss=45.0654
	step [133/192], loss=43.6177
	step [134/192], loss=44.1001
	step [135/192], loss=43.9719
	step [136/192], loss=48.7783
	step [137/192], loss=46.7691
	step [138/192], loss=45.0603
	step [139/192], loss=44.4062
	step [140/192], loss=46.3229
	step [141/192], loss=44.9557
	step [142/192], loss=43.0881
	step [143/192], loss=44.3483
	step [144/192], loss=44.2740
	step [145/192], loss=43.1189
	step [146/192], loss=44.0066
	step [147/192], loss=44.3344
	step [148/192], loss=44.8865
	step [149/192], loss=44.6878
	step [150/192], loss=44.5852
	step [151/192], loss=45.4324
	step [152/192], loss=43.8793
	step [153/192], loss=42.3646
	step [154/192], loss=43.0268
	step [155/192], loss=44.2543
	step [156/192], loss=44.5599
	step [157/192], loss=43.1866
	step [158/192], loss=44.2316
	step [159/192], loss=45.1381
	step [160/192], loss=42.4531
	step [161/192], loss=42.0865
	step [162/192], loss=41.9091
	step [163/192], loss=45.1148
	step [164/192], loss=42.1653
	step [165/192], loss=41.8610
	step [166/192], loss=42.4300
	step [167/192], loss=42.9055
	step [168/192], loss=42.5817
	step [169/192], loss=41.7141
	step [170/192], loss=41.5940
	step [171/192], loss=42.5499
	step [172/192], loss=44.0099
	step [173/192], loss=44.1470
	step [174/192], loss=42.2700
	step [175/192], loss=43.8733
	step [176/192], loss=43.3279
	step [177/192], loss=43.1098
	step [178/192], loss=43.8924
	step [179/192], loss=44.0283
	step [180/192], loss=41.5465
	step [181/192], loss=42.8906
	step [182/192], loss=43.4866
	step [183/192], loss=43.7638
	step [184/192], loss=42.4525
	step [185/192], loss=41.0687
	step [186/192], loss=43.1778
	step [187/192], loss=42.3649
	step [188/192], loss=41.1945
	step [189/192], loss=41.4661
	step [190/192], loss=42.9597
	step [191/192], loss=41.6934
	step [192/192], loss=5.4104
	Evaluating
	loss=0.1495, precision=0.1679, recall=0.9956, f1=0.2874
saving model as: 0_saved_model.pth
Training epoch 5
	step [1/192], loss=41.9563
	step [2/192], loss=42.3040
	step [3/192], loss=40.4429
	step [4/192], loss=40.3655
	step [5/192], loss=41.3597
	step [6/192], loss=40.5567
	step [7/192], loss=41.7678
	step [8/192], loss=42.2083
	step [9/192], loss=41.3720
	step [10/192], loss=39.7995
	step [11/192], loss=43.8836
	step [12/192], loss=39.9015
	step [13/192], loss=40.7624
	step [14/192], loss=41.3107
	step [15/192], loss=41.2185
	step [16/192], loss=39.6487
	step [17/192], loss=40.4795
	step [18/192], loss=42.7353
	step [19/192], loss=40.6704
	step [20/192], loss=39.8805
	step [21/192], loss=40.7686
	step [22/192], loss=38.9350
	step [23/192], loss=40.8811
	step [24/192], loss=39.8078
	step [25/192], loss=39.8869
	step [26/192], loss=39.8803
	step [27/192], loss=40.0667
	step [28/192], loss=39.0924
	step [29/192], loss=38.7257
	step [30/192], loss=39.8452
	step [31/192], loss=39.8515
	step [32/192], loss=41.0636
	step [33/192], loss=39.8430
	step [34/192], loss=38.8903
	step [35/192], loss=40.8050
	step [36/192], loss=39.9479
	step [37/192], loss=40.5511
	step [38/192], loss=38.5916
	step [39/192], loss=38.0821
	step [40/192], loss=40.6901
	step [41/192], loss=39.1128
	step [42/192], loss=42.2086
	step [43/192], loss=41.1019
	step [44/192], loss=38.1603
	step [45/192], loss=40.9930
	step [46/192], loss=39.7673
	step [47/192], loss=41.5742
	step [48/192], loss=39.3212
	step [49/192], loss=40.2167
	step [50/192], loss=40.5538
	step [51/192], loss=38.3115
	step [52/192], loss=39.7851
	step [53/192], loss=38.8811
	step [54/192], loss=39.7320
	step [55/192], loss=38.3783
	step [56/192], loss=38.9835
	step [57/192], loss=39.1453
	step [58/192], loss=36.9266
	step [59/192], loss=39.0865
	step [60/192], loss=38.9109
	step [61/192], loss=40.0358
	step [62/192], loss=40.9794
	step [63/192], loss=40.0184
	step [64/192], loss=37.7918
	step [65/192], loss=38.4362
	step [66/192], loss=41.4964
	step [67/192], loss=39.3776
	step [68/192], loss=38.8463
	step [69/192], loss=37.7822
	step [70/192], loss=37.9050
	step [71/192], loss=39.5644
	step [72/192], loss=38.9131
	step [73/192], loss=38.1975
	step [74/192], loss=38.7618
	step [75/192], loss=39.2955
	step [76/192], loss=37.5473
	step [77/192], loss=39.6517
	step [78/192], loss=36.0539
	step [79/192], loss=37.5181
	step [80/192], loss=39.6674
	step [81/192], loss=36.1698
	step [82/192], loss=38.8181
	step [83/192], loss=40.1171
	step [84/192], loss=39.0890
	step [85/192], loss=38.5479
	step [86/192], loss=38.5052
	step [87/192], loss=36.4436
	step [88/192], loss=37.0533
	step [89/192], loss=36.2177
	step [90/192], loss=37.8226
	step [91/192], loss=39.1634
	step [92/192], loss=37.8545
	step [93/192], loss=36.9549
	step [94/192], loss=37.4748
	step [95/192], loss=37.0135
	step [96/192], loss=36.7167
	step [97/192], loss=37.6532
	step [98/192], loss=38.2804
	step [99/192], loss=36.9875
	step [100/192], loss=36.7835
	step [101/192], loss=38.4350
	step [102/192], loss=36.7378
	step [103/192], loss=38.5411
	step [104/192], loss=37.4928
	step [105/192], loss=36.4285
	step [106/192], loss=36.8224
	step [107/192], loss=37.3630
	step [108/192], loss=36.4353
	step [109/192], loss=36.0543
	step [110/192], loss=36.4657
	step [111/192], loss=37.0301
	step [112/192], loss=36.2441
	step [113/192], loss=36.8564
	step [114/192], loss=37.1871
	step [115/192], loss=35.8719
	step [116/192], loss=38.6382
	step [117/192], loss=35.4878
	step [118/192], loss=36.6901
	step [119/192], loss=34.5754
	step [120/192], loss=37.6772
	step [121/192], loss=36.2022
	step [122/192], loss=35.2301
	step [123/192], loss=36.6689
	step [124/192], loss=35.3477
	step [125/192], loss=33.9778
	step [126/192], loss=36.1880
	step [127/192], loss=35.0359
	step [128/192], loss=35.5074
	step [129/192], loss=38.7211
	step [130/192], loss=37.4446
	step [131/192], loss=35.7726
	step [132/192], loss=37.6872
	step [133/192], loss=37.3589
	step [134/192], loss=36.3997
	step [135/192], loss=36.2029
	step [136/192], loss=39.5101
	step [137/192], loss=37.8698
	step [138/192], loss=33.8444
	step [139/192], loss=36.4831
	step [140/192], loss=35.6505
	step [141/192], loss=37.7005
	step [142/192], loss=36.0688
	step [143/192], loss=34.9616
	step [144/192], loss=34.9285
	step [145/192], loss=36.4409
	step [146/192], loss=33.5462
	step [147/192], loss=34.0927
	step [148/192], loss=33.5024
	step [149/192], loss=37.0303
	step [150/192], loss=35.0325
	step [151/192], loss=37.3864
	step [152/192], loss=37.4396
	step [153/192], loss=33.5796
	step [154/192], loss=34.2732
	step [155/192], loss=34.9827
	step [156/192], loss=34.4623
	step [157/192], loss=33.6336
	step [158/192], loss=33.8958
	step [159/192], loss=34.5621
	step [160/192], loss=35.9833
	step [161/192], loss=36.5209
	step [162/192], loss=34.3318
	step [163/192], loss=34.2883
	step [164/192], loss=35.3188
	step [165/192], loss=35.8353
	step [166/192], loss=35.4202
	step [167/192], loss=34.5231
	step [168/192], loss=36.7398
	step [169/192], loss=33.9609
	step [170/192], loss=35.0680
	step [171/192], loss=33.1691
	step [172/192], loss=35.0357
	step [173/192], loss=33.3852
	step [174/192], loss=37.5427
	step [175/192], loss=34.3534
	step [176/192], loss=31.4014
	step [177/192], loss=33.7352
	step [178/192], loss=34.6520
	step [179/192], loss=33.1881
	step [180/192], loss=34.9855
	step [181/192], loss=33.8672
	step [182/192], loss=33.1360
	step [183/192], loss=35.9248
	step [184/192], loss=35.9286
	step [185/192], loss=35.2020
	step [186/192], loss=35.6907
	step [187/192], loss=34.3176
	step [188/192], loss=36.8688
	step [189/192], loss=35.4065
	step [190/192], loss=34.6421
	step [191/192], loss=34.9003
	step [192/192], loss=4.6227
	Evaluating
	loss=0.1230, precision=0.1428, recall=0.9965, f1=0.2498
Training epoch 6
	step [1/192], loss=34.5043
	step [2/192], loss=34.3189
	step [3/192], loss=32.7900
	step [4/192], loss=35.8611
	step [5/192], loss=36.1235
	step [6/192], loss=34.3311
	step [7/192], loss=35.2158
	step [8/192], loss=34.7659
	step [9/192], loss=32.3436
	step [10/192], loss=35.1940
	step [11/192], loss=34.9677
	step [12/192], loss=33.7034
	step [13/192], loss=32.3861
	step [14/192], loss=32.8773
	step [15/192], loss=33.8269
	step [16/192], loss=31.9483
	step [17/192], loss=31.2973
	step [18/192], loss=35.5009
	step [19/192], loss=35.3228
	step [20/192], loss=34.3391
	step [21/192], loss=36.4201
	step [22/192], loss=31.6177
	step [23/192], loss=33.3612
	step [24/192], loss=33.0234
	step [25/192], loss=33.2374
	step [26/192], loss=32.9303
	step [27/192], loss=33.3147
	step [28/192], loss=34.2303
	step [29/192], loss=31.9677
	step [30/192], loss=33.0800
	step [31/192], loss=31.7008
	step [32/192], loss=31.3972
	step [33/192], loss=33.1358
	step [34/192], loss=32.9366
	step [35/192], loss=33.4642
	step [36/192], loss=32.3031
	step [37/192], loss=33.4337
	step [38/192], loss=32.2782
	step [39/192], loss=32.6063
	step [40/192], loss=33.0237
	step [41/192], loss=31.5392
	step [42/192], loss=34.0331
	step [43/192], loss=30.1086
	step [44/192], loss=30.5729
	step [45/192], loss=34.2188
	step [46/192], loss=32.4854
	step [47/192], loss=36.2279
	step [48/192], loss=30.9378
	step [49/192], loss=31.5104
	step [50/192], loss=34.4872
	step [51/192], loss=32.5782
	step [52/192], loss=31.3776
	step [53/192], loss=31.0720
	step [54/192], loss=32.0652
	step [55/192], loss=33.5759
	step [56/192], loss=30.2945
	step [57/192], loss=32.3839
	step [58/192], loss=31.7267
	step [59/192], loss=30.9501
	step [60/192], loss=31.4673
	step [61/192], loss=30.9598
	step [62/192], loss=35.0641
	step [63/192], loss=31.8357
	step [64/192], loss=32.0352
	step [65/192], loss=31.2021
	step [66/192], loss=31.6459
	step [67/192], loss=32.1239
	step [68/192], loss=30.5638
	step [69/192], loss=29.8028
	step [70/192], loss=31.2340
	step [71/192], loss=30.4879
	step [72/192], loss=31.5192
	step [73/192], loss=32.3557
	step [74/192], loss=31.0276
	step [75/192], loss=29.5629
	step [76/192], loss=31.2977
	step [77/192], loss=31.1574
	step [78/192], loss=31.7755
	step [79/192], loss=31.4219
	step [80/192], loss=31.3791
	step [81/192], loss=30.6063
	step [82/192], loss=30.7436
	step [83/192], loss=31.8850
	step [84/192], loss=30.8788
	step [85/192], loss=30.8902
	step [86/192], loss=31.3345
	step [87/192], loss=32.1406
	step [88/192], loss=32.3441
	step [89/192], loss=31.4906
	step [90/192], loss=32.7627
	step [91/192], loss=30.5712
	step [92/192], loss=30.5355
	step [93/192], loss=30.4633
	step [94/192], loss=32.9131
	step [95/192], loss=29.8493
	step [96/192], loss=32.8911
	step [97/192], loss=30.6691
	step [98/192], loss=31.0236
	step [99/192], loss=32.0801
	step [100/192], loss=30.3037
	step [101/192], loss=34.5183
	step [102/192], loss=29.7431
	step [103/192], loss=30.6874
	step [104/192], loss=29.8204
	step [105/192], loss=29.3636
	step [106/192], loss=29.4820
	step [107/192], loss=30.5304
	step [108/192], loss=31.4197
	step [109/192], loss=29.5576
	step [110/192], loss=30.4587
	step [111/192], loss=31.0406
	step [112/192], loss=29.9250
	step [113/192], loss=30.7708
	step [114/192], loss=29.9085
	step [115/192], loss=32.4911
	step [116/192], loss=31.7972
	step [117/192], loss=30.3436
	step [118/192], loss=28.7526
	step [119/192], loss=29.3612
	step [120/192], loss=27.5925
	step [121/192], loss=28.6228
	step [122/192], loss=28.1941
	step [123/192], loss=27.3547
	step [124/192], loss=29.7537
	step [125/192], loss=31.1449
	step [126/192], loss=30.0317
	step [127/192], loss=30.7348
	step [128/192], loss=28.7895
	step [129/192], loss=29.5702
	step [130/192], loss=31.9553
	step [131/192], loss=28.8706
	step [132/192], loss=31.0214
	step [133/192], loss=29.4146
	step [134/192], loss=33.3729
	step [135/192], loss=29.5950
	step [136/192], loss=27.8145
	step [137/192], loss=28.9440
	step [138/192], loss=27.2581
	step [139/192], loss=29.2223
	step [140/192], loss=28.3343
	step [141/192], loss=28.7332
	step [142/192], loss=29.6983
	step [143/192], loss=27.3165
	step [144/192], loss=29.9040
	step [145/192], loss=31.8503
	step [146/192], loss=30.9724
	step [147/192], loss=30.1071
	step [148/192], loss=28.5934
	step [149/192], loss=30.9788
	step [150/192], loss=28.5799
	step [151/192], loss=29.4237
	step [152/192], loss=28.6724
	step [153/192], loss=28.3664
	step [154/192], loss=29.6196
	step [155/192], loss=29.2667
	step [156/192], loss=26.7563
	step [157/192], loss=28.6120
	step [158/192], loss=29.7011
	step [159/192], loss=28.4121
	step [160/192], loss=29.7392
	step [161/192], loss=26.3650
	step [162/192], loss=29.0966
	step [163/192], loss=29.0159
	step [164/192], loss=28.1940
	step [165/192], loss=28.3360
	step [166/192], loss=27.0636
	step [167/192], loss=29.7314
	step [168/192], loss=28.9593
	step [169/192], loss=28.8309
	step [170/192], loss=30.4903
	step [171/192], loss=30.7180
	step [172/192], loss=29.0761
	step [173/192], loss=28.9654
	step [174/192], loss=29.0130
	step [175/192], loss=30.4736
	step [176/192], loss=25.6903
	step [177/192], loss=28.2793
	step [178/192], loss=29.6481
	step [179/192], loss=29.1812
	step [180/192], loss=27.8150
	step [181/192], loss=29.0068
	step [182/192], loss=26.5432
	step [183/192], loss=27.7901
	step [184/192], loss=27.1016
	step [185/192], loss=27.3835
	step [186/192], loss=27.8957
	step [187/192], loss=29.3947
	step [188/192], loss=29.4338
	step [189/192], loss=28.1623
	step [190/192], loss=29.4341
	step [191/192], loss=30.0615
	step [192/192], loss=3.8936
	Evaluating
	loss=0.1008, precision=0.1475, recall=0.9962, f1=0.2569
Training epoch 7
	step [1/192], loss=30.5402
	step [2/192], loss=27.1170
	step [3/192], loss=26.9292
	step [4/192], loss=28.5443
	step [5/192], loss=27.0308
	step [6/192], loss=29.9604
	step [7/192], loss=28.7171
	step [8/192], loss=27.4357
	step [9/192], loss=29.0369
	step [10/192], loss=31.4648
	step [11/192], loss=30.8872
	step [12/192], loss=29.5577
	step [13/192], loss=29.8292
	step [14/192], loss=29.7667
	step [15/192], loss=27.7683
	step [16/192], loss=26.8893
	step [17/192], loss=28.5649
	step [18/192], loss=26.2352
	step [19/192], loss=27.2414
	step [20/192], loss=29.1257
	step [21/192], loss=25.9363
	step [22/192], loss=26.7617
	step [23/192], loss=30.6784
	step [24/192], loss=27.5881
	step [25/192], loss=31.5308
	step [26/192], loss=29.0729
	step [27/192], loss=26.1620
	step [28/192], loss=27.1228
	step [29/192], loss=26.8710
	step [30/192], loss=28.2056
	step [31/192], loss=26.3227
	step [32/192], loss=27.4500
	step [33/192], loss=25.5971
	step [34/192], loss=28.1123
	step [35/192], loss=28.4797
	step [36/192], loss=28.4491
	step [37/192], loss=30.2309
	step [38/192], loss=26.3329
	step [39/192], loss=28.2336
	step [40/192], loss=28.7261
	step [41/192], loss=26.9224
	step [42/192], loss=26.6840
	step [43/192], loss=28.1440
	step [44/192], loss=29.0490
	step [45/192], loss=26.3968
	step [46/192], loss=26.7532
	step [47/192], loss=26.9753
	step [48/192], loss=25.9392
	step [49/192], loss=27.2455
	step [50/192], loss=27.3511
	step [51/192], loss=29.0665
	step [52/192], loss=28.1386
	step [53/192], loss=26.4658
	step [54/192], loss=27.8004
	step [55/192], loss=29.4613
	step [56/192], loss=26.1636
	step [57/192], loss=27.9878
	step [58/192], loss=28.7157
	step [59/192], loss=25.3569
	step [60/192], loss=27.9705
	step [61/192], loss=28.9586
	step [62/192], loss=25.6966
	step [63/192], loss=26.7792
	step [64/192], loss=26.3518
	step [65/192], loss=27.8364
	step [66/192], loss=27.2788
	step [67/192], loss=27.0778
	step [68/192], loss=26.3981
	step [69/192], loss=28.5333
	step [70/192], loss=27.1882
	step [71/192], loss=26.1193
	step [72/192], loss=26.7326
	step [73/192], loss=27.2585
	step [74/192], loss=27.4377
	step [75/192], loss=25.6769
	step [76/192], loss=25.8290
	step [77/192], loss=26.1356
	step [78/192], loss=26.7975
	step [79/192], loss=27.6796
	step [80/192], loss=28.2078
	step [81/192], loss=26.1465
	step [82/192], loss=27.4372
	step [83/192], loss=28.1270
	step [84/192], loss=26.1737
	step [85/192], loss=24.3670
	step [86/192], loss=27.5546
	step [87/192], loss=26.6637
	step [88/192], loss=26.7308
	step [89/192], loss=27.4964
	step [90/192], loss=26.6584
	step [91/192], loss=26.0350
	step [92/192], loss=25.4696
	step [93/192], loss=27.1116
	step [94/192], loss=25.0092
	step [95/192], loss=25.4019
	step [96/192], loss=23.7434
	step [97/192], loss=25.3353
	step [98/192], loss=26.6562
	step [99/192], loss=27.8281
	step [100/192], loss=24.9728
	step [101/192], loss=24.9046
	step [102/192], loss=26.4322
	step [103/192], loss=26.3035
	step [104/192], loss=25.9842
	step [105/192], loss=26.6936
	step [106/192], loss=26.8943
	step [107/192], loss=26.8233
	step [108/192], loss=24.6813
	step [109/192], loss=23.5251
	step [110/192], loss=24.8059
	step [111/192], loss=26.2411
	step [112/192], loss=25.5533
	step [113/192], loss=26.1119
	step [114/192], loss=23.5576
	step [115/192], loss=23.8018
	step [116/192], loss=24.2291
	step [117/192], loss=24.7641
	step [118/192], loss=24.4804
	step [119/192], loss=27.6593
	step [120/192], loss=23.7600
	step [121/192], loss=27.4524
	step [122/192], loss=25.9685
	step [123/192], loss=25.1415
	step [124/192], loss=25.1026
	step [125/192], loss=27.3563
	step [126/192], loss=26.2036
	step [127/192], loss=25.7355
	step [128/192], loss=24.4744
	step [129/192], loss=24.0687
	step [130/192], loss=26.7538
	step [131/192], loss=25.5757
	step [132/192], loss=24.4891
	step [133/192], loss=28.3549
	step [134/192], loss=25.3286
	step [135/192], loss=27.4904
	step [136/192], loss=24.4240
	step [137/192], loss=24.1006
	step [138/192], loss=26.5983
	step [139/192], loss=25.8988
	step [140/192], loss=24.2294
	step [141/192], loss=25.9808
	step [142/192], loss=25.8089
	step [143/192], loss=24.0418
	step [144/192], loss=24.8177
	step [145/192], loss=26.2543
	step [146/192], loss=25.5889
	step [147/192], loss=25.5985
	step [148/192], loss=24.4145
	step [149/192], loss=23.2622
	step [150/192], loss=25.2336
	step [151/192], loss=23.5962
	step [152/192], loss=24.5101
	step [153/192], loss=25.7687
	step [154/192], loss=25.5004
	step [155/192], loss=23.3872
	step [156/192], loss=21.7029
	step [157/192], loss=24.5134
	step [158/192], loss=26.0096
	step [159/192], loss=25.3303
	step [160/192], loss=22.4567
	step [161/192], loss=24.9440
	step [162/192], loss=24.4923
	step [163/192], loss=24.2526
	step [164/192], loss=24.8281
	step [165/192], loss=24.3746
	step [166/192], loss=25.5642
	step [167/192], loss=24.6360
	step [168/192], loss=24.5274
	step [169/192], loss=24.7213
	step [170/192], loss=24.8690
	step [171/192], loss=23.0561
	step [172/192], loss=22.9565
	step [173/192], loss=27.4065
	step [174/192], loss=25.5328
	step [175/192], loss=23.9146
	step [176/192], loss=24.3109
	step [177/192], loss=24.3047
	step [178/192], loss=23.5084
	step [179/192], loss=22.7534
	step [180/192], loss=25.3814
	step [181/192], loss=23.3257
	step [182/192], loss=22.8497
	step [183/192], loss=22.7275
	step [184/192], loss=22.9981
	step [185/192], loss=23.4305
	step [186/192], loss=24.5519
	step [187/192], loss=22.1187
	step [188/192], loss=23.2348
	step [189/192], loss=23.0775
	step [190/192], loss=23.3025
	step [191/192], loss=23.4277
	step [192/192], loss=2.8023
	Evaluating
	loss=0.0852, precision=0.1502, recall=0.9963, f1=0.2610
Training epoch 8
	step [1/192], loss=25.9265
	step [2/192], loss=23.9032
	step [3/192], loss=24.4872
	step [4/192], loss=24.5275
	step [5/192], loss=25.0086
	step [6/192], loss=24.7863
	step [7/192], loss=23.4879
	step [8/192], loss=23.5533
	step [9/192], loss=25.6729
	step [10/192], loss=22.7918
	step [11/192], loss=24.9533
	step [12/192], loss=23.1279
	step [13/192], loss=22.7642
	step [14/192], loss=24.1972
	step [15/192], loss=22.6656
	step [16/192], loss=24.8670
	step [17/192], loss=22.6019
	step [18/192], loss=25.5352
	step [19/192], loss=24.3688
	step [20/192], loss=23.9392
	step [21/192], loss=24.5421
	step [22/192], loss=23.2636
	step [23/192], loss=25.0281
	step [24/192], loss=24.9895
	step [25/192], loss=23.6798
	step [26/192], loss=22.5746
	step [27/192], loss=22.9910
	step [28/192], loss=25.1527
	step [29/192], loss=23.2668
	step [30/192], loss=24.0727
	step [31/192], loss=23.6071
	step [32/192], loss=23.5190
	step [33/192], loss=25.2478
	step [34/192], loss=25.0557
	step [35/192], loss=22.5392
	step [36/192], loss=22.1533
	step [37/192], loss=23.6093
	step [38/192], loss=23.5039
	step [39/192], loss=21.8311
	step [40/192], loss=23.4660
	step [41/192], loss=22.6522
	step [42/192], loss=22.0511
	step [43/192], loss=24.3662
	step [44/192], loss=21.6435
	step [45/192], loss=23.1477
	step [46/192], loss=22.4447
	step [47/192], loss=23.4153
	step [48/192], loss=26.7535
	step [49/192], loss=22.8217
	step [50/192], loss=24.5637
	step [51/192], loss=25.7403
	step [52/192], loss=22.1703
	step [53/192], loss=22.9675
	step [54/192], loss=24.0946
	step [55/192], loss=24.3407
	step [56/192], loss=23.8584
	step [57/192], loss=22.2494
	step [58/192], loss=21.4290
	step [59/192], loss=24.7234
	step [60/192], loss=20.3549
	step [61/192], loss=23.1744
	step [62/192], loss=24.5034
	step [63/192], loss=22.7128
	step [64/192], loss=23.5015
	step [65/192], loss=22.9675
	step [66/192], loss=23.6507
	step [67/192], loss=22.0847
	step [68/192], loss=21.9107
	step [69/192], loss=21.1676
	step [70/192], loss=23.0822
	step [71/192], loss=22.0523
	step [72/192], loss=25.4071
	step [73/192], loss=22.2716
	step [74/192], loss=22.3714
	step [75/192], loss=22.6228
	step [76/192], loss=25.1429
	step [77/192], loss=22.5843
	step [78/192], loss=21.3946
	step [79/192], loss=22.7764
	step [80/192], loss=21.8464
	step [81/192], loss=20.8341
	step [82/192], loss=23.0225
	step [83/192], loss=21.0964
	step [84/192], loss=20.6215
	step [85/192], loss=21.8188
	step [86/192], loss=21.3953
	step [87/192], loss=22.9114
	step [88/192], loss=24.7482
	step [89/192], loss=22.1564
	step [90/192], loss=23.7292
	step [91/192], loss=23.2450
	step [92/192], loss=23.6916
	step [93/192], loss=22.2589
	step [94/192], loss=22.9995
	step [95/192], loss=23.2787
	step [96/192], loss=22.7150
	step [97/192], loss=23.0350
	step [98/192], loss=22.4033
	step [99/192], loss=22.3966
	step [100/192], loss=21.7220
	step [101/192], loss=24.0769
	step [102/192], loss=22.0782
	step [103/192], loss=22.0197
	step [104/192], loss=23.9608
	step [105/192], loss=21.5254
	step [106/192], loss=20.7277
	step [107/192], loss=21.9329
	step [108/192], loss=22.2314
	step [109/192], loss=23.6715
	step [110/192], loss=21.9193
	step [111/192], loss=21.6419
	step [112/192], loss=22.4858
	step [113/192], loss=22.4156
	step [114/192], loss=22.1443
	step [115/192], loss=24.3129
	step [116/192], loss=21.4492
	step [117/192], loss=21.9444
	step [118/192], loss=22.9995
	step [119/192], loss=22.2291
	step [120/192], loss=22.8633
	step [121/192], loss=21.1947
	step [122/192], loss=21.8208
	step [123/192], loss=21.1704
	step [124/192], loss=22.2387
	step [125/192], loss=20.6655
	step [126/192], loss=21.9603
	step [127/192], loss=22.1525
	step [128/192], loss=20.8087
	step [129/192], loss=22.5237
	step [130/192], loss=19.6524
	step [131/192], loss=24.5971
	step [132/192], loss=20.6941
	step [133/192], loss=21.2534
	step [134/192], loss=21.4092
	step [135/192], loss=21.4883
	step [136/192], loss=22.1752
	step [137/192], loss=24.7321
	step [138/192], loss=22.2209
	step [139/192], loss=22.6408
	step [140/192], loss=22.3566
	step [141/192], loss=20.9873
	step [142/192], loss=22.5364
	step [143/192], loss=21.4167
	step [144/192], loss=20.6011
	step [145/192], loss=21.9363
	step [146/192], loss=22.1783
	step [147/192], loss=22.3337
	step [148/192], loss=22.9869
	step [149/192], loss=21.1810
	step [150/192], loss=22.3345
	step [151/192], loss=21.0402
	step [152/192], loss=23.6072
	step [153/192], loss=20.6948
	step [154/192], loss=21.1451
	step [155/192], loss=21.2966
	step [156/192], loss=20.6546
	step [157/192], loss=24.7214
	step [158/192], loss=20.5319
	step [159/192], loss=23.5274
	step [160/192], loss=22.2991
	step [161/192], loss=21.3715
	step [162/192], loss=22.5965
	step [163/192], loss=21.6664
	step [164/192], loss=22.8086
	step [165/192], loss=21.9704
	step [166/192], loss=25.1615
	step [167/192], loss=21.0131
	step [168/192], loss=21.3332
	step [169/192], loss=21.6634
	step [170/192], loss=19.5382
	step [171/192], loss=19.9934
	step [172/192], loss=22.5296
	step [173/192], loss=21.4192
	step [174/192], loss=24.2227
	step [175/192], loss=22.2641
	step [176/192], loss=22.2773
	step [177/192], loss=23.0864
	step [178/192], loss=22.5760
	step [179/192], loss=24.2098
	step [180/192], loss=22.2781
	step [181/192], loss=21.9941
	step [182/192], loss=21.7907
	step [183/192], loss=22.9304
	step [184/192], loss=19.4595
	step [185/192], loss=21.6300
	step [186/192], loss=22.8977
	step [187/192], loss=25.5360
	step [188/192], loss=20.6921
	step [189/192], loss=20.8400
	step [190/192], loss=20.5359
	step [191/192], loss=21.0266
	step [192/192], loss=2.7473
	Evaluating
	loss=0.0788, precision=0.1365, recall=0.9968, f1=0.2401
Training epoch 9
	step [1/192], loss=19.8610
	step [2/192], loss=21.2615
	step [3/192], loss=21.6720
	step [4/192], loss=19.8622
	step [5/192], loss=22.1418
	step [6/192], loss=21.6888
	step [7/192], loss=20.9094
	step [8/192], loss=23.4051
	step [9/192], loss=20.6378
	step [10/192], loss=23.6523
	step [11/192], loss=20.0353
	step [12/192], loss=19.9067
	step [13/192], loss=20.9441
	step [14/192], loss=21.8836
	step [15/192], loss=20.6091
	step [16/192], loss=22.8508
	step [17/192], loss=19.1609
	step [18/192], loss=21.2807
	step [19/192], loss=22.0088
	step [20/192], loss=20.5997
	step [21/192], loss=21.3650
	step [22/192], loss=20.7464
	step [23/192], loss=22.2800
	step [24/192], loss=20.7929
	step [25/192], loss=18.9852
	step [26/192], loss=17.9758
	step [27/192], loss=20.4529
	step [28/192], loss=20.6606
	step [29/192], loss=20.8006
	step [30/192], loss=19.7489
	step [31/192], loss=19.1448
	step [32/192], loss=22.1950
	step [33/192], loss=22.8747
	step [34/192], loss=23.0451
	step [35/192], loss=21.8896
	step [36/192], loss=21.2735
	step [37/192], loss=19.5257
	step [38/192], loss=19.3547
	step [39/192], loss=21.0624
	step [40/192], loss=20.8831
	step [41/192], loss=20.2639
	step [42/192], loss=22.3785
	step [43/192], loss=20.0203
	step [44/192], loss=18.6339
	step [45/192], loss=22.8030
	step [46/192], loss=20.9391
	step [47/192], loss=20.0349
	step [48/192], loss=21.4059
	step [49/192], loss=21.6340
	step [50/192], loss=20.3388
	step [51/192], loss=21.0795
	step [52/192], loss=20.2123
	step [53/192], loss=22.1508
	step [54/192], loss=21.0288
	step [55/192], loss=21.6732
	step [56/192], loss=20.7229
	step [57/192], loss=22.2937
	step [58/192], loss=22.6640
	step [59/192], loss=21.5798
	step [60/192], loss=20.8605
	step [61/192], loss=18.6399
	step [62/192], loss=20.7441
	step [63/192], loss=17.9009
	step [64/192], loss=19.9391
	step [65/192], loss=20.3661
	step [66/192], loss=23.7289
	step [67/192], loss=20.7058
	step [68/192], loss=19.2356
	step [69/192], loss=20.0475
	step [70/192], loss=20.3100
	step [71/192], loss=20.9549
	step [72/192], loss=20.4449
	step [73/192], loss=21.0350
	step [74/192], loss=20.0819
	step [75/192], loss=19.8009
	step [76/192], loss=20.4041
	step [77/192], loss=21.4025
	step [78/192], loss=19.1970
	step [79/192], loss=21.9984
	step [80/192], loss=18.7228
	step [81/192], loss=21.4016
	step [82/192], loss=19.6230
	step [83/192], loss=20.3873
	step [84/192], loss=20.2295
	step [85/192], loss=17.1817
	step [86/192], loss=18.9274
	step [87/192], loss=19.6590
	step [88/192], loss=19.5334
	step [89/192], loss=20.2629
	step [90/192], loss=19.5568
	step [91/192], loss=17.9201
	step [92/192], loss=18.8599
	step [93/192], loss=19.6123
	step [94/192], loss=20.1944
	step [95/192], loss=18.6241
	step [96/192], loss=20.6360
	step [97/192], loss=21.7018
	step [98/192], loss=19.3952
	step [99/192], loss=19.3844
	step [100/192], loss=19.0711
	step [101/192], loss=22.7959
	step [102/192], loss=19.6051
	step [103/192], loss=20.4132
	step [104/192], loss=18.7034
	step [105/192], loss=21.1195
	step [106/192], loss=19.3123
	step [107/192], loss=18.9543
	step [108/192], loss=20.4501
	step [109/192], loss=20.8796
	step [110/192], loss=20.2120
	step [111/192], loss=20.5804
	step [112/192], loss=19.1045
	step [113/192], loss=21.7555
	step [114/192], loss=20.1486
	step [115/192], loss=22.1981
	step [116/192], loss=21.0317
	step [117/192], loss=21.8694
	step [118/192], loss=19.3643
	step [119/192], loss=19.5477
	step [120/192], loss=18.1853
	step [121/192], loss=19.0153
	step [122/192], loss=20.7378
	step [123/192], loss=21.3829
	step [124/192], loss=21.0549
	step [125/192], loss=19.3383
	step [126/192], loss=17.9969
	step [127/192], loss=22.3732
	step [128/192], loss=18.8739
	step [129/192], loss=20.2496
	step [130/192], loss=18.4725
	step [131/192], loss=17.7158
	step [132/192], loss=16.8408
	step [133/192], loss=19.3798
	step [134/192], loss=20.8144
	step [135/192], loss=20.7859
	step [136/192], loss=19.3457
	step [137/192], loss=19.2593
	step [138/192], loss=20.9401
	step [139/192], loss=18.3641
	step [140/192], loss=19.8787
	step [141/192], loss=19.9129
	step [142/192], loss=19.8320
	step [143/192], loss=20.4341
	step [144/192], loss=20.1102
	step [145/192], loss=21.2540
	step [146/192], loss=18.7159
	step [147/192], loss=20.7634
	step [148/192], loss=18.3079
	step [149/192], loss=17.0421
	step [150/192], loss=18.9526
	step [151/192], loss=19.6328
	step [152/192], loss=18.9393
	step [153/192], loss=19.0397
	step [154/192], loss=22.1788
	step [155/192], loss=21.1989
	step [156/192], loss=20.7401
	step [157/192], loss=23.9668
	step [158/192], loss=19.0457
	step [159/192], loss=20.0885
	step [160/192], loss=20.8302
	step [161/192], loss=19.1440
	step [162/192], loss=18.6072
	step [163/192], loss=17.8983
	step [164/192], loss=20.2512
	step [165/192], loss=18.4748
	step [166/192], loss=17.2309
	step [167/192], loss=18.1543
	step [168/192], loss=17.6614
	step [169/192], loss=20.6980
	step [170/192], loss=18.7244
	step [171/192], loss=19.4856
	step [172/192], loss=19.7741
	step [173/192], loss=19.6066
	step [174/192], loss=17.6164
	step [175/192], loss=19.5433
	step [176/192], loss=21.7414
	step [177/192], loss=18.3222
	step [178/192], loss=19.2976
	step [179/192], loss=20.8025
	step [180/192], loss=19.5749
	step [181/192], loss=18.6223
	step [182/192], loss=21.2873
	step [183/192], loss=18.2515
	step [184/192], loss=18.1099
	step [185/192], loss=20.6218
	step [186/192], loss=19.4711
	step [187/192], loss=17.7942
	step [188/192], loss=18.0830
	step [189/192], loss=20.1533
	step [190/192], loss=18.6569
	step [191/192], loss=17.1717
	step [192/192], loss=2.1544
	Evaluating
	loss=0.0622, precision=0.1652, recall=0.9961, f1=0.2835
Training epoch 10
	step [1/192], loss=18.2035
	step [2/192], loss=17.8339
	step [3/192], loss=17.4534
	step [4/192], loss=18.4636
	step [5/192], loss=18.1544
	step [6/192], loss=17.6264
	step [7/192], loss=18.4833
	step [8/192], loss=17.9416
	step [9/192], loss=19.6691
	step [10/192], loss=19.2308
	step [11/192], loss=17.6094
	step [12/192], loss=18.8820
	step [13/192], loss=16.9358
	step [14/192], loss=18.3403
	step [15/192], loss=19.4013
	step [16/192], loss=18.6608
	step [17/192], loss=17.8505
	step [18/192], loss=19.0990
	step [19/192], loss=19.1745
	step [20/192], loss=18.4626
	step [21/192], loss=20.5680
	step [22/192], loss=21.1531
	step [23/192], loss=19.5129
	step [24/192], loss=17.4947
	step [25/192], loss=18.0044
	step [26/192], loss=21.7087
	step [27/192], loss=18.6762
	step [28/192], loss=18.6740
	step [29/192], loss=17.6683
	step [30/192], loss=19.9913
	step [31/192], loss=16.7948
	step [32/192], loss=19.6908
	step [33/192], loss=19.3166
	step [34/192], loss=18.8379
	step [35/192], loss=18.1613
	step [36/192], loss=20.7899
	step [37/192], loss=21.2499
	step [38/192], loss=20.6257
	step [39/192], loss=18.0801
	step [40/192], loss=19.7871
	step [41/192], loss=16.9113
	step [42/192], loss=20.8953
	step [43/192], loss=18.3672
	step [44/192], loss=19.6116
	step [45/192], loss=17.1430
	step [46/192], loss=19.5688
	step [47/192], loss=20.9672
	step [48/192], loss=18.9908
	step [49/192], loss=17.4579
	step [50/192], loss=16.5319
	step [51/192], loss=19.1432
	step [52/192], loss=19.8780
	step [53/192], loss=19.3268
	step [54/192], loss=17.1058
	step [55/192], loss=18.5242
	step [56/192], loss=20.7429
	step [57/192], loss=18.9569
	step [58/192], loss=18.0938
	step [59/192], loss=17.4281
	step [60/192], loss=17.6403
	step [61/192], loss=18.7933
	step [62/192], loss=18.8078
	step [63/192], loss=19.0781
	step [64/192], loss=19.7819
	step [65/192], loss=16.6942
	step [66/192], loss=18.7345
	step [67/192], loss=17.7740
	step [68/192], loss=20.2001
	step [69/192], loss=20.5001
	step [70/192], loss=16.9617
	step [71/192], loss=17.3057
	step [72/192], loss=18.4759
	step [73/192], loss=18.3586
	step [74/192], loss=18.0673
	step [75/192], loss=21.4454
	step [76/192], loss=18.6174
	step [77/192], loss=17.5500
	step [78/192], loss=17.6370
	step [79/192], loss=18.3278
	step [80/192], loss=17.3426
	step [81/192], loss=19.4205
	step [82/192], loss=17.7698
	step [83/192], loss=15.7999
	step [84/192], loss=16.3910
	step [85/192], loss=18.5809
	step [86/192], loss=20.3911
	step [87/192], loss=18.5286
	step [88/192], loss=18.7722
	step [89/192], loss=17.0806
	step [90/192], loss=21.3933
	step [91/192], loss=22.0978
	step [92/192], loss=18.6297
	step [93/192], loss=17.2805
	step [94/192], loss=18.1438
	step [95/192], loss=17.8441
	step [96/192], loss=17.0621
	step [97/192], loss=18.7166
	step [98/192], loss=18.4860
	step [99/192], loss=17.1196
	step [100/192], loss=18.0857
	step [101/192], loss=19.3903
	step [102/192], loss=16.0982
	step [103/192], loss=19.6175
	step [104/192], loss=19.4287
	step [105/192], loss=17.9617
	step [106/192], loss=18.2727
	step [107/192], loss=19.8368
	step [108/192], loss=17.5013
	step [109/192], loss=20.7126
	step [110/192], loss=16.1613
	step [111/192], loss=18.7760
	step [112/192], loss=18.9445
	step [113/192], loss=16.2611
	step [114/192], loss=18.5010
	step [115/192], loss=16.5540
	step [116/192], loss=21.1815
	step [117/192], loss=17.0765
	step [118/192], loss=18.0393
	step [119/192], loss=16.7290
	step [120/192], loss=17.5577
	step [121/192], loss=19.0718
	step [122/192], loss=16.8639
	step [123/192], loss=19.8000
	step [124/192], loss=17.3096
	step [125/192], loss=16.7851
	step [126/192], loss=18.8992
	step [127/192], loss=17.8491
	step [128/192], loss=17.0798
	step [129/192], loss=17.2814
	step [130/192], loss=17.9218
	step [131/192], loss=17.2872
	step [132/192], loss=15.6081
	step [133/192], loss=17.9057
	step [134/192], loss=15.9496
	step [135/192], loss=16.6818
	step [136/192], loss=17.6655
	step [137/192], loss=19.1507
	step [138/192], loss=16.2335
	step [139/192], loss=16.2373
	step [140/192], loss=16.9661
	step [141/192], loss=17.3847
	step [142/192], loss=18.9020
	step [143/192], loss=18.8399
	step [144/192], loss=17.0517
	step [145/192], loss=17.0312
	step [146/192], loss=16.7294
	step [147/192], loss=17.7699
	step [148/192], loss=18.7268
	step [149/192], loss=16.0649
	step [150/192], loss=17.7254
	step [151/192], loss=17.0192
	step [152/192], loss=16.4700
	step [153/192], loss=19.3753
	step [154/192], loss=16.3004
	step [155/192], loss=18.6081
	step [156/192], loss=17.8440
	step [157/192], loss=17.1894
	step [158/192], loss=16.1762
	step [159/192], loss=17.7685
	step [160/192], loss=18.5348
	step [161/192], loss=18.5691
	step [162/192], loss=16.9026
	step [163/192], loss=17.9536
	step [164/192], loss=17.4912
	step [165/192], loss=15.4801
	step [166/192], loss=18.8269
	step [167/192], loss=15.6723
	step [168/192], loss=16.3359
	step [169/192], loss=15.6507
	step [170/192], loss=18.4009
	step [171/192], loss=17.1483
	step [172/192], loss=19.2235
	step [173/192], loss=17.5663
	step [174/192], loss=17.6616
	step [175/192], loss=16.4100
	step [176/192], loss=20.4333
	step [177/192], loss=17.1867
	step [178/192], loss=17.7853
	step [179/192], loss=17.1555
	step [180/192], loss=18.4849
	step [181/192], loss=17.2992
	step [182/192], loss=17.0137
	step [183/192], loss=17.9340
	step [184/192], loss=15.8949
	step [185/192], loss=15.4398
	step [186/192], loss=17.4445
	step [187/192], loss=16.0881
	step [188/192], loss=18.9954
	step [189/192], loss=16.4301
	step [190/192], loss=17.0290
	step [191/192], loss=20.3506
	step [192/192], loss=2.2804
	Evaluating
	loss=0.0571, precision=0.1467, recall=0.9965, f1=0.2558
Training epoch 11
	step [1/192], loss=16.3321
	step [2/192], loss=16.8504
	step [3/192], loss=17.7770
	step [4/192], loss=17.1636
	step [5/192], loss=15.8946
	step [6/192], loss=17.1925
	step [7/192], loss=17.9280
	step [8/192], loss=16.8452
	step [9/192], loss=18.2076
	step [10/192], loss=18.5459
	step [11/192], loss=15.6037
	step [12/192], loss=17.6312
	step [13/192], loss=16.7444
	step [14/192], loss=16.1220
	step [15/192], loss=20.3356
	step [16/192], loss=16.0511
	step [17/192], loss=17.0641
	step [18/192], loss=18.3735
	step [19/192], loss=17.3035
	step [20/192], loss=16.6686
	step [21/192], loss=18.7984
	step [22/192], loss=16.6152
	step [23/192], loss=16.1426
	step [24/192], loss=15.9473
	step [25/192], loss=16.6425
	step [26/192], loss=16.5838
	step [27/192], loss=16.1660
	step [28/192], loss=17.4601
	step [29/192], loss=18.4090
	step [30/192], loss=17.4270
	step [31/192], loss=16.0697
	step [32/192], loss=16.5880
	step [33/192], loss=15.7315
	step [34/192], loss=18.1436
	step [35/192], loss=17.6876
	step [36/192], loss=16.2652
	step [37/192], loss=18.7571
	step [38/192], loss=15.9421
	step [39/192], loss=15.7315
	step [40/192], loss=17.9017
	step [41/192], loss=14.7261
	step [42/192], loss=17.9789
	step [43/192], loss=15.7497
	step [44/192], loss=16.2313
	step [45/192], loss=15.7900
	step [46/192], loss=15.5128
	step [47/192], loss=16.4822
	step [48/192], loss=19.7317
	step [49/192], loss=17.1784
	step [50/192], loss=16.6119
	step [51/192], loss=16.7880
	step [52/192], loss=16.9674
	step [53/192], loss=15.5336
	step [54/192], loss=17.9227
	step [55/192], loss=17.2535
	step [56/192], loss=18.5127
	step [57/192], loss=18.1639
	step [58/192], loss=17.4750
	step [59/192], loss=17.6031
	step [60/192], loss=17.2700
	step [61/192], loss=14.3412
	step [62/192], loss=16.6084
	step [63/192], loss=18.6165
	step [64/192], loss=16.7949
	step [65/192], loss=16.6424
	step [66/192], loss=21.6020
	step [67/192], loss=15.4419
	step [68/192], loss=17.0019
	step [69/192], loss=16.7879
	step [70/192], loss=16.4581
	step [71/192], loss=16.5123
	step [72/192], loss=16.2803
	step [73/192], loss=16.9934
	step [74/192], loss=17.8707
	step [75/192], loss=15.2917
	step [76/192], loss=17.3476
	step [77/192], loss=16.4642
	step [78/192], loss=16.9422
	step [79/192], loss=16.9238
	step [80/192], loss=17.9221
	step [81/192], loss=15.2010
	step [82/192], loss=15.5524
	step [83/192], loss=15.8796
	step [84/192], loss=17.9529
	step [85/192], loss=15.6443
	step [86/192], loss=15.6658
	step [87/192], loss=16.1115
	step [88/192], loss=16.6340
	step [89/192], loss=14.4095
	step [90/192], loss=17.3078
	step [91/192], loss=14.0510
	step [92/192], loss=15.8686
	step [93/192], loss=15.7787
	step [94/192], loss=18.4803
	step [95/192], loss=17.4345
	step [96/192], loss=18.2896
	step [97/192], loss=16.3364
	step [98/192], loss=17.6231
	step [99/192], loss=15.4446
	step [100/192], loss=15.7167
	step [101/192], loss=15.8325
	step [102/192], loss=15.4623
	step [103/192], loss=17.5246
	step [104/192], loss=17.5107
	step [105/192], loss=14.4743
	step [106/192], loss=15.8238
	step [107/192], loss=16.3514
	step [108/192], loss=18.7752
	step [109/192], loss=16.2966
	step [110/192], loss=15.3498
	step [111/192], loss=17.0816
	step [112/192], loss=16.8971
	step [113/192], loss=16.3128
	step [114/192], loss=20.7435
	step [115/192], loss=15.8120
	step [116/192], loss=15.8238
	step [117/192], loss=17.1714
	step [118/192], loss=16.4810
	step [119/192], loss=18.4645
	step [120/192], loss=16.9596
	step [121/192], loss=18.7680
	step [122/192], loss=18.4674
	step [123/192], loss=15.5272
	step [124/192], loss=16.5507
	step [125/192], loss=16.7033
	step [126/192], loss=15.5150
	step [127/192], loss=18.1905
	step [128/192], loss=16.8550
	step [129/192], loss=16.0446
	step [130/192], loss=15.4660
	step [131/192], loss=15.2789
	step [132/192], loss=15.5974
	step [133/192], loss=16.1873
	step [134/192], loss=17.9561
	step [135/192], loss=16.6740
	step [136/192], loss=15.4723
	step [137/192], loss=15.9654
	step [138/192], loss=16.0459
	step [139/192], loss=15.5207
	step [140/192], loss=14.3172
	step [141/192], loss=15.2739
	step [142/192], loss=16.1399
	step [143/192], loss=16.2734
	step [144/192], loss=18.8068
	step [145/192], loss=16.9033
	step [146/192], loss=16.1275
	step [147/192], loss=16.6579
	step [148/192], loss=16.6397
	step [149/192], loss=16.9809
	step [150/192], loss=15.6631
	step [151/192], loss=16.3815
	step [152/192], loss=16.4768
	step [153/192], loss=15.8677
	step [154/192], loss=14.7775
	step [155/192], loss=16.8457
	step [156/192], loss=17.8568
	step [157/192], loss=16.8632
	step [158/192], loss=17.1277
	step [159/192], loss=18.3700
	step [160/192], loss=14.6191
	step [161/192], loss=16.5454
	step [162/192], loss=16.5542
	step [163/192], loss=15.1398
	step [164/192], loss=15.3709
	step [165/192], loss=15.1269
	step [166/192], loss=16.0255
	step [167/192], loss=15.8033
	step [168/192], loss=14.6803
	step [169/192], loss=17.9692
	step [170/192], loss=16.6531
	step [171/192], loss=13.7799
	step [172/192], loss=15.5371
	step [173/192], loss=15.8431
	step [174/192], loss=14.5677
	step [175/192], loss=16.4961
	step [176/192], loss=15.7792
	step [177/192], loss=14.0489
	step [178/192], loss=16.0843
	step [179/192], loss=15.8496
	step [180/192], loss=15.5433
	step [181/192], loss=18.6937
	step [182/192], loss=17.3939
	step [183/192], loss=15.6577
	step [184/192], loss=17.2415
	step [185/192], loss=15.3833
	step [186/192], loss=15.5192
	step [187/192], loss=16.1293
	step [188/192], loss=14.4488
	step [189/192], loss=17.3065
	step [190/192], loss=17.5405
	step [191/192], loss=16.5233
	step [192/192], loss=2.4412
	Evaluating
	loss=0.0528, precision=0.1467, recall=0.9963, f1=0.2557
Training epoch 12
	step [1/192], loss=15.4102
	step [2/192], loss=14.2756
	step [3/192], loss=15.2799
	step [4/192], loss=17.5651
	step [5/192], loss=14.9386
	step [6/192], loss=16.9875
	step [7/192], loss=15.0483
	step [8/192], loss=16.5704
	step [9/192], loss=15.5470
	step [10/192], loss=14.9223
	step [11/192], loss=15.2748
	step [12/192], loss=15.0282
	step [13/192], loss=17.0268
	step [14/192], loss=16.3956
	step [15/192], loss=13.7985
	step [16/192], loss=15.4726
	step [17/192], loss=15.8281
	step [18/192], loss=14.7546
	step [19/192], loss=15.0576
	step [20/192], loss=16.1605
	step [21/192], loss=14.0129
	step [22/192], loss=15.6848
	step [23/192], loss=15.7939
	step [24/192], loss=15.2033
	step [25/192], loss=16.4355
	step [26/192], loss=14.2520
	step [27/192], loss=16.9519
	step [28/192], loss=16.3430
	step [29/192], loss=16.0557
	step [30/192], loss=15.4554
	step [31/192], loss=14.5608
	step [32/192], loss=16.3350
	step [33/192], loss=16.3195
	step [34/192], loss=13.1059
	step [35/192], loss=16.9845
	step [36/192], loss=15.9025
	step [37/192], loss=16.9011
	step [38/192], loss=16.0813
	step [39/192], loss=16.3647
	step [40/192], loss=16.4472
	step [41/192], loss=15.2134
	step [42/192], loss=15.1706
	step [43/192], loss=16.1264
	step [44/192], loss=15.0656
	step [45/192], loss=16.3331
	step [46/192], loss=17.2232
	step [47/192], loss=15.3783
	step [48/192], loss=16.3381
	step [49/192], loss=16.0410
	step [50/192], loss=15.7886
	step [51/192], loss=17.5627
	step [52/192], loss=13.0086
	step [53/192], loss=14.5901
	step [54/192], loss=17.2318
	step [55/192], loss=15.1841
	step [56/192], loss=15.9773
	step [57/192], loss=14.8812
	step [58/192], loss=13.1833
	step [59/192], loss=16.7548
	step [60/192], loss=15.1478
	step [61/192], loss=15.0044
	step [62/192], loss=15.2886
	step [63/192], loss=13.8363
	step [64/192], loss=14.9860
	step [65/192], loss=14.5227
	step [66/192], loss=17.1446
	step [67/192], loss=12.5406
	step [68/192], loss=16.4168
	step [69/192], loss=15.8754
	step [70/192], loss=16.3775
	step [71/192], loss=15.2129
	step [72/192], loss=16.6982
	step [73/192], loss=15.4234
	step [74/192], loss=13.8728
	step [75/192], loss=14.3064
	step [76/192], loss=14.8439
	step [77/192], loss=16.6491
	step [78/192], loss=14.0570
	step [79/192], loss=14.2489
	step [80/192], loss=15.8586
	step [81/192], loss=14.0467
	step [82/192], loss=15.3214
	step [83/192], loss=15.3655
	step [84/192], loss=15.3932
	step [85/192], loss=17.1724
	step [86/192], loss=14.2017
	step [87/192], loss=16.7205
	step [88/192], loss=15.2519
	step [89/192], loss=14.5279
	step [90/192], loss=15.7368
	step [91/192], loss=17.7361
	step [92/192], loss=13.2119
	step [93/192], loss=15.7474
	step [94/192], loss=15.8272
	step [95/192], loss=16.4230
	step [96/192], loss=16.9898
	step [97/192], loss=14.6697
	step [98/192], loss=13.6471
	step [99/192], loss=14.9113
	step [100/192], loss=14.2586
	step [101/192], loss=15.6983
	step [102/192], loss=15.1496
	step [103/192], loss=14.9363
	step [104/192], loss=13.7809
	step [105/192], loss=16.9212
	step [106/192], loss=14.3829
	step [107/192], loss=15.9781
	step [108/192], loss=13.6027
	step [109/192], loss=17.2117
	step [110/192], loss=17.0928
	step [111/192], loss=15.7994
	step [112/192], loss=15.8583
	step [113/192], loss=15.1901
	step [114/192], loss=14.6431
	step [115/192], loss=14.7862
	step [116/192], loss=14.1091
	step [117/192], loss=13.5999
	step [118/192], loss=16.2375
	step [119/192], loss=13.7277
	step [120/192], loss=14.7411
	step [121/192], loss=18.5450
	step [122/192], loss=17.3313
	step [123/192], loss=15.0773
	step [124/192], loss=15.2186
	step [125/192], loss=15.9444
	step [126/192], loss=15.2294
	step [127/192], loss=13.1682
	step [128/192], loss=16.2354
	step [129/192], loss=15.9100
	step [130/192], loss=14.4869
	step [131/192], loss=17.6644
	step [132/192], loss=12.5775
	step [133/192], loss=14.1657
	step [134/192], loss=13.8527
	step [135/192], loss=17.2070
	step [136/192], loss=15.8816
	step [137/192], loss=14.7848
	step [138/192], loss=15.4583
	step [139/192], loss=14.2292
	step [140/192], loss=14.2843
	step [141/192], loss=16.5740
	step [142/192], loss=16.0780
	step [143/192], loss=14.1546
	step [144/192], loss=16.4078
	step [145/192], loss=17.5521
	step [146/192], loss=14.2787
	step [147/192], loss=14.8675
	step [148/192], loss=14.3138
	step [149/192], loss=14.4685
	step [150/192], loss=14.5816
	step [151/192], loss=14.2357
	step [152/192], loss=13.9587
	step [153/192], loss=14.7406
	step [154/192], loss=15.9758
	step [155/192], loss=13.8712
	step [156/192], loss=14.5530
	step [157/192], loss=16.2767
	step [158/192], loss=15.5020
	step [159/192], loss=17.3685
	step [160/192], loss=15.0214
	step [161/192], loss=14.0140
	step [162/192], loss=14.8638
	step [163/192], loss=13.6706
	step [164/192], loss=15.2493
	step [165/192], loss=14.8681
	step [166/192], loss=15.2831
	step [167/192], loss=14.1511
	step [168/192], loss=15.3903
	step [169/192], loss=14.3855
	step [170/192], loss=12.3619
	step [171/192], loss=15.5884
	step [172/192], loss=15.0546
	step [173/192], loss=12.5836
	step [174/192], loss=15.7386
	step [175/192], loss=15.0923
	step [176/192], loss=14.3938
	step [177/192], loss=15.0031
	step [178/192], loss=16.8079
	step [179/192], loss=17.5000
	step [180/192], loss=18.6874
	step [181/192], loss=15.6165
	step [182/192], loss=14.4891
	step [183/192], loss=16.1800
	step [184/192], loss=15.2970
	step [185/192], loss=14.3583
	step [186/192], loss=14.6874
	step [187/192], loss=15.4435
	step [188/192], loss=16.7543
	step [189/192], loss=14.8280
	step [190/192], loss=15.9326
	step [191/192], loss=17.2723
	step [192/192], loss=3.3175
	Evaluating
	loss=0.0516, precision=0.1395, recall=0.9968, f1=0.2447
Training epoch 13
	step [1/192], loss=13.3923
	step [2/192], loss=13.0485
	step [3/192], loss=13.2188
	step [4/192], loss=15.8388
	step [5/192], loss=13.8076
	step [6/192], loss=15.3083
	step [7/192], loss=14.1149
	step [8/192], loss=15.6955
	step [9/192], loss=14.3312
	step [10/192], loss=14.8140
	step [11/192], loss=14.3273
	step [12/192], loss=14.0276
	step [13/192], loss=14.2417
	step [14/192], loss=13.8062
	step [15/192], loss=14.7264
	step [16/192], loss=14.4816
	step [17/192], loss=13.9952
	step [18/192], loss=15.5213
	step [19/192], loss=15.3087
	step [20/192], loss=17.0441
	step [21/192], loss=13.1440
	step [22/192], loss=14.6250
	step [23/192], loss=13.5208
	step [24/192], loss=15.3644
	step [25/192], loss=13.3508
	step [26/192], loss=14.4598
	step [27/192], loss=15.7486
	step [28/192], loss=14.3063
	step [29/192], loss=13.8452
	step [30/192], loss=15.3200
	step [31/192], loss=13.3234
	step [32/192], loss=14.4779
	step [33/192], loss=14.1984
	step [34/192], loss=14.4645
	step [35/192], loss=14.4184
	step [36/192], loss=14.1955
	step [37/192], loss=12.9113
	step [38/192], loss=13.6598
	step [39/192], loss=14.4283
	step [40/192], loss=16.7334
	step [41/192], loss=14.8394
	step [42/192], loss=15.3181
	step [43/192], loss=15.4841
	step [44/192], loss=15.6955
	step [45/192], loss=13.6568
	step [46/192], loss=14.9568
	step [47/192], loss=14.2839
	step [48/192], loss=13.0909
	step [49/192], loss=16.3516
	step [50/192], loss=16.0696
	step [51/192], loss=16.6428
	step [52/192], loss=15.7347
	step [53/192], loss=14.2755
	step [54/192], loss=14.1768
	step [55/192], loss=15.2107
	step [56/192], loss=17.0606
	step [57/192], loss=13.7635
	step [58/192], loss=14.0693
	step [59/192], loss=16.7807
	step [60/192], loss=12.7273
	step [61/192], loss=14.2973
	step [62/192], loss=14.6333
	step [63/192], loss=14.4565
	step [64/192], loss=15.7712
	step [65/192], loss=14.6607
	step [66/192], loss=14.9084
	step [67/192], loss=14.5677
	step [68/192], loss=16.5529
	step [69/192], loss=15.5994
	step [70/192], loss=16.1560
	step [71/192], loss=12.6282
	step [72/192], loss=12.8724
	step [73/192], loss=13.2408
	step [74/192], loss=13.0088
	step [75/192], loss=15.6829
	step [76/192], loss=14.0711
	step [77/192], loss=14.5364
	step [78/192], loss=11.7427
	step [79/192], loss=15.9792
	step [80/192], loss=14.4561
	step [81/192], loss=11.3558
	step [82/192], loss=14.5092
	step [83/192], loss=14.6865
	step [84/192], loss=16.7863
	step [85/192], loss=15.5018
	step [86/192], loss=16.3000
	step [87/192], loss=13.4450
	step [88/192], loss=16.1086
	step [89/192], loss=14.5260
	step [90/192], loss=15.3055
	step [91/192], loss=14.3420
	step [92/192], loss=16.5328
	step [93/192], loss=13.5084
	step [94/192], loss=13.7857
	step [95/192], loss=12.9560
	step [96/192], loss=13.8836
	step [97/192], loss=12.6664
	step [98/192], loss=14.9118
	step [99/192], loss=15.2830
	step [100/192], loss=16.6580
	step [101/192], loss=16.8033
	step [102/192], loss=17.3727
	step [103/192], loss=14.9299
	step [104/192], loss=13.2961
	step [105/192], loss=14.1927
	step [106/192], loss=14.2265
	step [107/192], loss=14.2471
	step [108/192], loss=12.0091
	step [109/192], loss=15.2419
	step [110/192], loss=13.9984
	step [111/192], loss=15.6523
	step [112/192], loss=15.2521
	step [113/192], loss=15.4536
	step [114/192], loss=12.9203
	step [115/192], loss=14.0163
	step [116/192], loss=14.1480
	step [117/192], loss=16.6393
	step [118/192], loss=14.4748
	step [119/192], loss=15.4818
	step [120/192], loss=15.1479
	step [121/192], loss=15.2190
	step [122/192], loss=16.1302
	step [123/192], loss=15.1250
	step [124/192], loss=14.5359
	step [125/192], loss=14.5863
	step [126/192], loss=14.0040
	step [127/192], loss=14.2585
	step [128/192], loss=14.6030
	step [129/192], loss=13.0782
	step [130/192], loss=14.5345
	step [131/192], loss=15.2622
	step [132/192], loss=15.4628
	step [133/192], loss=17.5030
	step [134/192], loss=13.4772
	step [135/192], loss=14.7558
	step [136/192], loss=14.4201
	step [137/192], loss=14.1912
	step [138/192], loss=13.7038
	step [139/192], loss=13.5943
	step [140/192], loss=13.1760
	step [141/192], loss=13.3352
	step [142/192], loss=12.6350
	step [143/192], loss=16.5154
	step [144/192], loss=12.3662
	step [145/192], loss=13.6699
	step [146/192], loss=12.9521
	step [147/192], loss=14.8393
	step [148/192], loss=16.1443
	step [149/192], loss=12.9979
	step [150/192], loss=12.0493
	step [151/192], loss=16.0911
	step [152/192], loss=14.4910
	step [153/192], loss=13.8792
	step [154/192], loss=14.1540
	step [155/192], loss=16.6825
	step [156/192], loss=13.5940
	step [157/192], loss=14.0840
	step [158/192], loss=13.7173
	step [159/192], loss=10.9337
	step [160/192], loss=15.2452
	step [161/192], loss=12.2465
	step [162/192], loss=13.4759
	step [163/192], loss=15.7595
	step [164/192], loss=12.7494
	step [165/192], loss=15.2279
	step [166/192], loss=12.3083
	step [167/192], loss=14.7180
	step [168/192], loss=14.6006
	step [169/192], loss=12.3671
	step [170/192], loss=14.1320
	step [171/192], loss=14.7406
	step [172/192], loss=15.3339
	step [173/192], loss=12.0220
	step [174/192], loss=12.9975
	step [175/192], loss=12.5227
	step [176/192], loss=15.2470
	step [177/192], loss=12.8930
	step [178/192], loss=14.0971
	step [179/192], loss=14.6586
	step [180/192], loss=14.1996
	step [181/192], loss=14.2537
	step [182/192], loss=14.5838
	step [183/192], loss=13.3414
	step [184/192], loss=14.7842
	step [185/192], loss=13.7396
	step [186/192], loss=13.5196
	step [187/192], loss=15.8524
	step [188/192], loss=12.9315
	step [189/192], loss=13.2052
	step [190/192], loss=13.6226
	step [191/192], loss=13.1220
	step [192/192], loss=1.8186
	Evaluating
	loss=0.0431, precision=0.1602, recall=0.9961, f1=0.2760
Training epoch 14
	step [1/192], loss=16.0506
	step [2/192], loss=11.7981
	step [3/192], loss=13.7469
	step [4/192], loss=12.1344
	step [5/192], loss=15.5710
	step [6/192], loss=11.6762
	step [7/192], loss=14.0815
	step [8/192], loss=12.9414
	step [9/192], loss=14.3299
	step [10/192], loss=14.8339
	step [11/192], loss=14.3363
	step [12/192], loss=14.4834
	step [13/192], loss=15.1794
	step [14/192], loss=12.6733
	step [15/192], loss=14.1622
	step [16/192], loss=12.0875
	step [17/192], loss=13.1335
	step [18/192], loss=12.4699
	step [19/192], loss=13.8724
	step [20/192], loss=14.7680
	step [21/192], loss=15.5837
	step [22/192], loss=14.3650
	step [23/192], loss=12.9249
	step [24/192], loss=15.9550
	step [25/192], loss=14.6012
	step [26/192], loss=14.7782
	step [27/192], loss=15.6285
	step [28/192], loss=13.5562
	step [29/192], loss=13.9018
	step [30/192], loss=13.1889
	step [31/192], loss=12.9983
	step [32/192], loss=12.7225
	step [33/192], loss=15.7829
	step [34/192], loss=14.3007
	step [35/192], loss=14.2588
	step [36/192], loss=13.6526
	step [37/192], loss=13.5796
	step [38/192], loss=13.7745
	step [39/192], loss=12.8721
	step [40/192], loss=13.6582
	step [41/192], loss=14.1584
	step [42/192], loss=15.6998
	step [43/192], loss=14.2973
	step [44/192], loss=12.0393
	step [45/192], loss=15.1910
	step [46/192], loss=13.3257
	step [47/192], loss=14.0454
	step [48/192], loss=15.1074
	step [49/192], loss=15.7230
	step [50/192], loss=12.0889
	step [51/192], loss=14.2993
	step [52/192], loss=12.7155
	step [53/192], loss=13.6935
	step [54/192], loss=16.4493
	step [55/192], loss=13.7669
	step [56/192], loss=15.8537
	step [57/192], loss=12.8159
	step [58/192], loss=12.0056
	step [59/192], loss=14.7922
	step [60/192], loss=12.7683
	step [61/192], loss=12.9021
	step [62/192], loss=12.1444
	step [63/192], loss=13.7054
	step [64/192], loss=14.2553
	step [65/192], loss=12.3711
	step [66/192], loss=13.4541
	step [67/192], loss=11.5012
	step [68/192], loss=12.4176
	step [69/192], loss=12.5351
	step [70/192], loss=13.9848
	step [71/192], loss=11.2981
	step [72/192], loss=13.3829
	step [73/192], loss=12.7216
	step [74/192], loss=13.2001
	step [75/192], loss=13.3964
	step [76/192], loss=12.8789
	step [77/192], loss=14.1932
	step [78/192], loss=13.7223
	step [79/192], loss=11.3812
	step [80/192], loss=13.7132
	step [81/192], loss=12.8989
	step [82/192], loss=14.1956
	step [83/192], loss=14.7895
	step [84/192], loss=13.4305
	step [85/192], loss=12.4924
	step [86/192], loss=14.0176
	step [87/192], loss=13.1413
	step [88/192], loss=15.7667
	step [89/192], loss=12.6487
	step [90/192], loss=15.3743
	step [91/192], loss=12.8421
	step [92/192], loss=12.8272
	step [93/192], loss=14.1727
	step [94/192], loss=13.1709
	step [95/192], loss=12.7359
	step [96/192], loss=11.0806
	step [97/192], loss=12.7959
	step [98/192], loss=14.9906
	step [99/192], loss=12.9274
	step [100/192], loss=14.0519
	step [101/192], loss=13.2091
	step [102/192], loss=15.2043
	step [103/192], loss=13.4600
	step [104/192], loss=12.7910
	step [105/192], loss=16.9572
	step [106/192], loss=14.2069
	step [107/192], loss=13.6182
	step [108/192], loss=13.4835
	step [109/192], loss=13.9945
	step [110/192], loss=14.8403
	step [111/192], loss=13.4362
	step [112/192], loss=13.7754
	step [113/192], loss=14.8015
	step [114/192], loss=14.6929
	step [115/192], loss=12.3385
	step [116/192], loss=12.6220
	step [117/192], loss=12.8678
	step [118/192], loss=13.5114
	step [119/192], loss=12.1999
	step [120/192], loss=13.8782
	step [121/192], loss=15.0307
	step [122/192], loss=14.4420
	step [123/192], loss=14.9484
	step [124/192], loss=12.4532
	step [125/192], loss=11.6364
	step [126/192], loss=14.4886
	step [127/192], loss=13.7005
	step [128/192], loss=11.3510
	step [129/192], loss=13.4047
	step [130/192], loss=17.7595
	step [131/192], loss=12.5381
	step [132/192], loss=15.9792
	step [133/192], loss=14.4268
	step [134/192], loss=15.7216
	step [135/192], loss=12.8423
	step [136/192], loss=13.9660
	step [137/192], loss=13.0535
	step [138/192], loss=14.9629
	step [139/192], loss=13.8241
	step [140/192], loss=13.6531
	step [141/192], loss=12.0883
	step [142/192], loss=14.3165
	step [143/192], loss=15.9984
	step [144/192], loss=13.9074
	step [145/192], loss=13.3783
	step [146/192], loss=12.8632
	step [147/192], loss=12.7442
	step [148/192], loss=13.9752
	step [149/192], loss=15.9957
	step [150/192], loss=13.2996
	step [151/192], loss=13.3187
	step [152/192], loss=13.9101
	step [153/192], loss=11.6678
	step [154/192], loss=12.5893
	step [155/192], loss=11.1279
	step [156/192], loss=12.3296
	step [157/192], loss=13.0237
	step [158/192], loss=13.1128
	step [159/192], loss=14.7477
	step [160/192], loss=12.6909
	step [161/192], loss=12.8853
	step [162/192], loss=12.3075
	step [163/192], loss=13.4860
	step [164/192], loss=14.4011
	step [165/192], loss=12.5046
	step [166/192], loss=12.9458
	step [167/192], loss=12.8615
	step [168/192], loss=13.3332
	step [169/192], loss=11.3588
	step [170/192], loss=13.7710
	step [171/192], loss=14.5520
	step [172/192], loss=12.0730
	step [173/192], loss=13.6688
	step [174/192], loss=12.8598
	step [175/192], loss=14.3161
	step [176/192], loss=12.7018
	step [177/192], loss=14.1462
	step [178/192], loss=13.9409
	step [179/192], loss=11.8575
	step [180/192], loss=13.8822
	step [181/192], loss=12.7200
	step [182/192], loss=12.4502
	step [183/192], loss=12.1855
	step [184/192], loss=16.2159
	step [185/192], loss=11.9416
	step [186/192], loss=13.1858
	step [187/192], loss=14.1521
	step [188/192], loss=12.1309
	step [189/192], loss=13.5162
	step [190/192], loss=15.1217
	step [191/192], loss=13.5643
	step [192/192], loss=1.6261
	Evaluating
	loss=0.0424, precision=0.1585, recall=0.9963, f1=0.2736
Training epoch 15
	step [1/192], loss=12.8744
	step [2/192], loss=11.0600
	step [3/192], loss=11.9647
	step [4/192], loss=13.2597
	step [5/192], loss=11.7056
	step [6/192], loss=11.1740
	step [7/192], loss=14.8472
	step [8/192], loss=12.8269
	step [9/192], loss=15.1854
	step [10/192], loss=14.1358
	step [11/192], loss=13.9548
	step [12/192], loss=15.0346
	step [13/192], loss=12.5404
	step [14/192], loss=13.1710
	step [15/192], loss=14.6270
	step [16/192], loss=13.7035
	step [17/192], loss=13.8590
	step [18/192], loss=12.7625
	step [19/192], loss=11.5943
	step [20/192], loss=13.6774
	step [21/192], loss=11.9418
	step [22/192], loss=13.1350
	step [23/192], loss=15.2944
	step [24/192], loss=12.4671
	step [25/192], loss=11.4299
	step [26/192], loss=12.7959
	step [27/192], loss=12.8556
	step [28/192], loss=12.6514
	step [29/192], loss=11.1286
	step [30/192], loss=12.1640
	step [31/192], loss=15.3034
	step [32/192], loss=12.7538
	step [33/192], loss=10.9838
	step [34/192], loss=11.7397
	step [35/192], loss=12.7765
	step [36/192], loss=13.9036
	step [37/192], loss=11.5148
	step [38/192], loss=12.4103
	step [39/192], loss=12.6747
	step [40/192], loss=12.4308
	step [41/192], loss=13.4187
	step [42/192], loss=14.1941
	step [43/192], loss=14.6697
	step [44/192], loss=14.2110
	step [45/192], loss=15.2150
	step [46/192], loss=15.1226
	step [47/192], loss=12.8968
	step [48/192], loss=12.8116
	step [49/192], loss=14.0638
	step [50/192], loss=12.4480
	step [51/192], loss=12.3165
	step [52/192], loss=13.7564
	step [53/192], loss=14.2632
	step [54/192], loss=12.6484
	step [55/192], loss=13.0178
	step [56/192], loss=13.0573
	step [57/192], loss=13.0207
	step [58/192], loss=14.5635
	step [59/192], loss=11.1480
	step [60/192], loss=13.2951
	step [61/192], loss=11.8381
	step [62/192], loss=14.0002
	step [63/192], loss=11.9607
	step [64/192], loss=12.2956
	step [65/192], loss=11.4854
	step [66/192], loss=14.4269
	step [67/192], loss=13.1492
	step [68/192], loss=11.9771
	step [69/192], loss=13.3025
	step [70/192], loss=12.0813
	step [71/192], loss=13.0081
	step [72/192], loss=12.5014
	step [73/192], loss=10.3444
	step [74/192], loss=12.4693
	step [75/192], loss=11.3271
	step [76/192], loss=12.7991
	step [77/192], loss=12.6480
	step [78/192], loss=14.2356
	step [79/192], loss=13.5695
	step [80/192], loss=12.7059
	step [81/192], loss=12.9654
	step [82/192], loss=13.8210
	step [83/192], loss=11.6785
	step [84/192], loss=11.3116
	step [85/192], loss=12.6363
	step [86/192], loss=11.9034
	step [87/192], loss=11.6441
	step [88/192], loss=11.3467
	step [89/192], loss=13.2019
	step [90/192], loss=14.0217
	step [91/192], loss=12.6846
	step [92/192], loss=11.2078
	step [93/192], loss=13.0167
	step [94/192], loss=12.5778
	step [95/192], loss=13.6408
	step [96/192], loss=11.9983
	step [97/192], loss=15.9755
	step [98/192], loss=11.8919
	step [99/192], loss=11.6526
	step [100/192], loss=13.6885
	step [101/192], loss=11.4746
	step [102/192], loss=12.1756
	step [103/192], loss=13.1256
	step [104/192], loss=13.0401
	step [105/192], loss=12.7324
	step [106/192], loss=12.0500
	step [107/192], loss=14.7296
	step [108/192], loss=12.1424
	step [109/192], loss=13.0786
	step [110/192], loss=13.4020
	step [111/192], loss=10.4069
	step [112/192], loss=11.9724
	step [113/192], loss=13.4433
	step [114/192], loss=12.8969
	step [115/192], loss=13.1947
	step [116/192], loss=14.8159
	step [117/192], loss=11.2877
	step [118/192], loss=12.1696
	step [119/192], loss=11.4976
	step [120/192], loss=12.4854
	step [121/192], loss=15.2615
	step [122/192], loss=14.1517
	step [123/192], loss=12.5836
	step [124/192], loss=11.8909
	step [125/192], loss=12.2725
	step [126/192], loss=13.5815
	step [127/192], loss=14.9581
	step [128/192], loss=12.3317
	step [129/192], loss=15.8632
	step [130/192], loss=13.3132
	step [131/192], loss=13.4514
	step [132/192], loss=12.3843
	step [133/192], loss=14.7816
	step [134/192], loss=14.6642
	step [135/192], loss=12.8460
	step [136/192], loss=11.6887
	step [137/192], loss=11.8772
	step [138/192], loss=13.8702
	step [139/192], loss=13.1135
	step [140/192], loss=13.6591
	step [141/192], loss=13.1192
	step [142/192], loss=12.7306
	step [143/192], loss=12.8337
	step [144/192], loss=11.8514
	step [145/192], loss=14.7259
	step [146/192], loss=13.3301
	step [147/192], loss=12.9058
	step [148/192], loss=12.4581
	step [149/192], loss=13.1018
	step [150/192], loss=12.0469
	step [151/192], loss=12.1405
	step [152/192], loss=12.3205
	step [153/192], loss=16.0154
	step [154/192], loss=11.3291
	step [155/192], loss=11.8524
	step [156/192], loss=14.2135
	step [157/192], loss=12.6837
	step [158/192], loss=13.3775
	step [159/192], loss=13.8572
	step [160/192], loss=13.2622
	step [161/192], loss=14.6924
	step [162/192], loss=11.4876
	step [163/192], loss=12.2968
	step [164/192], loss=11.7788
	step [165/192], loss=17.0012
	step [166/192], loss=12.9813
	step [167/192], loss=12.3248
	step [168/192], loss=15.2035
	step [169/192], loss=14.4890
	step [170/192], loss=13.5263
	step [171/192], loss=13.4221
	step [172/192], loss=13.8597
	step [173/192], loss=11.0441
	step [174/192], loss=14.1907
	step [175/192], loss=11.8283
	step [176/192], loss=14.0772
	step [177/192], loss=11.4011
	step [178/192], loss=11.3359
	step [179/192], loss=13.6113
	step [180/192], loss=12.3121
	step [181/192], loss=11.9157
	step [182/192], loss=15.2490
	step [183/192], loss=14.3028
	step [184/192], loss=11.5092
	step [185/192], loss=12.7439
	step [186/192], loss=13.3088
	step [187/192], loss=12.7244
	step [188/192], loss=11.2492
	step [189/192], loss=13.9260
	step [190/192], loss=12.1775
	step [191/192], loss=11.1141
	step [192/192], loss=4.2993
	Evaluating
	loss=0.0278, precision=0.2330, recall=0.9918, f1=0.3773
saving model as: 0_saved_model.pth
Training epoch 16
	step [1/192], loss=12.7052
	step [2/192], loss=13.3193
	step [3/192], loss=12.6700
	step [4/192], loss=12.3819
	step [5/192], loss=13.8020
	step [6/192], loss=13.9509
	step [7/192], loss=14.5777
	step [8/192], loss=12.5304
	step [9/192], loss=11.5277
	step [10/192], loss=11.6923
	step [11/192], loss=14.0964
	step [12/192], loss=13.0256
	step [13/192], loss=14.0672
	step [14/192], loss=12.9724
	step [15/192], loss=10.1270
	step [16/192], loss=13.4769
	step [17/192], loss=15.1453
	step [18/192], loss=10.9873
	step [19/192], loss=11.3995
	step [20/192], loss=11.9755
	step [21/192], loss=12.0798
	step [22/192], loss=12.7163
	step [23/192], loss=13.2566
	step [24/192], loss=11.9059
	step [25/192], loss=15.9576
	step [26/192], loss=12.9626
	step [27/192], loss=12.1039
	step [28/192], loss=12.3341
	step [29/192], loss=12.5461
	step [30/192], loss=14.1314
	step [31/192], loss=12.1294
	step [32/192], loss=11.8347
	step [33/192], loss=12.1461
	step [34/192], loss=14.6879
	step [35/192], loss=12.1271
	step [36/192], loss=12.4350
	step [37/192], loss=12.2242
	step [38/192], loss=13.6444
	step [39/192], loss=12.3425
	step [40/192], loss=13.5282
	step [41/192], loss=12.1097
	step [42/192], loss=15.7486
	step [43/192], loss=11.2418
	step [44/192], loss=13.1100
	step [45/192], loss=16.1935
	step [46/192], loss=11.0779
	step [47/192], loss=13.4618
	step [48/192], loss=11.7187
	step [49/192], loss=13.4578
	step [50/192], loss=11.6539
	step [51/192], loss=12.3034
	step [52/192], loss=12.6527
	step [53/192], loss=13.4521
	step [54/192], loss=12.4250
	step [55/192], loss=12.5656
	step [56/192], loss=11.9809
	step [57/192], loss=12.1709
	step [58/192], loss=10.8438
	step [59/192], loss=12.8281
	step [60/192], loss=10.3383
	step [61/192], loss=12.4690
	step [62/192], loss=10.8364
	step [63/192], loss=14.0418
	step [64/192], loss=13.5476
	step [65/192], loss=11.4312
	step [66/192], loss=12.6823
	step [67/192], loss=14.8068
	step [68/192], loss=11.5173
	step [69/192], loss=11.6880
	step [70/192], loss=14.2921
	step [71/192], loss=12.9097
	step [72/192], loss=11.7406
	step [73/192], loss=12.5637
	step [74/192], loss=12.5075
	step [75/192], loss=13.2015
	step [76/192], loss=11.4750
	step [77/192], loss=13.0518
	step [78/192], loss=15.0335
	step [79/192], loss=13.2194
	step [80/192], loss=13.8854
	step [81/192], loss=10.8574
	step [82/192], loss=12.5326
	step [83/192], loss=13.0521
	step [84/192], loss=12.2839
	step [85/192], loss=12.6788
	step [86/192], loss=10.9903
	step [87/192], loss=10.9359
	step [88/192], loss=14.1782
	step [89/192], loss=11.4722
	step [90/192], loss=13.2509
	step [91/192], loss=13.8912
	step [92/192], loss=12.1461
	step [93/192], loss=12.1774
	step [94/192], loss=12.8293
	step [95/192], loss=12.8434
	step [96/192], loss=12.3266
	step [97/192], loss=14.3320
	step [98/192], loss=12.2179
	step [99/192], loss=12.9170
	step [100/192], loss=9.8776
	step [101/192], loss=12.0208
	step [102/192], loss=11.5406
	step [103/192], loss=12.5980
	step [104/192], loss=11.5379
	step [105/192], loss=11.6541
	step [106/192], loss=10.0611
	step [107/192], loss=14.2805
	step [108/192], loss=11.7060
	step [109/192], loss=12.1367
	step [110/192], loss=12.2846
	step [111/192], loss=11.2118
	step [112/192], loss=12.6884
	step [113/192], loss=11.4897
	step [114/192], loss=14.2037
	step [115/192], loss=15.8908
	step [116/192], loss=11.8431
	step [117/192], loss=13.8548
	step [118/192], loss=12.0924
	step [119/192], loss=11.3935
	step [120/192], loss=11.6454
	step [121/192], loss=12.6071
	step [122/192], loss=12.1226
	step [123/192], loss=12.2452
	step [124/192], loss=11.7354
	step [125/192], loss=13.3681
	step [126/192], loss=10.7282
	step [127/192], loss=11.5446
	step [128/192], loss=13.6710
	step [129/192], loss=11.5735
	step [130/192], loss=10.6297
	step [131/192], loss=11.4964
	step [132/192], loss=11.6677
	step [133/192], loss=11.8540
	step [134/192], loss=12.2184
	step [135/192], loss=12.5171
	step [136/192], loss=14.0027
	step [137/192], loss=11.9374
	step [138/192], loss=14.0566
	step [139/192], loss=13.9231
	step [140/192], loss=11.2293
	step [141/192], loss=11.5264
	step [142/192], loss=12.8291
	step [143/192], loss=11.6957
	step [144/192], loss=10.3444
	step [145/192], loss=11.9988
	step [146/192], loss=11.7009
	step [147/192], loss=9.9837
	step [148/192], loss=11.7646
	step [149/192], loss=12.7750
	step [150/192], loss=11.6976
	step [151/192], loss=12.9072
	step [152/192], loss=12.8940
	step [153/192], loss=11.5681
	step [154/192], loss=12.3017
	step [155/192], loss=11.6166
	step [156/192], loss=12.6805
	step [157/192], loss=13.6758
	step [158/192], loss=11.5274
	step [159/192], loss=11.1752
	step [160/192], loss=11.1641
	step [161/192], loss=13.4281
	step [162/192], loss=11.8533
	step [163/192], loss=13.7078
	step [164/192], loss=10.1294
	step [165/192], loss=12.3493
	step [166/192], loss=10.4661
	step [167/192], loss=12.2460
	step [168/192], loss=10.6801
	step [169/192], loss=11.4071
	step [170/192], loss=12.1723
	step [171/192], loss=11.8169
	step [172/192], loss=12.9081
	step [173/192], loss=10.2508
	step [174/192], loss=11.8942
	step [175/192], loss=12.6296
	step [176/192], loss=12.1139
	step [177/192], loss=10.6143
	step [178/192], loss=11.8896
	step [179/192], loss=13.0160
	step [180/192], loss=12.1403
	step [181/192], loss=11.2659
	step [182/192], loss=11.9205
	step [183/192], loss=11.6667
	step [184/192], loss=13.3694
	step [185/192], loss=11.7828
	step [186/192], loss=11.3491
	step [187/192], loss=13.4055
	step [188/192], loss=12.4852
	step [189/192], loss=11.9906
	step [190/192], loss=13.6531
	step [191/192], loss=12.4274
	step [192/192], loss=1.1702
	Evaluating
	loss=0.0354, precision=0.1561, recall=0.9963, f1=0.2699
Training epoch 17
	step [1/192], loss=11.8369
	step [2/192], loss=13.4770
	step [3/192], loss=11.2418
	step [4/192], loss=12.2069
	step [5/192], loss=12.4552
	step [6/192], loss=12.0285
	step [7/192], loss=12.7072
	step [8/192], loss=13.5917
	step [9/192], loss=9.8357
	step [10/192], loss=13.5234
	step [11/192], loss=11.6382
	step [12/192], loss=13.5584
	step [13/192], loss=9.9901
	step [14/192], loss=11.5013
	step [15/192], loss=13.5205
	step [16/192], loss=12.4234
	step [17/192], loss=11.3072
	step [18/192], loss=11.1486
	step [19/192], loss=11.1082
	step [20/192], loss=12.3911
	step [21/192], loss=9.6524
	step [22/192], loss=12.3794
	step [23/192], loss=12.2333
	step [24/192], loss=10.0763
	step [25/192], loss=14.2008
	step [26/192], loss=10.7392
	step [27/192], loss=11.0916
	step [28/192], loss=10.2474
	step [29/192], loss=15.2407
	step [30/192], loss=12.6206
	step [31/192], loss=9.9940
	step [32/192], loss=12.6594
	step [33/192], loss=10.2909
	step [34/192], loss=12.4955
	step [35/192], loss=11.6496
	step [36/192], loss=11.1578
	step [37/192], loss=12.9168
	step [38/192], loss=12.5056
	step [39/192], loss=11.0784
	step [40/192], loss=11.0472
	step [41/192], loss=12.2769
	step [42/192], loss=13.8981
	step [43/192], loss=12.0903
	step [44/192], loss=11.9171
	step [45/192], loss=11.3101
	step [46/192], loss=12.7395
	step [47/192], loss=13.4466
	step [48/192], loss=12.1532
	step [49/192], loss=12.3371
	step [50/192], loss=11.3547
	step [51/192], loss=11.6715
	step [52/192], loss=13.6023
	step [53/192], loss=11.8173
	step [54/192], loss=15.1210
	step [55/192], loss=11.1421
	step [56/192], loss=11.5519
	step [57/192], loss=11.1038
	step [58/192], loss=13.7466
	step [59/192], loss=12.4928
	step [60/192], loss=12.2824
	step [61/192], loss=12.9187
	step [62/192], loss=11.9899
	step [63/192], loss=12.1763
	step [64/192], loss=12.5070
	step [65/192], loss=12.7455
	step [66/192], loss=10.4867
	step [67/192], loss=11.7477
	step [68/192], loss=11.1399
	step [69/192], loss=11.9543
	step [70/192], loss=10.8631
	step [71/192], loss=12.4473
	step [72/192], loss=13.0941
	step [73/192], loss=10.0587
	step [74/192], loss=11.4677
	step [75/192], loss=15.9320
	step [76/192], loss=12.3725
	step [77/192], loss=12.0511
	step [78/192], loss=12.9734
	step [79/192], loss=13.2760
	step [80/192], loss=13.4327
	step [81/192], loss=11.4972
	step [82/192], loss=11.8685
	step [83/192], loss=12.5317
	step [84/192], loss=10.4253
	step [85/192], loss=13.0537
	step [86/192], loss=12.2685
	step [87/192], loss=12.9799
	step [88/192], loss=11.3019
	step [89/192], loss=11.9726
	step [90/192], loss=11.1039
	step [91/192], loss=9.9393
	step [92/192], loss=11.8478
	step [93/192], loss=12.4193
	step [94/192], loss=12.0667
	step [95/192], loss=12.0086
	step [96/192], loss=11.0459
	step [97/192], loss=9.5123
	step [98/192], loss=13.6003
	step [99/192], loss=13.5111
	step [100/192], loss=10.4729
	step [101/192], loss=12.4933
	step [102/192], loss=11.0539
	step [103/192], loss=17.2245
	step [104/192], loss=11.0496
	step [105/192], loss=13.3194
	step [106/192], loss=12.6194
	step [107/192], loss=13.4514
	step [108/192], loss=9.7013
	step [109/192], loss=11.9325
	step [110/192], loss=12.4503
	step [111/192], loss=10.7858
	step [112/192], loss=11.8669
	step [113/192], loss=12.1446
	step [114/192], loss=11.4135
	step [115/192], loss=12.5630
	step [116/192], loss=13.1703
	step [117/192], loss=12.2308
	step [118/192], loss=11.4982
	step [119/192], loss=12.2265
	step [120/192], loss=11.3062
	step [121/192], loss=11.6801
	step [122/192], loss=9.7887
	step [123/192], loss=9.6161
	step [124/192], loss=12.8290
	step [125/192], loss=10.7695
	step [126/192], loss=11.7723
	step [127/192], loss=10.7845
	step [128/192], loss=13.9990
	step [129/192], loss=9.9259
	step [130/192], loss=11.1975
	step [131/192], loss=12.5679
	step [132/192], loss=11.6755
	step [133/192], loss=10.8566
	step [134/192], loss=11.6348
	step [135/192], loss=12.3048
	step [136/192], loss=10.7652
	step [137/192], loss=11.2199
	step [138/192], loss=12.3052
	step [139/192], loss=11.1429
	step [140/192], loss=10.8006
	step [141/192], loss=12.6665
	step [142/192], loss=12.2850
	step [143/192], loss=11.8036
	step [144/192], loss=10.4798
	step [145/192], loss=9.9415
	step [146/192], loss=13.0459
	step [147/192], loss=11.0544
	step [148/192], loss=11.5551
	step [149/192], loss=11.0805
	step [150/192], loss=12.1969
	step [151/192], loss=13.1844
	step [152/192], loss=10.4182
	step [153/192], loss=11.0552
	step [154/192], loss=11.4415
	step [155/192], loss=10.3294
	step [156/192], loss=14.8859
	step [157/192], loss=12.9663
	step [158/192], loss=13.0741
	step [159/192], loss=13.1398
	step [160/192], loss=11.8595
	step [161/192], loss=11.9081
	step [162/192], loss=10.8187
	step [163/192], loss=14.2527
	step [164/192], loss=10.3940
	step [165/192], loss=12.0659
	step [166/192], loss=13.3211
	step [167/192], loss=10.5036
	step [168/192], loss=13.5961
	step [169/192], loss=11.3825
	step [170/192], loss=11.5182
	step [171/192], loss=12.8186
	step [172/192], loss=11.9574
	step [173/192], loss=12.6530
	step [174/192], loss=13.0830
	step [175/192], loss=12.4587
	step [176/192], loss=10.1792
	step [177/192], loss=11.5691
	step [178/192], loss=10.0391
	step [179/192], loss=12.4110
	step [180/192], loss=11.7265
	step [181/192], loss=10.7548
	step [182/192], loss=10.4295
	step [183/192], loss=13.1441
	step [184/192], loss=9.9965
	step [185/192], loss=10.7644
	step [186/192], loss=11.7432
	step [187/192], loss=11.9441
	step [188/192], loss=12.2498
	step [189/192], loss=11.5444
	step [190/192], loss=10.8461
	step [191/192], loss=10.0373
	step [192/192], loss=2.4441
	Evaluating
	loss=0.0359, precision=0.1599, recall=0.9959, f1=0.2756
Training epoch 18
	step [1/192], loss=11.7541
	step [2/192], loss=11.4435
	step [3/192], loss=11.6029
	step [4/192], loss=11.7754
	step [5/192], loss=13.8219
	step [6/192], loss=11.7401
	step [7/192], loss=10.3901
	step [8/192], loss=13.4841
	step [9/192], loss=12.3007
	step [10/192], loss=11.4532
	step [11/192], loss=10.4199
	step [12/192], loss=10.0593
	step [13/192], loss=11.1521
	step [14/192], loss=11.0694
	step [15/192], loss=12.8350
	step [16/192], loss=10.3612
	step [17/192], loss=10.3430
	step [18/192], loss=12.2416
	step [19/192], loss=12.2100
	step [20/192], loss=12.2167
	step [21/192], loss=11.5627
	step [22/192], loss=13.4190
	step [23/192], loss=12.2614
	step [24/192], loss=11.1518
	step [25/192], loss=11.2632
	step [26/192], loss=11.7755
	step [27/192], loss=11.1988
	step [28/192], loss=11.8391
	step [29/192], loss=12.0267
	step [30/192], loss=11.4050
	step [31/192], loss=10.2044
	step [32/192], loss=11.7268
	step [33/192], loss=13.6295
	step [34/192], loss=13.2047
	step [35/192], loss=10.1173
	step [36/192], loss=12.0633
	step [37/192], loss=12.5454
	step [38/192], loss=11.7346
	step [39/192], loss=12.6833
	step [40/192], loss=11.1905
	step [41/192], loss=10.0902
	step [42/192], loss=12.7595
	step [43/192], loss=11.6917
	step [44/192], loss=10.5249
	step [45/192], loss=11.8104
	step [46/192], loss=14.4518
	step [47/192], loss=11.1640
	step [48/192], loss=10.9003
	step [49/192], loss=10.7952
	step [50/192], loss=12.5404
	step [51/192], loss=11.6360
	step [52/192], loss=11.1130
	step [53/192], loss=12.9576
	step [54/192], loss=12.0329
	step [55/192], loss=11.1446
	step [56/192], loss=12.1709
	step [57/192], loss=13.6583
	step [58/192], loss=11.7848
	step [59/192], loss=10.4641
	step [60/192], loss=11.9959
	step [61/192], loss=11.4929
	step [62/192], loss=10.1641
	step [63/192], loss=11.0152
	step [64/192], loss=12.7440
	step [65/192], loss=10.5660
	step [66/192], loss=10.5413
	step [67/192], loss=10.1268
	step [68/192], loss=12.9173
	step [69/192], loss=11.7823
	step [70/192], loss=11.8869
	step [71/192], loss=12.3534
	step [72/192], loss=12.0507
	step [73/192], loss=12.9278
	step [74/192], loss=9.4765
	step [75/192], loss=12.5402
	step [76/192], loss=11.7320
	step [77/192], loss=13.0886
	step [78/192], loss=15.4109
	step [79/192], loss=11.5768
	step [80/192], loss=16.9093
	step [81/192], loss=11.3264
	step [82/192], loss=13.7949
	step [83/192], loss=10.4440
	step [84/192], loss=11.7797
	step [85/192], loss=12.5705
	step [86/192], loss=11.8686
	step [87/192], loss=10.9820
	step [88/192], loss=12.7859
	step [89/192], loss=10.9257
	step [90/192], loss=11.2031
	step [91/192], loss=10.3461
	step [92/192], loss=12.9173
	step [93/192], loss=12.2487
	step [94/192], loss=10.9994
	step [95/192], loss=10.3613
	step [96/192], loss=11.3276
	step [97/192], loss=12.2410
	step [98/192], loss=12.5148
	step [99/192], loss=10.7942
	step [100/192], loss=10.5920
	step [101/192], loss=9.0735
	step [102/192], loss=11.2696
	step [103/192], loss=11.7643
	step [104/192], loss=11.2610
	step [105/192], loss=11.2297
	step [106/192], loss=9.6272
	step [107/192], loss=13.0233
	step [108/192], loss=8.9181
	step [109/192], loss=11.1750
	step [110/192], loss=11.4149
	step [111/192], loss=14.4909
	step [112/192], loss=13.6051
	step [113/192], loss=12.1316
	step [114/192], loss=11.7762
	step [115/192], loss=10.7563
	step [116/192], loss=14.5467
	step [117/192], loss=13.1727
	step [118/192], loss=13.3123
	step [119/192], loss=11.5990
	step [120/192], loss=10.9132
	step [121/192], loss=12.8116
	step [122/192], loss=11.6253
	step [123/192], loss=9.5501
	step [124/192], loss=11.8456
	step [125/192], loss=12.1419
	step [126/192], loss=13.1089
	step [127/192], loss=12.5501
	step [128/192], loss=14.4207
	step [129/192], loss=12.5237
	step [130/192], loss=11.8390
	step [131/192], loss=11.6716
	step [132/192], loss=9.7915
	step [133/192], loss=12.7542
	step [134/192], loss=12.9889
	step [135/192], loss=9.5495
	step [136/192], loss=9.3543
	step [137/192], loss=13.3427
	step [138/192], loss=12.7180
	step [139/192], loss=12.0819
	step [140/192], loss=11.6104
	step [141/192], loss=10.7860
	step [142/192], loss=10.9639
	step [143/192], loss=11.3543
	step [144/192], loss=12.9129
	step [145/192], loss=14.2545
	step [146/192], loss=11.0124
	step [147/192], loss=10.7597
	step [148/192], loss=11.5035
	step [149/192], loss=9.6007
	step [150/192], loss=11.2009
	step [151/192], loss=12.9858
	step [152/192], loss=9.5556
	step [153/192], loss=13.4162
	step [154/192], loss=11.7759
	step [155/192], loss=11.4985
	step [156/192], loss=11.1978
	step [157/192], loss=9.8335
	step [158/192], loss=10.3251
	step [159/192], loss=11.5456
	step [160/192], loss=10.8345
	step [161/192], loss=11.1537
	step [162/192], loss=10.3571
	step [163/192], loss=10.4745
	step [164/192], loss=10.1143
	step [165/192], loss=14.9843
	step [166/192], loss=13.2380
	step [167/192], loss=13.1196
	step [168/192], loss=11.1764
	step [169/192], loss=12.0201
	step [170/192], loss=11.2028
	step [171/192], loss=12.2643
	step [172/192], loss=9.7320
	step [173/192], loss=11.2180
	step [174/192], loss=10.1041
	step [175/192], loss=11.7086
	step [176/192], loss=13.7679
	step [177/192], loss=11.2022
	step [178/192], loss=12.0788
	step [179/192], loss=12.9144
	step [180/192], loss=11.5073
	step [181/192], loss=11.9573
	step [182/192], loss=10.4059
	step [183/192], loss=9.9357
	step [184/192], loss=11.5327
	step [185/192], loss=11.2266
	step [186/192], loss=12.4966
	step [187/192], loss=12.3980
	step [188/192], loss=11.4585
	step [189/192], loss=12.0120
	step [190/192], loss=9.9272
	step [191/192], loss=13.6118
	step [192/192], loss=1.2495
	Evaluating
	loss=0.0304, precision=0.1756, recall=0.9952, f1=0.2985
Training epoch 19
	step [1/192], loss=14.6550
	step [2/192], loss=12.1696
	step [3/192], loss=9.8358
	step [4/192], loss=12.2178
	step [5/192], loss=10.0180
	step [6/192], loss=10.5820
	step [7/192], loss=11.5747
	step [8/192], loss=12.5094
	step [9/192], loss=11.2323
	step [10/192], loss=14.1056
	step [11/192], loss=12.6580
	step [12/192], loss=11.3191
	step [13/192], loss=11.4989
	step [14/192], loss=12.1134
	step [15/192], loss=10.0276
	step [16/192], loss=11.1742
	step [17/192], loss=11.7774
	step [18/192], loss=11.5390
	step [19/192], loss=11.7339
	step [20/192], loss=13.5512
	step [21/192], loss=10.1662
	step [22/192], loss=11.6566
	step [23/192], loss=10.7347
	step [24/192], loss=11.3827
	step [25/192], loss=12.0739
	step [26/192], loss=10.7678
	step [27/192], loss=9.4782
	step [28/192], loss=9.7717
	step [29/192], loss=10.8277
	step [30/192], loss=10.0373
	step [31/192], loss=11.0291
	step [32/192], loss=10.0486
	step [33/192], loss=12.9627
	step [34/192], loss=10.6624
	step [35/192], loss=10.5097
	step [36/192], loss=10.3547
	step [37/192], loss=13.8444
	step [38/192], loss=11.0291
	step [39/192], loss=11.2158
	step [40/192], loss=10.1689
	step [41/192], loss=10.0537
	step [42/192], loss=10.4203
	step [43/192], loss=11.2585
	step [44/192], loss=9.0201
	step [45/192], loss=12.3471
	step [46/192], loss=10.5200
	step [47/192], loss=13.1811
	step [48/192], loss=10.4344
	step [49/192], loss=12.3444
	step [50/192], loss=13.2023
	step [51/192], loss=11.4880
	step [52/192], loss=10.6842
	step [53/192], loss=10.8148
	step [54/192], loss=10.4370
	step [55/192], loss=12.1735
	step [56/192], loss=11.4151
	step [57/192], loss=11.9108
	step [58/192], loss=11.0793
	step [59/192], loss=12.2814
	step [60/192], loss=9.5050
	step [61/192], loss=10.3683
	step [62/192], loss=11.5959
	step [63/192], loss=12.5125
	step [64/192], loss=12.8312
	step [65/192], loss=12.7839
	step [66/192], loss=13.4441
	step [67/192], loss=11.7105
	step [68/192], loss=11.8115
	step [69/192], loss=12.6397
	step [70/192], loss=10.0740
	step [71/192], loss=11.5696
	step [72/192], loss=11.3758
	step [73/192], loss=11.8332
	step [74/192], loss=11.8177
	step [75/192], loss=10.1973
	step [76/192], loss=10.3432
	step [77/192], loss=10.1390
	step [78/192], loss=12.6487
	step [79/192], loss=10.1201
	step [80/192], loss=9.6256
	step [81/192], loss=11.4982
	step [82/192], loss=12.8099
	step [83/192], loss=13.4707
	step [84/192], loss=10.4239
	step [85/192], loss=9.7089
	step [86/192], loss=11.8495
	step [87/192], loss=12.6752
	step [88/192], loss=11.2591
	step [89/192], loss=12.4287
	step [90/192], loss=10.2484
	step [91/192], loss=12.1652
	step [92/192], loss=8.4617
	step [93/192], loss=10.4200
	step [94/192], loss=11.1514
	step [95/192], loss=11.9021
	step [96/192], loss=11.2934
	step [97/192], loss=9.3255
	step [98/192], loss=14.5729
	step [99/192], loss=10.5690
	step [100/192], loss=13.5441
	step [101/192], loss=10.2951
	step [102/192], loss=9.7873
	step [103/192], loss=11.7174
	step [104/192], loss=9.8676
	step [105/192], loss=10.3903
	step [106/192], loss=12.8912
	step [107/192], loss=12.9661
	step [108/192], loss=12.1779
	step [109/192], loss=12.2320
	step [110/192], loss=11.2694
	step [111/192], loss=9.8180
	step [112/192], loss=11.1736
	step [113/192], loss=12.7378
	step [114/192], loss=10.6117
	step [115/192], loss=10.9685
	step [116/192], loss=10.7078
	step [117/192], loss=10.2204
	step [118/192], loss=12.1491
	step [119/192], loss=9.3888
	step [120/192], loss=11.9352
	step [121/192], loss=11.6960
	step [122/192], loss=13.3746
	step [123/192], loss=10.5603
	step [124/192], loss=13.8496
	step [125/192], loss=9.4453
	step [126/192], loss=9.0621
	step [127/192], loss=10.5636
	step [128/192], loss=10.0114
	step [129/192], loss=12.2143
	step [130/192], loss=11.1460
	step [131/192], loss=11.7970
	step [132/192], loss=11.9354
	step [133/192], loss=9.7184
	step [134/192], loss=10.2142
	step [135/192], loss=10.4361
	step [136/192], loss=11.6056
	step [137/192], loss=11.2223
	step [138/192], loss=11.0100
	step [139/192], loss=10.2187
	step [140/192], loss=10.9735
	step [141/192], loss=10.5497
	step [142/192], loss=9.5784
	step [143/192], loss=12.3476
	step [144/192], loss=12.3492
	step [145/192], loss=10.2570
	step [146/192], loss=12.5096
	step [147/192], loss=11.1081
	step [148/192], loss=11.1426
	step [149/192], loss=11.2112
	step [150/192], loss=9.0696
	step [151/192], loss=9.8358
	step [152/192], loss=9.9563
	step [153/192], loss=10.9697
	step [154/192], loss=12.1203
	step [155/192], loss=9.6146
	step [156/192], loss=11.2774
	step [157/192], loss=10.7275
	step [158/192], loss=10.6907
	step [159/192], loss=10.6866
	step [160/192], loss=12.8759
	step [161/192], loss=11.2487
	step [162/192], loss=10.7975
	step [163/192], loss=13.3206
	step [164/192], loss=11.3579
	step [165/192], loss=10.6851
	step [166/192], loss=10.1913
	step [167/192], loss=11.4738
	step [168/192], loss=10.5753
	step [169/192], loss=10.8154
	step [170/192], loss=11.3322
	step [171/192], loss=9.3479
	step [172/192], loss=10.4565
	step [173/192], loss=12.8103
	step [174/192], loss=9.7805
	step [175/192], loss=9.2458
	step [176/192], loss=11.1235
	step [177/192], loss=11.4179
	step [178/192], loss=11.8096
	step [179/192], loss=10.4658
	step [180/192], loss=10.5306
	step [181/192], loss=10.2930
	step [182/192], loss=9.3571
	step [183/192], loss=10.8205
	step [184/192], loss=11.8258
	step [185/192], loss=9.5998
	step [186/192], loss=10.0892
	step [187/192], loss=12.9446
	step [188/192], loss=11.5691
	step [189/192], loss=11.2198
	step [190/192], loss=8.9177
	step [191/192], loss=13.4519
	step [192/192], loss=1.2593
	Evaluating
	loss=0.0339, precision=0.1531, recall=0.9962, f1=0.2653
Training epoch 20
	step [1/192], loss=10.6552
	step [2/192], loss=12.9002
	step [3/192], loss=11.0997
	step [4/192], loss=10.9000
	step [5/192], loss=9.5503
	step [6/192], loss=9.8763
	step [7/192], loss=9.7684
	step [8/192], loss=10.9244
	step [9/192], loss=11.9599
	step [10/192], loss=13.8216
	step [11/192], loss=12.8025
	step [12/192], loss=11.9189
	step [13/192], loss=11.4274
	step [14/192], loss=13.2515
	step [15/192], loss=12.3232
	step [16/192], loss=10.8027
	step [17/192], loss=10.1179
	step [18/192], loss=11.1796
	step [19/192], loss=9.8460
	step [20/192], loss=10.5171
	step [21/192], loss=14.9798
	step [22/192], loss=12.5690
	step [23/192], loss=11.8310
	step [24/192], loss=10.4179
	step [25/192], loss=9.8496
	step [26/192], loss=9.9985
	step [27/192], loss=10.0527
	step [28/192], loss=12.3413
	step [29/192], loss=9.4944
	step [30/192], loss=12.0337
	step [31/192], loss=11.6152
	step [32/192], loss=11.9701
	step [33/192], loss=11.4904
	step [34/192], loss=11.4999
	step [35/192], loss=13.2015
	step [36/192], loss=12.4643
	step [37/192], loss=10.9039
	step [38/192], loss=11.8091
	step [39/192], loss=9.7228
	step [40/192], loss=10.4707
	step [41/192], loss=12.4130
	step [42/192], loss=11.5739
	step [43/192], loss=12.4806
	step [44/192], loss=10.3086
	step [45/192], loss=12.0628
	step [46/192], loss=10.0112
	step [47/192], loss=10.1477
	step [48/192], loss=9.9816
	step [49/192], loss=11.1719
	step [50/192], loss=10.2042
	step [51/192], loss=9.8091
	step [52/192], loss=10.4969
	step [53/192], loss=10.1025
	step [54/192], loss=10.4250
	step [55/192], loss=10.0073
	step [56/192], loss=9.5145
	step [57/192], loss=11.1380
	step [58/192], loss=14.3461
	step [59/192], loss=11.3335
	step [60/192], loss=10.9437
	step [61/192], loss=10.4105
	step [62/192], loss=11.8329
	step [63/192], loss=12.5351
	step [64/192], loss=9.6868
	step [65/192], loss=12.9281
	step [66/192], loss=13.6481
	step [67/192], loss=12.2369
	step [68/192], loss=11.1585
	step [69/192], loss=12.3852
	step [70/192], loss=11.1898
	step [71/192], loss=9.5681
	step [72/192], loss=12.0796
	step [73/192], loss=10.9260
	step [74/192], loss=10.5896
	step [75/192], loss=11.3405
	step [76/192], loss=10.2515
	step [77/192], loss=11.9522
	step [78/192], loss=10.3910
	step [79/192], loss=11.6568
	step [80/192], loss=12.6043
	step [81/192], loss=10.9590
	step [82/192], loss=10.4426
	step [83/192], loss=9.8848
	step [84/192], loss=12.0451
	step [85/192], loss=11.2481
	step [86/192], loss=10.0489
	step [87/192], loss=9.7006
	step [88/192], loss=12.1801
	step [89/192], loss=9.4696
	step [90/192], loss=10.3315
	step [91/192], loss=9.4359
	step [92/192], loss=12.8073
	step [93/192], loss=11.3566
	step [94/192], loss=10.3288
	step [95/192], loss=9.0612
	step [96/192], loss=11.8067
	step [97/192], loss=11.8705
	step [98/192], loss=10.3127
	step [99/192], loss=9.7897
	step [100/192], loss=13.2404
	step [101/192], loss=11.2897
	step [102/192], loss=9.7515
	step [103/192], loss=9.9431
	step [104/192], loss=10.7251
	step [105/192], loss=11.3246
	step [106/192], loss=10.0569
	step [107/192], loss=11.5950
	step [108/192], loss=9.9660
	step [109/192], loss=12.2396
	step [110/192], loss=10.4541
	step [111/192], loss=10.2960
	step [112/192], loss=10.9141
	step [113/192], loss=9.6612
	step [114/192], loss=10.4448
	step [115/192], loss=10.8160
	step [116/192], loss=11.6478
	step [117/192], loss=11.0809
	step [118/192], loss=12.6494
	step [119/192], loss=9.6813
	step [120/192], loss=11.5673
	step [121/192], loss=11.6186
	step [122/192], loss=12.1427
	step [123/192], loss=10.8822
	step [124/192], loss=11.1892
	step [125/192], loss=11.0712
	step [126/192], loss=10.1532
	step [127/192], loss=11.6120
	step [128/192], loss=9.7245
	step [129/192], loss=10.7609
	step [130/192], loss=11.4355
	step [131/192], loss=11.2560
	step [132/192], loss=10.6105
	step [133/192], loss=10.3048
	step [134/192], loss=11.1177
	step [135/192], loss=10.7131
	step [136/192], loss=10.4484
	step [137/192], loss=9.2081
	step [138/192], loss=9.6153
	step [139/192], loss=10.4524
	step [140/192], loss=11.0354
	step [141/192], loss=10.5234
	step [142/192], loss=13.5579
	step [143/192], loss=12.2909
	step [144/192], loss=11.0488
	step [145/192], loss=9.6468
	step [146/192], loss=10.3649
	step [147/192], loss=10.7972
	step [148/192], loss=9.1975
	step [149/192], loss=9.5029
	step [150/192], loss=9.4062
	step [151/192], loss=10.9329
	step [152/192], loss=9.9536
	step [153/192], loss=9.8506
	step [154/192], loss=11.3123
	step [155/192], loss=8.4066
	step [156/192], loss=11.4866
	step [157/192], loss=10.1443
	step [158/192], loss=9.4065
	step [159/192], loss=11.5359
	step [160/192], loss=10.9411
	step [161/192], loss=8.3331
	step [162/192], loss=10.6422
	step [163/192], loss=10.6367
	step [164/192], loss=10.0215
	step [165/192], loss=9.6591
	step [166/192], loss=9.8319
	step [167/192], loss=11.0487
	step [168/192], loss=10.9951
	step [169/192], loss=10.8420
	step [170/192], loss=8.0397
	step [171/192], loss=10.0407
	step [172/192], loss=10.2634
	step [173/192], loss=9.2636
	step [174/192], loss=11.4933
	step [175/192], loss=9.6705
	step [176/192], loss=11.1501
	step [177/192], loss=10.5519
	step [178/192], loss=9.3559
	step [179/192], loss=8.9382
	step [180/192], loss=9.7520
	step [181/192], loss=12.0640
	step [182/192], loss=8.6516
	step [183/192], loss=9.4647
	step [184/192], loss=11.1282
	step [185/192], loss=9.7864
	step [186/192], loss=11.4850
	step [187/192], loss=11.4483
	step [188/192], loss=11.2845
	step [189/192], loss=10.3197
	step [190/192], loss=9.0674
	step [191/192], loss=9.7877
	step [192/192], loss=1.7748
	Evaluating
	loss=0.0299, precision=0.1573, recall=0.9959, f1=0.2716
Training epoch 21
	step [1/192], loss=11.2744
	step [2/192], loss=9.6507
	step [3/192], loss=11.2112
	step [4/192], loss=10.8111
	step [5/192], loss=9.9556
	step [6/192], loss=11.9212
	step [7/192], loss=10.4131
	step [8/192], loss=10.5928
	step [9/192], loss=11.5808
	step [10/192], loss=9.5009
	step [11/192], loss=11.4168
	step [12/192], loss=10.6861
	step [13/192], loss=10.6918
	step [14/192], loss=11.4442
	step [15/192], loss=9.8657
	step [16/192], loss=8.9848
	step [17/192], loss=10.1899
	step [18/192], loss=11.7977
	step [19/192], loss=11.0821
	step [20/192], loss=11.4044
	step [21/192], loss=11.0361
	step [22/192], loss=10.7605
	step [23/192], loss=12.2297
	step [24/192], loss=10.4524
	step [25/192], loss=10.8216
	step [26/192], loss=10.0341
	step [27/192], loss=10.8459
	step [28/192], loss=10.6738
	step [29/192], loss=11.2315
	step [30/192], loss=11.0551
	step [31/192], loss=9.9715
	step [32/192], loss=12.3800
	step [33/192], loss=11.3881
	step [34/192], loss=9.2179
	step [35/192], loss=11.8641
	step [36/192], loss=12.7118
	step [37/192], loss=12.5377
	step [38/192], loss=8.4740
	step [39/192], loss=12.6242
	step [40/192], loss=11.1989
	step [41/192], loss=11.2152
	step [42/192], loss=11.2113
	step [43/192], loss=9.5642
	step [44/192], loss=11.1349
	step [45/192], loss=9.3090
	step [46/192], loss=10.3171
	step [47/192], loss=8.9780
	step [48/192], loss=10.8981
	step [49/192], loss=11.6148
	step [50/192], loss=10.1984
	step [51/192], loss=10.2892
	step [52/192], loss=9.4671
	step [53/192], loss=10.1808
	step [54/192], loss=10.0590
	step [55/192], loss=9.4536
	step [56/192], loss=10.5258
	step [57/192], loss=10.7670
	step [58/192], loss=12.4558
	step [59/192], loss=11.1239
	step [60/192], loss=11.8801
	step [61/192], loss=10.4419
	step [62/192], loss=9.4334
	step [63/192], loss=10.5956
	step [64/192], loss=10.3474
	step [65/192], loss=11.6040
	step [66/192], loss=10.8483
	step [67/192], loss=9.5562
	step [68/192], loss=9.6009
	step [69/192], loss=12.1299
	step [70/192], loss=9.3491
	step [71/192], loss=9.3414
	step [72/192], loss=12.3546
	step [73/192], loss=12.3251
	step [74/192], loss=11.0237
	step [75/192], loss=8.9452
	step [76/192], loss=9.8135
	step [77/192], loss=11.4509
	step [78/192], loss=10.3307
	step [79/192], loss=12.4517
	step [80/192], loss=10.2488
	step [81/192], loss=10.7339
	step [82/192], loss=10.1787
	step [83/192], loss=10.7711
	step [84/192], loss=8.6212
	step [85/192], loss=11.0793
	step [86/192], loss=10.8675
	step [87/192], loss=11.0664
	step [88/192], loss=8.6761
	step [89/192], loss=12.3694
	step [90/192], loss=9.9430
	step [91/192], loss=9.5099
	step [92/192], loss=10.8622
	step [93/192], loss=11.1191
	step [94/192], loss=10.4099
	step [95/192], loss=11.2963
	step [96/192], loss=10.0687
	step [97/192], loss=12.3188
	step [98/192], loss=10.1198
	step [99/192], loss=9.0248
	step [100/192], loss=10.7330
	step [101/192], loss=11.1416
	step [102/192], loss=9.4608
	step [103/192], loss=11.2341
	step [104/192], loss=11.8694
	step [105/192], loss=13.4071
	step [106/192], loss=10.9985
	step [107/192], loss=10.8324
	step [108/192], loss=13.2439
	step [109/192], loss=9.3157
	step [110/192], loss=9.8665
	step [111/192], loss=10.8838
	step [112/192], loss=10.4320
	step [113/192], loss=9.2173
	step [114/192], loss=10.1206
	step [115/192], loss=10.3428
	step [116/192], loss=11.9384
	step [117/192], loss=11.0105
	step [118/192], loss=12.3836
	step [119/192], loss=9.0329
	step [120/192], loss=9.1265
	step [121/192], loss=12.0125
	step [122/192], loss=8.9468
	step [123/192], loss=9.7522
	step [124/192], loss=9.6697
	step [125/192], loss=10.5613
	step [126/192], loss=9.9443
	step [127/192], loss=9.9360
	step [128/192], loss=10.6008
	step [129/192], loss=9.3485
	step [130/192], loss=8.8525
	step [131/192], loss=9.1427
	step [132/192], loss=10.8178
	step [133/192], loss=9.7778
	step [134/192], loss=11.7300
	step [135/192], loss=12.6887
	step [136/192], loss=10.2777
	step [137/192], loss=10.3241
	step [138/192], loss=10.6692
	step [139/192], loss=10.9039
	step [140/192], loss=9.9245
	step [141/192], loss=11.7460
	step [142/192], loss=9.6136
	step [143/192], loss=12.6726
	step [144/192], loss=10.5848
	step [145/192], loss=10.3781
	step [146/192], loss=9.1655
	step [147/192], loss=9.8996
	step [148/192], loss=10.3624
	step [149/192], loss=10.5550
	step [150/192], loss=9.5673
	step [151/192], loss=11.9402
	step [152/192], loss=13.2798
	step [153/192], loss=10.7503
	step [154/192], loss=10.9033
	step [155/192], loss=10.0095
	step [156/192], loss=10.8273
	step [157/192], loss=9.3248
	step [158/192], loss=9.6368
	step [159/192], loss=13.0633
	step [160/192], loss=10.7011
	step [161/192], loss=11.0079
	step [162/192], loss=11.3687
	step [163/192], loss=11.5598
	step [164/192], loss=12.4356
	step [165/192], loss=10.6990
	step [166/192], loss=8.4709
	step [167/192], loss=9.2660
	step [168/192], loss=11.7800
	step [169/192], loss=10.2921
	step [170/192], loss=8.1424
	step [171/192], loss=13.1562
	step [172/192], loss=9.9070
	step [173/192], loss=9.0659
	step [174/192], loss=10.6016
	step [175/192], loss=10.5354
	step [176/192], loss=8.6658
	step [177/192], loss=10.9026
	step [178/192], loss=10.7448
	step [179/192], loss=10.3185
	step [180/192], loss=11.5217
	step [181/192], loss=10.1762
	step [182/192], loss=8.8185
	step [183/192], loss=10.0050
	step [184/192], loss=10.6084
	step [185/192], loss=9.7585
	step [186/192], loss=10.3791
	step [187/192], loss=9.8366
	step [188/192], loss=11.1511
	step [189/192], loss=8.6444
	step [190/192], loss=9.8879
	step [191/192], loss=9.8952
	step [192/192], loss=0.9109
	Evaluating
	loss=0.0242, precision=0.2016, recall=0.9947, f1=0.3352
Training epoch 22
	step [1/192], loss=11.3829
	step [2/192], loss=10.1473
	step [3/192], loss=8.8174
	step [4/192], loss=9.4216
	step [5/192], loss=9.1697
	step [6/192], loss=10.8874
	step [7/192], loss=11.3820
	step [8/192], loss=11.8292
	step [9/192], loss=10.3490
	step [10/192], loss=10.8973
	step [11/192], loss=10.0787
	step [12/192], loss=10.9770
	step [13/192], loss=10.7159
	step [14/192], loss=10.7822
	step [15/192], loss=10.3863
	step [16/192], loss=12.0397
	step [17/192], loss=10.1447
	step [18/192], loss=9.3866
	step [19/192], loss=11.4432
	step [20/192], loss=10.0160
	step [21/192], loss=10.5955
	step [22/192], loss=8.9090
	step [23/192], loss=10.6618
	step [24/192], loss=10.3820
	step [25/192], loss=9.9045
	step [26/192], loss=8.3426
	step [27/192], loss=11.5831
	step [28/192], loss=9.0404
	step [29/192], loss=11.3085
	step [30/192], loss=10.4997
	step [31/192], loss=10.5765
	step [32/192], loss=12.7282
	step [33/192], loss=8.5547
	step [34/192], loss=11.6862
	step [35/192], loss=9.1020
	step [36/192], loss=9.7690
	step [37/192], loss=9.8752
	step [38/192], loss=11.4646
	step [39/192], loss=10.5667
	step [40/192], loss=10.9441
	step [41/192], loss=11.5167
	step [42/192], loss=9.9831
	step [43/192], loss=10.1611
	step [44/192], loss=11.4032
	step [45/192], loss=12.0308
	step [46/192], loss=11.2420
	step [47/192], loss=9.1774
	step [48/192], loss=9.9823
	step [49/192], loss=7.7631
	step [50/192], loss=9.3431
	step [51/192], loss=9.9906
	step [52/192], loss=12.0822
	step [53/192], loss=10.5947
	step [54/192], loss=9.4082
	step [55/192], loss=7.6171
	step [56/192], loss=11.1521
	step [57/192], loss=11.6136
	step [58/192], loss=12.1896
	step [59/192], loss=10.7944
	step [60/192], loss=11.1861
	step [61/192], loss=9.6986
	step [62/192], loss=9.2901
	step [63/192], loss=11.0488
	step [64/192], loss=10.8431
	step [65/192], loss=10.3293
	step [66/192], loss=11.8012
	step [67/192], loss=10.4555
	step [68/192], loss=11.6050
	step [69/192], loss=9.0685
	step [70/192], loss=9.4336
	step [71/192], loss=8.4191
	step [72/192], loss=10.6303
	step [73/192], loss=9.9179
	step [74/192], loss=10.5929
	step [75/192], loss=7.9330
	step [76/192], loss=9.8721
	step [77/192], loss=10.6921
	step [78/192], loss=11.8889
	step [79/192], loss=11.6249
	step [80/192], loss=10.1478
	step [81/192], loss=9.4174
	step [82/192], loss=12.0996
	step [83/192], loss=10.4376
	step [84/192], loss=10.5175
	step [85/192], loss=9.6353
	step [86/192], loss=10.3655
	step [87/192], loss=9.0020
	step [88/192], loss=9.5752
	step [89/192], loss=9.5821
	step [90/192], loss=11.4380
	step [91/192], loss=10.4088
	step [92/192], loss=11.1198
	step [93/192], loss=8.0686
	step [94/192], loss=9.8705
	step [95/192], loss=8.0846
	step [96/192], loss=10.7957
	step [97/192], loss=11.8634
	step [98/192], loss=9.9537
	step [99/192], loss=8.8584
	step [100/192], loss=10.7235
	step [101/192], loss=10.1778
	step [102/192], loss=11.8474
	step [103/192], loss=11.1477
	step [104/192], loss=10.3642
	step [105/192], loss=9.6254
	step [106/192], loss=9.1491
	step [107/192], loss=12.3004
	step [108/192], loss=9.6485
	step [109/192], loss=10.6618
	step [110/192], loss=11.8069
	step [111/192], loss=10.4148
	step [112/192], loss=12.6969
	step [113/192], loss=9.9057
	step [114/192], loss=9.7291
	step [115/192], loss=9.6721
	step [116/192], loss=10.0397
	step [117/192], loss=9.6445
	step [118/192], loss=10.7269
	step [119/192], loss=13.0204
	step [120/192], loss=9.9638
	step [121/192], loss=10.2902
	step [122/192], loss=10.1710
	step [123/192], loss=11.3797
	step [124/192], loss=9.0914
	step [125/192], loss=11.4955
	step [126/192], loss=10.0956
	step [127/192], loss=12.2556
	step [128/192], loss=10.1850
	step [129/192], loss=11.7441
	step [130/192], loss=9.9673
	step [131/192], loss=8.5752
	step [132/192], loss=10.9803
	step [133/192], loss=11.7212
	step [134/192], loss=11.1304
	step [135/192], loss=10.1620
	step [136/192], loss=11.1537
	step [137/192], loss=8.5454
	step [138/192], loss=10.0807
	step [139/192], loss=11.8899
	step [140/192], loss=11.8950
	step [141/192], loss=9.7050
	step [142/192], loss=9.2480
	step [143/192], loss=10.7767
	step [144/192], loss=9.3116
	step [145/192], loss=8.8205
	step [146/192], loss=8.3698
	step [147/192], loss=11.6459
	step [148/192], loss=10.1792
	step [149/192], loss=8.8464
	step [150/192], loss=8.6419
	step [151/192], loss=13.5039
	step [152/192], loss=10.5605
	step [153/192], loss=9.0236
	step [154/192], loss=9.3848
	step [155/192], loss=10.9370
	step [156/192], loss=11.6845
	step [157/192], loss=9.2027
	step [158/192], loss=9.3916
	step [159/192], loss=12.1110
	step [160/192], loss=9.7733
	step [161/192], loss=7.7494
	step [162/192], loss=9.8037
	step [163/192], loss=9.2610
	step [164/192], loss=11.4873
	step [165/192], loss=8.5568
	step [166/192], loss=11.8935
	step [167/192], loss=10.9457
	step [168/192], loss=8.2912
	step [169/192], loss=9.3623
	step [170/192], loss=10.4269
	step [171/192], loss=10.1389
	step [172/192], loss=9.4402
	step [173/192], loss=11.3930
	step [174/192], loss=10.4091
	step [175/192], loss=11.4158
	step [176/192], loss=10.3060
	step [177/192], loss=13.5531
	step [178/192], loss=9.6632
	step [179/192], loss=9.9770
	step [180/192], loss=8.9081
	step [181/192], loss=9.7869
	step [182/192], loss=9.2569
	step [183/192], loss=11.5192
	step [184/192], loss=10.7172
	step [185/192], loss=8.3895
	step [186/192], loss=11.4314
	step [187/192], loss=9.3682
	step [188/192], loss=9.1159
	step [189/192], loss=11.9816
	step [190/192], loss=10.3519
	step [191/192], loss=9.6655
	step [192/192], loss=1.3008
	Evaluating
	loss=0.0308, precision=0.1606, recall=0.9960, f1=0.2766
Training epoch 23
	step [1/192], loss=7.3692
	step [2/192], loss=8.6220
	step [3/192], loss=9.0813
	step [4/192], loss=8.9792
	step [5/192], loss=10.0033
	step [6/192], loss=9.6815
	step [7/192], loss=9.7054
	step [8/192], loss=9.3715
	step [9/192], loss=8.4845
	step [10/192], loss=11.1149
	step [11/192], loss=10.4053
	step [12/192], loss=10.2325
	step [13/192], loss=8.9126
	step [14/192], loss=11.4662
	step [15/192], loss=11.7549
	step [16/192], loss=9.7112
	step [17/192], loss=9.8392
	step [18/192], loss=9.5030
	step [19/192], loss=8.6299
	step [20/192], loss=10.9903
	step [21/192], loss=8.8200
	step [22/192], loss=10.3170
	step [23/192], loss=11.8765
	step [24/192], loss=8.5801
	step [25/192], loss=9.0742
	step [26/192], loss=11.4668
	step [27/192], loss=10.9304
	step [28/192], loss=11.3986
	step [29/192], loss=8.6904
	step [30/192], loss=9.6867
	step [31/192], loss=9.7685
	step [32/192], loss=10.8918
	step [33/192], loss=11.5462
	step [34/192], loss=9.6833
	step [35/192], loss=12.3553
	step [36/192], loss=9.7868
	step [37/192], loss=9.7203
	step [38/192], loss=7.6294
	step [39/192], loss=9.3942
	step [40/192], loss=10.1202
	step [41/192], loss=10.7691
	step [42/192], loss=11.8563
	step [43/192], loss=10.1252
	step [44/192], loss=9.2697
	step [45/192], loss=11.4721
	step [46/192], loss=10.8713
	step [47/192], loss=12.8551
	step [48/192], loss=8.7303
	step [49/192], loss=12.5778
	step [50/192], loss=9.1760
	step [51/192], loss=10.0626
	step [52/192], loss=11.8352
	step [53/192], loss=10.6637
	step [54/192], loss=9.0522
	step [55/192], loss=10.3268
	step [56/192], loss=10.5090
	step [57/192], loss=11.0264
	step [58/192], loss=11.1123
	step [59/192], loss=9.6718
	step [60/192], loss=9.0422
	step [61/192], loss=12.2638
	step [62/192], loss=9.7378
	step [63/192], loss=8.1818
	step [64/192], loss=11.1337
	step [65/192], loss=10.8686
	step [66/192], loss=11.7544
	step [67/192], loss=9.0431
	step [68/192], loss=9.4208
	step [69/192], loss=10.5089
	step [70/192], loss=8.7974
	step [71/192], loss=10.9116
	step [72/192], loss=10.9948
	step [73/192], loss=8.0431
	step [74/192], loss=8.9284
	step [75/192], loss=7.3785
	step [76/192], loss=8.1854
	step [77/192], loss=10.7597
	step [78/192], loss=11.6037
	step [79/192], loss=11.3562
	step [80/192], loss=10.0087
	step [81/192], loss=10.7944
	step [82/192], loss=10.4702
	step [83/192], loss=9.6505
	step [84/192], loss=10.2557
	step [85/192], loss=9.7351
	step [86/192], loss=9.3412
	step [87/192], loss=9.6264
	step [88/192], loss=10.0148
	step [89/192], loss=11.2575
	step [90/192], loss=9.5347
	step [91/192], loss=11.6577
	step [92/192], loss=11.1926
	step [93/192], loss=10.1890
	step [94/192], loss=10.2665
	step [95/192], loss=10.5164
	step [96/192], loss=11.3523
	step [97/192], loss=10.7256
	step [98/192], loss=9.1051
	step [99/192], loss=9.7672
	step [100/192], loss=11.5726
	step [101/192], loss=11.4464
	step [102/192], loss=8.4975
	step [103/192], loss=11.1818
	step [104/192], loss=9.3219
	step [105/192], loss=10.1861
	step [106/192], loss=8.5029
	step [107/192], loss=8.9541
	step [108/192], loss=8.1439
	step [109/192], loss=10.9222
	step [110/192], loss=9.0666
	step [111/192], loss=9.5978
	step [112/192], loss=11.6731
	step [113/192], loss=9.7219
	step [114/192], loss=8.8528
	step [115/192], loss=10.3306
	step [116/192], loss=10.3483
	step [117/192], loss=9.5698
	step [118/192], loss=11.8919
	step [119/192], loss=7.7862
	step [120/192], loss=11.8369
	step [121/192], loss=9.6963
	step [122/192], loss=12.0023
	step [123/192], loss=9.8547
	step [124/192], loss=12.5463
	step [125/192], loss=8.1022
	step [126/192], loss=9.2323
	step [127/192], loss=8.2212
	step [128/192], loss=9.5267
	step [129/192], loss=11.7473
	step [130/192], loss=8.9553
	step [131/192], loss=9.1650
	step [132/192], loss=8.5101
	step [133/192], loss=11.0011
	step [134/192], loss=10.4729
	step [135/192], loss=12.0969
	step [136/192], loss=8.2456
	step [137/192], loss=9.7515
	step [138/192], loss=11.3647
	step [139/192], loss=10.9490
	step [140/192], loss=11.6041
	step [141/192], loss=12.4319
	step [142/192], loss=8.5548
	step [143/192], loss=9.7741
	step [144/192], loss=9.6998
	step [145/192], loss=9.2256
	step [146/192], loss=8.6978
	step [147/192], loss=11.8597
	step [148/192], loss=8.8420
	step [149/192], loss=9.8267
	step [150/192], loss=10.5376
	step [151/192], loss=8.5064
	step [152/192], loss=10.1914
	step [153/192], loss=8.7006
	step [154/192], loss=7.9948
	step [155/192], loss=11.0401
	step [156/192], loss=12.6212
	step [157/192], loss=12.2113
	step [158/192], loss=10.6118
	step [159/192], loss=11.8139
	step [160/192], loss=10.9559
	step [161/192], loss=9.8879
	step [162/192], loss=9.3561
	step [163/192], loss=9.7297
	step [164/192], loss=9.9697
	step [165/192], loss=11.2451
	step [166/192], loss=10.3617
	step [167/192], loss=10.0563
	step [168/192], loss=8.6941
	step [169/192], loss=8.9302
	step [170/192], loss=8.0092
	step [171/192], loss=9.3101
	step [172/192], loss=9.8706
	step [173/192], loss=8.4989
	step [174/192], loss=12.9211
	step [175/192], loss=12.6243
	step [176/192], loss=11.4803
	step [177/192], loss=9.0448
	step [178/192], loss=9.3711
	step [179/192], loss=11.2927
	step [180/192], loss=9.2562
	step [181/192], loss=9.7262
	step [182/192], loss=10.4563
	step [183/192], loss=10.4656
	step [184/192], loss=8.7518
	step [185/192], loss=9.8468
	step [186/192], loss=8.0793
	step [187/192], loss=11.8029
	step [188/192], loss=10.6088
	step [189/192], loss=9.6217
	step [190/192], loss=8.9187
	step [191/192], loss=9.6340
	step [192/192], loss=0.3891
	Evaluating
	loss=0.0263, precision=0.1719, recall=0.9955, f1=0.2932
Training epoch 24
	step [1/192], loss=10.0528
	step [2/192], loss=8.7552
	step [3/192], loss=9.0221
	step [4/192], loss=11.7546
	step [5/192], loss=9.9797
	step [6/192], loss=9.5944
	step [7/192], loss=9.7441
	step [8/192], loss=12.4033
	step [9/192], loss=12.1756
	step [10/192], loss=10.5400
	step [11/192], loss=10.5656
	step [12/192], loss=9.1294
	step [13/192], loss=9.6448
	step [14/192], loss=9.6541
	step [15/192], loss=8.0564
	step [16/192], loss=12.7650
	step [17/192], loss=9.3908
	step [18/192], loss=9.8527
	step [19/192], loss=8.1189
	step [20/192], loss=11.3554
	step [21/192], loss=8.6167
	step [22/192], loss=9.4395
	step [23/192], loss=9.1641
	step [24/192], loss=9.1315
	step [25/192], loss=9.4168
	step [26/192], loss=9.1447
	step [27/192], loss=10.4408
	step [28/192], loss=10.3392
	step [29/192], loss=8.0089
	step [30/192], loss=10.1692
	step [31/192], loss=9.3570
	step [32/192], loss=9.4933
	step [33/192], loss=9.5177
	step [34/192], loss=11.9577
	step [35/192], loss=12.1925
	step [36/192], loss=8.8835
	step [37/192], loss=9.3202
	step [38/192], loss=9.3357
	step [39/192], loss=11.0454
	step [40/192], loss=11.5780
	step [41/192], loss=10.1127
	step [42/192], loss=7.7706
	step [43/192], loss=10.6868
	step [44/192], loss=9.2185
	step [45/192], loss=9.0991
	step [46/192], loss=10.7116
	step [47/192], loss=8.9615
	step [48/192], loss=9.9365
	step [49/192], loss=9.8512
	step [50/192], loss=9.8795
	step [51/192], loss=10.7510
	step [52/192], loss=10.2391
	step [53/192], loss=7.6585
	step [54/192], loss=10.7984
	step [55/192], loss=8.4356
	step [56/192], loss=10.5606
	step [57/192], loss=10.5604
	step [58/192], loss=9.2037
	step [59/192], loss=10.3404
	step [60/192], loss=9.6229
	step [61/192], loss=9.0473
	step [62/192], loss=10.0855
	step [63/192], loss=11.9112
	step [64/192], loss=8.7028
	step [65/192], loss=10.2441
	step [66/192], loss=9.7215
	step [67/192], loss=10.1784
	step [68/192], loss=9.8770
	step [69/192], loss=10.4206
	step [70/192], loss=9.9719
	step [71/192], loss=9.1448
	step [72/192], loss=8.8053
	step [73/192], loss=9.2696
	step [74/192], loss=11.0581
	step [75/192], loss=10.0875
	step [76/192], loss=10.0738
	step [77/192], loss=10.6967
	step [78/192], loss=8.5109
	step [79/192], loss=9.3789
	step [80/192], loss=10.4541
	step [81/192], loss=14.4314
	step [82/192], loss=9.6645
	step [83/192], loss=8.3662
	step [84/192], loss=9.2817
	step [85/192], loss=10.6579
	step [86/192], loss=8.0899
	step [87/192], loss=11.8073
	step [88/192], loss=10.3970
	step [89/192], loss=10.0369
	step [90/192], loss=12.3893
	step [91/192], loss=9.9320
	step [92/192], loss=10.5400
	step [93/192], loss=8.6453
	step [94/192], loss=10.1153
	step [95/192], loss=10.8725
	step [96/192], loss=9.9219
	step [97/192], loss=9.5095
	step [98/192], loss=7.9956
	step [99/192], loss=10.8396
	step [100/192], loss=10.1123
	step [101/192], loss=10.9497
	step [102/192], loss=9.7877
	step [103/192], loss=8.5590
	step [104/192], loss=11.1556
	step [105/192], loss=10.4661
	step [106/192], loss=9.7511
	step [107/192], loss=9.0304
	step [108/192], loss=8.7138
	step [109/192], loss=9.1435
	step [110/192], loss=8.5165
	step [111/192], loss=9.5904
	step [112/192], loss=9.4578
	step [113/192], loss=10.0659
	step [114/192], loss=10.3487
	step [115/192], loss=9.2790
	step [116/192], loss=10.7666
	step [117/192], loss=9.9727
	step [118/192], loss=11.0435
	step [119/192], loss=7.9007
	step [120/192], loss=10.1941
	step [121/192], loss=10.0498
	step [122/192], loss=9.5503
	step [123/192], loss=11.9120
	step [124/192], loss=9.0483
	step [125/192], loss=9.1367
	step [126/192], loss=10.4101
	step [127/192], loss=10.5172
	step [128/192], loss=7.8060
	step [129/192], loss=13.1762
	step [130/192], loss=8.6836
	step [131/192], loss=9.8126
	step [132/192], loss=10.2189
	step [133/192], loss=9.5047
	step [134/192], loss=9.6757
	step [135/192], loss=11.1976
	step [136/192], loss=8.6330
	step [137/192], loss=9.9584
	step [138/192], loss=10.3157
	step [139/192], loss=12.3133
	step [140/192], loss=8.8105
	step [141/192], loss=9.0905
	step [142/192], loss=10.3400
	step [143/192], loss=8.7814
	step [144/192], loss=10.3326
	step [145/192], loss=11.4498
	step [146/192], loss=11.0361
	step [147/192], loss=8.7942
	step [148/192], loss=9.2545
	step [149/192], loss=8.8676
	step [150/192], loss=9.0020
	step [151/192], loss=11.8506
	step [152/192], loss=8.1369
	step [153/192], loss=11.7435
	step [154/192], loss=10.0538
	step [155/192], loss=9.9211
	step [156/192], loss=9.5729
	step [157/192], loss=10.6686
	step [158/192], loss=9.4053
	step [159/192], loss=8.0779
	step [160/192], loss=9.0349
	step [161/192], loss=8.5401
	step [162/192], loss=10.4000
	step [163/192], loss=8.7335
	step [164/192], loss=10.6409
	step [165/192], loss=9.6024
	step [166/192], loss=9.8993
	step [167/192], loss=11.3419
	step [168/192], loss=10.7982
	step [169/192], loss=8.0205
	step [170/192], loss=9.9021
	step [171/192], loss=11.1338
	step [172/192], loss=12.0604
	step [173/192], loss=8.1152
	step [174/192], loss=9.1468
	step [175/192], loss=9.1209
	step [176/192], loss=8.0137
	step [177/192], loss=10.5574
	step [178/192], loss=10.8070
	step [179/192], loss=9.8949
	step [180/192], loss=9.2371
	step [181/192], loss=9.9400
	step [182/192], loss=9.7125
	step [183/192], loss=9.1064
	step [184/192], loss=8.9266
	step [185/192], loss=10.1642
	step [186/192], loss=9.2974
	step [187/192], loss=8.2782
	step [188/192], loss=8.9186
	step [189/192], loss=12.3316
	step [190/192], loss=9.5450
	step [191/192], loss=8.9938
	step [192/192], loss=1.7980
	Evaluating
	loss=0.0295, precision=0.1580, recall=0.9960, f1=0.2727
Training epoch 25
	step [1/192], loss=13.3894
	step [2/192], loss=9.1069
	step [3/192], loss=7.5175
	step [4/192], loss=9.3942
	step [5/192], loss=11.0508
	step [6/192], loss=10.5800
	step [7/192], loss=9.8361
	step [8/192], loss=9.2277
	step [9/192], loss=11.2875
	step [10/192], loss=8.7226
	step [11/192], loss=9.5515
	step [12/192], loss=9.2905
	step [13/192], loss=9.7928
	step [14/192], loss=9.4405
	step [15/192], loss=11.9329
	step [16/192], loss=10.0415
	step [17/192], loss=9.2403
	step [18/192], loss=8.3732
	step [19/192], loss=10.3166
	step [20/192], loss=9.8958
	step [21/192], loss=9.4139
	step [22/192], loss=8.7334
	step [23/192], loss=9.4047
	step [24/192], loss=9.2338
	step [25/192], loss=11.1198
	step [26/192], loss=9.2968
	step [27/192], loss=10.4043
	step [28/192], loss=8.9914
	step [29/192], loss=9.4508
	step [30/192], loss=9.9318
	step [31/192], loss=9.5141
	step [32/192], loss=10.2334
	step [33/192], loss=9.1197
	step [34/192], loss=8.9913
	step [35/192], loss=7.6568
	step [36/192], loss=8.8499
	step [37/192], loss=8.2776
	step [38/192], loss=9.1534
	step [39/192], loss=10.5605
	step [40/192], loss=12.1364
	step [41/192], loss=8.0907
	step [42/192], loss=7.3782
	step [43/192], loss=9.3546
	step [44/192], loss=7.2002
	step [45/192], loss=9.9188
	step [46/192], loss=7.5206
	step [47/192], loss=10.9480
	step [48/192], loss=9.6088
	step [49/192], loss=12.9037
	step [50/192], loss=9.0661
	step [51/192], loss=9.1083
	step [52/192], loss=9.8173
	step [53/192], loss=9.0937
	step [54/192], loss=9.2388
	step [55/192], loss=8.5520
	step [56/192], loss=10.5592
	step [57/192], loss=8.7526
	step [58/192], loss=8.3065
	step [59/192], loss=9.3075
	step [60/192], loss=10.1177
	step [61/192], loss=7.0638
	step [62/192], loss=10.4442
	step [63/192], loss=8.9579
	step [64/192], loss=9.4133
	step [65/192], loss=9.0134
	step [66/192], loss=9.2440
	step [67/192], loss=11.6033
	step [68/192], loss=11.4159
	step [69/192], loss=9.9608
	step [70/192], loss=11.3109
	step [71/192], loss=11.4394
	step [72/192], loss=10.6032
	step [73/192], loss=11.3155
	step [74/192], loss=8.4853
	step [75/192], loss=9.8240
	step [76/192], loss=10.1305
	step [77/192], loss=9.9067
	step [78/192], loss=10.2772
	step [79/192], loss=9.6090
	step [80/192], loss=11.5561
	step [81/192], loss=9.4782
	step [82/192], loss=9.2310
	step [83/192], loss=10.3070
	step [84/192], loss=9.9128
	step [85/192], loss=9.3900
	step [86/192], loss=9.7199
	step [87/192], loss=9.0315
	step [88/192], loss=10.1418
	step [89/192], loss=9.8616
	step [90/192], loss=10.5214
	step [91/192], loss=8.9964
	step [92/192], loss=9.0749
	step [93/192], loss=9.6255
	step [94/192], loss=9.3327
	step [95/192], loss=9.5395
	step [96/192], loss=8.8467
	step [97/192], loss=10.7467
	step [98/192], loss=10.9963
	step [99/192], loss=9.7531
	step [100/192], loss=9.2378
	step [101/192], loss=9.7289
	step [102/192], loss=9.4727
	step [103/192], loss=9.4990
	step [104/192], loss=10.4026
	step [105/192], loss=9.2376
	step [106/192], loss=9.4904
	step [107/192], loss=10.8033
	step [108/192], loss=9.8277
	step [109/192], loss=9.2984
	step [110/192], loss=10.6265
	step [111/192], loss=9.8101
	step [112/192], loss=9.0902
	step [113/192], loss=8.9765
	step [114/192], loss=8.4133
	step [115/192], loss=10.2294
	step [116/192], loss=10.8756
	step [117/192], loss=9.0502
	step [118/192], loss=9.7187
	step [119/192], loss=8.9991
	step [120/192], loss=8.4425
	step [121/192], loss=7.6636
	step [122/192], loss=10.3689
	step [123/192], loss=8.4408
	step [124/192], loss=11.9175
	step [125/192], loss=8.6328
	step [126/192], loss=9.1935
	step [127/192], loss=9.4642
	step [128/192], loss=9.7290
	step [129/192], loss=10.3275
	step [130/192], loss=11.9968
	step [131/192], loss=8.7014
	step [132/192], loss=8.3528
	step [133/192], loss=11.3656
	step [134/192], loss=11.0790
	step [135/192], loss=9.2329
	step [136/192], loss=11.3793
	step [137/192], loss=9.4521
	step [138/192], loss=10.9237
	step [139/192], loss=9.1748
	step [140/192], loss=10.1127
	step [141/192], loss=8.7896
	step [142/192], loss=10.7917
	step [143/192], loss=7.9589
	step [144/192], loss=9.9390
	step [145/192], loss=10.2117
	step [146/192], loss=11.0133
	step [147/192], loss=8.1689
	step [148/192], loss=9.7348
	step [149/192], loss=9.9387
	step [150/192], loss=10.7267
	step [151/192], loss=9.4754
	step [152/192], loss=9.3817
	step [153/192], loss=11.7914
	step [154/192], loss=10.8257
	step [155/192], loss=8.4771
	step [156/192], loss=11.1025
	step [157/192], loss=7.6074
	step [158/192], loss=10.0538
	step [159/192], loss=8.7349
	step [160/192], loss=10.1212
	step [161/192], loss=11.7100
	step [162/192], loss=10.9574
	step [163/192], loss=6.8558
	step [164/192], loss=9.4145
	step [165/192], loss=9.1635
	step [166/192], loss=10.6565
	step [167/192], loss=8.8902
	step [168/192], loss=7.6311
	step [169/192], loss=9.4442
	step [170/192], loss=9.5480
	step [171/192], loss=8.4528
	step [172/192], loss=8.8719
	step [173/192], loss=11.0053
	step [174/192], loss=10.5032
	step [175/192], loss=7.8580
	step [176/192], loss=8.0405
	step [177/192], loss=9.9290
	step [178/192], loss=8.2237
	step [179/192], loss=10.1600
	step [180/192], loss=10.9314
	step [181/192], loss=8.8569
	step [182/192], loss=10.1191
	step [183/192], loss=9.2662
	step [184/192], loss=10.9991
	step [185/192], loss=7.9029
	step [186/192], loss=9.4699
	step [187/192], loss=11.4144
	step [188/192], loss=9.9400
	step [189/192], loss=10.7879
	step [190/192], loss=8.9131
	step [191/192], loss=8.7080
	step [192/192], loss=0.9554
	Evaluating
	loss=0.0276, precision=0.1656, recall=0.9950, f1=0.2839
Training epoch 26
	step [1/192], loss=9.7653
	step [2/192], loss=12.0706
	step [3/192], loss=9.0545
	step [4/192], loss=8.5856
	step [5/192], loss=7.1008
	step [6/192], loss=8.0251
	step [7/192], loss=9.4547
	step [8/192], loss=9.0386
	step [9/192], loss=8.6528
	step [10/192], loss=9.8374
	step [11/192], loss=8.0137
	step [12/192], loss=10.2934
	step [13/192], loss=8.0349
	step [14/192], loss=8.9969
	step [15/192], loss=8.3652
	step [16/192], loss=9.6382
	step [17/192], loss=9.3608
	step [18/192], loss=9.7763
	step [19/192], loss=10.5721
	step [20/192], loss=9.3183
	step [21/192], loss=9.7420
	step [22/192], loss=8.8789
	step [23/192], loss=10.0926
	step [24/192], loss=9.7668
	step [25/192], loss=9.7876
	step [26/192], loss=9.5889
	step [27/192], loss=8.5025
	step [28/192], loss=9.2260
	step [29/192], loss=6.9505
	step [30/192], loss=9.6805
	step [31/192], loss=8.7412
	step [32/192], loss=11.2832
	step [33/192], loss=10.9954
	step [34/192], loss=9.7990
	step [35/192], loss=9.0115
	step [36/192], loss=9.5043
	step [37/192], loss=11.0552
	step [38/192], loss=9.2115
	step [39/192], loss=10.1325
	step [40/192], loss=10.2304
	step [41/192], loss=9.8046
	step [42/192], loss=10.4655
	step [43/192], loss=9.5261
	step [44/192], loss=8.3130
	step [45/192], loss=10.4641
	step [46/192], loss=8.7685
	step [47/192], loss=8.9670
	step [48/192], loss=11.1392
	step [49/192], loss=9.8867
	step [50/192], loss=9.6022
	step [51/192], loss=11.2457
	step [52/192], loss=9.3372
	step [53/192], loss=10.4737
	step [54/192], loss=9.5183
	step [55/192], loss=9.8381
	step [56/192], loss=8.8588
	step [57/192], loss=8.3296
	step [58/192], loss=9.4738
	step [59/192], loss=9.2963
	step [60/192], loss=9.6162
	step [61/192], loss=10.3155
	step [62/192], loss=7.4564
	step [63/192], loss=8.7776
	step [64/192], loss=9.7250
	step [65/192], loss=8.9885
	step [66/192], loss=9.9898
	step [67/192], loss=10.3577
	step [68/192], loss=9.2262
	step [69/192], loss=10.9778
	step [70/192], loss=7.2321
	step [71/192], loss=11.3517
	step [72/192], loss=10.4559
	step [73/192], loss=10.8381
	step [74/192], loss=8.0647
	step [75/192], loss=8.7601
	step [76/192], loss=7.9072
	step [77/192], loss=10.3585
	step [78/192], loss=9.0194
	step [79/192], loss=10.0738
	step [80/192], loss=12.5847
	step [81/192], loss=8.8227
	step [82/192], loss=10.0825
	step [83/192], loss=9.7702
	step [84/192], loss=9.4449
	step [85/192], loss=11.4044
	step [86/192], loss=9.0239
	step [87/192], loss=11.1912
	step [88/192], loss=11.2405
	step [89/192], loss=9.2268
	step [90/192], loss=10.0988
	step [91/192], loss=6.8195
	step [92/192], loss=10.0799
	step [93/192], loss=9.9837
	step [94/192], loss=8.6869
	step [95/192], loss=8.9738
	step [96/192], loss=9.5756
	step [97/192], loss=8.8536
	step [98/192], loss=7.9343
	step [99/192], loss=9.6344
	step [100/192], loss=9.2540
	step [101/192], loss=10.2477
	step [102/192], loss=10.0027
	step [103/192], loss=8.1177
	step [104/192], loss=10.3057
	step [105/192], loss=8.6653
	step [106/192], loss=9.8220
	step [107/192], loss=8.2970
	step [108/192], loss=11.9044
	step [109/192], loss=8.6110
	step [110/192], loss=8.2329
	step [111/192], loss=7.6081
	step [112/192], loss=9.5830
	step [113/192], loss=9.3679
	step [114/192], loss=11.5891
	step [115/192], loss=8.9197
	step [116/192], loss=11.0973
	step [117/192], loss=9.3754
	step [118/192], loss=9.6405
	step [119/192], loss=11.6873
	step [120/192], loss=9.8290
	step [121/192], loss=9.5966
	step [122/192], loss=8.4207
	step [123/192], loss=10.3024
	step [124/192], loss=10.2329
	step [125/192], loss=10.0285
	step [126/192], loss=9.6002
	step [127/192], loss=8.8579
	step [128/192], loss=9.0757
	step [129/192], loss=7.7232
	step [130/192], loss=9.2020
	step [131/192], loss=8.1914
	step [132/192], loss=11.6597
	step [133/192], loss=9.7418
	step [134/192], loss=11.2875
	step [135/192], loss=10.5131
	step [136/192], loss=8.7238
	step [137/192], loss=8.7097
	step [138/192], loss=8.3803
	step [139/192], loss=12.4066
	step [140/192], loss=9.1880
	step [141/192], loss=9.4187
	step [142/192], loss=11.1040
	step [143/192], loss=8.7033
	step [144/192], loss=8.0948
	step [145/192], loss=9.4563
	step [146/192], loss=10.3846
	step [147/192], loss=11.3243
	step [148/192], loss=9.7939
	step [149/192], loss=10.3827
	step [150/192], loss=9.4309
	step [151/192], loss=11.1440
	step [152/192], loss=9.5705
	step [153/192], loss=10.9587
	step [154/192], loss=9.8756
	step [155/192], loss=8.7564
	step [156/192], loss=8.8138
	step [157/192], loss=8.0192
	step [158/192], loss=11.2945
	step [159/192], loss=11.2535
	step [160/192], loss=10.8695
	step [161/192], loss=9.3986
	step [162/192], loss=9.9820
	step [163/192], loss=8.7412
	step [164/192], loss=7.5383
	step [165/192], loss=9.9831
	step [166/192], loss=11.0173
	step [167/192], loss=8.3777
	step [168/192], loss=8.7011
	step [169/192], loss=9.0900
	step [170/192], loss=10.9554
	step [171/192], loss=7.2376
	step [172/192], loss=10.9633
	step [173/192], loss=9.5981
	step [174/192], loss=8.1355
	step [175/192], loss=8.6149
	step [176/192], loss=8.0946
	step [177/192], loss=10.6720
	step [178/192], loss=10.0533
	step [179/192], loss=9.3164
	step [180/192], loss=7.6291
	step [181/192], loss=10.1885
	step [182/192], loss=10.3192
	step [183/192], loss=9.5227
	step [184/192], loss=8.9594
	step [185/192], loss=10.6820
	step [186/192], loss=9.4771
	step [187/192], loss=11.1276
	step [188/192], loss=9.2125
	step [189/192], loss=9.9290
	step [190/192], loss=10.6782
	step [191/192], loss=7.3470
	step [192/192], loss=1.8169
	Evaluating
	loss=0.0267, precision=0.1699, recall=0.9953, f1=0.2902
Training epoch 27
	step [1/192], loss=10.4495
	step [2/192], loss=9.2990
	step [3/192], loss=9.8408
	step [4/192], loss=6.8443
	step [5/192], loss=9.5759
	step [6/192], loss=8.1462
	step [7/192], loss=12.7713
	step [8/192], loss=9.8495
	step [9/192], loss=9.3740
	step [10/192], loss=7.4730
	step [11/192], loss=9.0995
	step [12/192], loss=8.7564
	step [13/192], loss=10.4684
	step [14/192], loss=9.0322
	step [15/192], loss=9.4905
	step [16/192], loss=8.7003
	step [17/192], loss=9.9181
	step [18/192], loss=10.0140
	step [19/192], loss=8.3488
	step [20/192], loss=10.2020
	step [21/192], loss=7.6974
	step [22/192], loss=8.3439
	step [23/192], loss=9.2846
	step [24/192], loss=11.0699
	step [25/192], loss=11.3399
	step [26/192], loss=9.8412
	step [27/192], loss=9.6275
	step [28/192], loss=8.8972
	step [29/192], loss=9.5801
	step [30/192], loss=8.4475
	step [31/192], loss=9.4733
	step [32/192], loss=8.5480
	step [33/192], loss=8.9226
	step [34/192], loss=8.8107
	step [35/192], loss=8.4511
	step [36/192], loss=11.9197
	step [37/192], loss=11.5278
	step [38/192], loss=10.4655
	step [39/192], loss=9.5811
	step [40/192], loss=8.7004
	step [41/192], loss=9.7777
	step [42/192], loss=11.0171
	step [43/192], loss=10.5567
	step [44/192], loss=8.8384
	step [45/192], loss=11.6179
	step [46/192], loss=9.2599
	step [47/192], loss=8.8954
	step [48/192], loss=7.7804
	step [49/192], loss=10.8769
	step [50/192], loss=9.6397
	step [51/192], loss=8.9942
	step [52/192], loss=9.1048
	step [53/192], loss=9.4329
	step [54/192], loss=9.8610
	step [55/192], loss=10.3374
	step [56/192], loss=8.6310
	step [57/192], loss=11.2660
	step [58/192], loss=9.2992
	step [59/192], loss=9.3603
	step [60/192], loss=9.7916
	step [61/192], loss=10.6878
	step [62/192], loss=8.9171
	step [63/192], loss=10.1170
	step [64/192], loss=8.7401
	step [65/192], loss=8.3731
	step [66/192], loss=10.9281
	step [67/192], loss=10.6537
	step [68/192], loss=9.1848
	step [69/192], loss=10.7466
	step [70/192], loss=10.2313
	step [71/192], loss=10.3723
	step [72/192], loss=9.1474
	step [73/192], loss=10.5387
	step [74/192], loss=8.8070
	step [75/192], loss=9.4716
	step [76/192], loss=9.6485
	step [77/192], loss=8.7178
	step [78/192], loss=9.4012
	step [79/192], loss=11.0845
	step [80/192], loss=10.6017
	step [81/192], loss=8.2418
	step [82/192], loss=9.1019
	step [83/192], loss=8.6260
	step [84/192], loss=10.3541
	step [85/192], loss=9.6740
	step [86/192], loss=8.7593
	step [87/192], loss=8.7899
	step [88/192], loss=7.1480
	step [89/192], loss=11.7917
	step [90/192], loss=8.9313
	step [91/192], loss=8.8473
	step [92/192], loss=9.4184
	step [93/192], loss=8.7154
	step [94/192], loss=9.3350
	step [95/192], loss=8.5426
	step [96/192], loss=7.7181
	step [97/192], loss=9.2969
	step [98/192], loss=12.0501
	step [99/192], loss=8.1666
	step [100/192], loss=8.1256
	step [101/192], loss=8.4171
	step [102/192], loss=9.9894
	step [103/192], loss=8.2509
	step [104/192], loss=9.6470
	step [105/192], loss=10.5242
	step [106/192], loss=8.2006
	step [107/192], loss=9.1765
	step [108/192], loss=9.1852
	step [109/192], loss=9.5960
	step [110/192], loss=12.2684
	step [111/192], loss=10.2675
	step [112/192], loss=8.2339
	step [113/192], loss=8.4325
	step [114/192], loss=11.3317
	step [115/192], loss=9.1005
	step [116/192], loss=10.3272
	step [117/192], loss=8.7782
	step [118/192], loss=9.5994
	step [119/192], loss=7.8987
	step [120/192], loss=7.8869
	step [121/192], loss=8.5512
	step [122/192], loss=9.9344
	step [123/192], loss=10.2288
	step [124/192], loss=8.3118
	step [125/192], loss=9.7239
	step [126/192], loss=10.2259
	step [127/192], loss=9.7354
	step [128/192], loss=7.8781
	step [129/192], loss=9.8552
	step [130/192], loss=9.5060
	step [131/192], loss=7.5782
	step [132/192], loss=8.2544
	step [133/192], loss=9.0238
	step [134/192], loss=9.8895
	step [135/192], loss=7.6158
	step [136/192], loss=8.9993
	step [137/192], loss=10.4392
	step [138/192], loss=8.4630
	step [139/192], loss=9.9352
	step [140/192], loss=8.8547
	step [141/192], loss=8.9092
	step [142/192], loss=7.7144
	step [143/192], loss=9.1744
	step [144/192], loss=10.3386
	step [145/192], loss=10.5636
	step [146/192], loss=8.3483
	step [147/192], loss=8.7679
	step [148/192], loss=9.7754
	step [149/192], loss=8.3445
	step [150/192], loss=9.5164
	step [151/192], loss=8.2680
	step [152/192], loss=9.2524
	step [153/192], loss=8.1623
	step [154/192], loss=8.8820
	step [155/192], loss=9.3897
	step [156/192], loss=10.3525
	step [157/192], loss=8.3916
	step [158/192], loss=8.4197
	step [159/192], loss=6.8760
	step [160/192], loss=7.7930
	step [161/192], loss=8.4107
	step [162/192], loss=11.8175
	step [163/192], loss=11.2434
	step [164/192], loss=8.7193
	step [165/192], loss=9.3225
	step [166/192], loss=8.2453
	step [167/192], loss=9.7617
	step [168/192], loss=9.9720
	step [169/192], loss=9.9596
	step [170/192], loss=10.5105
	step [171/192], loss=7.5682
	step [172/192], loss=9.0957
	step [173/192], loss=8.7813
	step [174/192], loss=7.1909
	step [175/192], loss=7.6103
	step [176/192], loss=11.5099
	step [177/192], loss=9.8444
	step [178/192], loss=10.3583
	step [179/192], loss=8.9140
	step [180/192], loss=9.6174
	step [181/192], loss=8.7851
	step [182/192], loss=9.5993
	step [183/192], loss=7.8398
	step [184/192], loss=10.6532
	step [185/192], loss=7.5480
	step [186/192], loss=8.6125
	step [187/192], loss=10.0671
	step [188/192], loss=8.7043
	step [189/192], loss=9.5134
	step [190/192], loss=10.6442
	step [191/192], loss=9.5643
	step [192/192], loss=1.1667
	Evaluating
	loss=0.0257, precision=0.1705, recall=0.9939, f1=0.2911
Training epoch 28
	step [1/192], loss=11.2547
	step [2/192], loss=8.8420
	step [3/192], loss=10.5215
	step [4/192], loss=8.7974
	step [5/192], loss=7.0681
	step [6/192], loss=11.6393
	step [7/192], loss=9.4570
	step [8/192], loss=9.7578
	step [9/192], loss=11.0463
	step [10/192], loss=9.9271
	step [11/192], loss=7.5371
	step [12/192], loss=9.0438
	step [13/192], loss=10.3247
	step [14/192], loss=8.5491
	step [15/192], loss=8.7489
	step [16/192], loss=8.5292
	step [17/192], loss=9.4217
	step [18/192], loss=9.8896
	step [19/192], loss=6.9390
	step [20/192], loss=8.7658
	step [21/192], loss=8.9236
	step [22/192], loss=9.0145
	step [23/192], loss=10.1186
	step [24/192], loss=8.4622
	step [25/192], loss=9.7533
	step [26/192], loss=9.7991
	step [27/192], loss=11.2135
	step [28/192], loss=9.4887
	step [29/192], loss=8.8654
	step [30/192], loss=9.0251
	step [31/192], loss=11.5165
	step [32/192], loss=7.0321
	step [33/192], loss=7.2296
	step [34/192], loss=10.3077
	step [35/192], loss=11.8462
	step [36/192], loss=8.2332
	step [37/192], loss=7.0698
	step [38/192], loss=7.9157
	step [39/192], loss=9.8201
	step [40/192], loss=8.6357
	step [41/192], loss=9.5886
	step [42/192], loss=9.1479
	step [43/192], loss=7.8060
	step [44/192], loss=9.6485
	step [45/192], loss=7.3884
	step [46/192], loss=9.9651
	step [47/192], loss=7.4942
	step [48/192], loss=10.6447
	step [49/192], loss=7.3898
	step [50/192], loss=9.0905
	step [51/192], loss=12.0114
	step [52/192], loss=9.4221
	step [53/192], loss=10.5637
	step [54/192], loss=9.5320
	step [55/192], loss=8.6080
	step [56/192], loss=8.9694
	step [57/192], loss=8.3871
	step [58/192], loss=9.9964
	step [59/192], loss=8.6363
	step [60/192], loss=9.1763
	step [61/192], loss=12.2793
	step [62/192], loss=11.0676
	step [63/192], loss=7.3739
	step [64/192], loss=9.8074
	step [65/192], loss=9.9241
	step [66/192], loss=9.7051
	step [67/192], loss=9.9029
	step [68/192], loss=9.5580
	step [69/192], loss=8.9757
	step [70/192], loss=9.6458
	step [71/192], loss=8.2627
	step [72/192], loss=9.5299
	step [73/192], loss=10.3801
	step [74/192], loss=11.4810
	step [75/192], loss=9.1569
	step [76/192], loss=9.4555
	step [77/192], loss=6.8632
	step [78/192], loss=7.9343
	step [79/192], loss=8.0026
	step [80/192], loss=6.9751
	step [81/192], loss=8.6793
	step [82/192], loss=10.2512
	step [83/192], loss=7.1080
	step [84/192], loss=7.7243
	step [85/192], loss=7.3842
	step [86/192], loss=10.8682
	step [87/192], loss=8.7026
	step [88/192], loss=7.6604
	step [89/192], loss=11.1857
	step [90/192], loss=8.6341
	step [91/192], loss=8.8670
	step [92/192], loss=9.2712
	step [93/192], loss=9.0777
	step [94/192], loss=9.3104
	step [95/192], loss=9.6798
	step [96/192], loss=8.3993
	step [97/192], loss=8.0145
	step [98/192], loss=8.2538
	step [99/192], loss=8.1143
	step [100/192], loss=7.0382
	step [101/192], loss=6.6033
	step [102/192], loss=8.6583
	step [103/192], loss=9.1690
	step [104/192], loss=9.0197
	step [105/192], loss=10.3979
	step [106/192], loss=9.3578
	step [107/192], loss=9.5488
	step [108/192], loss=9.8590
	step [109/192], loss=8.9602
	step [110/192], loss=11.2752
	step [111/192], loss=9.4069
	step [112/192], loss=8.7427
	step [113/192], loss=8.8795
	step [114/192], loss=10.3569
	step [115/192], loss=9.9650
	step [116/192], loss=10.9946
	step [117/192], loss=8.0495
	step [118/192], loss=10.6929
	step [119/192], loss=9.8374
	step [120/192], loss=8.2370
	step [121/192], loss=10.1982
	step [122/192], loss=8.8841
	step [123/192], loss=10.8708
	step [124/192], loss=10.2326
	step [125/192], loss=8.6333
	step [126/192], loss=9.0769
	step [127/192], loss=10.0563
	step [128/192], loss=9.8218
	step [129/192], loss=10.2972
	step [130/192], loss=10.9182
	step [131/192], loss=10.1717
	step [132/192], loss=8.1797
	step [133/192], loss=8.7219
	step [134/192], loss=10.0545
	step [135/192], loss=11.4256
	step [136/192], loss=10.4205
	step [137/192], loss=9.3393
	step [138/192], loss=8.5344
	step [139/192], loss=7.4849
	step [140/192], loss=9.5464
	step [141/192], loss=9.4223
	step [142/192], loss=11.4659
	step [143/192], loss=8.8838
	step [144/192], loss=9.4564
	step [145/192], loss=11.4563
	step [146/192], loss=6.9170
	step [147/192], loss=9.5871
	step [148/192], loss=8.8719
	step [149/192], loss=6.8269
	step [150/192], loss=9.4554
	step [151/192], loss=9.8941
	step [152/192], loss=7.9677
	step [153/192], loss=10.2981
	step [154/192], loss=8.9145
	step [155/192], loss=8.3199
	step [156/192], loss=8.5385
	step [157/192], loss=9.7233
	step [158/192], loss=9.6137
	step [159/192], loss=10.2589
	step [160/192], loss=8.9889
	step [161/192], loss=9.2836
	step [162/192], loss=9.9711
	step [163/192], loss=9.6530
	step [164/192], loss=9.3803
	step [165/192], loss=8.7904
	step [166/192], loss=8.1361
	step [167/192], loss=9.3927
	step [168/192], loss=9.6272
	step [169/192], loss=7.9407
	step [170/192], loss=8.8332
	step [171/192], loss=9.7334
	step [172/192], loss=11.2060
	step [173/192], loss=10.2328
	step [174/192], loss=10.1445
	step [175/192], loss=7.8402
	step [176/192], loss=8.0107
	step [177/192], loss=6.4467
	step [178/192], loss=7.8685
	step [179/192], loss=8.6504
	step [180/192], loss=10.2177
	step [181/192], loss=11.0053
	step [182/192], loss=9.1107
	step [183/192], loss=10.9124
	step [184/192], loss=8.2571
	step [185/192], loss=9.0144
	step [186/192], loss=11.4355
	step [187/192], loss=8.5099
	step [188/192], loss=8.9689
	step [189/192], loss=7.8989
	step [190/192], loss=9.2714
	step [191/192], loss=9.3003
	step [192/192], loss=1.1799
	Evaluating
	loss=0.0275, precision=0.1587, recall=0.9955, f1=0.2738
Training epoch 29
	step [1/192], loss=10.7902
	step [2/192], loss=8.5969
	step [3/192], loss=10.9889
	step [4/192], loss=9.8351
	step [5/192], loss=8.1016
	step [6/192], loss=9.3116
	step [7/192], loss=9.1827
	step [8/192], loss=7.5973
	step [9/192], loss=8.1494
	step [10/192], loss=8.8089
	step [11/192], loss=9.1257
	step [12/192], loss=9.5672
	step [13/192], loss=9.1848
	step [14/192], loss=8.9436
	step [15/192], loss=9.6725
	step [16/192], loss=9.0256
	step [17/192], loss=8.7651
	step [18/192], loss=7.4226
	step [19/192], loss=8.0332
	step [20/192], loss=9.5295
	step [21/192], loss=8.5439
	step [22/192], loss=8.7768
	step [23/192], loss=10.6743
	step [24/192], loss=8.7631
	step [25/192], loss=9.6858
	step [26/192], loss=7.2160
	step [27/192], loss=8.7372
	step [28/192], loss=7.7409
	step [29/192], loss=8.4826
	step [30/192], loss=8.5616
	step [31/192], loss=7.1409
	step [32/192], loss=8.3773
	step [33/192], loss=9.0609
	step [34/192], loss=8.6199
	step [35/192], loss=9.7503
	step [36/192], loss=7.4253
	step [37/192], loss=8.1864
	step [38/192], loss=10.6277
	step [39/192], loss=8.9323
	step [40/192], loss=6.9856
	step [41/192], loss=8.0861
	step [42/192], loss=6.5335
	step [43/192], loss=11.5750
	step [44/192], loss=9.8811
	step [45/192], loss=8.1943
	step [46/192], loss=8.5657
	step [47/192], loss=9.8674
	step [48/192], loss=10.5635
	step [49/192], loss=7.2998
	step [50/192], loss=8.5270
	step [51/192], loss=9.5444
	step [52/192], loss=8.9055
	step [53/192], loss=9.4794
	step [54/192], loss=9.8333
	step [55/192], loss=10.0768
	step [56/192], loss=8.8003
	step [57/192], loss=8.1135
	step [58/192], loss=10.0700
	step [59/192], loss=7.9569
	step [60/192], loss=8.7395
	step [61/192], loss=8.7512
	step [62/192], loss=9.2825
	step [63/192], loss=9.2985
	step [64/192], loss=9.3122
	step [65/192], loss=8.5035
	step [66/192], loss=9.9195
	step [67/192], loss=8.3632
	step [68/192], loss=9.3693
	step [69/192], loss=8.6689
	step [70/192], loss=8.4545
	step [71/192], loss=9.0159
	step [72/192], loss=8.0083
	step [73/192], loss=10.6103
	step [74/192], loss=9.2757
	step [75/192], loss=9.7211
	step [76/192], loss=8.9995
	step [77/192], loss=6.8737
	step [78/192], loss=7.5221
	step [79/192], loss=7.9826
	step [80/192], loss=8.8256
	step [81/192], loss=6.5980
	step [82/192], loss=9.6774
	step [83/192], loss=11.1083
	step [84/192], loss=8.4993
	step [85/192], loss=8.6529
	step [86/192], loss=8.4322
	step [87/192], loss=10.0854
	step [88/192], loss=8.7385
	step [89/192], loss=8.8013
	step [90/192], loss=9.8365
	step [91/192], loss=9.3108
	step [92/192], loss=8.2942
	step [93/192], loss=7.3282
	step [94/192], loss=8.6670
	step [95/192], loss=9.0106
	step [96/192], loss=7.1961
	step [97/192], loss=9.8974
	step [98/192], loss=6.9119
	step [99/192], loss=8.0899
	step [100/192], loss=9.1571
	step [101/192], loss=8.5339
	step [102/192], loss=9.6537
	step [103/192], loss=10.1358
	step [104/192], loss=8.2361
	step [105/192], loss=7.8637
	step [106/192], loss=9.3149
	step [107/192], loss=7.7553
	step [108/192], loss=9.3932
	step [109/192], loss=7.6812
	step [110/192], loss=9.4889
	step [111/192], loss=10.9332
	step [112/192], loss=9.5583
	step [113/192], loss=9.4690
	step [114/192], loss=8.3225
	step [115/192], loss=9.3315
	step [116/192], loss=9.2189
	step [117/192], loss=9.9750
	step [118/192], loss=7.9324
	step [119/192], loss=9.2895
	step [120/192], loss=9.0476
	step [121/192], loss=9.8227
	step [122/192], loss=7.4410
	step [123/192], loss=8.3378
	step [124/192], loss=9.8647
	step [125/192], loss=8.2416
	step [126/192], loss=8.9605
	step [127/192], loss=8.4876
	step [128/192], loss=7.2207
	step [129/192], loss=8.2760
	step [130/192], loss=11.7382
	step [131/192], loss=10.6453
	step [132/192], loss=8.5772
	step [133/192], loss=8.1725
	step [134/192], loss=8.8366
	step [135/192], loss=9.6604
	step [136/192], loss=7.8858
	step [137/192], loss=7.2801
	step [138/192], loss=9.3653
	step [139/192], loss=8.3651
	step [140/192], loss=8.3926
	step [141/192], loss=7.4410
	step [142/192], loss=9.1599
	step [143/192], loss=8.0801
	step [144/192], loss=9.8587
	step [145/192], loss=10.0580
	step [146/192], loss=9.9211
	step [147/192], loss=7.8315
	step [148/192], loss=9.8467
	step [149/192], loss=9.3528
	step [150/192], loss=8.0515
	step [151/192], loss=9.7108
	step [152/192], loss=9.7082
	step [153/192], loss=9.5971
	step [154/192], loss=9.5728
	step [155/192], loss=9.1545
	step [156/192], loss=8.4178
	step [157/192], loss=10.9802
	step [158/192], loss=9.5630
	step [159/192], loss=9.0539
	step [160/192], loss=7.9413
	step [161/192], loss=9.4505
	step [162/192], loss=9.5439
	step [163/192], loss=9.5026
	step [164/192], loss=8.9880
	step [165/192], loss=9.6965
	step [166/192], loss=9.2703
	step [167/192], loss=10.8309
	step [168/192], loss=8.2798
	step [169/192], loss=8.1628
	step [170/192], loss=6.5958
	step [171/192], loss=9.4553
	step [172/192], loss=7.6595
	step [173/192], loss=9.6508
	step [174/192], loss=8.6106
	step [175/192], loss=10.5732
	step [176/192], loss=8.2485
	step [177/192], loss=10.3835
	step [178/192], loss=7.3556
	step [179/192], loss=8.8860
	step [180/192], loss=9.2650
	step [181/192], loss=9.3764
	step [182/192], loss=10.2004
	step [183/192], loss=10.4116
	step [184/192], loss=8.2652
	step [185/192], loss=9.5549
	step [186/192], loss=9.6247
	step [187/192], loss=9.4929
	step [188/192], loss=9.8938
	step [189/192], loss=10.3669
	step [190/192], loss=9.1364
	step [191/192], loss=11.4941
	step [192/192], loss=0.7992
	Evaluating
	loss=0.0249, precision=0.1753, recall=0.9938, f1=0.2980
Training epoch 30
	step [1/192], loss=8.7300
	step [2/192], loss=10.3326
	step [3/192], loss=7.5793
	step [4/192], loss=7.3931
	step [5/192], loss=11.3893
	step [6/192], loss=7.8307
	step [7/192], loss=8.5452
	step [8/192], loss=7.9288
	step [9/192], loss=10.3483
	step [10/192], loss=9.7168
	step [11/192], loss=8.7890
	step [12/192], loss=9.5078
	step [13/192], loss=8.4965
	step [14/192], loss=8.3003
	step [15/192], loss=8.9417
	step [16/192], loss=8.7289
	step [17/192], loss=9.0384
	step [18/192], loss=9.2379
	step [19/192], loss=7.4462
	step [20/192], loss=8.4606
	step [21/192], loss=7.9947
	step [22/192], loss=9.3640
	step [23/192], loss=10.9407
	step [24/192], loss=7.6352
	step [25/192], loss=9.8642
	step [26/192], loss=8.6138
	step [27/192], loss=9.4619
	step [28/192], loss=8.1425
	step [29/192], loss=8.1521
	step [30/192], loss=9.0400
	step [31/192], loss=7.7308
	step [32/192], loss=10.7579
	step [33/192], loss=11.8775
	step [34/192], loss=8.8299
	step [35/192], loss=8.7439
	step [36/192], loss=8.2153
	step [37/192], loss=8.9354
	step [38/192], loss=10.1552
	step [39/192], loss=9.5666
	step [40/192], loss=8.1629
	step [41/192], loss=9.9275
	step [42/192], loss=7.3030
	step [43/192], loss=12.2235
	step [44/192], loss=9.1073
	step [45/192], loss=10.1503
	step [46/192], loss=8.1372
	step [47/192], loss=10.4245
	step [48/192], loss=8.4093
	step [49/192], loss=7.8409
	step [50/192], loss=9.7365
	step [51/192], loss=11.0025
	step [52/192], loss=7.5569
	step [53/192], loss=9.5095
	step [54/192], loss=7.8659
	step [55/192], loss=8.2421
	step [56/192], loss=8.9929
	step [57/192], loss=8.5822
	step [58/192], loss=11.8591
	step [59/192], loss=8.3538
	step [60/192], loss=10.4218
	step [61/192], loss=9.8100
	step [62/192], loss=8.9018
	step [63/192], loss=7.3679
	step [64/192], loss=7.5730
	step [65/192], loss=7.9276
	step [66/192], loss=10.2404
	step [67/192], loss=9.3458
	step [68/192], loss=10.4219
	step [69/192], loss=8.3469
	step [70/192], loss=8.6819
	step [71/192], loss=9.1244
	step [72/192], loss=8.6674
	step [73/192], loss=8.1469
	step [74/192], loss=10.1243
	step [75/192], loss=9.9120
	step [76/192], loss=8.5006
	step [77/192], loss=10.1250
	step [78/192], loss=8.6006
	step [79/192], loss=9.4879
	step [80/192], loss=7.9611
	step [81/192], loss=7.0024
	step [82/192], loss=11.8820
	step [83/192], loss=9.1762
	step [84/192], loss=8.9625
	step [85/192], loss=9.3131
	step [86/192], loss=7.6636
	step [87/192], loss=8.3175
	step [88/192], loss=8.8806
	step [89/192], loss=8.8908
	step [90/192], loss=7.5840
	step [91/192], loss=11.0669
	step [92/192], loss=7.8566
	step [93/192], loss=8.4741
	step [94/192], loss=8.0067
	step [95/192], loss=9.1192
	step [96/192], loss=7.3950
	step [97/192], loss=8.5982
	step [98/192], loss=9.9960
	step [99/192], loss=9.0064
	step [100/192], loss=7.4544
	step [101/192], loss=9.2791
	step [102/192], loss=10.1147
	step [103/192], loss=9.3505
	step [104/192], loss=8.3847
	step [105/192], loss=10.3167
	step [106/192], loss=7.1004
	step [107/192], loss=8.0671
	step [108/192], loss=9.6100
	step [109/192], loss=7.7307
	step [110/192], loss=7.4696
	step [111/192], loss=9.2606
	step [112/192], loss=8.4182
	step [113/192], loss=8.0880
	step [114/192], loss=8.8231
	step [115/192], loss=9.4196
	step [116/192], loss=8.5076
	step [117/192], loss=9.6165
	step [118/192], loss=8.1626
	step [119/192], loss=8.1752
	step [120/192], loss=9.9092
	step [121/192], loss=7.3213
	step [122/192], loss=7.5978
	step [123/192], loss=8.6483
	step [124/192], loss=7.8371
	step [125/192], loss=7.8240
	step [126/192], loss=10.0607
	step [127/192], loss=10.9004
	step [128/192], loss=7.1439
	step [129/192], loss=8.6600
	step [130/192], loss=8.6999
	step [131/192], loss=8.8820
	step [132/192], loss=7.8212
	step [133/192], loss=10.3527
	step [134/192], loss=7.9542
	step [135/192], loss=8.0799
	step [136/192], loss=11.1295
	step [137/192], loss=11.1846
	step [138/192], loss=9.8253
	step [139/192], loss=10.3671
	step [140/192], loss=9.8345
	step [141/192], loss=8.3165
	step [142/192], loss=8.4159
	step [143/192], loss=11.0018
	step [144/192], loss=8.9034
	step [145/192], loss=12.1463
	step [146/192], loss=9.3848
	step [147/192], loss=8.1736
	step [148/192], loss=7.8897
	step [149/192], loss=8.2166
	step [150/192], loss=10.7002
	step [151/192], loss=8.7274
	step [152/192], loss=10.6126
	step [153/192], loss=8.9780
	step [154/192], loss=10.0507
	step [155/192], loss=9.2585
	step [156/192], loss=9.9455
	step [157/192], loss=8.1537
	step [158/192], loss=7.7905
	step [159/192], loss=9.4448
	step [160/192], loss=7.9616
	step [161/192], loss=8.8172
	step [162/192], loss=8.1770
	step [163/192], loss=7.6318
	step [164/192], loss=10.5548
	step [165/192], loss=5.8507
	step [166/192], loss=8.6811
	step [167/192], loss=8.2581
	step [168/192], loss=7.8256
	step [169/192], loss=7.4961
	step [170/192], loss=9.0942
	step [171/192], loss=7.8674
	step [172/192], loss=9.6017
	step [173/192], loss=9.0725
	step [174/192], loss=8.1399
	step [175/192], loss=8.7052
	step [176/192], loss=7.8812
	step [177/192], loss=7.8043
	step [178/192], loss=8.0115
	step [179/192], loss=8.3962
	step [180/192], loss=8.2212
	step [181/192], loss=10.9042
	step [182/192], loss=8.4723
	step [183/192], loss=9.0374
	step [184/192], loss=8.8184
	step [185/192], loss=8.2881
	step [186/192], loss=8.9802
	step [187/192], loss=7.5239
	step [188/192], loss=8.9292
	step [189/192], loss=8.2382
	step [190/192], loss=8.4671
	step [191/192], loss=8.4759
	step [192/192], loss=0.8607
	Evaluating
	loss=0.0197, precision=0.2020, recall=0.9928, f1=0.3357
Training finished
best_f1: 0.37730740432491683
directing: X rim_enhanced: False test_id 1
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9175 # image files with weight 9141
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2707 # image files with weight 2691
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9141
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/143], loss=633.2269
	step [2/143], loss=392.4108
	step [3/143], loss=254.5191
	step [4/143], loss=201.8380
	step [5/143], loss=162.9503
	step [6/143], loss=156.6348
	step [7/143], loss=146.6424
	step [8/143], loss=142.1616
	step [9/143], loss=141.6994
	step [10/143], loss=136.4103
	step [11/143], loss=135.9771
	step [12/143], loss=133.7328
	step [13/143], loss=132.8651
	step [14/143], loss=132.8123
	step [15/143], loss=131.8821
	step [16/143], loss=131.2820
	step [17/143], loss=130.0172
	step [18/143], loss=129.7879
	step [19/143], loss=128.2894
	step [20/143], loss=128.2132
	step [21/143], loss=126.8347
	step [22/143], loss=124.0118
	step [23/143], loss=122.5765
	step [24/143], loss=121.3553
	step [25/143], loss=121.7616
	step [26/143], loss=120.0467
	step [27/143], loss=119.2627
	step [28/143], loss=117.3802
	step [29/143], loss=116.7929
	step [30/143], loss=116.7570
	step [31/143], loss=114.7466
	step [32/143], loss=113.7160
	step [33/143], loss=112.8080
	step [34/143], loss=111.9193
	step [35/143], loss=110.7995
	step [36/143], loss=111.1562
	step [37/143], loss=110.9627
	step [38/143], loss=106.6806
	step [39/143], loss=107.9673
	step [40/143], loss=105.8926
	step [41/143], loss=108.5445
	step [42/143], loss=104.6628
	step [43/143], loss=105.2863
	step [44/143], loss=103.5515
	step [45/143], loss=104.6738
	step [46/143], loss=103.8920
	step [47/143], loss=102.5336
	step [48/143], loss=101.9847
	step [49/143], loss=101.6228
	step [50/143], loss=98.2185
	step [51/143], loss=98.8691
	step [52/143], loss=97.7636
	step [53/143], loss=98.2226
	step [54/143], loss=95.6491
	step [55/143], loss=95.8648
	step [56/143], loss=95.7338
	step [57/143], loss=95.5971
	step [58/143], loss=95.9594
	step [59/143], loss=95.0872
	step [60/143], loss=94.6187
	step [61/143], loss=91.9995
	step [62/143], loss=94.0603
	step [63/143], loss=91.0218
	step [64/143], loss=91.6991
	step [65/143], loss=92.0519
	step [66/143], loss=92.7930
	step [67/143], loss=92.0445
	step [68/143], loss=91.0401
	step [69/143], loss=89.6968
	step [70/143], loss=89.9354
	step [71/143], loss=90.8604
	step [72/143], loss=87.0947
	step [73/143], loss=89.4057
	step [74/143], loss=88.6160
	step [75/143], loss=86.1548
	step [76/143], loss=86.2057
	step [77/143], loss=87.4543
	step [78/143], loss=87.4020
	step [79/143], loss=85.3876
	step [80/143], loss=86.4323
	step [81/143], loss=84.2213
	step [82/143], loss=85.7356
	step [83/143], loss=86.8299
	step [84/143], loss=83.8925
	step [85/143], loss=84.7847
	step [86/143], loss=84.1318
	step [87/143], loss=83.4585
	step [88/143], loss=84.6969
	step [89/143], loss=84.4769
	step [90/143], loss=81.9849
	step [91/143], loss=84.3643
	step [92/143], loss=83.8400
	step [93/143], loss=84.2765
	step [94/143], loss=81.4267
	step [95/143], loss=81.4522
	step [96/143], loss=78.6538
	step [97/143], loss=83.3171
	step [98/143], loss=80.4400
	step [99/143], loss=80.7126
	step [100/143], loss=80.9022
	step [101/143], loss=80.2720
	step [102/143], loss=79.9717
	step [103/143], loss=79.7842
	step [104/143], loss=81.0342
	step [105/143], loss=78.7158
	step [106/143], loss=78.3706
	step [107/143], loss=78.6189
	step [108/143], loss=79.5199
	step [109/143], loss=76.3941
	step [110/143], loss=81.4009
	step [111/143], loss=78.6846
	step [112/143], loss=77.4758
	step [113/143], loss=78.2264
	step [114/143], loss=77.5115
	step [115/143], loss=75.9169
	step [116/143], loss=76.1301
	step [117/143], loss=77.1698
	step [118/143], loss=75.5935
	step [119/143], loss=76.4099
	step [120/143], loss=74.4260
	step [121/143], loss=74.7918
	step [122/143], loss=75.1576
	step [123/143], loss=75.4857
	step [124/143], loss=73.2413
	step [125/143], loss=77.0559
	step [126/143], loss=75.8612
	step [127/143], loss=72.8951
	step [128/143], loss=73.8583
	step [129/143], loss=72.4871
	step [130/143], loss=74.5913
	step [131/143], loss=74.6300
	step [132/143], loss=73.9009
	step [133/143], loss=71.9199
	step [134/143], loss=72.5905
	step [135/143], loss=71.5200
	step [136/143], loss=73.1061
	step [137/143], loss=72.4467
	step [138/143], loss=74.0147
	step [139/143], loss=70.7211
	step [140/143], loss=73.1264
	step [141/143], loss=72.3920
	step [142/143], loss=70.9772
	step [143/143], loss=60.2071
	Evaluating
	loss=0.2572, precision=0.1981, recall=0.9939, f1=0.3304
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/143], loss=72.7380
	step [2/143], loss=71.2098
	step [3/143], loss=69.7006
	step [4/143], loss=69.6379
	step [5/143], loss=69.9108
	step [6/143], loss=71.4920
	step [7/143], loss=70.1341
	step [8/143], loss=69.6102
	step [9/143], loss=69.5019
	step [10/143], loss=67.7667
	step [11/143], loss=70.8867
	step [12/143], loss=70.3801
	step [13/143], loss=68.4523
	step [14/143], loss=70.1176
	step [15/143], loss=69.3909
	step [16/143], loss=70.9930
	step [17/143], loss=67.3171
	step [18/143], loss=67.0644
	step [19/143], loss=69.6621
	step [20/143], loss=69.3846
	step [21/143], loss=69.3761
	step [22/143], loss=67.8509
	step [23/143], loss=66.5260
	step [24/143], loss=69.0151
	step [25/143], loss=67.3060
	step [26/143], loss=69.0377
	step [27/143], loss=67.4609
	step [28/143], loss=66.5771
	step [29/143], loss=68.8964
	step [30/143], loss=66.6400
	step [31/143], loss=65.2024
	step [32/143], loss=65.7951
	step [33/143], loss=65.4728
	step [34/143], loss=66.4954
	step [35/143], loss=67.3650
	step [36/143], loss=66.7820
	step [37/143], loss=65.8009
	step [38/143], loss=65.4766
	step [39/143], loss=65.2628
	step [40/143], loss=65.4844
	step [41/143], loss=65.7281
	step [42/143], loss=66.1335
	step [43/143], loss=66.1788
	step [44/143], loss=64.8101
	step [45/143], loss=64.1488
	step [46/143], loss=65.5014
	step [47/143], loss=64.9180
	step [48/143], loss=65.2384
	step [49/143], loss=64.2375
	step [50/143], loss=62.6222
	step [51/143], loss=65.2597
	step [52/143], loss=65.2514
	step [53/143], loss=63.7993
	step [54/143], loss=63.9816
	step [55/143], loss=64.0318
	step [56/143], loss=62.9261
	step [57/143], loss=63.3231
	step [58/143], loss=64.2248
	step [59/143], loss=63.1597
	step [60/143], loss=62.3733
	step [61/143], loss=61.8789
	step [62/143], loss=61.6920
	step [63/143], loss=65.4969
	step [64/143], loss=63.3967
	step [65/143], loss=61.1572
	step [66/143], loss=62.8767
	step [67/143], loss=62.3493
	step [68/143], loss=61.3526
	step [69/143], loss=62.2729
	step [70/143], loss=63.0264
	step [71/143], loss=64.0172
	step [72/143], loss=61.1939
	step [73/143], loss=64.3556
	step [74/143], loss=61.2189
	step [75/143], loss=60.7440
	step [76/143], loss=61.8143
	step [77/143], loss=60.9343
	step [78/143], loss=61.5547
	step [79/143], loss=60.2105
	step [80/143], loss=59.7364
	step [81/143], loss=60.7001
	step [82/143], loss=60.9575
	step [83/143], loss=60.3581
	step [84/143], loss=59.7878
	step [85/143], loss=60.5717
	step [86/143], loss=60.9615
	step [87/143], loss=61.8746
	step [88/143], loss=62.1353
	step [89/143], loss=61.9412
	step [90/143], loss=60.8023
	step [91/143], loss=58.4114
	step [92/143], loss=60.3229
	step [93/143], loss=60.1288
	step [94/143], loss=60.6784
	step [95/143], loss=60.0272
	step [96/143], loss=59.4540
	step [97/143], loss=58.4865
	step [98/143], loss=58.9182
	step [99/143], loss=57.7145
	step [100/143], loss=60.1795
	step [101/143], loss=58.7775
	step [102/143], loss=56.6968
	step [103/143], loss=59.9234
	step [104/143], loss=58.0729
	step [105/143], loss=58.1572
	step [106/143], loss=57.2245
	step [107/143], loss=58.4173
	step [108/143], loss=59.9155
	step [109/143], loss=56.6954
	step [110/143], loss=58.0334
	step [111/143], loss=58.8356
	step [112/143], loss=58.3759
	step [113/143], loss=57.5744
	step [114/143], loss=56.5965
	step [115/143], loss=57.5107
	step [116/143], loss=57.0245
	step [117/143], loss=58.3820
	step [118/143], loss=58.2686
	step [119/143], loss=57.8389
	step [120/143], loss=56.4635
	step [121/143], loss=55.9985
	step [122/143], loss=58.1501
	step [123/143], loss=56.6196
	step [124/143], loss=57.2644
	step [125/143], loss=56.4828
	step [126/143], loss=57.8360
	step [127/143], loss=56.5739
	step [128/143], loss=55.5769
	step [129/143], loss=57.7901
	step [130/143], loss=54.9324
	step [131/143], loss=57.7684
	step [132/143], loss=55.8348
	step [133/143], loss=57.6286
	step [134/143], loss=54.9043
	step [135/143], loss=56.2345
	step [136/143], loss=56.3056
	step [137/143], loss=56.1347
	step [138/143], loss=54.7135
	step [139/143], loss=56.2504
	step [140/143], loss=56.2951
	step [141/143], loss=55.1074
	step [142/143], loss=55.5035
	step [143/143], loss=46.2437
	Evaluating
	loss=0.2033, precision=0.1812, recall=0.9955, f1=0.3065
Training epoch 3
	step [1/143], loss=55.3082
	step [2/143], loss=56.7487
	step [3/143], loss=55.4147
	step [4/143], loss=56.1076
	step [5/143], loss=55.1013
	step [6/143], loss=56.0976
	step [7/143], loss=54.2511
	step [8/143], loss=55.4085
	step [9/143], loss=53.3290
	step [10/143], loss=53.3129
	step [11/143], loss=54.8910
	step [12/143], loss=53.8489
	step [13/143], loss=54.9218
	step [14/143], loss=52.9485
	step [15/143], loss=53.5469
	step [16/143], loss=57.7878
	step [17/143], loss=53.0979
	step [18/143], loss=56.2281
	step [19/143], loss=53.6822
	step [20/143], loss=52.1653
	step [21/143], loss=53.3987
	step [22/143], loss=53.1277
	step [23/143], loss=54.8252
	step [24/143], loss=52.8635
	step [25/143], loss=53.9045
	step [26/143], loss=51.6750
	step [27/143], loss=52.6853
	step [28/143], loss=51.1768
	step [29/143], loss=52.5742
	step [30/143], loss=52.8057
	step [31/143], loss=51.7402
	step [32/143], loss=53.8132
	step [33/143], loss=51.7596
	step [34/143], loss=53.5600
	step [35/143], loss=52.2096
	step [36/143], loss=51.6381
	step [37/143], loss=50.8079
	step [38/143], loss=51.5219
	step [39/143], loss=53.3792
	step [40/143], loss=53.7290
	step [41/143], loss=50.3897
	step [42/143], loss=52.1579
	step [43/143], loss=52.9500
	step [44/143], loss=51.6854
	step [45/143], loss=51.8214
	step [46/143], loss=50.8741
	step [47/143], loss=51.5784
	step [48/143], loss=54.3987
	step [49/143], loss=55.8562
	step [50/143], loss=51.7388
	step [51/143], loss=51.7544
	step [52/143], loss=51.8418
	step [53/143], loss=50.7749
	step [54/143], loss=50.9569
	step [55/143], loss=51.5695
	step [56/143], loss=49.8611
	step [57/143], loss=52.3503
	step [58/143], loss=49.9359
	step [59/143], loss=49.9884
	step [60/143], loss=50.4741
	step [61/143], loss=51.7606
	step [62/143], loss=50.6790
	step [63/143], loss=50.1127
	step [64/143], loss=51.8797
	step [65/143], loss=48.9508
	step [66/143], loss=50.8657
	step [67/143], loss=50.0750
	step [68/143], loss=49.0936
	step [69/143], loss=50.2812
	step [70/143], loss=50.3608
	step [71/143], loss=49.8152
	step [72/143], loss=50.6482
	step [73/143], loss=53.0069
	step [74/143], loss=50.0866
	step [75/143], loss=50.6265
	step [76/143], loss=50.0430
	step [77/143], loss=49.6750
	step [78/143], loss=49.2348
	step [79/143], loss=49.6391
	step [80/143], loss=48.4990
	step [81/143], loss=50.6863
	step [82/143], loss=49.5788
	step [83/143], loss=49.8137
	step [84/143], loss=48.6523
	step [85/143], loss=49.4440
	step [86/143], loss=50.6872
	step [87/143], loss=48.6407
	step [88/143], loss=48.7670
	step [89/143], loss=48.8079
	step [90/143], loss=48.5662
	step [91/143], loss=48.9719
	step [92/143], loss=51.0734
	step [93/143], loss=48.7284
	step [94/143], loss=49.0573
	step [95/143], loss=50.2844
	step [96/143], loss=49.0006
	step [97/143], loss=49.4555
	step [98/143], loss=49.8380
	step [99/143], loss=50.8573
	step [100/143], loss=47.5110
	step [101/143], loss=50.2838
	step [102/143], loss=50.2809
	step [103/143], loss=47.1811
	step [104/143], loss=47.2848
	step [105/143], loss=48.0497
	step [106/143], loss=49.9775
	step [107/143], loss=46.6232
	step [108/143], loss=48.3373
	step [109/143], loss=47.3863
	step [110/143], loss=47.5702
	step [111/143], loss=47.6963
	step [112/143], loss=46.2529
	step [113/143], loss=46.3526
	step [114/143], loss=47.6695
	step [115/143], loss=49.0650
	step [116/143], loss=48.4326
	step [117/143], loss=47.2535
	step [118/143], loss=45.9111
	step [119/143], loss=50.0462
	step [120/143], loss=48.1757
	step [121/143], loss=46.1245
	step [122/143], loss=46.8504
	step [123/143], loss=46.2462
	step [124/143], loss=48.2763
	step [125/143], loss=47.8651
	step [126/143], loss=48.5046
	step [127/143], loss=48.0334
	step [128/143], loss=45.6212
	step [129/143], loss=46.1312
	step [130/143], loss=44.5258
	step [131/143], loss=46.3794
	step [132/143], loss=45.7040
	step [133/143], loss=48.2691
	step [134/143], loss=45.4146
	step [135/143], loss=45.4321
	step [136/143], loss=46.5698
	step [137/143], loss=46.7694
	step [138/143], loss=48.1099
	step [139/143], loss=46.5686
	step [140/143], loss=44.2283
	step [141/143], loss=46.8124
	step [142/143], loss=46.1795
	step [143/143], loss=36.9748
	Evaluating
	loss=0.1608, precision=0.2284, recall=0.9934, f1=0.3715
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/143], loss=45.4225
	step [2/143], loss=44.8880
	step [3/143], loss=44.5641
	step [4/143], loss=45.5181
	step [5/143], loss=46.3212
	step [6/143], loss=46.0321
	step [7/143], loss=45.6377
	step [8/143], loss=45.4285
	step [9/143], loss=45.2062
	step [10/143], loss=46.1795
	step [11/143], loss=44.4570
	step [12/143], loss=45.3319
	step [13/143], loss=46.1560
	step [14/143], loss=45.2922
	step [15/143], loss=45.5994
	step [16/143], loss=44.7034
	step [17/143], loss=46.5464
	step [18/143], loss=45.8756
	step [19/143], loss=45.6251
	step [20/143], loss=45.1883
	step [21/143], loss=44.0717
	step [22/143], loss=46.6719
	step [23/143], loss=43.6012
	step [24/143], loss=45.3083
	step [25/143], loss=45.0716
	step [26/143], loss=44.2505
	step [27/143], loss=44.6953
	step [28/143], loss=46.4646
	step [29/143], loss=44.4901
	step [30/143], loss=43.8007
	step [31/143], loss=45.1043
	step [32/143], loss=44.7129
	step [33/143], loss=44.6123
	step [34/143], loss=44.4552
	step [35/143], loss=44.2195
	step [36/143], loss=43.9782
	step [37/143], loss=43.3419
	step [38/143], loss=43.8540
	step [39/143], loss=44.5487
	step [40/143], loss=42.4846
	step [41/143], loss=43.1499
	step [42/143], loss=42.8438
	step [43/143], loss=41.9458
	step [44/143], loss=43.2687
	step [45/143], loss=43.0062
	step [46/143], loss=42.3619
	step [47/143], loss=42.6506
	step [48/143], loss=43.3627
	step [49/143], loss=43.2682
	step [50/143], loss=44.7774
	step [51/143], loss=43.3174
	step [52/143], loss=44.2338
	step [53/143], loss=44.1146
	step [54/143], loss=43.1416
	step [55/143], loss=42.6504
	step [56/143], loss=42.0808
	step [57/143], loss=43.6022
	step [58/143], loss=42.9774
	step [59/143], loss=42.4357
	step [60/143], loss=42.7004
	step [61/143], loss=41.9485
	step [62/143], loss=43.1271
	step [63/143], loss=43.3429
	step [64/143], loss=42.9168
	step [65/143], loss=42.1248
	step [66/143], loss=42.6082
	step [67/143], loss=42.6511
	step [68/143], loss=42.6139
	step [69/143], loss=43.4722
	step [70/143], loss=41.7212
	step [71/143], loss=41.7451
	step [72/143], loss=41.5639
	step [73/143], loss=41.8170
	step [74/143], loss=41.2136
	step [75/143], loss=43.0084
	step [76/143], loss=41.7024
	step [77/143], loss=40.6424
	step [78/143], loss=42.2232
	step [79/143], loss=42.2192
	step [80/143], loss=42.0811
	step [81/143], loss=40.7850
	step [82/143], loss=41.4558
	step [83/143], loss=43.9668
	step [84/143], loss=41.4922
	step [85/143], loss=42.2771
	step [86/143], loss=41.5185
	step [87/143], loss=42.1866
	step [88/143], loss=40.5274
	step [89/143], loss=40.2420
	step [90/143], loss=40.2068
	step [91/143], loss=43.3488
	step [92/143], loss=40.3164
	step [93/143], loss=39.7194
	step [94/143], loss=39.8357
	step [95/143], loss=40.4221
	step [96/143], loss=41.0609
	step [97/143], loss=39.9266
	step [98/143], loss=40.2563
	step [99/143], loss=41.5796
	step [100/143], loss=39.5886
	step [101/143], loss=39.6703
	step [102/143], loss=40.7073
	step [103/143], loss=40.0763
	step [104/143], loss=40.1674
	step [105/143], loss=40.7290
	step [106/143], loss=39.7822
	step [107/143], loss=41.3818
	step [108/143], loss=39.6131
	step [109/143], loss=42.4179
	step [110/143], loss=39.8638
	step [111/143], loss=39.8679
	step [112/143], loss=40.9525
	step [113/143], loss=39.1008
	step [114/143], loss=40.6273
	step [115/143], loss=42.3026
	step [116/143], loss=41.0202
	step [117/143], loss=39.8174
	step [118/143], loss=40.1985
	step [119/143], loss=41.4623
	step [120/143], loss=39.8489
	step [121/143], loss=41.2799
	step [122/143], loss=39.4185
	step [123/143], loss=39.3087
	step [124/143], loss=39.6407
	step [125/143], loss=39.4686
	step [126/143], loss=40.7006
	step [127/143], loss=41.6266
	step [128/143], loss=40.5762
	step [129/143], loss=40.2387
	step [130/143], loss=39.8761
	step [131/143], loss=39.6684
	step [132/143], loss=38.8053
	step [133/143], loss=38.4763
	step [134/143], loss=37.7976
	step [135/143], loss=39.2253
	step [136/143], loss=38.3275
	step [137/143], loss=41.1894
	step [138/143], loss=39.4273
	step [139/143], loss=38.3150
	step [140/143], loss=40.6176
	step [141/143], loss=40.8688
	step [142/143], loss=36.8347
	step [143/143], loss=32.1582
	Evaluating
	loss=0.1306, precision=0.2360, recall=0.9928, f1=0.3814
saving model as: 1_saved_model.pth
Training epoch 5
	step [1/143], loss=39.6355
	step [2/143], loss=39.9917
	step [3/143], loss=38.1508
	step [4/143], loss=38.4805
	step [5/143], loss=39.7094
	step [6/143], loss=40.9334
	step [7/143], loss=39.1902
	step [8/143], loss=38.7374
	step [9/143], loss=37.1083
	step [10/143], loss=37.2008
	step [11/143], loss=37.5654
	step [12/143], loss=39.0736
	step [13/143], loss=38.6589
	step [14/143], loss=39.0615
	step [15/143], loss=38.2261
	step [16/143], loss=38.1594
	step [17/143], loss=39.8625
	step [18/143], loss=38.7066
	step [19/143], loss=36.5892
	step [20/143], loss=39.5194
	step [21/143], loss=38.5097
	step [22/143], loss=38.5287
	step [23/143], loss=37.2668
	step [24/143], loss=38.2424
	step [25/143], loss=36.7135
	step [26/143], loss=39.1318
	step [27/143], loss=37.8767
	step [28/143], loss=38.9056
	step [29/143], loss=39.1356
	step [30/143], loss=37.2903
	step [31/143], loss=38.9350
	step [32/143], loss=35.8735
	step [33/143], loss=38.6098
	step [34/143], loss=36.9116
	step [35/143], loss=37.6101
	step [36/143], loss=36.7821
	step [37/143], loss=37.8561
	step [38/143], loss=36.3682
	step [39/143], loss=37.3236
	step [40/143], loss=36.1174
	step [41/143], loss=36.1093
	step [42/143], loss=37.7950
	step [43/143], loss=37.5844
	step [44/143], loss=36.4174
	step [45/143], loss=36.0394
	step [46/143], loss=37.8476
	step [47/143], loss=38.0137
	step [48/143], loss=37.4820
	step [49/143], loss=36.5832
	step [50/143], loss=38.1774
	step [51/143], loss=36.2663
	step [52/143], loss=36.4001
	step [53/143], loss=36.5636
	step [54/143], loss=37.0054
	step [55/143], loss=37.8992
	step [56/143], loss=35.0880
	step [57/143], loss=37.2233
	step [58/143], loss=37.2405
	step [59/143], loss=35.7755
	step [60/143], loss=35.5783
	step [61/143], loss=37.5473
	step [62/143], loss=36.0941
	step [63/143], loss=35.8885
	step [64/143], loss=36.2650
	step [65/143], loss=36.5981
	step [66/143], loss=35.2223
	step [67/143], loss=35.6222
	step [68/143], loss=35.2529
	step [69/143], loss=36.5334
	step [70/143], loss=36.0303
	step [71/143], loss=36.1157
	step [72/143], loss=34.6831
	step [73/143], loss=35.9942
	step [74/143], loss=36.0100
	step [75/143], loss=33.6944
	step [76/143], loss=36.7556
	step [77/143], loss=37.2108
	step [78/143], loss=35.0490
	step [79/143], loss=36.7102
	step [80/143], loss=37.5902
	step [81/143], loss=36.1112
	step [82/143], loss=35.7015
	step [83/143], loss=34.9739
	step [84/143], loss=36.5449
	step [85/143], loss=37.7521
	step [86/143], loss=34.9122
	step [87/143], loss=36.6285
	step [88/143], loss=36.1612
	step [89/143], loss=33.3309
	step [90/143], loss=34.9808
	step [91/143], loss=36.0662
	step [92/143], loss=34.8066
	step [93/143], loss=34.7362
	step [94/143], loss=35.9584
	step [95/143], loss=34.8656
	step [96/143], loss=34.9359
	step [97/143], loss=35.4538
	step [98/143], loss=34.6560
	step [99/143], loss=35.9281
	step [100/143], loss=36.6097
	step [101/143], loss=34.4498
	step [102/143], loss=34.1738
	step [103/143], loss=35.6386
	step [104/143], loss=34.5344
	step [105/143], loss=35.4838
	step [106/143], loss=33.8412
	step [107/143], loss=35.1833
	step [108/143], loss=34.9681
	step [109/143], loss=33.8179
	step [110/143], loss=35.9503
	step [111/143], loss=35.1164
	step [112/143], loss=34.1764
	step [113/143], loss=34.1022
	step [114/143], loss=34.0865
	step [115/143], loss=34.3308
	step [116/143], loss=36.1447
	step [117/143], loss=35.9441
	step [118/143], loss=34.8547
	step [119/143], loss=33.1321
	step [120/143], loss=33.3362
	step [121/143], loss=34.0070
	step [122/143], loss=36.4764
	step [123/143], loss=31.8821
	step [124/143], loss=34.4782
	step [125/143], loss=33.5618
	step [126/143], loss=33.9689
	step [127/143], loss=34.0074
	step [128/143], loss=34.6980
	step [129/143], loss=34.3114
	step [130/143], loss=35.3968
	step [131/143], loss=33.6416
	step [132/143], loss=32.5201
	step [133/143], loss=34.2533
	step [134/143], loss=32.1699
	step [135/143], loss=33.5617
	step [136/143], loss=34.6180
	step [137/143], loss=32.5124
	step [138/143], loss=33.1892
	step [139/143], loss=32.8043
	step [140/143], loss=33.8298
	step [141/143], loss=32.8659
	step [142/143], loss=33.3801
	step [143/143], loss=27.1777
	Evaluating
	loss=0.1409, precision=0.2010, recall=0.9947, f1=0.3345
Training epoch 6
	step [1/143], loss=33.9365
	step [2/143], loss=32.7367
	step [3/143], loss=34.5635
	step [4/143], loss=34.5251
	step [5/143], loss=33.8148
	step [6/143], loss=34.1288
	step [7/143], loss=31.8384
	step [8/143], loss=33.1782
	step [9/143], loss=33.1290
	step [10/143], loss=32.8274
	step [11/143], loss=34.1874
	step [12/143], loss=32.6927
	step [13/143], loss=31.3127
	step [14/143], loss=33.9507
	step [15/143], loss=34.8787
	step [16/143], loss=31.6766
	step [17/143], loss=33.6514
	step [18/143], loss=34.2840
	step [19/143], loss=32.0552
	step [20/143], loss=32.7869
	step [21/143], loss=32.8584
	step [22/143], loss=31.7174
	step [23/143], loss=30.5284
	step [24/143], loss=32.3200
	step [25/143], loss=33.7814
	step [26/143], loss=33.0241
	step [27/143], loss=32.5833
	step [28/143], loss=33.8296
	step [29/143], loss=32.1187
	step [30/143], loss=30.9783
	step [31/143], loss=33.9347
	step [32/143], loss=32.2769
	step [33/143], loss=32.4865
	step [34/143], loss=34.0627
	step [35/143], loss=32.3184
	step [36/143], loss=30.9228
	step [37/143], loss=31.9102
	step [38/143], loss=33.1352
	step [39/143], loss=31.3688
	step [40/143], loss=32.8221
	step [41/143], loss=32.1077
	step [42/143], loss=32.3092
	step [43/143], loss=31.5652
	step [44/143], loss=32.0739
	step [45/143], loss=32.2050
	step [46/143], loss=33.5478
	step [47/143], loss=31.1838
	step [48/143], loss=31.9186
	step [49/143], loss=31.0892
	step [50/143], loss=32.5155
	step [51/143], loss=33.7393
	step [52/143], loss=31.4043
	step [53/143], loss=31.1749
	step [54/143], loss=30.9938
	step [55/143], loss=31.9037
	step [56/143], loss=32.1747
	step [57/143], loss=32.7261
	step [58/143], loss=32.2468
	step [59/143], loss=30.5590
	step [60/143], loss=30.9612
	step [61/143], loss=30.1049
	step [62/143], loss=30.4124
	step [63/143], loss=31.9350
	step [64/143], loss=32.1810
	step [65/143], loss=32.8648
	step [66/143], loss=31.7627
	step [67/143], loss=29.5306
	step [68/143], loss=31.1284
	step [69/143], loss=31.6643
	step [70/143], loss=31.7119
	step [71/143], loss=31.4079
	step [72/143], loss=30.4030
	step [73/143], loss=29.2204
	step [74/143], loss=30.9207
	step [75/143], loss=32.7036
	step [76/143], loss=31.2912
	step [77/143], loss=31.5325
	step [78/143], loss=30.0023
	step [79/143], loss=31.1205
	step [80/143], loss=30.0630
	step [81/143], loss=30.9221
	step [82/143], loss=31.2541
	step [83/143], loss=29.8388
	step [84/143], loss=33.1963
	step [85/143], loss=31.3098
	step [86/143], loss=29.7927
	step [87/143], loss=29.9869
	step [88/143], loss=31.1335
	step [89/143], loss=29.9238
	step [90/143], loss=30.2110
	step [91/143], loss=31.5862
	step [92/143], loss=31.7351
	step [93/143], loss=31.6469
	step [94/143], loss=30.0682
	step [95/143], loss=30.8822
	step [96/143], loss=32.0876
	step [97/143], loss=30.8023
	step [98/143], loss=30.1385
	step [99/143], loss=29.3249
	step [100/143], loss=28.8069
	step [101/143], loss=30.1977
	step [102/143], loss=31.1649
	step [103/143], loss=31.5659
	step [104/143], loss=29.9090
	step [105/143], loss=29.7376
	step [106/143], loss=30.2709
	step [107/143], loss=30.0096
	step [108/143], loss=30.3947
	step [109/143], loss=32.4619
	step [110/143], loss=29.7038
	step [111/143], loss=31.9246
	step [112/143], loss=29.1751
	step [113/143], loss=28.8461
	step [114/143], loss=29.7672
	step [115/143], loss=31.2800
	step [116/143], loss=29.0346
	step [117/143], loss=30.1221
	step [118/143], loss=29.0111
	step [119/143], loss=28.1399
	step [120/143], loss=30.4110
	step [121/143], loss=27.8313
	step [122/143], loss=30.0781
	step [123/143], loss=29.9781
	step [124/143], loss=28.6465
	step [125/143], loss=30.0021
	step [126/143], loss=28.8464
	step [127/143], loss=28.2050
	step [128/143], loss=31.7278
	step [129/143], loss=30.5370
	step [130/143], loss=28.0941
	step [131/143], loss=28.1829
	step [132/143], loss=30.6533
	step [133/143], loss=29.9255
	step [134/143], loss=29.3205
	step [135/143], loss=30.7770
	step [136/143], loss=28.8161
	step [137/143], loss=29.5546
	step [138/143], loss=30.5260
	step [139/143], loss=29.0051
	step [140/143], loss=29.7857
	step [141/143], loss=28.6089
	step [142/143], loss=29.7499
	step [143/143], loss=22.5577
	Evaluating
	loss=0.0943, precision=0.2418, recall=0.9926, f1=0.3889
saving model as: 1_saved_model.pth
Training epoch 7
	step [1/143], loss=29.7736
	step [2/143], loss=28.8901
	step [3/143], loss=29.1500
	step [4/143], loss=29.8784
	step [5/143], loss=30.6084
	step [6/143], loss=28.1999
	step [7/143], loss=28.4733
	step [8/143], loss=27.7199
	step [9/143], loss=29.0804
	step [10/143], loss=27.0522
	step [11/143], loss=29.2943
	step [12/143], loss=28.9527
	step [13/143], loss=29.8469
	step [14/143], loss=30.4075
	step [15/143], loss=29.2116
	step [16/143], loss=27.5230
	step [17/143], loss=28.4637
	step [18/143], loss=27.2199
	step [19/143], loss=32.2309
	step [20/143], loss=29.9583
	step [21/143], loss=27.7184
	step [22/143], loss=28.1489
	step [23/143], loss=27.1378
	step [24/143], loss=28.5227
	step [25/143], loss=26.9118
	step [26/143], loss=28.3205
	step [27/143], loss=29.0950
	step [28/143], loss=26.2646
	step [29/143], loss=27.0119
	step [30/143], loss=27.7003
	step [31/143], loss=28.4566
	step [32/143], loss=26.5988
	step [33/143], loss=30.6161
	step [34/143], loss=28.0494
	step [35/143], loss=29.8613
	step [36/143], loss=29.0367
	step [37/143], loss=27.9454
	step [38/143], loss=28.1778
	step [39/143], loss=27.2456
	step [40/143], loss=28.9237
	step [41/143], loss=27.0404
	step [42/143], loss=28.1069
	step [43/143], loss=26.8171
	step [44/143], loss=26.7624
	step [45/143], loss=27.9082
	step [46/143], loss=27.5200
	step [47/143], loss=28.2439
	step [48/143], loss=27.4113
	step [49/143], loss=25.8347
	step [50/143], loss=28.8825
	step [51/143], loss=27.8023
	step [52/143], loss=27.4653
	step [53/143], loss=27.3652
	step [54/143], loss=28.2051
	step [55/143], loss=29.0478
	step [56/143], loss=28.5194
	step [57/143], loss=28.4007
	step [58/143], loss=26.2366
	step [59/143], loss=28.7227
	step [60/143], loss=25.3748
	step [61/143], loss=26.6395
	step [62/143], loss=27.9158
	step [63/143], loss=28.4918
	step [64/143], loss=29.0563
	step [65/143], loss=26.5079
	step [66/143], loss=27.7733
	step [67/143], loss=28.5906
	step [68/143], loss=29.0134
	step [69/143], loss=27.0095
	step [70/143], loss=27.1010
	step [71/143], loss=25.0036
	step [72/143], loss=25.4615
	step [73/143], loss=28.1587
	step [74/143], loss=24.5391
	step [75/143], loss=26.8162
	step [76/143], loss=26.1337
	step [77/143], loss=25.6150
	step [78/143], loss=26.3016
	step [79/143], loss=27.4526
	step [80/143], loss=26.8649
	step [81/143], loss=26.3415
	step [82/143], loss=28.1874
	step [83/143], loss=26.4243
	step [84/143], loss=26.9511
	step [85/143], loss=27.4296
	step [86/143], loss=25.3521
	step [87/143], loss=25.8323
	step [88/143], loss=27.6100
	step [89/143], loss=27.5721
	step [90/143], loss=26.8153
	step [91/143], loss=26.0687
	step [92/143], loss=28.0447
	step [93/143], loss=25.9873
	step [94/143], loss=26.0607
	step [95/143], loss=26.4067
	step [96/143], loss=26.7821
	step [97/143], loss=27.5773
	step [98/143], loss=26.2723
	step [99/143], loss=26.9044
	step [100/143], loss=25.0080
	step [101/143], loss=27.4414
	step [102/143], loss=27.1974
	step [103/143], loss=27.0520
	step [104/143], loss=24.6849
	step [105/143], loss=24.9916
	step [106/143], loss=27.7872
	step [107/143], loss=26.4487
	step [108/143], loss=24.3391
	step [109/143], loss=26.6312
	step [110/143], loss=26.3000
	step [111/143], loss=25.6792
	step [112/143], loss=24.6106
	step [113/143], loss=26.5421
	step [114/143], loss=23.3984
	step [115/143], loss=25.7595
	step [116/143], loss=26.6412
	step [117/143], loss=25.7792
	step [118/143], loss=25.3289
	step [119/143], loss=26.3037
	step [120/143], loss=25.7825
	step [121/143], loss=25.6508
	step [122/143], loss=24.5638
	step [123/143], loss=25.9256
	step [124/143], loss=27.8236
	step [125/143], loss=26.1058
	step [126/143], loss=25.0360
	step [127/143], loss=26.6108
	step [128/143], loss=25.7044
	step [129/143], loss=25.2274
	step [130/143], loss=27.0660
	step [131/143], loss=26.2131
	step [132/143], loss=27.7750
	step [133/143], loss=26.9116
	step [134/143], loss=24.5828
	step [135/143], loss=26.4133
	step [136/143], loss=24.2968
	step [137/143], loss=24.9142
	step [138/143], loss=25.4134
	step [139/143], loss=23.2266
	step [140/143], loss=25.4529
	step [141/143], loss=25.6593
	step [142/143], loss=28.2604
	step [143/143], loss=20.9570
	Evaluating
	loss=0.0963, precision=0.1519, recall=0.9955, f1=0.2636
Training epoch 8
	step [1/143], loss=24.1848
	step [2/143], loss=26.7895
	step [3/143], loss=24.2685
	step [4/143], loss=24.1171
	step [5/143], loss=25.4227
	step [6/143], loss=26.7287
	step [7/143], loss=22.9569
	step [8/143], loss=26.5389
	step [9/143], loss=24.3441
	step [10/143], loss=25.0621
	step [11/143], loss=24.8440
	step [12/143], loss=24.0762
	step [13/143], loss=24.3980
	step [14/143], loss=22.9128
	step [15/143], loss=26.1619
	step [16/143], loss=24.7848
	step [17/143], loss=23.8237
	step [18/143], loss=25.4947
	step [19/143], loss=23.4567
	step [20/143], loss=25.8023
	step [21/143], loss=24.1202
	step [22/143], loss=25.5821
	step [23/143], loss=25.5726
	step [24/143], loss=23.8515
	step [25/143], loss=26.7497
	step [26/143], loss=24.0284
	step [27/143], loss=27.0868
	step [28/143], loss=24.6619
	step [29/143], loss=23.8440
	step [30/143], loss=24.3308
	step [31/143], loss=26.4140
	step [32/143], loss=24.1355
	step [33/143], loss=23.8690
	step [34/143], loss=23.7921
	step [35/143], loss=23.8142
	step [36/143], loss=24.3226
	step [37/143], loss=25.3150
	step [38/143], loss=24.2834
	step [39/143], loss=24.4822
	step [40/143], loss=24.7917
	step [41/143], loss=24.2999
	step [42/143], loss=24.1678
	step [43/143], loss=26.1077
	step [44/143], loss=24.2691
	step [45/143], loss=24.5664
	step [46/143], loss=24.7205
	step [47/143], loss=24.3780
	step [48/143], loss=24.5540
	step [49/143], loss=25.4693
	step [50/143], loss=24.8236
	step [51/143], loss=23.0194
	step [52/143], loss=23.4707
	step [53/143], loss=24.5896
	step [54/143], loss=25.3461
	step [55/143], loss=24.6021
	step [56/143], loss=22.9587
	step [57/143], loss=25.0990
	step [58/143], loss=24.1255
	step [59/143], loss=23.6828
	step [60/143], loss=24.9023
	step [61/143], loss=25.6972
	step [62/143], loss=23.0847
	step [63/143], loss=25.0769
	step [64/143], loss=23.5760
	step [65/143], loss=23.6512
	step [66/143], loss=21.8389
	step [67/143], loss=23.2369
	step [68/143], loss=23.6730
	step [69/143], loss=24.5967
	step [70/143], loss=22.5722
	step [71/143], loss=21.8574
	step [72/143], loss=24.4430
	step [73/143], loss=23.7609
	step [74/143], loss=26.4561
	step [75/143], loss=22.6942
	step [76/143], loss=22.9965
	step [77/143], loss=23.6658
	step [78/143], loss=24.2403
	step [79/143], loss=23.5747
	step [80/143], loss=22.7682
	step [81/143], loss=23.3067
	step [82/143], loss=23.7440
	step [83/143], loss=23.7565
	step [84/143], loss=24.4071
	step [85/143], loss=24.1916
	step [86/143], loss=24.5639
	step [87/143], loss=23.8975
	step [88/143], loss=25.2435
	step [89/143], loss=24.3550
	step [90/143], loss=24.7722
	step [91/143], loss=23.5589
	step [92/143], loss=22.8867
	step [93/143], loss=24.8786
	step [94/143], loss=24.3267
	step [95/143], loss=23.2755
	step [96/143], loss=23.5805
	step [97/143], loss=24.9936
	step [98/143], loss=23.7160
	step [99/143], loss=22.1338
	step [100/143], loss=21.4516
	step [101/143], loss=23.3294
	step [102/143], loss=22.1295
	step [103/143], loss=25.3907
	step [104/143], loss=23.7407
	step [105/143], loss=23.5056
	step [106/143], loss=23.9094
	step [107/143], loss=22.4918
	step [108/143], loss=25.4526
	step [109/143], loss=24.6819
	step [110/143], loss=23.0221
	step [111/143], loss=24.2597
	step [112/143], loss=23.1834
	step [113/143], loss=23.2014
	step [114/143], loss=22.1865
	step [115/143], loss=23.0975
	step [116/143], loss=23.3694
	step [117/143], loss=23.9608
	step [118/143], loss=22.8534
	step [119/143], loss=24.4399
	step [120/143], loss=22.1452
	step [121/143], loss=21.5668
	step [122/143], loss=25.3675
	step [123/143], loss=22.8004
	step [124/143], loss=22.7107
	step [125/143], loss=24.7385
	step [126/143], loss=24.4361
	step [127/143], loss=23.8215
	step [128/143], loss=22.4334
	step [129/143], loss=24.0098
	step [130/143], loss=21.9676
	step [131/143], loss=22.4368
	step [132/143], loss=21.7818
	step [133/143], loss=24.6036
	step [134/143], loss=22.7328
	step [135/143], loss=22.1638
	step [136/143], loss=22.9017
	step [137/143], loss=21.8393
	step [138/143], loss=21.5290
	step [139/143], loss=21.3637
	step [140/143], loss=23.1733
	step [141/143], loss=22.6530
	step [142/143], loss=22.1369
	step [143/143], loss=21.2855
	Evaluating
	loss=0.0679, precision=0.2437, recall=0.9916, f1=0.3912
saving model as: 1_saved_model.pth
Training epoch 9
	step [1/143], loss=23.5761
	step [2/143], loss=23.4226
	step [3/143], loss=22.7388
	step [4/143], loss=20.8495
	step [5/143], loss=24.6388
	step [6/143], loss=21.8721
	step [7/143], loss=22.2274
	step [8/143], loss=22.7597
	step [9/143], loss=23.3563
	step [10/143], loss=20.8375
	step [11/143], loss=21.8239
	step [12/143], loss=21.9938
	step [13/143], loss=23.4501
	step [14/143], loss=21.6573
	step [15/143], loss=21.8029
	step [16/143], loss=22.1018
	step [17/143], loss=22.0403
	step [18/143], loss=20.9461
	step [19/143], loss=21.4475
	step [20/143], loss=22.2255
	step [21/143], loss=22.4976
	step [22/143], loss=24.1444
	step [23/143], loss=20.8877
	step [24/143], loss=22.7796
	step [25/143], loss=22.6732
	step [26/143], loss=23.0603
	step [27/143], loss=20.2090
	step [28/143], loss=24.4460
	step [29/143], loss=24.1123
	step [30/143], loss=21.8874
	step [31/143], loss=20.8329
	step [32/143], loss=22.3649
	step [33/143], loss=21.7399
	step [34/143], loss=22.8261
	step [35/143], loss=23.5812
	step [36/143], loss=20.9404
	step [37/143], loss=21.3179
	step [38/143], loss=22.7919
	step [39/143], loss=20.9440
	step [40/143], loss=23.2855
	step [41/143], loss=22.6327
	step [42/143], loss=21.6232
	step [43/143], loss=23.3407
	step [44/143], loss=20.5391
	step [45/143], loss=20.4885
	step [46/143], loss=22.8285
	step [47/143], loss=21.8329
	step [48/143], loss=20.3256
	step [49/143], loss=21.1361
	step [50/143], loss=20.6343
	step [51/143], loss=20.7959
	step [52/143], loss=22.2520
	step [53/143], loss=20.3140
	step [54/143], loss=21.6314
	step [55/143], loss=21.2648
	step [56/143], loss=23.0533
	step [57/143], loss=20.8460
	step [58/143], loss=20.8506
	step [59/143], loss=20.6576
	step [60/143], loss=22.0413
	step [61/143], loss=21.0961
	step [62/143], loss=20.4551
	step [63/143], loss=19.8457
	step [64/143], loss=21.3405
	step [65/143], loss=22.4700
	step [66/143], loss=21.5999
	step [67/143], loss=21.0570
	step [68/143], loss=21.1686
	step [69/143], loss=23.1020
	step [70/143], loss=21.1949
	step [71/143], loss=20.4288
	step [72/143], loss=21.1424
	step [73/143], loss=20.9307
	step [74/143], loss=20.8649
	step [75/143], loss=21.5685
	step [76/143], loss=20.0644
	step [77/143], loss=21.3599
	step [78/143], loss=21.0058
	step [79/143], loss=19.6886
	step [80/143], loss=23.1067
	step [81/143], loss=22.4614
	step [82/143], loss=22.2954
	step [83/143], loss=21.1888
	step [84/143], loss=20.7424
	step [85/143], loss=23.0623
	step [86/143], loss=20.7033
	step [87/143], loss=22.7934
	step [88/143], loss=22.3094
	step [89/143], loss=18.8651
	step [90/143], loss=23.9720
	step [91/143], loss=19.9894
	step [92/143], loss=20.4305
	step [93/143], loss=21.2971
	step [94/143], loss=21.8507
	step [95/143], loss=19.4017
	step [96/143], loss=21.1951
	step [97/143], loss=20.3572
	step [98/143], loss=21.9429
	step [99/143], loss=21.3439
	step [100/143], loss=21.1062
	step [101/143], loss=21.8376
	step [102/143], loss=21.7871
	step [103/143], loss=20.4888
	step [104/143], loss=19.7356
	step [105/143], loss=21.2139
	step [106/143], loss=21.2042
	step [107/143], loss=20.9339
	step [108/143], loss=21.3152
	step [109/143], loss=19.5684
	step [110/143], loss=21.5554
	step [111/143], loss=19.8381
	step [112/143], loss=19.5324
	step [113/143], loss=22.6070
	step [114/143], loss=20.5735
	step [115/143], loss=20.3606
	step [116/143], loss=19.3429
	step [117/143], loss=20.7930
	step [118/143], loss=20.7225
	step [119/143], loss=20.8704
	step [120/143], loss=21.6296
	step [121/143], loss=18.8120
	step [122/143], loss=19.3504
	step [123/143], loss=21.7678
	step [124/143], loss=20.7262
	step [125/143], loss=20.1261
	step [126/143], loss=19.9809
	step [127/143], loss=19.0382
	step [128/143], loss=19.6833
	step [129/143], loss=20.4264
	step [130/143], loss=21.2401
	step [131/143], loss=21.1172
	step [132/143], loss=20.0704
	step [133/143], loss=20.9856
	step [134/143], loss=20.8556
	step [135/143], loss=19.1717
	step [136/143], loss=20.4294
	step [137/143], loss=20.4263
	step [138/143], loss=19.7275
	step [139/143], loss=20.6925
	step [140/143], loss=22.5963
	step [141/143], loss=20.7007
	step [142/143], loss=20.8513
	step [143/143], loss=16.1217
	Evaluating
	loss=0.0669, precision=0.2070, recall=0.9939, f1=0.3426
Training epoch 10
	step [1/143], loss=20.6792
	step [2/143], loss=19.5181
	step [3/143], loss=18.5115
	step [4/143], loss=19.2444
	step [5/143], loss=20.9132
	step [6/143], loss=22.7818
	step [7/143], loss=20.0332
	step [8/143], loss=20.2984
	step [9/143], loss=21.6364
	step [10/143], loss=19.5501
	step [11/143], loss=19.6068
	step [12/143], loss=18.2430
	step [13/143], loss=21.8183
	step [14/143], loss=20.0620
	step [15/143], loss=21.8335
	step [16/143], loss=22.9290
	step [17/143], loss=19.8540
	step [18/143], loss=20.3565
	step [19/143], loss=19.9075
	step [20/143], loss=20.1756
	step [21/143], loss=19.1796
	step [22/143], loss=19.4148
	step [23/143], loss=19.9710
	step [24/143], loss=19.1005
	step [25/143], loss=19.9598
	step [26/143], loss=19.6301
	step [27/143], loss=22.7034
	step [28/143], loss=18.5640
	step [29/143], loss=19.0914
	step [30/143], loss=19.9419
	step [31/143], loss=18.3181
	step [32/143], loss=18.5254
	step [33/143], loss=21.7544
	step [34/143], loss=18.2065
	step [35/143], loss=19.2268
	step [36/143], loss=17.8527
	step [37/143], loss=19.8087
	step [38/143], loss=19.5062
	step [39/143], loss=19.6591
	step [40/143], loss=21.4026
	step [41/143], loss=21.6491
	step [42/143], loss=20.8175
	step [43/143], loss=18.7011
	step [44/143], loss=18.4819
	step [45/143], loss=19.8592
	step [46/143], loss=19.9497
	step [47/143], loss=19.2350
	step [48/143], loss=19.4930
	step [49/143], loss=19.3793
	step [50/143], loss=17.6772
	step [51/143], loss=19.8914
	step [52/143], loss=18.4940
	step [53/143], loss=18.9886
	step [54/143], loss=19.3090
	step [55/143], loss=20.6128
	step [56/143], loss=18.2068
	step [57/143], loss=19.6768
	step [58/143], loss=19.1735
	step [59/143], loss=19.3574
	step [60/143], loss=19.0663
	step [61/143], loss=20.0286
	step [62/143], loss=19.8882
	step [63/143], loss=22.3454
	step [64/143], loss=20.5627
	step [65/143], loss=19.8691
	step [66/143], loss=19.4448
	step [67/143], loss=19.3698
	step [68/143], loss=18.3905
	step [69/143], loss=19.1197
	step [70/143], loss=18.5971
	step [71/143], loss=20.3499
	step [72/143], loss=20.1594
	step [73/143], loss=17.9970
	step [74/143], loss=18.5034
	step [75/143], loss=19.8243
	step [76/143], loss=16.6612
	step [77/143], loss=20.6455
	step [78/143], loss=17.8378
	step [79/143], loss=19.1474
	step [80/143], loss=18.9335
	step [81/143], loss=19.8799
	step [82/143], loss=20.7565
	step [83/143], loss=19.8643
	step [84/143], loss=20.3758
	step [85/143], loss=18.7179
	step [86/143], loss=18.4492
	step [87/143], loss=20.4686
	step [88/143], loss=18.0119
	step [89/143], loss=19.7646
	step [90/143], loss=19.7578
	step [91/143], loss=20.5780
	step [92/143], loss=17.9441
	step [93/143], loss=19.7409
	step [94/143], loss=19.1865
	step [95/143], loss=17.7985
	step [96/143], loss=17.9959
	step [97/143], loss=19.9543
	step [98/143], loss=18.6140
	step [99/143], loss=19.7909
	step [100/143], loss=18.8986
	step [101/143], loss=16.9234
	step [102/143], loss=18.4223
	step [103/143], loss=18.5188
	step [104/143], loss=19.2206
	step [105/143], loss=20.9006
	step [106/143], loss=20.0892
	step [107/143], loss=18.4559
	step [108/143], loss=20.6072
	step [109/143], loss=18.0600
	step [110/143], loss=18.8445
	step [111/143], loss=17.8172
	step [112/143], loss=19.6257
	step [113/143], loss=18.4066
	step [114/143], loss=17.5016
	step [115/143], loss=19.7137
	step [116/143], loss=18.5235
	step [117/143], loss=18.6494
	step [118/143], loss=17.4944
	step [119/143], loss=18.2135
	step [120/143], loss=17.9029
	step [121/143], loss=18.5714
	step [122/143], loss=18.5629
	step [123/143], loss=18.0565
	step [124/143], loss=18.4648
	step [125/143], loss=18.1963
	step [126/143], loss=17.9996
	step [127/143], loss=16.7316
	step [128/143], loss=18.3441
	step [129/143], loss=20.5448
	step [130/143], loss=19.4058
	step [131/143], loss=18.4351
	step [132/143], loss=18.4543
	step [133/143], loss=18.7070
	step [134/143], loss=19.2378
	step [135/143], loss=18.1879
	step [136/143], loss=19.9968
	step [137/143], loss=19.5512
	step [138/143], loss=17.6303
	step [139/143], loss=17.9357
	step [140/143], loss=17.0645
	step [141/143], loss=18.9686
	step [142/143], loss=18.5085
	step [143/143], loss=17.3771
	Evaluating
	loss=0.0560, precision=0.2333, recall=0.9920, f1=0.3778
Training epoch 11
	step [1/143], loss=19.1288
	step [2/143], loss=17.3937
	step [3/143], loss=17.0869
	step [4/143], loss=18.4906
	step [5/143], loss=19.4880
	step [6/143], loss=18.8940
	step [7/143], loss=17.1249
	step [8/143], loss=17.0555
	step [9/143], loss=18.9197
	step [10/143], loss=18.4905
	step [11/143], loss=16.0100
	step [12/143], loss=17.0281
	step [13/143], loss=17.4589
	step [14/143], loss=18.0438
	step [15/143], loss=20.7255
	step [16/143], loss=19.1205
	step [17/143], loss=18.1744
	step [18/143], loss=17.5502
	step [19/143], loss=17.6533
	step [20/143], loss=18.4834
	step [21/143], loss=17.2679
	step [22/143], loss=16.8827
	step [23/143], loss=16.7785
	step [24/143], loss=17.0805
	step [25/143], loss=18.1076
	step [26/143], loss=18.2092
	step [27/143], loss=18.3369
	step [28/143], loss=17.6074
	step [29/143], loss=18.0833
	step [30/143], loss=17.3393
	step [31/143], loss=16.8278
	step [32/143], loss=18.6059
	step [33/143], loss=16.4429
	step [34/143], loss=16.0514
	step [35/143], loss=16.8023
	step [36/143], loss=17.8189
	step [37/143], loss=19.8038
	step [38/143], loss=16.5839
	step [39/143], loss=18.1890
	step [40/143], loss=17.8026
	step [41/143], loss=19.8517
	step [42/143], loss=17.1740
	step [43/143], loss=16.3381
	step [44/143], loss=17.4442
	step [45/143], loss=19.1141
	step [46/143], loss=16.9430
	step [47/143], loss=17.7369
	step [48/143], loss=18.5059
	step [49/143], loss=18.5063
	step [50/143], loss=19.3782
	step [51/143], loss=17.1057
	step [52/143], loss=17.0664
	step [53/143], loss=17.3355
	step [54/143], loss=16.7431
	step [55/143], loss=18.3017
	step [56/143], loss=17.4732
	step [57/143], loss=17.4765
	step [58/143], loss=17.7166
	step [59/143], loss=16.9620
	step [60/143], loss=17.2815
	step [61/143], loss=16.3063
	step [62/143], loss=17.5960
	step [63/143], loss=16.8529
	step [64/143], loss=16.0319
	step [65/143], loss=19.4100
	step [66/143], loss=17.7406
	step [67/143], loss=17.7156
	step [68/143], loss=16.6554
	step [69/143], loss=16.9243
	step [70/143], loss=19.3867
	step [71/143], loss=17.3546
	step [72/143], loss=16.5115
	step [73/143], loss=17.0184
	step [74/143], loss=18.8153
	step [75/143], loss=17.9784
	step [76/143], loss=17.2959
	step [77/143], loss=17.8228
	step [78/143], loss=18.0872
	step [79/143], loss=19.1615
	step [80/143], loss=17.3489
	step [81/143], loss=17.4228
	step [82/143], loss=16.8631
	step [83/143], loss=16.0719
	step [84/143], loss=17.8771
	step [85/143], loss=17.1342
	step [86/143], loss=17.4220
	step [87/143], loss=16.6798
	step [88/143], loss=17.4405
	step [89/143], loss=15.5448
	step [90/143], loss=20.1305
	step [91/143], loss=18.2252
	step [92/143], loss=17.0843
	step [93/143], loss=18.8973
	step [94/143], loss=17.3088
	step [95/143], loss=19.2823
	step [96/143], loss=16.3253
	step [97/143], loss=17.5120
	step [98/143], loss=17.6320
	step [99/143], loss=17.3443
	step [100/143], loss=19.1378
	step [101/143], loss=18.3545
	step [102/143], loss=19.7375
	step [103/143], loss=17.6680
	step [104/143], loss=17.2770
	step [105/143], loss=17.7477
	step [106/143], loss=16.1659
	step [107/143], loss=19.7341
	step [108/143], loss=16.9555
	step [109/143], loss=17.8303
	step [110/143], loss=16.5696
	step [111/143], loss=16.5364
	step [112/143], loss=17.1559
	step [113/143], loss=17.8024
	step [114/143], loss=16.7829
	step [115/143], loss=16.2982
	step [116/143], loss=16.5493
	step [117/143], loss=17.9903
	step [118/143], loss=18.0358
	step [119/143], loss=17.2082
	step [120/143], loss=17.2488
	step [121/143], loss=17.5710
	step [122/143], loss=17.6316
	step [123/143], loss=19.5035
	step [124/143], loss=17.5725
	step [125/143], loss=17.4312
	step [126/143], loss=16.6207
	step [127/143], loss=15.9846
	step [128/143], loss=17.7412
	step [129/143], loss=16.8870
	step [130/143], loss=17.5090
	step [131/143], loss=16.3842
	step [132/143], loss=16.8790
	step [133/143], loss=17.3182
	step [134/143], loss=16.2413
	step [135/143], loss=16.7270
	step [136/143], loss=15.8758
	step [137/143], loss=16.8303
	step [138/143], loss=18.7451
	step [139/143], loss=16.3953
	step [140/143], loss=16.3128
	step [141/143], loss=16.2119
	step [142/143], loss=16.5920
	step [143/143], loss=14.1000
	Evaluating
	loss=0.0490, precision=0.2442, recall=0.9929, f1=0.3920
saving model as: 1_saved_model.pth
Training epoch 12
	step [1/143], loss=17.0026
	step [2/143], loss=14.9003
	step [3/143], loss=17.3284
	step [4/143], loss=17.4333
	step [5/143], loss=18.5718
	step [6/143], loss=15.6213
	step [7/143], loss=17.1049
	step [8/143], loss=15.5125
	step [9/143], loss=16.1007
	step [10/143], loss=17.4809
	step [11/143], loss=16.3247
	step [12/143], loss=16.2695
	step [13/143], loss=14.9452
	step [14/143], loss=18.0541
	step [15/143], loss=17.3630
	step [16/143], loss=16.2055
	step [17/143], loss=16.6690
	step [18/143], loss=18.3983
	step [19/143], loss=15.2709
	step [20/143], loss=17.0358
	step [21/143], loss=17.3714
	step [22/143], loss=15.1940
	step [23/143], loss=16.3017
	step [24/143], loss=14.9890
	step [25/143], loss=15.7863
	step [26/143], loss=17.2306
	step [27/143], loss=15.7441
	step [28/143], loss=17.4939
	step [29/143], loss=16.7240
	step [30/143], loss=15.6565
	step [31/143], loss=15.0818
	step [32/143], loss=17.5018
	step [33/143], loss=15.9030
	step [34/143], loss=18.0931
	step [35/143], loss=17.3745
	step [36/143], loss=17.0989
	step [37/143], loss=18.1945
	step [38/143], loss=15.4365
	step [39/143], loss=16.3253
	step [40/143], loss=16.8876
	step [41/143], loss=15.5497
	step [42/143], loss=16.5121
	step [43/143], loss=17.4503
	step [44/143], loss=17.7852
	step [45/143], loss=16.6082
	step [46/143], loss=16.5423
	step [47/143], loss=17.3325
	step [48/143], loss=15.7204
	step [49/143], loss=15.2683
	step [50/143], loss=18.0261
	step [51/143], loss=15.8372
	step [52/143], loss=15.1230
	step [53/143], loss=15.8289
	step [54/143], loss=16.0102
	step [55/143], loss=14.6582
	step [56/143], loss=16.2063
	step [57/143], loss=16.2582
	step [58/143], loss=16.0821
	step [59/143], loss=15.7155
	step [60/143], loss=16.3319
	step [61/143], loss=19.0436
	step [62/143], loss=17.1288
	step [63/143], loss=16.1435
	step [64/143], loss=17.0652
	step [65/143], loss=17.2049
	step [66/143], loss=17.2297
	step [67/143], loss=16.9308
	step [68/143], loss=17.3803
	step [69/143], loss=16.4052
	step [70/143], loss=16.7809
	step [71/143], loss=16.4714
	step [72/143], loss=15.7826
	step [73/143], loss=16.5708
	step [74/143], loss=15.8576
	step [75/143], loss=16.7539
	step [76/143], loss=16.4869
	step [77/143], loss=17.7264
	step [78/143], loss=17.0048
	step [79/143], loss=15.3780
	step [80/143], loss=15.4123
	step [81/143], loss=15.5214
	step [82/143], loss=16.6042
	step [83/143], loss=16.7027
	step [84/143], loss=16.3999
	step [85/143], loss=15.9178
	step [86/143], loss=15.9670
	step [87/143], loss=14.7460
	step [88/143], loss=14.7458
	step [89/143], loss=17.2064
	step [90/143], loss=16.2162
	step [91/143], loss=15.1146
	step [92/143], loss=17.8130
	step [93/143], loss=16.8363
	step [94/143], loss=16.0496
	step [95/143], loss=17.1342
	step [96/143], loss=15.3175
	step [97/143], loss=15.5453
	step [98/143], loss=15.6188
	step [99/143], loss=15.3863
	step [100/143], loss=18.0196
	step [101/143], loss=17.7776
	step [102/143], loss=17.4636
	step [103/143], loss=17.0977
	step [104/143], loss=15.8134
	step [105/143], loss=15.2087
	step [106/143], loss=16.5357
	step [107/143], loss=14.6973
	step [108/143], loss=15.3502
	step [109/143], loss=14.9953
	step [110/143], loss=15.3719
	step [111/143], loss=16.3734
	step [112/143], loss=18.4020
	step [113/143], loss=16.1781
	step [114/143], loss=16.0582
	step [115/143], loss=15.0054
	step [116/143], loss=15.2440
	step [117/143], loss=14.4058
	step [118/143], loss=15.4980
	step [119/143], loss=16.6144
	step [120/143], loss=14.6967
	step [121/143], loss=13.5652
	step [122/143], loss=15.3640
	step [123/143], loss=15.5561
	step [124/143], loss=15.1292
	step [125/143], loss=15.5201
	step [126/143], loss=17.5470
	step [127/143], loss=15.1931
	step [128/143], loss=15.9694
	step [129/143], loss=15.2051
	step [130/143], loss=17.3211
	step [131/143], loss=14.6174
	step [132/143], loss=14.3224
	step [133/143], loss=14.3117
	step [134/143], loss=14.9811
	step [135/143], loss=15.3833
	step [136/143], loss=14.8525
	step [137/143], loss=14.4853
	step [138/143], loss=15.2767
	step [139/143], loss=15.8190
	step [140/143], loss=15.1693
	step [141/143], loss=15.0306
	step [142/143], loss=14.5071
	step [143/143], loss=12.8518
	Evaluating
	loss=0.0518, precision=0.1994, recall=0.9946, f1=0.3323
Training epoch 13
	step [1/143], loss=14.4015
	step [2/143], loss=16.4983
	step [3/143], loss=14.7881
	step [4/143], loss=16.2874
	step [5/143], loss=15.8676
	step [6/143], loss=17.4071
	step [7/143], loss=15.1570
	step [8/143], loss=15.7256
	step [9/143], loss=15.0564
	step [10/143], loss=16.3522
	step [11/143], loss=14.7892
	step [12/143], loss=15.3793
	step [13/143], loss=15.9392
	step [14/143], loss=14.1742
	step [15/143], loss=15.2235
	step [16/143], loss=15.3105
	step [17/143], loss=15.2993
	step [18/143], loss=14.6241
	step [19/143], loss=16.5123
	step [20/143], loss=15.3588
	step [21/143], loss=16.2019
	step [22/143], loss=14.7693
	step [23/143], loss=15.8127
	step [24/143], loss=13.7617
	step [25/143], loss=14.3586
	step [26/143], loss=14.0075
	step [27/143], loss=14.8992
	step [28/143], loss=14.1422
	step [29/143], loss=16.3288
	step [30/143], loss=15.7788
	step [31/143], loss=15.1002
	step [32/143], loss=14.5759
	step [33/143], loss=13.4810
	step [34/143], loss=14.6780
	step [35/143], loss=15.6326
	step [36/143], loss=15.6175
	step [37/143], loss=15.1257
	step [38/143], loss=16.6620
	step [39/143], loss=15.3598
	step [40/143], loss=15.8204
	step [41/143], loss=15.1409
	step [42/143], loss=15.9087
	step [43/143], loss=15.8497
	step [44/143], loss=14.5634
	step [45/143], loss=14.7250
	step [46/143], loss=14.9379
	step [47/143], loss=16.0859
	step [48/143], loss=14.1919
	step [49/143], loss=17.1872
	step [50/143], loss=14.7963
	step [51/143], loss=15.4892
	step [52/143], loss=13.8818
	step [53/143], loss=15.5092
	step [54/143], loss=14.6455
	step [55/143], loss=13.3863
	step [56/143], loss=13.6288
	step [57/143], loss=13.6529
	step [58/143], loss=14.4068
	step [59/143], loss=14.1394
	step [60/143], loss=14.6660
	step [61/143], loss=14.4387
	step [62/143], loss=13.9227
	step [63/143], loss=14.2069
	step [64/143], loss=15.5911
	step [65/143], loss=15.1016
	step [66/143], loss=16.1110
	step [67/143], loss=16.4198
	step [68/143], loss=15.2402
	step [69/143], loss=14.8824
	step [70/143], loss=15.3407
	step [71/143], loss=15.4339
	step [72/143], loss=14.5026
	step [73/143], loss=16.3954
	step [74/143], loss=14.9990
	step [75/143], loss=15.1981
	step [76/143], loss=13.7051
	step [77/143], loss=17.2265
	step [78/143], loss=16.0846
	step [79/143], loss=15.4919
	step [80/143], loss=14.4307
	step [81/143], loss=13.8701
	step [82/143], loss=14.8668
	step [83/143], loss=16.5613
	step [84/143], loss=14.3680
	step [85/143], loss=13.6705
	step [86/143], loss=15.9541
	step [87/143], loss=14.5327
	step [88/143], loss=17.8731
	step [89/143], loss=14.0240
	step [90/143], loss=14.7714
	step [91/143], loss=15.6296
	step [92/143], loss=15.1926
	step [93/143], loss=14.5327
	step [94/143], loss=16.0299
	step [95/143], loss=13.6930
	step [96/143], loss=13.7180
	step [97/143], loss=14.8315
	step [98/143], loss=13.9679
	step [99/143], loss=14.2215
	step [100/143], loss=15.1334
	step [101/143], loss=15.4044
	step [102/143], loss=16.7342
	step [103/143], loss=14.3226
	step [104/143], loss=13.4155
	step [105/143], loss=15.6717
	step [106/143], loss=15.4078
	step [107/143], loss=14.0999
	step [108/143], loss=14.6467
	step [109/143], loss=15.6633
	step [110/143], loss=13.8647
	step [111/143], loss=15.0436
	step [112/143], loss=15.5146
	step [113/143], loss=14.6436
	step [114/143], loss=15.0417
	step [115/143], loss=15.0897
	step [116/143], loss=15.9326
	step [117/143], loss=13.9732
	step [118/143], loss=15.4750
	step [119/143], loss=13.3566
	step [120/143], loss=14.2829
	step [121/143], loss=14.6431
	step [122/143], loss=15.0665
	step [123/143], loss=15.0509
	step [124/143], loss=13.9647
	step [125/143], loss=14.7775
	step [126/143], loss=13.4917
	step [127/143], loss=14.4603
	step [128/143], loss=14.7733
	step [129/143], loss=13.0944
	step [130/143], loss=13.7136
	step [131/143], loss=14.6471
	step [132/143], loss=14.9134
	step [133/143], loss=14.5025
	step [134/143], loss=14.7624
	step [135/143], loss=13.0253
	step [136/143], loss=14.6334
	step [137/143], loss=15.6062
	step [138/143], loss=14.2337
	step [139/143], loss=13.1201
	step [140/143], loss=14.0601
	step [141/143], loss=14.2721
	step [142/143], loss=14.0207
	step [143/143], loss=10.9182
	Evaluating
	loss=0.0400, precision=0.2619, recall=0.9916, f1=0.4144
saving model as: 1_saved_model.pth
Training epoch 14
	step [1/143], loss=14.0286
	step [2/143], loss=16.4981
	step [3/143], loss=15.7882
	step [4/143], loss=15.8297
	step [5/143], loss=14.3138
	step [6/143], loss=13.9746
	step [7/143], loss=13.9083
	step [8/143], loss=13.6788
	step [9/143], loss=15.3566
	step [10/143], loss=13.0120
	step [11/143], loss=15.1128
	step [12/143], loss=13.2877
	step [13/143], loss=13.6391
	step [14/143], loss=13.2655
	step [15/143], loss=13.4341
	step [16/143], loss=15.7105
	step [17/143], loss=13.5428
	step [18/143], loss=15.1528
	step [19/143], loss=15.4183
	step [20/143], loss=13.9449
	step [21/143], loss=13.1177
	step [22/143], loss=13.7035
	step [23/143], loss=13.9523
	step [24/143], loss=14.1934
	step [25/143], loss=15.3408
	step [26/143], loss=13.6589
	step [27/143], loss=14.8453
	step [28/143], loss=13.7856
	step [29/143], loss=14.8499
	step [30/143], loss=13.3348
	step [31/143], loss=15.1519
	step [32/143], loss=14.0795
	step [33/143], loss=13.5715
	step [34/143], loss=14.1744
	step [35/143], loss=13.2076
	step [36/143], loss=14.0729
	step [37/143], loss=16.1173
	step [38/143], loss=14.7504
	step [39/143], loss=13.5883
	step [40/143], loss=13.5376
	step [41/143], loss=13.9880
	step [42/143], loss=13.1447
	step [43/143], loss=14.9959
	step [44/143], loss=14.0160
	step [45/143], loss=13.8063
	step [46/143], loss=13.8617
	step [47/143], loss=13.9677
	step [48/143], loss=14.7542
	step [49/143], loss=13.1254
	step [50/143], loss=14.3945
	step [51/143], loss=14.3914
	step [52/143], loss=12.5905
	step [53/143], loss=13.1210
	step [54/143], loss=14.9269
	step [55/143], loss=12.6423
	step [56/143], loss=13.3645
	step [57/143], loss=12.8689
	step [58/143], loss=14.3510
	step [59/143], loss=13.9677
	step [60/143], loss=15.0935
	step [61/143], loss=15.1294
	step [62/143], loss=14.6125
	step [63/143], loss=14.5779
	step [64/143], loss=13.1477
	step [65/143], loss=13.6000
	step [66/143], loss=13.9413
	step [67/143], loss=15.1466
	step [68/143], loss=13.6523
	step [69/143], loss=13.7109
	step [70/143], loss=12.2834
	step [71/143], loss=13.9301
	step [72/143], loss=14.8987
	step [73/143], loss=12.9199
	step [74/143], loss=14.8910
	step [75/143], loss=13.8560
	step [76/143], loss=14.6123
	step [77/143], loss=13.3990
	step [78/143], loss=14.5333
	step [79/143], loss=13.8943
	step [80/143], loss=13.6032
	step [81/143], loss=14.4520
	step [82/143], loss=12.9129
	step [83/143], loss=13.5322
	step [84/143], loss=17.1746
	step [85/143], loss=15.1924
	step [86/143], loss=14.1371
	step [87/143], loss=14.0559
	step [88/143], loss=14.1213
	step [89/143], loss=14.3719
	step [90/143], loss=14.8747
	step [91/143], loss=12.6954
	step [92/143], loss=13.0726
	step [93/143], loss=15.4584
	step [94/143], loss=14.6393
	step [95/143], loss=13.0442
	step [96/143], loss=14.2452
	step [97/143], loss=16.0479
	step [98/143], loss=13.1452
	step [99/143], loss=14.6179
	step [100/143], loss=13.8736
	step [101/143], loss=14.0001
	step [102/143], loss=12.8693
	step [103/143], loss=13.8926
	step [104/143], loss=13.8438
	step [105/143], loss=14.0295
	step [106/143], loss=13.0074
	step [107/143], loss=12.8095
	step [108/143], loss=13.1110
	step [109/143], loss=13.3130
	step [110/143], loss=13.6894
	step [111/143], loss=13.5879
	step [112/143], loss=13.9918
	step [113/143], loss=12.6048
	step [114/143], loss=12.8687
	step [115/143], loss=14.1472
	step [116/143], loss=11.6353
	step [117/143], loss=13.5451
	step [118/143], loss=12.6467
	step [119/143], loss=13.5017
	step [120/143], loss=13.2985
	step [121/143], loss=12.7629
	step [122/143], loss=15.2369
	step [123/143], loss=12.8207
	step [124/143], loss=13.4785
	step [125/143], loss=14.2452
	step [126/143], loss=13.6675
	step [127/143], loss=13.0489
	step [128/143], loss=14.4615
	step [129/143], loss=16.0697
	step [130/143], loss=12.3164
	step [131/143], loss=12.9956
	step [132/143], loss=13.2358
	step [133/143], loss=12.2327
	step [134/143], loss=16.1036
	step [135/143], loss=13.5226
	step [136/143], loss=14.2720
	step [137/143], loss=14.1520
	step [138/143], loss=13.8272
	step [139/143], loss=14.3619
	step [140/143], loss=12.4510
	step [141/143], loss=12.5330
	step [142/143], loss=12.4564
	step [143/143], loss=10.3700
	Evaluating
	loss=0.0395, precision=0.2487, recall=0.9915, f1=0.3977
Training epoch 15
	step [1/143], loss=12.1808
	step [2/143], loss=11.9702
	step [3/143], loss=14.4628
	step [4/143], loss=13.8663
	step [5/143], loss=13.3313
	step [6/143], loss=12.7892
	step [7/143], loss=13.4907
	step [8/143], loss=13.4746
	step [9/143], loss=12.5128
	step [10/143], loss=13.0519
	step [11/143], loss=12.9378
	step [12/143], loss=12.0916
	step [13/143], loss=12.8940
	step [14/143], loss=13.6214
	step [15/143], loss=12.2595
	step [16/143], loss=14.6532
	step [17/143], loss=14.2986
	step [18/143], loss=13.0837
	step [19/143], loss=13.5227
	step [20/143], loss=12.9492
	step [21/143], loss=12.4533
	step [22/143], loss=15.2292
	step [23/143], loss=14.4023
	step [24/143], loss=13.5228
	step [25/143], loss=13.1977
	step [26/143], loss=12.4591
	step [27/143], loss=11.9027
	step [28/143], loss=12.7651
	step [29/143], loss=14.4086
	step [30/143], loss=13.1542
	step [31/143], loss=12.6282
	step [32/143], loss=13.1996
	step [33/143], loss=13.0339
	step [34/143], loss=12.7394
	step [35/143], loss=14.2746
	step [36/143], loss=13.3669
	step [37/143], loss=13.4306
	step [38/143], loss=13.3374
	step [39/143], loss=12.5370
	step [40/143], loss=12.9468
	step [41/143], loss=13.5176
	step [42/143], loss=12.6273
	step [43/143], loss=12.8284
	step [44/143], loss=14.7097
	step [45/143], loss=13.3863
	step [46/143], loss=11.0690
	step [47/143], loss=14.4484
	step [48/143], loss=12.6759
	step [49/143], loss=12.1931
	step [50/143], loss=13.1724
	step [51/143], loss=13.3663
	step [52/143], loss=17.0999
	step [53/143], loss=13.6717
	step [54/143], loss=13.0305
	step [55/143], loss=14.2634
	step [56/143], loss=14.1367
	step [57/143], loss=14.4108
	step [58/143], loss=11.7744
	step [59/143], loss=15.4500
	step [60/143], loss=12.9711
	step [61/143], loss=11.6063
	step [62/143], loss=13.0270
	step [63/143], loss=13.0190
	step [64/143], loss=13.1069
	step [65/143], loss=12.9645
	step [66/143], loss=14.1780
	step [67/143], loss=12.4163
	step [68/143], loss=12.8160
	step [69/143], loss=12.0815
	step [70/143], loss=13.4635
	step [71/143], loss=11.9723
	step [72/143], loss=13.1719
	step [73/143], loss=13.8313
	step [74/143], loss=14.0048
	step [75/143], loss=14.8908
	step [76/143], loss=12.6812
	step [77/143], loss=14.8114
	step [78/143], loss=14.2248
	step [79/143], loss=14.6750
	step [80/143], loss=13.6245
	step [81/143], loss=12.8378
	step [82/143], loss=12.6005
	step [83/143], loss=12.6661
	step [84/143], loss=11.8890
	step [85/143], loss=12.3716
	step [86/143], loss=12.6935
	step [87/143], loss=12.8225
	step [88/143], loss=11.6686
	step [89/143], loss=12.9184
	step [90/143], loss=12.6480
	step [91/143], loss=11.6566
	step [92/143], loss=11.1808
	step [93/143], loss=12.3163
	step [94/143], loss=12.6164
	step [95/143], loss=11.9532
	step [96/143], loss=12.5682
	step [97/143], loss=12.8485
	step [98/143], loss=12.9386
	step [99/143], loss=12.9878
	step [100/143], loss=12.9324
	step [101/143], loss=12.1643
	step [102/143], loss=12.3198
	step [103/143], loss=10.8928
	step [104/143], loss=12.7007
	step [105/143], loss=10.9441
	step [106/143], loss=10.9635
	step [107/143], loss=13.5104
	step [108/143], loss=13.0986
	step [109/143], loss=11.5043
	step [110/143], loss=13.9695
	step [111/143], loss=13.1327
	step [112/143], loss=12.9139
	step [113/143], loss=12.3860
	step [114/143], loss=13.2433
	step [115/143], loss=12.3596
	step [116/143], loss=10.5777
	step [117/143], loss=12.2895
	step [118/143], loss=11.8208
	step [119/143], loss=14.5943
	step [120/143], loss=13.2130
	step [121/143], loss=13.0296
	step [122/143], loss=13.8502
	step [123/143], loss=11.8755
	step [124/143], loss=12.7955
	step [125/143], loss=13.1151
	step [126/143], loss=12.3635
	step [127/143], loss=11.9810
	step [128/143], loss=11.8630
	step [129/143], loss=11.5035
	step [130/143], loss=12.5455
	step [131/143], loss=14.0762
	step [132/143], loss=12.5788
	step [133/143], loss=12.8193
	step [134/143], loss=12.3010
	step [135/143], loss=12.6011
	step [136/143], loss=12.9191
	step [137/143], loss=13.4289
	step [138/143], loss=12.4031
	step [139/143], loss=11.7465
	step [140/143], loss=12.6408
	step [141/143], loss=11.6041
	step [142/143], loss=13.1970
	step [143/143], loss=11.7849
	Evaluating
	loss=0.0366, precision=0.2519, recall=0.9916, f1=0.4017
Training epoch 16
	step [1/143], loss=11.3025
	step [2/143], loss=11.8286
	step [3/143], loss=11.2256
	step [4/143], loss=13.7532
	step [5/143], loss=11.9967
	step [6/143], loss=12.6989
	step [7/143], loss=11.8172
	step [8/143], loss=12.3111
	step [9/143], loss=13.7587
	step [10/143], loss=12.7182
	step [11/143], loss=11.0216
	step [12/143], loss=13.1352
	step [13/143], loss=12.8724
	step [14/143], loss=12.2011
	step [15/143], loss=12.5731
	step [16/143], loss=12.7318
	step [17/143], loss=12.4394
	step [18/143], loss=12.3566
	step [19/143], loss=12.0045
	step [20/143], loss=12.5351
	step [21/143], loss=12.4066
	step [22/143], loss=12.5262
	step [23/143], loss=11.8356
	step [24/143], loss=14.7993
	step [25/143], loss=12.0312
	step [26/143], loss=12.6294
	step [27/143], loss=12.4661
	step [28/143], loss=11.6615
	step [29/143], loss=11.8064
	step [30/143], loss=11.8938
	step [31/143], loss=14.2771
	step [32/143], loss=11.6033
	step [33/143], loss=12.7009
	step [34/143], loss=11.8696
	step [35/143], loss=12.3307
	step [36/143], loss=12.2648
	step [37/143], loss=12.8039
	step [38/143], loss=11.8020
	step [39/143], loss=11.7039
	step [40/143], loss=12.5534
	step [41/143], loss=12.1876
	step [42/143], loss=13.4588
	step [43/143], loss=12.8726
	step [44/143], loss=12.1588
	step [45/143], loss=12.9445
	step [46/143], loss=11.0280
	step [47/143], loss=12.3789
	step [48/143], loss=12.4198
	step [49/143], loss=11.6681
	step [50/143], loss=13.8391
	step [51/143], loss=13.4983
	step [52/143], loss=13.0670
	step [53/143], loss=11.3214
	step [54/143], loss=12.1025
	step [55/143], loss=12.0594
	step [56/143], loss=11.5513
	step [57/143], loss=11.6165
	step [58/143], loss=12.8077
	step [59/143], loss=11.8628
	step [60/143], loss=12.0478
	step [61/143], loss=12.3018
	step [62/143], loss=11.4955
	step [63/143], loss=14.8007
	step [64/143], loss=11.1896
	step [65/143], loss=10.9437
	step [66/143], loss=14.3637
	step [67/143], loss=11.9948
	step [68/143], loss=12.8730
	step [69/143], loss=11.7106
	step [70/143], loss=11.6196
	step [71/143], loss=11.1904
	step [72/143], loss=12.9066
	step [73/143], loss=11.5757
	step [74/143], loss=12.9761
	step [75/143], loss=11.2212
	step [76/143], loss=10.8533
	step [77/143], loss=11.8934
	step [78/143], loss=14.1598
	step [79/143], loss=12.7506
	step [80/143], loss=11.3236
	step [81/143], loss=12.0775
	step [82/143], loss=13.4774
	step [83/143], loss=11.3660
	step [84/143], loss=12.8125
	step [85/143], loss=11.3784
	step [86/143], loss=12.4547
	step [87/143], loss=12.2401
	step [88/143], loss=12.6382
	step [89/143], loss=12.2215
	step [90/143], loss=11.5974
	step [91/143], loss=12.2890
	step [92/143], loss=11.8270
	step [93/143], loss=13.5811
	step [94/143], loss=13.8897
	step [95/143], loss=13.6230
	step [96/143], loss=12.5894
	step [97/143], loss=12.1168
	step [98/143], loss=12.4673
	step [99/143], loss=10.5001
	step [100/143], loss=11.8215
	step [101/143], loss=12.7159
	step [102/143], loss=11.7475
	step [103/143], loss=12.5652
	step [104/143], loss=12.7391
	step [105/143], loss=12.0949
	step [106/143], loss=12.4484
	step [107/143], loss=14.2765
	step [108/143], loss=10.6577
	step [109/143], loss=10.5719
	step [110/143], loss=11.6275
	step [111/143], loss=12.3254
	step [112/143], loss=13.0797
	step [113/143], loss=12.5832
	step [114/143], loss=13.2253
	step [115/143], loss=12.0094
	step [116/143], loss=11.7800
	step [117/143], loss=12.0428
	step [118/143], loss=13.2677
	step [119/143], loss=11.6774
	step [120/143], loss=11.9852
	step [121/143], loss=12.1806
	step [122/143], loss=10.7667
	step [123/143], loss=11.7744
	step [124/143], loss=11.3761
	step [125/143], loss=12.2106
	step [126/143], loss=12.4631
	step [127/143], loss=11.3465
	step [128/143], loss=9.9379
	step [129/143], loss=12.7665
	step [130/143], loss=13.1900
	step [131/143], loss=12.3870
	step [132/143], loss=11.8745
	step [133/143], loss=13.5312
	step [134/143], loss=11.6730
	step [135/143], loss=13.0502
	step [136/143], loss=12.9213
	step [137/143], loss=11.5419
	step [138/143], loss=13.0587
	step [139/143], loss=12.9530
	step [140/143], loss=11.1152
	step [141/143], loss=13.5743
	step [142/143], loss=12.1121
	step [143/143], loss=9.1120
	Evaluating
	loss=0.0331, precision=0.2522, recall=0.9899, f1=0.4020
Training epoch 17
	step [1/143], loss=11.7135
	step [2/143], loss=11.1721
	step [3/143], loss=10.0283
	step [4/143], loss=10.9900
	step [5/143], loss=10.7690
	step [6/143], loss=12.1751
	step [7/143], loss=13.2863
	step [8/143], loss=10.8375
	step [9/143], loss=11.8962
	step [10/143], loss=12.9565
	step [11/143], loss=11.4129
	step [12/143], loss=10.9716
	step [13/143], loss=11.2291
	step [14/143], loss=13.0643
	step [15/143], loss=11.5583
	step [16/143], loss=11.2588
	step [17/143], loss=10.8966
	step [18/143], loss=14.4360
	step [19/143], loss=11.8227
	step [20/143], loss=11.7784
	step [21/143], loss=12.2795
	step [22/143], loss=12.4738
	step [23/143], loss=12.5208
	step [24/143], loss=12.7721
	step [25/143], loss=11.4565
	step [26/143], loss=11.4573
	step [27/143], loss=12.1413
	step [28/143], loss=11.2954
	step [29/143], loss=11.3328
	step [30/143], loss=10.5318
	step [31/143], loss=12.9933
	step [32/143], loss=11.7244
	step [33/143], loss=12.0718
	step [34/143], loss=12.1469
	step [35/143], loss=10.3067
	step [36/143], loss=10.1305
	step [37/143], loss=11.4041
	step [38/143], loss=10.5852
	step [39/143], loss=14.7583
	step [40/143], loss=11.1891
	step [41/143], loss=12.0210
	step [42/143], loss=10.9716
	step [43/143], loss=10.9539
	step [44/143], loss=12.6723
	step [45/143], loss=11.9003
	step [46/143], loss=11.1067
	step [47/143], loss=11.0185
	step [48/143], loss=11.5422
	step [49/143], loss=12.9164
	step [50/143], loss=13.5108
	step [51/143], loss=12.0687
	step [52/143], loss=11.7657
	step [53/143], loss=10.8089
	step [54/143], loss=10.9216
	step [55/143], loss=11.2764
	step [56/143], loss=11.6779
	step [57/143], loss=12.6205
	step [58/143], loss=10.7405
	step [59/143], loss=10.6592
	step [60/143], loss=10.4663
	step [61/143], loss=10.8919
	step [62/143], loss=11.0661
	step [63/143], loss=11.0606
	step [64/143], loss=10.6058
	step [65/143], loss=11.2049
	step [66/143], loss=11.8204
	step [67/143], loss=11.0745
	step [68/143], loss=10.1881
	step [69/143], loss=13.2716
	step [70/143], loss=11.0111
	step [71/143], loss=11.0006
	step [72/143], loss=10.7426
	step [73/143], loss=11.1993
	step [74/143], loss=11.0668
	step [75/143], loss=12.4875
	step [76/143], loss=10.0042
	step [77/143], loss=10.4941
	step [78/143], loss=12.2223
	step [79/143], loss=11.2468
	step [80/143], loss=12.0026
	step [81/143], loss=13.7999
	step [82/143], loss=11.4284
	step [83/143], loss=11.1628
	step [84/143], loss=10.6493
	step [85/143], loss=9.6541
	step [86/143], loss=11.1996
	step [87/143], loss=11.0370
	step [88/143], loss=11.6803
	step [89/143], loss=11.1102
	step [90/143], loss=12.3079
	step [91/143], loss=11.7564
	step [92/143], loss=10.1439
	step [93/143], loss=12.3081
	step [94/143], loss=10.5646
	step [95/143], loss=11.8574
	step [96/143], loss=11.3807
	step [97/143], loss=10.4501
	step [98/143], loss=10.8606
	step [99/143], loss=10.5692
	step [100/143], loss=10.8081
	step [101/143], loss=11.4239
	step [102/143], loss=11.7049
	step [103/143], loss=10.2080
	step [104/143], loss=12.5399
	step [105/143], loss=10.8679
	step [106/143], loss=11.9713
	step [107/143], loss=11.9487
	step [108/143], loss=11.3171
	step [109/143], loss=11.6397
	step [110/143], loss=10.9927
	step [111/143], loss=11.2281
	step [112/143], loss=9.8739
	step [113/143], loss=12.8326
	step [114/143], loss=11.6734
	step [115/143], loss=10.4520
	step [116/143], loss=11.2741
	step [117/143], loss=10.2528
	step [118/143], loss=12.6070
	step [119/143], loss=12.8307
	step [120/143], loss=11.8797
	step [121/143], loss=11.4319
	step [122/143], loss=11.0127
	step [123/143], loss=11.9217
	step [124/143], loss=10.9039
	step [125/143], loss=11.7216
	step [126/143], loss=10.1247
	step [127/143], loss=10.8270
	step [128/143], loss=11.3290
	step [129/143], loss=10.7161
	step [130/143], loss=10.9975
	step [131/143], loss=10.5095
	step [132/143], loss=11.3748
	step [133/143], loss=11.3693
	step [134/143], loss=12.3734
	step [135/143], loss=10.8752
	step [136/143], loss=11.9390
	step [137/143], loss=10.7598
	step [138/143], loss=11.0561
	step [139/143], loss=11.9129
	step [140/143], loss=10.7527
	step [141/143], loss=11.7700
	step [142/143], loss=11.2288
	step [143/143], loss=10.7521
	Evaluating
	loss=0.0353, precision=0.2445, recall=0.9921, f1=0.3923
Training epoch 18
	step [1/143], loss=10.9206
	step [2/143], loss=10.6470
	step [3/143], loss=11.7523
	step [4/143], loss=10.6523
	step [5/143], loss=11.7014
	step [6/143], loss=10.8235
	step [7/143], loss=11.3184
	step [8/143], loss=11.3481
	step [9/143], loss=10.3390
	step [10/143], loss=10.6231
	step [11/143], loss=12.1934
	step [12/143], loss=12.3707
	step [13/143], loss=11.1477
	step [14/143], loss=11.0041
	step [15/143], loss=10.4280
	step [16/143], loss=10.3606
	step [17/143], loss=10.5463
	step [18/143], loss=11.6963
	step [19/143], loss=11.4556
	step [20/143], loss=11.5387
	step [21/143], loss=11.5537
	step [22/143], loss=10.0752
	step [23/143], loss=10.7124
	step [24/143], loss=10.3203
	step [25/143], loss=11.5723
	step [26/143], loss=11.5582
	step [27/143], loss=10.4219
	step [28/143], loss=10.2596
	step [29/143], loss=11.1932
	step [30/143], loss=10.9303
	step [31/143], loss=10.2931
	step [32/143], loss=10.5607
	step [33/143], loss=10.8490
	step [34/143], loss=10.4526
	step [35/143], loss=10.9822
	step [36/143], loss=10.3098
	step [37/143], loss=10.2427
	step [38/143], loss=12.1165
	step [39/143], loss=10.8688
	step [40/143], loss=10.6059
	step [41/143], loss=10.1438
	step [42/143], loss=11.3038
	step [43/143], loss=10.1780
	step [44/143], loss=12.6860
	step [45/143], loss=11.5421
	step [46/143], loss=11.6767
	step [47/143], loss=10.8449
	step [48/143], loss=11.0001
	step [49/143], loss=11.6905
	step [50/143], loss=10.6536
	step [51/143], loss=10.8879
	step [52/143], loss=11.2015
	step [53/143], loss=10.5142
	step [54/143], loss=10.8212
	step [55/143], loss=11.6088
	step [56/143], loss=11.2889
	step [57/143], loss=12.2053
	step [58/143], loss=11.9341
	step [59/143], loss=11.6923
	step [60/143], loss=10.4960
	step [61/143], loss=11.2416
	step [62/143], loss=12.6269
	step [63/143], loss=10.2004
	step [64/143], loss=9.9273
	step [65/143], loss=11.5552
	step [66/143], loss=11.0399
	step [67/143], loss=12.4858
	step [68/143], loss=9.0766
	step [69/143], loss=10.0457
	step [70/143], loss=10.5746
	step [71/143], loss=10.3738
	step [72/143], loss=11.3175
	step [73/143], loss=10.5693
	step [74/143], loss=11.4015
	step [75/143], loss=10.0154
	step [76/143], loss=11.1566
	step [77/143], loss=10.4447
	step [78/143], loss=9.4978
	step [79/143], loss=11.1008
	step [80/143], loss=11.0880
	step [81/143], loss=10.5293
	step [82/143], loss=11.4081
	step [83/143], loss=11.9477
	step [84/143], loss=10.1444
	step [85/143], loss=9.9839
	step [86/143], loss=9.9358
	step [87/143], loss=11.3637
	step [88/143], loss=10.0741
	step [89/143], loss=10.8634
	step [90/143], loss=11.6137
	step [91/143], loss=10.7638
	step [92/143], loss=11.8343
	step [93/143], loss=11.4155
	step [94/143], loss=11.8175
	step [95/143], loss=10.7395
	step [96/143], loss=10.4677
	step [97/143], loss=10.8544
	step [98/143], loss=11.2192
	step [99/143], loss=9.5564
	step [100/143], loss=10.7463
	step [101/143], loss=11.2289
	step [102/143], loss=10.0401
	step [103/143], loss=11.7708
	step [104/143], loss=12.3405
	step [105/143], loss=10.5858
	step [106/143], loss=10.8067
	step [107/143], loss=11.2734
	step [108/143], loss=11.1248
	step [109/143], loss=11.9188
	step [110/143], loss=10.3337
	step [111/143], loss=11.7156
	step [112/143], loss=11.0428
	step [113/143], loss=11.3616
	step [114/143], loss=9.8413
	step [115/143], loss=11.2048
	step [116/143], loss=9.6141
	step [117/143], loss=9.4125
	step [118/143], loss=11.6927
	step [119/143], loss=9.2406
	step [120/143], loss=10.4623
	step [121/143], loss=10.5502
	step [122/143], loss=11.3792
	step [123/143], loss=11.5675
	step [124/143], loss=10.7350
	step [125/143], loss=10.8828
	step [126/143], loss=10.3655
	step [127/143], loss=10.5852
	step [128/143], loss=10.6888
	step [129/143], loss=9.8731
	step [130/143], loss=11.4484
	step [131/143], loss=10.7607
	step [132/143], loss=11.2292
	step [133/143], loss=10.5527
	step [134/143], loss=11.8312
	step [135/143], loss=10.9284
	step [136/143], loss=11.2195
	step [137/143], loss=11.0226
	step [138/143], loss=10.6877
	step [139/143], loss=10.4072
	step [140/143], loss=10.9625
	step [141/143], loss=10.7795
	step [142/143], loss=11.7308
	step [143/143], loss=7.8906
	Evaluating
	loss=0.0299, precision=0.2738, recall=0.9907, f1=0.4291
saving model as: 1_saved_model.pth
Training epoch 19
	step [1/143], loss=10.9655
	step [2/143], loss=11.3234
	step [3/143], loss=12.3974
	step [4/143], loss=10.1261
	step [5/143], loss=10.2243
	step [6/143], loss=10.6803
	step [7/143], loss=9.4657
	step [8/143], loss=11.4038
	step [9/143], loss=10.8214
	step [10/143], loss=11.1200
	step [11/143], loss=10.6347
	step [12/143], loss=9.9796
	step [13/143], loss=9.9538
	step [14/143], loss=9.7098
	step [15/143], loss=10.2903
	step [16/143], loss=10.5339
	step [17/143], loss=10.2001
	step [18/143], loss=12.5685
	step [19/143], loss=10.5511
	step [20/143], loss=9.8242
	step [21/143], loss=11.3896
	step [22/143], loss=10.4143
	step [23/143], loss=11.0773
	step [24/143], loss=10.5603
	step [25/143], loss=9.4120
	step [26/143], loss=9.4485
	step [27/143], loss=9.4305
	step [28/143], loss=11.3279
	step [29/143], loss=9.9517
	step [30/143], loss=10.4405
	step [31/143], loss=10.5459
	step [32/143], loss=10.4108
	step [33/143], loss=10.1381
	step [34/143], loss=9.3763
	step [35/143], loss=10.4583
	step [36/143], loss=12.6240
	step [37/143], loss=11.0890
	step [38/143], loss=11.0676
	step [39/143], loss=11.0186
	step [40/143], loss=10.3616
	step [41/143], loss=10.0609
	step [42/143], loss=9.9787
	step [43/143], loss=10.8092
	step [44/143], loss=12.7113
	step [45/143], loss=11.2949
	step [46/143], loss=11.1388
	step [47/143], loss=11.4813
	step [48/143], loss=11.0316
	step [49/143], loss=10.0308
	step [50/143], loss=9.7240
	step [51/143], loss=10.4815
	step [52/143], loss=10.1080
	step [53/143], loss=10.1185
	step [54/143], loss=10.4449
	step [55/143], loss=10.8948
	step [56/143], loss=10.1301
	step [57/143], loss=9.7396
	step [58/143], loss=11.2798
	step [59/143], loss=9.8464
	step [60/143], loss=11.4892
	step [61/143], loss=11.2005
	step [62/143], loss=9.5133
	step [63/143], loss=11.0627
	step [64/143], loss=10.4303
	step [65/143], loss=9.1076
	step [66/143], loss=10.6310
	step [67/143], loss=9.4162
	step [68/143], loss=12.7431
	step [69/143], loss=9.6508
	step [70/143], loss=11.0244
	step [71/143], loss=10.3025
	step [72/143], loss=10.5113
	step [73/143], loss=10.7211
	step [74/143], loss=9.9003
	step [75/143], loss=9.5278
	step [76/143], loss=9.8617
	step [77/143], loss=9.6880
	step [78/143], loss=8.4456
	step [79/143], loss=10.5334
	step [80/143], loss=8.8407
	step [81/143], loss=12.2863
	step [82/143], loss=9.9777
	step [83/143], loss=10.7567
	step [84/143], loss=8.9034
	step [85/143], loss=10.8670
	step [86/143], loss=9.8097
	step [87/143], loss=9.7501
	step [88/143], loss=11.3941
	step [89/143], loss=10.3744
	step [90/143], loss=9.9346
	step [91/143], loss=10.5745
	step [92/143], loss=9.8075
	step [93/143], loss=11.9360
	step [94/143], loss=10.5180
	step [95/143], loss=8.9262
	step [96/143], loss=10.0773
	step [97/143], loss=10.4417
	step [98/143], loss=9.7126
	step [99/143], loss=9.1921
	step [100/143], loss=8.1586
	step [101/143], loss=10.1971
	step [102/143], loss=9.3206
	step [103/143], loss=10.0106
	step [104/143], loss=8.8900
	step [105/143], loss=11.8365
	step [106/143], loss=11.0937
	step [107/143], loss=10.1003
	step [108/143], loss=9.4590
	step [109/143], loss=10.2071
	step [110/143], loss=11.0130
	step [111/143], loss=10.7821
	step [112/143], loss=9.9516
	step [113/143], loss=10.3278
	step [114/143], loss=9.7025
	step [115/143], loss=9.1367
	step [116/143], loss=10.4594
	step [117/143], loss=10.6304
	step [118/143], loss=10.4799
	step [119/143], loss=10.9030
	step [120/143], loss=9.5528
	step [121/143], loss=9.4808
	step [122/143], loss=9.7202
	step [123/143], loss=12.0798
	step [124/143], loss=10.1027
	step [125/143], loss=11.0727
	step [126/143], loss=9.8833
	step [127/143], loss=10.5943
	step [128/143], loss=10.1422
	step [129/143], loss=11.1784
	step [130/143], loss=9.3394
	step [131/143], loss=9.9627
	step [132/143], loss=9.8588
	step [133/143], loss=9.5305
	step [134/143], loss=10.6098
	step [135/143], loss=11.6597
	step [136/143], loss=10.5064
	step [137/143], loss=11.1910
	step [138/143], loss=11.4028
	step [139/143], loss=10.0845
	step [140/143], loss=9.7996
	step [141/143], loss=10.9622
	step [142/143], loss=10.9063
	step [143/143], loss=7.8606
	Evaluating
	loss=0.0276, precision=0.2781, recall=0.9901, f1=0.4342
saving model as: 1_saved_model.pth
Training epoch 20
	step [1/143], loss=9.1967
	step [2/143], loss=9.3426
	step [3/143], loss=9.7756
	step [4/143], loss=10.2793
	step [5/143], loss=10.7348
	step [6/143], loss=10.6837
	step [7/143], loss=10.3822
	step [8/143], loss=11.6939
	step [9/143], loss=9.6286
	step [10/143], loss=9.7241
	step [11/143], loss=10.6503
	step [12/143], loss=10.1484
	step [13/143], loss=10.5857
	step [14/143], loss=10.2367
	step [15/143], loss=9.7771
	step [16/143], loss=10.0991
	step [17/143], loss=9.3947
	step [18/143], loss=9.1403
	step [19/143], loss=12.7985
	step [20/143], loss=11.6115
	step [21/143], loss=10.3990
	step [22/143], loss=9.9554
	step [23/143], loss=9.8053
	step [24/143], loss=11.0575
	step [25/143], loss=10.5222
	step [26/143], loss=9.9908
	step [27/143], loss=11.0163
	step [28/143], loss=9.8455
	step [29/143], loss=11.0028
	step [30/143], loss=10.0719
	step [31/143], loss=9.7403
	step [32/143], loss=10.1011
	step [33/143], loss=10.5479
	step [34/143], loss=9.5093
	step [35/143], loss=9.4853
	step [36/143], loss=11.0239
	step [37/143], loss=10.1212
	step [38/143], loss=9.3824
	step [39/143], loss=11.2620
	step [40/143], loss=10.5202
	step [41/143], loss=10.2669
	step [42/143], loss=9.9518
	step [43/143], loss=10.4598
	step [44/143], loss=10.5846
	step [45/143], loss=9.9794
	step [46/143], loss=10.5217
	step [47/143], loss=9.2572
	step [48/143], loss=10.3966
	step [49/143], loss=9.8859
	step [50/143], loss=9.5608
	step [51/143], loss=9.1517
	step [52/143], loss=9.9278
	step [53/143], loss=8.9633
	step [54/143], loss=10.4993
	step [55/143], loss=10.8579
	step [56/143], loss=9.9128
	step [57/143], loss=10.3245
	step [58/143], loss=10.0434
	step [59/143], loss=9.6366
	step [60/143], loss=9.5200
	step [61/143], loss=9.1176
	step [62/143], loss=10.6369
	step [63/143], loss=9.4432
	step [64/143], loss=9.9972
	step [65/143], loss=9.5843
	step [66/143], loss=9.5499
	step [67/143], loss=11.2115
	step [68/143], loss=9.8421
	step [69/143], loss=10.2281
	step [70/143], loss=9.0946
	step [71/143], loss=10.1856
	step [72/143], loss=9.9948
	step [73/143], loss=10.1385
	step [74/143], loss=10.3736
	step [75/143], loss=11.2039
	step [76/143], loss=11.9304
	step [77/143], loss=9.5413
	step [78/143], loss=8.8633
	step [79/143], loss=9.3895
	step [80/143], loss=8.6779
	step [81/143], loss=9.3378
	step [82/143], loss=10.4057
	step [83/143], loss=9.9066
	step [84/143], loss=10.6602
	step [85/143], loss=10.0627
	step [86/143], loss=10.1219
	step [87/143], loss=9.5195
	step [88/143], loss=9.5641
	step [89/143], loss=10.2923
	step [90/143], loss=10.0376
	step [91/143], loss=10.6855
	step [92/143], loss=9.1528
	step [93/143], loss=10.0940
	step [94/143], loss=9.9968
	step [95/143], loss=10.1243
	step [96/143], loss=9.3591
	step [97/143], loss=11.7327
	step [98/143], loss=9.5313
	step [99/143], loss=10.3289
	step [100/143], loss=10.1853
	step [101/143], loss=9.8680
	step [102/143], loss=8.8209
	step [103/143], loss=10.8410
	step [104/143], loss=10.4769
	step [105/143], loss=9.4922
	step [106/143], loss=10.4614
	step [107/143], loss=9.0732
	step [108/143], loss=10.0833
	step [109/143], loss=9.7467
	step [110/143], loss=9.4349
	step [111/143], loss=9.3287
	step [112/143], loss=9.1735
	step [113/143], loss=10.1728
	step [114/143], loss=10.1120
	step [115/143], loss=9.0443
	step [116/143], loss=10.3034
	step [117/143], loss=10.7942
	step [118/143], loss=10.0225
	step [119/143], loss=10.3339
	step [120/143], loss=9.4655
	step [121/143], loss=9.9192
	step [122/143], loss=10.4124
	step [123/143], loss=9.2292
	step [124/143], loss=9.5299
	step [125/143], loss=9.1369
	step [126/143], loss=10.9291
	step [127/143], loss=9.5291
	step [128/143], loss=10.4018
	step [129/143], loss=9.4281
	step [130/143], loss=10.4005
	step [131/143], loss=9.1568
	step [132/143], loss=8.4347
	step [133/143], loss=10.1572
	step [134/143], loss=9.2379
	step [135/143], loss=8.3006
	step [136/143], loss=9.6651
	step [137/143], loss=9.1606
	step [138/143], loss=9.6657
	step [139/143], loss=8.7467
	step [140/143], loss=11.4768
	step [141/143], loss=9.1598
	step [142/143], loss=9.0694
	step [143/143], loss=7.9848
	Evaluating
	loss=0.0256, precision=0.2951, recall=0.9886, f1=0.4545
saving model as: 1_saved_model.pth
Training epoch 21
	step [1/143], loss=10.3433
	step [2/143], loss=9.4416
	step [3/143], loss=9.7003
	step [4/143], loss=9.0224
	step [5/143], loss=10.0727
	step [6/143], loss=9.5256
	step [7/143], loss=9.9873
	step [8/143], loss=8.9212
	step [9/143], loss=8.6427
	step [10/143], loss=9.1424
	step [11/143], loss=9.1649
	step [12/143], loss=9.4133
	step [13/143], loss=10.0897
	step [14/143], loss=10.4005
	step [15/143], loss=9.9869
	step [16/143], loss=9.5448
	step [17/143], loss=8.9765
	step [18/143], loss=8.7024
	step [19/143], loss=10.7522
	step [20/143], loss=9.9663
	step [21/143], loss=10.9313
	step [22/143], loss=9.5914
	step [23/143], loss=10.2088
	step [24/143], loss=10.1989
	step [25/143], loss=9.0377
	step [26/143], loss=9.6650
	step [27/143], loss=9.6736
	step [28/143], loss=8.8010
	step [29/143], loss=10.1605
	step [30/143], loss=8.8795
	step [31/143], loss=8.6882
	step [32/143], loss=9.2387
	step [33/143], loss=8.9035
	step [34/143], loss=9.9742
	step [35/143], loss=9.2890
	step [36/143], loss=8.1766
	step [37/143], loss=9.9895
	step [38/143], loss=8.3983
	step [39/143], loss=9.6071
	step [40/143], loss=9.0483
	step [41/143], loss=9.5768
	step [42/143], loss=8.5064
	step [43/143], loss=9.3814
	step [44/143], loss=11.2307
	step [45/143], loss=8.2628
	step [46/143], loss=9.9609
	step [47/143], loss=9.8940
	step [48/143], loss=10.7141
	step [49/143], loss=9.4127
	step [50/143], loss=8.9119
	step [51/143], loss=9.8770
	step [52/143], loss=9.6136
	step [53/143], loss=9.3954
	step [54/143], loss=9.6811
	step [55/143], loss=8.5558
	step [56/143], loss=9.6307
	step [57/143], loss=10.6182
	step [58/143], loss=9.7467
	step [59/143], loss=10.2573
	step [60/143], loss=9.3430
	step [61/143], loss=10.2781
	step [62/143], loss=9.2949
	step [63/143], loss=9.2891
	step [64/143], loss=10.2816
	step [65/143], loss=8.4945
	step [66/143], loss=8.9037
	step [67/143], loss=8.5523
	step [68/143], loss=9.6978
	step [69/143], loss=10.0560
	step [70/143], loss=7.9704
	step [71/143], loss=7.9981
	step [72/143], loss=8.3538
	step [73/143], loss=8.5766
	step [74/143], loss=8.6576
	step [75/143], loss=10.4964
	step [76/143], loss=9.5022
	step [77/143], loss=9.4595
	step [78/143], loss=9.9492
	step [79/143], loss=9.5693
	step [80/143], loss=10.2639
	step [81/143], loss=10.2092
	step [82/143], loss=9.2021
	step [83/143], loss=9.6850
	step [84/143], loss=9.1629
	step [85/143], loss=9.0076
	step [86/143], loss=9.7339
	step [87/143], loss=9.0691
	step [88/143], loss=9.5591
	step [89/143], loss=10.3773
	step [90/143], loss=8.5043
	step [91/143], loss=9.1914
	step [92/143], loss=9.3168
	step [93/143], loss=9.6503
	step [94/143], loss=9.9079
	step [95/143], loss=8.3226
	step [96/143], loss=8.6445
	step [97/143], loss=9.6151
	step [98/143], loss=9.8754
	step [99/143], loss=9.8113
	step [100/143], loss=8.8939
	step [101/143], loss=10.2297
	step [102/143], loss=9.0811
	step [103/143], loss=8.8325
	step [104/143], loss=9.0825
	step [105/143], loss=9.7875
	step [106/143], loss=11.4927
	step [107/143], loss=9.3700
	step [108/143], loss=8.5185
	step [109/143], loss=8.8975
	step [110/143], loss=9.5214
	step [111/143], loss=9.9327
	step [112/143], loss=9.5369
	step [113/143], loss=9.1540
	step [114/143], loss=10.6242
	step [115/143], loss=9.4287
	step [116/143], loss=8.5699
	step [117/143], loss=9.3972
	step [118/143], loss=8.9914
	step [119/143], loss=9.8376
	step [120/143], loss=10.3876
	step [121/143], loss=9.0462
	step [122/143], loss=9.5689
	step [123/143], loss=9.5636
	step [124/143], loss=8.1470
	step [125/143], loss=9.1881
	step [126/143], loss=11.1724
	step [127/143], loss=8.7336
	step [128/143], loss=9.6297
	step [129/143], loss=9.8529
	step [130/143], loss=10.8762
	step [131/143], loss=9.8253
	step [132/143], loss=9.6093
	step [133/143], loss=11.1530
	step [134/143], loss=9.5439
	step [135/143], loss=9.3740
	step [136/143], loss=9.6333
	step [137/143], loss=9.1098
	step [138/143], loss=8.7906
	step [139/143], loss=9.0779
	step [140/143], loss=9.9116
	step [141/143], loss=8.4707
	step [142/143], loss=8.5178
	step [143/143], loss=8.8523
	Evaluating
	loss=0.0287, precision=0.2585, recall=0.9905, f1=0.4100
Training epoch 22
	step [1/143], loss=9.2519
	step [2/143], loss=8.4150
	step [3/143], loss=9.3249
	step [4/143], loss=9.2505
	step [5/143], loss=9.1111
	step [6/143], loss=9.6949
	step [7/143], loss=10.0637
	step [8/143], loss=9.1258
	step [9/143], loss=10.4049
	step [10/143], loss=9.8129
	step [11/143], loss=8.5772
	step [12/143], loss=9.7882
	step [13/143], loss=9.3772
	step [14/143], loss=8.1732
	step [15/143], loss=7.6817
	step [16/143], loss=7.8541
	step [17/143], loss=8.6654
	step [18/143], loss=8.4828
	step [19/143], loss=9.7571
	step [20/143], loss=10.1369
	step [21/143], loss=7.3916
	step [22/143], loss=8.7101
	step [23/143], loss=8.6718
	step [24/143], loss=9.9348
	step [25/143], loss=9.7698
	step [26/143], loss=8.3091
	step [27/143], loss=9.7793
	step [28/143], loss=10.3395
	step [29/143], loss=9.5110
	step [30/143], loss=9.4596
	step [31/143], loss=9.8641
	step [32/143], loss=8.7029
	step [33/143], loss=9.0987
	step [34/143], loss=9.9143
	step [35/143], loss=10.5771
	step [36/143], loss=9.7768
	step [37/143], loss=9.2149
	step [38/143], loss=8.5855
	step [39/143], loss=9.5151
	step [40/143], loss=9.4192
	step [41/143], loss=8.0932
	step [42/143], loss=8.2943
	step [43/143], loss=11.6129
	step [44/143], loss=10.1014
	step [45/143], loss=9.4227
	step [46/143], loss=9.2960
	step [47/143], loss=9.1796
	step [48/143], loss=9.0526
	step [49/143], loss=8.8883
	step [50/143], loss=8.6451
	step [51/143], loss=10.8093
	step [52/143], loss=9.0847
	step [53/143], loss=9.3226
	step [54/143], loss=10.0177
	step [55/143], loss=10.2058
	step [56/143], loss=9.3595
	step [57/143], loss=8.8883
	step [58/143], loss=9.4762
	step [59/143], loss=9.6711
	step [60/143], loss=8.5762
	step [61/143], loss=10.0887
	step [62/143], loss=8.9691
	step [63/143], loss=8.5205
	step [64/143], loss=10.4445
	step [65/143], loss=8.8655
	step [66/143], loss=9.0667
	step [67/143], loss=8.1746
	step [68/143], loss=7.4795
	step [69/143], loss=10.8233
	step [70/143], loss=8.0922
	step [71/143], loss=8.7719
	step [72/143], loss=10.3778
	step [73/143], loss=8.7712
	step [74/143], loss=9.4992
	step [75/143], loss=8.7791
	step [76/143], loss=7.7773
	step [77/143], loss=9.8676
	step [78/143], loss=10.1109
	step [79/143], loss=8.1910
	step [80/143], loss=7.5967
	step [81/143], loss=9.1138
	step [82/143], loss=8.5951
	step [83/143], loss=8.8028
	step [84/143], loss=9.5620
	step [85/143], loss=9.3038
	step [86/143], loss=9.6817
	step [87/143], loss=8.9888
	step [88/143], loss=9.0336
	step [89/143], loss=9.9603
	step [90/143], loss=8.9584
	step [91/143], loss=10.1117
	step [92/143], loss=9.2824
	step [93/143], loss=8.2991
	step [94/143], loss=9.4944
	step [95/143], loss=9.6733
	step [96/143], loss=9.8291
	step [97/143], loss=9.4274
	step [98/143], loss=9.1441
	step [99/143], loss=8.5520
	step [100/143], loss=9.8095
	step [101/143], loss=9.5915
	step [102/143], loss=8.8686
	step [103/143], loss=9.6726
	step [104/143], loss=8.5711
	step [105/143], loss=8.9264
	step [106/143], loss=9.6875
	step [107/143], loss=8.8299
	step [108/143], loss=8.6856
	step [109/143], loss=8.7911
	step [110/143], loss=8.8334
	step [111/143], loss=9.3663
	step [112/143], loss=8.8646
	step [113/143], loss=9.8157
	step [114/143], loss=9.3581
	step [115/143], loss=9.2352
	step [116/143], loss=9.1544
	step [117/143], loss=10.2620
	step [118/143], loss=8.8583
	step [119/143], loss=8.2734
	step [120/143], loss=9.6718
	step [121/143], loss=9.0063
	step [122/143], loss=7.6579
	step [123/143], loss=8.3064
	step [124/143], loss=10.2205
	step [125/143], loss=9.5154
	step [126/143], loss=8.8912
	step [127/143], loss=8.4144
	step [128/143], loss=8.5309
	step [129/143], loss=8.2069
	step [130/143], loss=10.5001
	step [131/143], loss=8.5715
	step [132/143], loss=9.0479
	step [133/143], loss=7.8223
	step [134/143], loss=9.1504
	step [135/143], loss=7.4554
	step [136/143], loss=10.0909
	step [137/143], loss=8.0620
	step [138/143], loss=8.5912
	step [139/143], loss=8.7760
	step [140/143], loss=8.0566
	step [141/143], loss=8.4098
	step [142/143], loss=8.5424
	step [143/143], loss=7.1222
	Evaluating
	loss=0.0279, precision=0.2664, recall=0.9897, f1=0.4198
Training epoch 23
	step [1/143], loss=9.8360
	step [2/143], loss=9.1600
	step [3/143], loss=8.4903
	step [4/143], loss=8.3982
	step [5/143], loss=6.4904
	step [6/143], loss=8.4956
	step [7/143], loss=8.5620
	step [8/143], loss=10.2066
	step [9/143], loss=8.0864
	step [10/143], loss=8.4617
	step [11/143], loss=8.6136
	step [12/143], loss=8.7901
	step [13/143], loss=9.1865
	step [14/143], loss=8.7311
	step [15/143], loss=9.3549
	step [16/143], loss=7.7699
	step [17/143], loss=7.9221
	step [18/143], loss=8.9210
	step [19/143], loss=8.7649
	step [20/143], loss=7.6358
	step [21/143], loss=8.6487
	step [22/143], loss=9.3309
	step [23/143], loss=8.8974
	step [24/143], loss=8.3335
	step [25/143], loss=8.4840
	step [26/143], loss=8.3590
	step [27/143], loss=9.8974
	step [28/143], loss=8.8245
	step [29/143], loss=8.5290
	step [30/143], loss=8.7362
	step [31/143], loss=9.0422
	step [32/143], loss=9.0570
	step [33/143], loss=8.8612
	step [34/143], loss=9.6800
	step [35/143], loss=8.5312
	step [36/143], loss=8.5824
	step [37/143], loss=8.4761
	step [38/143], loss=10.3036
	step [39/143], loss=9.6229
	step [40/143], loss=9.4390
	step [41/143], loss=8.9586
	step [42/143], loss=9.2099
	step [43/143], loss=9.7466
	step [44/143], loss=10.0174
	step [45/143], loss=8.6799
	step [46/143], loss=8.0244
	step [47/143], loss=8.8203
	step [48/143], loss=9.3024
	step [49/143], loss=8.6733
	step [50/143], loss=9.8739
	step [51/143], loss=8.0380
	step [52/143], loss=8.2329
	step [53/143], loss=9.8100
	step [54/143], loss=9.4754
	step [55/143], loss=8.9669
	step [56/143], loss=9.3883
	step [57/143], loss=10.2194
	step [58/143], loss=8.1055
	step [59/143], loss=8.2548
	step [60/143], loss=8.1144
	step [61/143], loss=10.7148
	step [62/143], loss=7.3838
	step [63/143], loss=9.0437
	step [64/143], loss=8.6612
	step [65/143], loss=8.9528
	step [66/143], loss=9.0186
	step [67/143], loss=9.2447
	step [68/143], loss=7.4473
	step [69/143], loss=8.4692
	step [70/143], loss=8.2905
	step [71/143], loss=9.8008
	step [72/143], loss=10.4598
	step [73/143], loss=8.3391
	step [74/143], loss=7.8645
	step [75/143], loss=8.3366
	step [76/143], loss=8.6020
	step [77/143], loss=8.8133
	step [78/143], loss=9.9390
	step [79/143], loss=8.1818
	step [80/143], loss=8.8996
	step [81/143], loss=9.2574
	step [82/143], loss=9.1612
	step [83/143], loss=7.9147
	step [84/143], loss=8.7960
	step [85/143], loss=8.8700
	step [86/143], loss=10.0239
	step [87/143], loss=7.5561
	step [88/143], loss=7.9065
	step [89/143], loss=9.1415
	step [90/143], loss=8.7136
	step [91/143], loss=10.2502
	step [92/143], loss=8.6724
	step [93/143], loss=8.0372
	step [94/143], loss=8.5517
	step [95/143], loss=8.2141
	step [96/143], loss=7.7153
	step [97/143], loss=7.7016
	step [98/143], loss=9.9204
	step [99/143], loss=8.2347
	step [100/143], loss=8.6190
	step [101/143], loss=8.5565
	step [102/143], loss=8.4854
	step [103/143], loss=9.2941
	step [104/143], loss=7.7708
	step [105/143], loss=9.0095
	step [106/143], loss=8.7017
	step [107/143], loss=9.4644
	step [108/143], loss=9.0377
	step [109/143], loss=8.6739
	step [110/143], loss=9.9136
	step [111/143], loss=8.2178
	step [112/143], loss=9.4094
	step [113/143], loss=9.2713
	step [114/143], loss=9.4617
	step [115/143], loss=8.3829
	step [116/143], loss=9.2307
	step [117/143], loss=8.2628
	step [118/143], loss=8.5269
	step [119/143], loss=8.9292
	step [120/143], loss=8.5905
	step [121/143], loss=8.8319
	step [122/143], loss=8.5429
	step [123/143], loss=8.3588
	step [124/143], loss=8.7375
	step [125/143], loss=7.8615
	step [126/143], loss=8.9130
	step [127/143], loss=8.4224
	step [128/143], loss=8.9085
	step [129/143], loss=9.1990
	step [130/143], loss=8.8150
	step [131/143], loss=9.1895
	step [132/143], loss=9.4788
	step [133/143], loss=8.0683
	step [134/143], loss=8.2221
	step [135/143], loss=8.7210
	step [136/143], loss=8.0056
	step [137/143], loss=8.4663
	step [138/143], loss=8.5662
	step [139/143], loss=8.7354
	step [140/143], loss=8.8225
	step [141/143], loss=8.1274
	step [142/143], loss=8.6007
	step [143/143], loss=8.0361
	Evaluating
	loss=0.0281, precision=0.2555, recall=0.9907, f1=0.4063
Training epoch 24
	step [1/143], loss=7.5757
	step [2/143], loss=8.9964
	step [3/143], loss=7.7168
	step [4/143], loss=9.7410
	step [5/143], loss=8.6508
	step [6/143], loss=9.4098
	step [7/143], loss=9.2358
	step [8/143], loss=9.6064
	step [9/143], loss=8.9936
	step [10/143], loss=8.4875
	step [11/143], loss=8.2645
	step [12/143], loss=9.6060
	step [13/143], loss=8.1106
	step [14/143], loss=8.5543
	step [15/143], loss=9.8851
	step [16/143], loss=8.8919
	step [17/143], loss=7.9570
	step [18/143], loss=8.8953
	step [19/143], loss=8.4101
	step [20/143], loss=8.7726
	step [21/143], loss=7.3256
	step [22/143], loss=9.2143
	step [23/143], loss=8.7059
	step [24/143], loss=9.8820
	step [25/143], loss=7.6077
	step [26/143], loss=9.5324
	step [27/143], loss=9.0247
	step [28/143], loss=9.0093
	step [29/143], loss=8.6741
	step [30/143], loss=8.3332
	step [31/143], loss=8.2761
	step [32/143], loss=9.5954
	step [33/143], loss=7.9505
	step [34/143], loss=9.9521
	step [35/143], loss=8.1150
	step [36/143], loss=9.2108
	step [37/143], loss=8.9284
	step [38/143], loss=8.6007
	step [39/143], loss=9.1792
	step [40/143], loss=8.0264
	step [41/143], loss=8.3667
	step [42/143], loss=9.4551
	step [43/143], loss=8.8610
	step [44/143], loss=8.8513
	step [45/143], loss=8.2182
	step [46/143], loss=9.2769
	step [47/143], loss=8.9520
	step [48/143], loss=9.2638
	step [49/143], loss=9.0156
	step [50/143], loss=8.2755
	step [51/143], loss=7.7555
	step [52/143], loss=9.0252
	step [53/143], loss=8.5422
	step [54/143], loss=7.1246
	step [55/143], loss=9.6581
	step [56/143], loss=7.5166
	step [57/143], loss=10.3321
	step [58/143], loss=8.6675
	step [59/143], loss=8.9958
	step [60/143], loss=7.7296
	step [61/143], loss=8.5945
	step [62/143], loss=8.0555
	step [63/143], loss=9.3709
	step [64/143], loss=7.5678
	step [65/143], loss=8.2153
	step [66/143], loss=9.3411
	step [67/143], loss=9.3338
	step [68/143], loss=8.8491
	step [69/143], loss=8.8061
	step [70/143], loss=8.3325
	step [71/143], loss=8.5695
	step [72/143], loss=7.1069
	step [73/143], loss=9.1963
	step [74/143], loss=8.0653
	step [75/143], loss=8.1646
	step [76/143], loss=9.3797
	step [77/143], loss=9.7697
	step [78/143], loss=7.8742
	step [79/143], loss=7.5858
	step [80/143], loss=8.6578
	step [81/143], loss=8.2349
	step [82/143], loss=8.8040
	step [83/143], loss=9.0259
	step [84/143], loss=8.1214
	step [85/143], loss=7.4096
	step [86/143], loss=7.4059
	step [87/143], loss=9.1010
	step [88/143], loss=8.4966
	step [89/143], loss=8.0560
	step [90/143], loss=8.2018
	step [91/143], loss=8.9892
	step [92/143], loss=8.8694
	step [93/143], loss=8.5015
	step [94/143], loss=9.1076
	step [95/143], loss=7.4989
	step [96/143], loss=8.3912
	step [97/143], loss=6.7826
	step [98/143], loss=8.7057
	step [99/143], loss=7.3005
	step [100/143], loss=7.6176
	step [101/143], loss=10.4372
	step [102/143], loss=7.6856
	step [103/143], loss=8.9220
	step [104/143], loss=8.5271
	step [105/143], loss=8.3995
	step [106/143], loss=7.8417
	step [107/143], loss=8.6901
	step [108/143], loss=8.1802
	step [109/143], loss=7.2712
	step [110/143], loss=8.9428
	step [111/143], loss=8.7995
	step [112/143], loss=9.5106
	step [113/143], loss=7.6637
	step [114/143], loss=7.1944
	step [115/143], loss=9.0695
	step [116/143], loss=8.3145
	step [117/143], loss=7.7341
	step [118/143], loss=8.0928
	step [119/143], loss=7.7321
	step [120/143], loss=9.1746
	step [121/143], loss=7.4406
	step [122/143], loss=8.8594
	step [123/143], loss=9.5020
	step [124/143], loss=8.2840
	step [125/143], loss=8.2607
	step [126/143], loss=8.8754
	step [127/143], loss=7.8013
	step [128/143], loss=8.5193
	step [129/143], loss=7.9491
	step [130/143], loss=7.7210
	step [131/143], loss=7.7669
	step [132/143], loss=7.3375
	step [133/143], loss=9.7945
	step [134/143], loss=8.6042
	step [135/143], loss=8.0036
	step [136/143], loss=9.2784
	step [137/143], loss=9.6503
	step [138/143], loss=8.9961
	step [139/143], loss=8.4185
	step [140/143], loss=8.3842
	step [141/143], loss=8.5191
	step [142/143], loss=7.6487
	step [143/143], loss=7.4961
	Evaluating
	loss=0.0249, precision=0.2711, recall=0.9893, f1=0.4256
Training epoch 25
	step [1/143], loss=9.1854
	step [2/143], loss=8.9243
	step [3/143], loss=7.6214
	step [4/143], loss=8.8656
	step [5/143], loss=8.3646
	step [6/143], loss=9.2217
	step [7/143], loss=9.2041
	step [8/143], loss=7.9935
	step [9/143], loss=6.8814
	step [10/143], loss=7.9469
	step [11/143], loss=8.7148
	step [12/143], loss=9.2590
	step [13/143], loss=7.2222
	step [14/143], loss=9.2276
	step [15/143], loss=6.8894
	step [16/143], loss=8.1554
	step [17/143], loss=8.1868
	step [18/143], loss=8.1031
	step [19/143], loss=7.4966
	step [20/143], loss=7.7741
	step [21/143], loss=8.5374
	step [22/143], loss=9.1244
	step [23/143], loss=9.6964
	step [24/143], loss=8.9954
	step [25/143], loss=9.0763
	step [26/143], loss=8.6355
	step [27/143], loss=8.0755
	step [28/143], loss=7.6197
	step [29/143], loss=8.2961
	step [30/143], loss=8.0587
	step [31/143], loss=7.1606
	step [32/143], loss=9.7432
	step [33/143], loss=8.5526
	step [34/143], loss=8.7271
	step [35/143], loss=8.0981
	step [36/143], loss=8.8420
	step [37/143], loss=8.6159
	step [38/143], loss=8.2815
	step [39/143], loss=8.6357
	step [40/143], loss=8.7031
	step [41/143], loss=7.6741
	step [42/143], loss=7.8790
	step [43/143], loss=7.8844
	step [44/143], loss=8.4154
	step [45/143], loss=8.9759
	step [46/143], loss=7.5602
	step [47/143], loss=7.8702
	step [48/143], loss=9.3612
	step [49/143], loss=8.5437
	step [50/143], loss=8.0338
	step [51/143], loss=8.6873
	step [52/143], loss=8.5070
	step [53/143], loss=9.0405
	step [54/143], loss=8.1862
	step [55/143], loss=9.1410
	step [56/143], loss=7.6349
	step [57/143], loss=8.2278
	step [58/143], loss=8.7366
	step [59/143], loss=8.4915
	step [60/143], loss=8.9861
	step [61/143], loss=8.6265
	step [62/143], loss=8.6904
	step [63/143], loss=8.7857
	step [64/143], loss=7.9156
	step [65/143], loss=7.9015
	step [66/143], loss=8.5163
	step [67/143], loss=7.3030
	step [68/143], loss=6.9332
	step [69/143], loss=7.3778
	step [70/143], loss=8.9378
	step [71/143], loss=8.0065
	step [72/143], loss=7.6840
	step [73/143], loss=8.9061
	step [74/143], loss=7.1384
	step [75/143], loss=8.7034
	step [76/143], loss=8.5570
	step [77/143], loss=8.8822
	step [78/143], loss=8.4413
	step [79/143], loss=7.1860
	step [80/143], loss=9.4164
	step [81/143], loss=8.0453
	step [82/143], loss=7.6997
	step [83/143], loss=7.8129
	step [84/143], loss=7.1771
	step [85/143], loss=7.5931
	step [86/143], loss=8.2677
	step [87/143], loss=8.6427
	step [88/143], loss=7.8918
	step [89/143], loss=7.3887
	step [90/143], loss=8.0367
	step [91/143], loss=8.2974
	step [92/143], loss=8.0248
	step [93/143], loss=8.2260
	step [94/143], loss=8.7663
	step [95/143], loss=8.7719
	step [96/143], loss=7.7959
	step [97/143], loss=8.5582
	step [98/143], loss=8.0604
	step [99/143], loss=7.7838
	step [100/143], loss=8.7782
	step [101/143], loss=8.5473
	step [102/143], loss=8.1819
	step [103/143], loss=8.9672
	step [104/143], loss=7.1679
	step [105/143], loss=9.4982
	step [106/143], loss=7.7133
	step [107/143], loss=8.0013
	step [108/143], loss=7.7593
	step [109/143], loss=7.7556
	step [110/143], loss=7.2170
	step [111/143], loss=7.9490
	step [112/143], loss=7.8026
	step [113/143], loss=9.2211
	step [114/143], loss=7.1879
	step [115/143], loss=8.4194
	step [116/143], loss=8.4006
	step [117/143], loss=8.5326
	step [118/143], loss=7.8532
	step [119/143], loss=6.4412
	step [120/143], loss=7.5707
	step [121/143], loss=7.8058
	step [122/143], loss=8.5766
	step [123/143], loss=8.1289
	step [124/143], loss=7.4438
	step [125/143], loss=7.8815
	step [126/143], loss=7.7143
	step [127/143], loss=9.4150
	step [128/143], loss=8.2760
	step [129/143], loss=7.4787
	step [130/143], loss=7.5600
	step [131/143], loss=7.9473
	step [132/143], loss=9.2472
	step [133/143], loss=7.2690
	step [134/143], loss=8.7941
	step [135/143], loss=7.9229
	step [136/143], loss=8.6474
	step [137/143], loss=8.4130
	step [138/143], loss=7.7611
	step [139/143], loss=8.5508
	step [140/143], loss=8.7624
	step [141/143], loss=8.1688
	step [142/143], loss=7.5673
	step [143/143], loss=5.5957
	Evaluating
	loss=0.0197, precision=0.3135, recall=0.9858, f1=0.4758
saving model as: 1_saved_model.pth
Training epoch 26
	step [1/143], loss=8.1557
	step [2/143], loss=8.8870
	step [3/143], loss=8.7864
	step [4/143], loss=7.8722
	step [5/143], loss=7.1332
	step [6/143], loss=7.9063
	step [7/143], loss=9.2451
	step [8/143], loss=7.6553
	step [9/143], loss=8.9860
	step [10/143], loss=8.5569
	step [11/143], loss=7.6091
	step [12/143], loss=7.0282
	step [13/143], loss=8.1515
	step [14/143], loss=8.0414
	step [15/143], loss=7.2075
	step [16/143], loss=8.2330
	step [17/143], loss=8.3263
	step [18/143], loss=7.3822
	step [19/143], loss=8.3849
	step [20/143], loss=7.7628
	step [21/143], loss=7.7535
	step [22/143], loss=8.9036
	step [23/143], loss=7.7494
	step [24/143], loss=8.0294
	step [25/143], loss=9.0343
	step [26/143], loss=8.0786
	step [27/143], loss=8.2403
	step [28/143], loss=7.4623
	step [29/143], loss=7.9982
	step [30/143], loss=8.5234
	step [31/143], loss=8.3623
	step [32/143], loss=7.3393
	step [33/143], loss=8.0962
	step [34/143], loss=8.5763
	step [35/143], loss=8.1282
	step [36/143], loss=7.2250
	step [37/143], loss=9.1642
	step [38/143], loss=8.4518
	step [39/143], loss=8.2769
	step [40/143], loss=8.1264
	step [41/143], loss=7.5777
	step [42/143], loss=7.9173
	step [43/143], loss=7.8254
	step [44/143], loss=7.9430
	step [45/143], loss=7.4069
	step [46/143], loss=8.1399
	step [47/143], loss=8.8575
	step [48/143], loss=7.6436
	step [49/143], loss=8.4782
	step [50/143], loss=7.2884
	step [51/143], loss=7.4134
	step [52/143], loss=8.3686
	step [53/143], loss=8.5268
	step [54/143], loss=6.6203
	step [55/143], loss=7.3986
	step [56/143], loss=8.4053
	step [57/143], loss=7.6501
	step [58/143], loss=8.6607
	step [59/143], loss=8.1187
	step [60/143], loss=7.6895
	step [61/143], loss=8.2732
	step [62/143], loss=7.3535
	step [63/143], loss=8.3939
	step [64/143], loss=9.9168
	step [65/143], loss=7.7975
	step [66/143], loss=9.2640
	step [67/143], loss=7.0327
	step [68/143], loss=9.6863
	step [69/143], loss=7.5660
	step [70/143], loss=7.7399
	step [71/143], loss=8.2663
	step [72/143], loss=7.4226
	step [73/143], loss=8.1777
	step [74/143], loss=6.7839
	step [75/143], loss=7.6535
	step [76/143], loss=7.7588
	step [77/143], loss=7.5110
	step [78/143], loss=9.0724
	step [79/143], loss=7.1705
	step [80/143], loss=8.8159
	step [81/143], loss=8.5743
	step [82/143], loss=8.0178
	step [83/143], loss=7.4029
	step [84/143], loss=7.2213
	step [85/143], loss=7.4896
	step [86/143], loss=7.4088
	step [87/143], loss=7.3098
	step [88/143], loss=8.5191
	step [89/143], loss=7.9174
	step [90/143], loss=8.2163
	step [91/143], loss=7.3352
	step [92/143], loss=7.8793
	step [93/143], loss=7.3360
	step [94/143], loss=9.2274
	step [95/143], loss=7.1664
	step [96/143], loss=7.1775
	step [97/143], loss=7.1342
	step [98/143], loss=6.9366
	step [99/143], loss=6.6338
	step [100/143], loss=8.0854
	step [101/143], loss=6.6978
	step [102/143], loss=7.3004
	step [103/143], loss=7.8173
	step [104/143], loss=7.2860
	step [105/143], loss=7.9583
	step [106/143], loss=7.2238
	step [107/143], loss=7.5398
	step [108/143], loss=7.8474
	step [109/143], loss=7.7463
	step [110/143], loss=7.4764
	step [111/143], loss=8.4328
	step [112/143], loss=8.8056
	step [113/143], loss=7.6027
	step [114/143], loss=9.1055
	step [115/143], loss=7.6147
	step [116/143], loss=7.9229
	step [117/143], loss=8.7009
	step [118/143], loss=7.4350
	step [119/143], loss=7.3420
	step [120/143], loss=7.9854
	step [121/143], loss=8.0181
	step [122/143], loss=7.7863
	step [123/143], loss=7.8132
	step [124/143], loss=7.2830
	step [125/143], loss=8.4039
	step [126/143], loss=7.5077
	step [127/143], loss=7.2196
	step [128/143], loss=7.9597
	step [129/143], loss=7.8339
	step [130/143], loss=7.6258
	step [131/143], loss=8.1957
	step [132/143], loss=8.1884
	step [133/143], loss=7.1363
	step [134/143], loss=8.7312
	step [135/143], loss=8.0450
	step [136/143], loss=8.6807
	step [137/143], loss=8.2101
	step [138/143], loss=9.6453
	step [139/143], loss=8.5673
	step [140/143], loss=7.2388
	step [141/143], loss=8.3057
	step [142/143], loss=7.7781
	step [143/143], loss=6.2380
	Evaluating
	loss=0.0191, precision=0.3341, recall=0.9862, f1=0.4991
saving model as: 1_saved_model.pth
Training epoch 27
	step [1/143], loss=7.1854
	step [2/143], loss=7.6729
	step [3/143], loss=10.2477
	step [4/143], loss=6.8440
	step [5/143], loss=8.3306
	step [6/143], loss=8.7955
	step [7/143], loss=8.2169
	step [8/143], loss=7.4012
	step [9/143], loss=8.0301
	step [10/143], loss=6.7048
	step [11/143], loss=8.1596
	step [12/143], loss=7.7819
	step [13/143], loss=9.4864
	step [14/143], loss=8.1466
	step [15/143], loss=7.4028
	step [16/143], loss=7.1412
	step [17/143], loss=6.8665
	step [18/143], loss=8.3483
	step [19/143], loss=7.8090
	step [20/143], loss=8.0570
	step [21/143], loss=7.8314
	step [22/143], loss=7.3086
	step [23/143], loss=7.6394
	step [24/143], loss=7.5198
	step [25/143], loss=8.4339
	step [26/143], loss=7.5289
	step [27/143], loss=7.8942
	step [28/143], loss=6.5210
	step [29/143], loss=8.2848
	step [30/143], loss=7.8981
	step [31/143], loss=8.5518
	step [32/143], loss=7.1399
	step [33/143], loss=8.9530
	step [34/143], loss=7.5425
	step [35/143], loss=8.0688
	step [36/143], loss=8.2403
	step [37/143], loss=7.3988
	step [38/143], loss=7.9729
	step [39/143], loss=7.5325
	step [40/143], loss=6.8640
	step [41/143], loss=6.9126
	step [42/143], loss=7.5214
	step [43/143], loss=7.2257
	step [44/143], loss=9.4175
	step [45/143], loss=7.3235
	step [46/143], loss=7.3466
	step [47/143], loss=7.2496
	step [48/143], loss=7.2809
	step [49/143], loss=7.3755
	step [50/143], loss=6.5203
	step [51/143], loss=8.8525
	step [52/143], loss=6.8853
	step [53/143], loss=7.5015
	step [54/143], loss=6.9222
	step [55/143], loss=7.0798
	step [56/143], loss=7.2948
	step [57/143], loss=8.4373
	step [58/143], loss=8.6163
	step [59/143], loss=6.9764
	step [60/143], loss=6.4485
	step [61/143], loss=7.8858
	step [62/143], loss=7.4274
	step [63/143], loss=8.0491
	step [64/143], loss=7.3119
	step [65/143], loss=7.3350
	step [66/143], loss=7.1169
	step [67/143], loss=7.8839
	step [68/143], loss=8.0037
	step [69/143], loss=8.8819
	step [70/143], loss=7.7970
	step [71/143], loss=8.5523
	step [72/143], loss=7.5015
	step [73/143], loss=7.3158
	step [74/143], loss=8.0407
	step [75/143], loss=7.9837
	step [76/143], loss=7.5513
	step [77/143], loss=8.0667
	step [78/143], loss=8.1584
	step [79/143], loss=7.5740
	step [80/143], loss=7.8353
	step [81/143], loss=8.7703
	step [82/143], loss=6.9166
	step [83/143], loss=8.6432
	step [84/143], loss=6.6509
	step [85/143], loss=7.5181
	step [86/143], loss=7.8788
	step [87/143], loss=8.4572
	step [88/143], loss=8.2242
	step [89/143], loss=8.8014
	step [90/143], loss=7.1337
	step [91/143], loss=8.2670
	step [92/143], loss=7.2050
	step [93/143], loss=7.2749
	step [94/143], loss=7.3243
	step [95/143], loss=7.0675
	step [96/143], loss=7.9379
	step [97/143], loss=8.9864
	step [98/143], loss=8.3384
	step [99/143], loss=8.2510
	step [100/143], loss=8.2520
	step [101/143], loss=7.7593
	step [102/143], loss=7.6429
	step [103/143], loss=7.3667
	step [104/143], loss=7.2418
	step [105/143], loss=8.0356
	step [106/143], loss=7.3256
	step [107/143], loss=6.6382
	step [108/143], loss=7.8003
	step [109/143], loss=7.5886
	step [110/143], loss=8.8076
	step [111/143], loss=7.4005
	step [112/143], loss=7.8879
	step [113/143], loss=8.2114
	step [114/143], loss=6.4866
	step [115/143], loss=8.0336
	step [116/143], loss=9.3959
	step [117/143], loss=8.1426
	step [118/143], loss=7.5258
	step [119/143], loss=8.3874
	step [120/143], loss=7.4647
	step [121/143], loss=8.5679
	step [122/143], loss=7.7081
	step [123/143], loss=8.0035
	step [124/143], loss=8.1175
	step [125/143], loss=8.6903
	step [126/143], loss=7.8688
	step [127/143], loss=7.2575
	step [128/143], loss=8.0017
	step [129/143], loss=8.3146
	step [130/143], loss=8.0561
	step [131/143], loss=7.3547
	step [132/143], loss=7.5846
	step [133/143], loss=7.4294
	step [134/143], loss=7.5696
	step [135/143], loss=8.6547
	step [136/143], loss=7.3644
	step [137/143], loss=7.4380
	step [138/143], loss=7.2277
	step [139/143], loss=7.7699
	step [140/143], loss=7.7969
	step [141/143], loss=8.1400
	step [142/143], loss=7.0658
	step [143/143], loss=5.8257
	Evaluating
	loss=0.0235, precision=0.2820, recall=0.9884, f1=0.4388
Training epoch 28
	step [1/143], loss=7.4746
	step [2/143], loss=8.1286
	step [3/143], loss=7.1843
	step [4/143], loss=7.4647
	step [5/143], loss=8.4883
	step [6/143], loss=7.6181
	step [7/143], loss=8.3734
	step [8/143], loss=7.5917
	step [9/143], loss=7.0515
	step [10/143], loss=7.0549
	step [11/143], loss=8.4874
	step [12/143], loss=7.7871
	step [13/143], loss=8.2207
	step [14/143], loss=7.1204
	step [15/143], loss=7.1499
	step [16/143], loss=7.2101
	step [17/143], loss=7.6021
	step [18/143], loss=8.0031
	step [19/143], loss=6.7297
	step [20/143], loss=7.3580
	step [21/143], loss=6.9263
	step [22/143], loss=7.4107
	step [23/143], loss=8.2608
	step [24/143], loss=8.3867
	step [25/143], loss=6.9045
	step [26/143], loss=6.7281
	step [27/143], loss=7.2181
	step [28/143], loss=6.1581
	step [29/143], loss=8.3348
	step [30/143], loss=7.9864
	step [31/143], loss=7.6420
	step [32/143], loss=7.6364
	step [33/143], loss=6.7766
	step [34/143], loss=8.4592
	step [35/143], loss=6.7869
	step [36/143], loss=6.8233
	step [37/143], loss=8.1529
	step [38/143], loss=7.7035
	step [39/143], loss=7.8505
	step [40/143], loss=8.6211
	step [41/143], loss=7.3317
	step [42/143], loss=8.0804
	step [43/143], loss=7.6533
	step [44/143], loss=7.1409
	step [45/143], loss=7.6193
	step [46/143], loss=6.9756
	step [47/143], loss=6.7525
	step [48/143], loss=9.6774
	step [49/143], loss=6.8061
	step [50/143], loss=9.1118
	step [51/143], loss=8.7141
	step [52/143], loss=7.5091
	step [53/143], loss=7.3723
	step [54/143], loss=8.4366
	step [55/143], loss=7.1433
	step [56/143], loss=6.8883
	step [57/143], loss=7.7716
	step [58/143], loss=8.2293
	step [59/143], loss=6.9149
	step [60/143], loss=7.1924
	step [61/143], loss=6.6165
	step [62/143], loss=7.9182
	step [63/143], loss=8.8037
	step [64/143], loss=8.7322
	step [65/143], loss=7.5207
	step [66/143], loss=7.9830
	step [67/143], loss=6.3461
	step [68/143], loss=6.8786
	step [69/143], loss=6.5900
	step [70/143], loss=6.4153
	step [71/143], loss=6.7287
	step [72/143], loss=8.4486
	step [73/143], loss=8.5488
	step [74/143], loss=8.0074
	step [75/143], loss=6.9257
	step [76/143], loss=8.1643
	step [77/143], loss=7.1973
	step [78/143], loss=6.7084
	step [79/143], loss=8.5182
	step [80/143], loss=8.0302
	step [81/143], loss=5.7907
	step [82/143], loss=7.1372
	step [83/143], loss=9.1898
	step [84/143], loss=7.2361
	step [85/143], loss=7.5882
	step [86/143], loss=7.1974
	step [87/143], loss=7.7375
	step [88/143], loss=7.8975
	step [89/143], loss=7.8272
	step [90/143], loss=6.8668
	step [91/143], loss=8.5499
	step [92/143], loss=7.1151
	step [93/143], loss=6.7943
	step [94/143], loss=7.3187
	step [95/143], loss=6.3191
	step [96/143], loss=7.9862
	step [97/143], loss=7.1493
	step [98/143], loss=6.5584
	step [99/143], loss=6.6684
	step [100/143], loss=6.6881
	step [101/143], loss=7.8614
	step [102/143], loss=7.1228
	step [103/143], loss=7.1077
	step [104/143], loss=6.5667
	step [105/143], loss=7.2702
	step [106/143], loss=7.1978
	step [107/143], loss=8.2657
	step [108/143], loss=8.5288
	step [109/143], loss=7.2906
	step [110/143], loss=7.0909
	step [111/143], loss=7.3644
	step [112/143], loss=9.0058
	step [113/143], loss=6.0852
	step [114/143], loss=7.8364
	step [115/143], loss=7.0309
	step [116/143], loss=6.8443
	step [117/143], loss=7.5805
	step [118/143], loss=7.1654
	step [119/143], loss=7.5180
	step [120/143], loss=7.2915
	step [121/143], loss=7.0833
	step [122/143], loss=7.4166
	step [123/143], loss=7.2481
	step [124/143], loss=7.6099
	step [125/143], loss=7.1818
	step [126/143], loss=8.6345
	step [127/143], loss=7.2376
	step [128/143], loss=7.1192
	step [129/143], loss=8.6840
	step [130/143], loss=7.5795
	step [131/143], loss=8.1050
	step [132/143], loss=7.8706
	step [133/143], loss=7.9542
	step [134/143], loss=6.7494
	step [135/143], loss=7.1321
	step [136/143], loss=7.5295
	step [137/143], loss=8.6241
	step [138/143], loss=7.3396
	step [139/143], loss=7.7164
	step [140/143], loss=7.0553
	step [141/143], loss=7.0345
	step [142/143], loss=7.4735
	step [143/143], loss=5.8643
	Evaluating
	loss=0.0188, precision=0.3295, recall=0.9852, f1=0.4938
Training epoch 29
	step [1/143], loss=6.7224
	step [2/143], loss=7.6291
	step [3/143], loss=7.3557
	step [4/143], loss=7.8935
	step [5/143], loss=6.2715
	step [6/143], loss=7.1231
	step [7/143], loss=7.0721
	step [8/143], loss=6.2641
	step [9/143], loss=7.2277
	step [10/143], loss=6.4285
	step [11/143], loss=7.6718
	step [12/143], loss=6.5402
	step [13/143], loss=7.8757
	step [14/143], loss=7.4093
	step [15/143], loss=7.7944
	step [16/143], loss=7.3356
	step [17/143], loss=7.1764
	step [18/143], loss=6.7125
	step [19/143], loss=7.9625
	step [20/143], loss=6.8134
	step [21/143], loss=7.2564
	step [22/143], loss=7.9354
	step [23/143], loss=8.0094
	step [24/143], loss=7.7802
	step [25/143], loss=8.1757
	step [26/143], loss=7.9108
	step [27/143], loss=7.0529
	step [28/143], loss=6.8524
	step [29/143], loss=7.1949
	step [30/143], loss=7.6452
	step [31/143], loss=8.1350
	step [32/143], loss=7.6014
	step [33/143], loss=8.0152
	step [34/143], loss=6.5745
	step [35/143], loss=7.5864
	step [36/143], loss=8.0299
	step [37/143], loss=7.4594
	step [38/143], loss=7.8921
	step [39/143], loss=6.1544
	step [40/143], loss=7.7893
	step [41/143], loss=7.5518
	step [42/143], loss=7.5089
	step [43/143], loss=7.0851
	step [44/143], loss=7.6268
	step [45/143], loss=8.4241
	step [46/143], loss=6.9775
	step [47/143], loss=8.0960
	step [48/143], loss=8.1267
	step [49/143], loss=7.8462
	step [50/143], loss=7.3180
	step [51/143], loss=7.3009
	step [52/143], loss=7.1333
	step [53/143], loss=6.5940
	step [54/143], loss=7.4519
	step [55/143], loss=7.6193
	step [56/143], loss=7.7143
	step [57/143], loss=7.4618
	step [58/143], loss=7.6520
	step [59/143], loss=7.8590
	step [60/143], loss=8.4116
	step [61/143], loss=8.6714
	step [62/143], loss=7.1805
	step [63/143], loss=7.7262
	step [64/143], loss=7.3386
	step [65/143], loss=7.2582
	step [66/143], loss=6.4329
	step [67/143], loss=6.7684
	step [68/143], loss=6.8192
	step [69/143], loss=7.4598
	step [70/143], loss=6.4427
	step [71/143], loss=7.0601
	step [72/143], loss=7.6777
	step [73/143], loss=6.9412
	step [74/143], loss=7.7546
	step [75/143], loss=7.0959
	step [76/143], loss=6.8709
	step [77/143], loss=7.6740
	step [78/143], loss=6.8114
	step [79/143], loss=7.2949
	step [80/143], loss=6.5455
	step [81/143], loss=6.2115
	step [82/143], loss=7.5319
	step [83/143], loss=6.6852
	step [84/143], loss=6.5838
	step [85/143], loss=7.3512
	step [86/143], loss=8.3018
	step [87/143], loss=6.7690
	step [88/143], loss=7.3319
	step [89/143], loss=7.1180
	step [90/143], loss=7.4271
	step [91/143], loss=7.5770
	step [92/143], loss=7.0880
	step [93/143], loss=6.0874
	step [94/143], loss=8.1913
	step [95/143], loss=6.5970
	step [96/143], loss=6.4829
	step [97/143], loss=7.4726
	step [98/143], loss=8.4799
	step [99/143], loss=7.5002
	step [100/143], loss=6.7518
	step [101/143], loss=6.4788
	step [102/143], loss=6.9248
	step [103/143], loss=8.5492
	step [104/143], loss=7.2400
	step [105/143], loss=6.6850
	step [106/143], loss=6.5766
	step [107/143], loss=7.1929
	step [108/143], loss=6.3871
	step [109/143], loss=7.8005
	step [110/143], loss=8.4823
	step [111/143], loss=8.0838
	step [112/143], loss=7.2897
	step [113/143], loss=7.7801
	step [114/143], loss=6.9730
	step [115/143], loss=7.1996
	step [116/143], loss=7.5212
	step [117/143], loss=7.4178
	step [118/143], loss=6.7719
	step [119/143], loss=6.9167
	step [120/143], loss=7.2374
	step [121/143], loss=6.6545
	step [122/143], loss=7.1193
	step [123/143], loss=7.7798
	step [124/143], loss=7.4658
	step [125/143], loss=7.9410
	step [126/143], loss=7.6140
	step [127/143], loss=7.3134
	step [128/143], loss=7.2337
	step [129/143], loss=7.7035
	step [130/143], loss=6.8015
	step [131/143], loss=7.4396
	step [132/143], loss=7.3202
	step [133/143], loss=7.9702
	step [134/143], loss=6.1580
	step [135/143], loss=6.8655
	step [136/143], loss=6.1118
	step [137/143], loss=7.1831
	step [138/143], loss=6.5882
	step [139/143], loss=7.6778
	step [140/143], loss=7.3001
	step [141/143], loss=7.0496
	step [142/143], loss=7.5386
	step [143/143], loss=5.2924
	Evaluating
	loss=0.0214, precision=0.3053, recall=0.9869, f1=0.4663
Training epoch 30
	step [1/143], loss=5.5775
	step [2/143], loss=7.3037
	step [3/143], loss=6.8639
	step [4/143], loss=7.8291
	step [5/143], loss=7.6855
	step [6/143], loss=6.0490
	step [7/143], loss=6.3227
	step [8/143], loss=8.3153
	step [9/143], loss=7.5325
	step [10/143], loss=7.3515
	step [11/143], loss=5.9169
	step [12/143], loss=7.1039
	step [13/143], loss=7.4556
	step [14/143], loss=6.7654
	step [15/143], loss=7.7449
	step [16/143], loss=6.5968
	step [17/143], loss=7.7416
	step [18/143], loss=7.2098
	step [19/143], loss=8.3751
	step [20/143], loss=7.1265
	step [21/143], loss=7.3222
	step [22/143], loss=7.1939
	step [23/143], loss=7.1691
	step [24/143], loss=8.1686
	step [25/143], loss=7.8685
	step [26/143], loss=7.2417
	step [27/143], loss=7.1246
	step [28/143], loss=7.8628
	step [29/143], loss=7.6070
	step [30/143], loss=7.3148
	step [31/143], loss=6.5030
	step [32/143], loss=7.0331
	step [33/143], loss=6.2674
	step [34/143], loss=7.3848
	step [35/143], loss=7.0318
	step [36/143], loss=7.4185
	step [37/143], loss=7.1866
	step [38/143], loss=8.2691
	step [39/143], loss=6.9910
	step [40/143], loss=6.0891
	step [41/143], loss=6.5269
	step [42/143], loss=8.1657
	step [43/143], loss=7.3014
	step [44/143], loss=8.2932
	step [45/143], loss=6.8473
	step [46/143], loss=7.6621
	step [47/143], loss=7.8638
	step [48/143], loss=7.1858
	step [49/143], loss=7.6900
	step [50/143], loss=6.7355
	step [51/143], loss=8.0560
	step [52/143], loss=6.5086
	step [53/143], loss=7.6439
	step [54/143], loss=7.0470
	step [55/143], loss=8.6200
	step [56/143], loss=7.6169
	step [57/143], loss=6.6419
	step [58/143], loss=6.9953
	step [59/143], loss=6.0303
	step [60/143], loss=6.5774
	step [61/143], loss=7.4805
	step [62/143], loss=6.9015
	step [63/143], loss=7.1636
	step [64/143], loss=6.8887
	step [65/143], loss=6.8989
	step [66/143], loss=7.3866
	step [67/143], loss=5.9109
	step [68/143], loss=6.5390
	step [69/143], loss=6.3328
	step [70/143], loss=6.7687
	step [71/143], loss=7.3351
	step [72/143], loss=6.8691
	step [73/143], loss=6.9746
	step [74/143], loss=7.2795
	step [75/143], loss=7.5703
	step [76/143], loss=7.0131
	step [77/143], loss=7.2546
	step [78/143], loss=6.8125
	step [79/143], loss=6.2748
	step [80/143], loss=6.8620
	step [81/143], loss=7.2651
	step [82/143], loss=7.5331
	step [83/143], loss=7.9759
	step [84/143], loss=6.5174
	step [85/143], loss=7.5199
	step [86/143], loss=7.2329
	step [87/143], loss=6.1636
	step [88/143], loss=6.7138
	step [89/143], loss=6.9486
	step [90/143], loss=7.0955
	step [91/143], loss=8.2160
	step [92/143], loss=6.6435
	step [93/143], loss=7.6258
	step [94/143], loss=6.8366
	step [95/143], loss=7.7035
	step [96/143], loss=7.0073
	step [97/143], loss=7.5077
	step [98/143], loss=7.8284
	step [99/143], loss=8.3233
	step [100/143], loss=8.3069
	step [101/143], loss=7.1390
	step [102/143], loss=7.6236
	step [103/143], loss=6.6272
	step [104/143], loss=6.5028
	step [105/143], loss=7.1966
	step [106/143], loss=6.7525
	step [107/143], loss=7.0463
	step [108/143], loss=6.8971
	step [109/143], loss=7.3303
	step [110/143], loss=7.6227
	step [111/143], loss=7.3180
	step [112/143], loss=7.3927
	step [113/143], loss=7.2521
	step [114/143], loss=6.6322
	step [115/143], loss=7.9969
	step [116/143], loss=7.3537
	step [117/143], loss=6.6023
	step [118/143], loss=7.1537
	step [119/143], loss=6.5695
	step [120/143], loss=7.1899
	step [121/143], loss=6.5637
	step [122/143], loss=7.4332
	step [123/143], loss=7.6366
	step [124/143], loss=5.5959
	step [125/143], loss=7.8100
	step [126/143], loss=7.3062
	step [127/143], loss=7.7643
	step [128/143], loss=6.9074
	step [129/143], loss=6.1872
	step [130/143], loss=8.2004
	step [131/143], loss=6.6775
	step [132/143], loss=7.7034
	step [133/143], loss=7.5314
	step [134/143], loss=6.2299
	step [135/143], loss=6.6491
	step [136/143], loss=8.2003
	step [137/143], loss=7.2778
	step [138/143], loss=6.9662
	step [139/143], loss=7.1870
	step [140/143], loss=6.9132
	step [141/143], loss=6.4656
	step [142/143], loss=7.8053
	step [143/143], loss=5.2224
	Evaluating
	loss=0.0174, precision=0.3465, recall=0.9839, f1=0.5125
saving model as: 1_saved_model.pth
Training finished
best_f1: 0.5125039537317952
directing: Y rim_enhanced: False test_id 1
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15614 # image files with weight 15579
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4469 # image files with weight 4451
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15579
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/244], loss=865.2091
	step [2/244], loss=515.8427
	step [3/244], loss=323.3348
	step [4/244], loss=224.7172
	step [5/244], loss=210.8108
	step [6/244], loss=183.9106
	step [7/244], loss=165.0026
	step [8/244], loss=148.8079
	step [9/244], loss=142.2049
	step [10/244], loss=145.3954
	step [11/244], loss=142.3164
	step [12/244], loss=134.2011
	step [13/244], loss=131.7072
	step [14/244], loss=130.1620
	step [15/244], loss=132.7625
	step [16/244], loss=130.6023
	step [17/244], loss=128.2720
	step [18/244], loss=126.5289
	step [19/244], loss=127.1155
	step [20/244], loss=123.7100
	step [21/244], loss=124.4563
	step [22/244], loss=123.0512
	step [23/244], loss=123.0526
	step [24/244], loss=122.1116
	step [25/244], loss=123.1235
	step [26/244], loss=120.3553
	step [27/244], loss=119.4680
	step [28/244], loss=118.9852
	step [29/244], loss=118.9540
	step [30/244], loss=118.1605
	step [31/244], loss=115.7118
	step [32/244], loss=115.1198
	step [33/244], loss=116.6815
	step [34/244], loss=116.2116
	step [35/244], loss=114.3454
	step [36/244], loss=114.1050
	step [37/244], loss=114.3008
	step [38/244], loss=113.5336
	step [39/244], loss=113.8211
	step [40/244], loss=112.5215
	step [41/244], loss=110.3718
	step [42/244], loss=111.3460
	step [43/244], loss=108.8293
	step [44/244], loss=109.3476
	step [45/244], loss=108.5200
	step [46/244], loss=107.3896
	step [47/244], loss=106.8062
	step [48/244], loss=105.7938
	step [49/244], loss=106.8922
	step [50/244], loss=105.8414
	step [51/244], loss=105.1646
	step [52/244], loss=105.2194
	step [53/244], loss=105.1588
	step [54/244], loss=102.6870
	step [55/244], loss=105.2188
	step [56/244], loss=103.4479
	step [57/244], loss=102.7403
	step [58/244], loss=101.6262
	step [59/244], loss=105.4271
	step [60/244], loss=101.3691
	step [61/244], loss=101.0611
	step [62/244], loss=100.5818
	step [63/244], loss=100.5732
	step [64/244], loss=100.1151
	step [65/244], loss=99.6252
	step [66/244], loss=99.8554
	step [67/244], loss=98.0926
	step [68/244], loss=98.0437
	step [69/244], loss=97.7931
	step [70/244], loss=97.0784
	step [71/244], loss=95.7890
	step [72/244], loss=95.9886
	step [73/244], loss=94.9323
	step [74/244], loss=95.0732
	step [75/244], loss=94.2292
	step [76/244], loss=93.4561
	step [77/244], loss=95.1971
	step [78/244], loss=94.6761
	step [79/244], loss=95.0758
	step [80/244], loss=94.2202
	step [81/244], loss=93.8884
	step [82/244], loss=93.3099
	step [83/244], loss=93.1123
	step [84/244], loss=92.1458
	step [85/244], loss=90.7969
	step [86/244], loss=91.8287
	step [87/244], loss=91.7666
	step [88/244], loss=91.9534
	step [89/244], loss=89.4530
	step [90/244], loss=89.4832
	step [91/244], loss=88.9251
	step [92/244], loss=90.4843
	step [93/244], loss=87.5427
	step [94/244], loss=88.3705
	step [95/244], loss=90.3027
	step [96/244], loss=88.1642
	step [97/244], loss=87.8874
	step [98/244], loss=87.5434
	step [99/244], loss=87.5315
	step [100/244], loss=87.6686
	step [101/244], loss=88.0172
	step [102/244], loss=87.6783
	step [103/244], loss=86.4081
	step [104/244], loss=84.9437
	step [105/244], loss=87.6082
	step [106/244], loss=84.4282
	step [107/244], loss=86.5133
	step [108/244], loss=85.1246
	step [109/244], loss=86.8320
	step [110/244], loss=83.6289
	step [111/244], loss=86.0250
	step [112/244], loss=83.9977
	step [113/244], loss=82.6344
	step [114/244], loss=83.8616
	step [115/244], loss=83.2976
	step [116/244], loss=82.8936
	step [117/244], loss=83.0558
	step [118/244], loss=85.9511
	step [119/244], loss=81.6313
	step [120/244], loss=82.5246
	step [121/244], loss=82.2069
	step [122/244], loss=80.9104
	step [123/244], loss=83.0154
	step [124/244], loss=83.2268
	step [125/244], loss=80.4998
	step [126/244], loss=80.9844
	step [127/244], loss=79.2473
	step [128/244], loss=79.2283
	step [129/244], loss=80.5570
	step [130/244], loss=80.9755
	step [131/244], loss=81.5514
	step [132/244], loss=79.9153
	step [133/244], loss=80.6116
	step [134/244], loss=77.5757
	step [135/244], loss=78.0638
	step [136/244], loss=78.3346
	step [137/244], loss=79.1331
	step [138/244], loss=78.7883
	step [139/244], loss=78.8924
	step [140/244], loss=77.7434
	step [141/244], loss=77.5184
	step [142/244], loss=77.6293
	step [143/244], loss=76.2816
	step [144/244], loss=76.5960
	step [145/244], loss=77.6549
	step [146/244], loss=77.7480
	step [147/244], loss=76.2814
	step [148/244], loss=76.3084
	step [149/244], loss=78.8779
	step [150/244], loss=77.1839
	step [151/244], loss=74.8376
	step [152/244], loss=75.9132
	step [153/244], loss=77.6938
	step [154/244], loss=74.5890
	step [155/244], loss=75.7161
	step [156/244], loss=75.3413
	step [157/244], loss=74.8381
	step [158/244], loss=75.5444
	step [159/244], loss=74.4692
	step [160/244], loss=74.1629
	step [161/244], loss=73.2136
	step [162/244], loss=75.2605
	step [163/244], loss=74.8232
	step [164/244], loss=73.8061
	step [165/244], loss=73.8669
	step [166/244], loss=74.1768
	step [167/244], loss=72.6224
	step [168/244], loss=71.7763
	step [169/244], loss=72.8717
	step [170/244], loss=72.4541
	step [171/244], loss=72.6396
	step [172/244], loss=71.6088
	step [173/244], loss=72.0583
	step [174/244], loss=74.6134
	step [175/244], loss=71.2708
	step [176/244], loss=71.7897
	step [177/244], loss=73.7806
	step [178/244], loss=73.0030
	step [179/244], loss=71.6731
	step [180/244], loss=71.6557
	step [181/244], loss=71.7094
	step [182/244], loss=71.2429
	step [183/244], loss=70.4353
	step [184/244], loss=69.5924
	step [185/244], loss=70.2967
	step [186/244], loss=71.2644
	step [187/244], loss=69.3905
	step [188/244], loss=69.7041
	step [189/244], loss=70.8221
	step [190/244], loss=70.5273
	step [191/244], loss=71.5349
	step [192/244], loss=69.0369
	step [193/244], loss=68.0979
	step [194/244], loss=68.1531
	step [195/244], loss=69.0541
	step [196/244], loss=69.9834
	step [197/244], loss=69.0814
	step [198/244], loss=68.5216
	step [199/244], loss=67.5478
	step [200/244], loss=69.2119
	step [201/244], loss=67.4839
	step [202/244], loss=67.4097
	step [203/244], loss=68.7524
	step [204/244], loss=69.6407
	step [205/244], loss=67.3507
	step [206/244], loss=66.8826
	step [207/244], loss=66.0324
	step [208/244], loss=67.5620
	step [209/244], loss=67.1539
	step [210/244], loss=67.8095
	step [211/244], loss=67.1815
	step [212/244], loss=65.5691
	step [213/244], loss=67.5785
	step [214/244], loss=65.4861
	step [215/244], loss=66.6697
	step [216/244], loss=67.3807
	step [217/244], loss=66.8797
	step [218/244], loss=64.9275
	step [219/244], loss=65.4067
	step [220/244], loss=65.7772
	step [221/244], loss=63.7400
	step [222/244], loss=65.2816
	step [223/244], loss=63.9852
	step [224/244], loss=65.3574
	step [225/244], loss=64.6858
	step [226/244], loss=64.5751
	step [227/244], loss=64.6134
	step [228/244], loss=65.3424
	step [229/244], loss=64.3519
	step [230/244], loss=65.1168
	step [231/244], loss=64.2133
	step [232/244], loss=63.8412
	step [233/244], loss=63.0258
	step [234/244], loss=63.5018
	step [235/244], loss=63.4377
	step [236/244], loss=62.5929
	step [237/244], loss=65.0443
	step [238/244], loss=62.2652
	step [239/244], loss=62.1149
	step [240/244], loss=63.1223
	step [241/244], loss=62.9848
	step [242/244], loss=64.6978
	step [243/244], loss=65.1674
	step [244/244], loss=27.0319
	Evaluating
	loss=0.2324, precision=0.1421, recall=0.9963, f1=0.2487
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/244], loss=63.0895
	step [2/244], loss=62.1364
	step [3/244], loss=62.1319
	step [4/244], loss=61.4754
	step [5/244], loss=62.5067
	step [6/244], loss=61.4051
	step [7/244], loss=61.9640
	step [8/244], loss=63.8836
	step [9/244], loss=61.8180
	step [10/244], loss=61.4781
	step [11/244], loss=61.9193
	step [12/244], loss=60.6826
	step [13/244], loss=61.5286
	step [14/244], loss=60.7752
	step [15/244], loss=59.7617
	step [16/244], loss=59.0740
	step [17/244], loss=59.7263
	step [18/244], loss=60.1239
	step [19/244], loss=60.8114
	step [20/244], loss=59.9562
	step [21/244], loss=62.8306
	step [22/244], loss=61.0597
	step [23/244], loss=61.1470
	step [24/244], loss=58.4040
	step [25/244], loss=58.2348
	step [26/244], loss=62.0050
	step [27/244], loss=60.0655
	step [28/244], loss=61.2230
	step [29/244], loss=59.3550
	step [30/244], loss=59.0990
	step [31/244], loss=59.2609
	step [32/244], loss=58.9439
	step [33/244], loss=58.1022
	step [34/244], loss=58.8562
	step [35/244], loss=58.4584
	step [36/244], loss=59.5648
	step [37/244], loss=58.3440
	step [38/244], loss=59.6729
	step [39/244], loss=58.2391
	step [40/244], loss=58.8123
	step [41/244], loss=57.4742
	step [42/244], loss=57.1278
	step [43/244], loss=59.5199
	step [44/244], loss=58.0298
	step [45/244], loss=57.1203
	step [46/244], loss=59.6943
	step [47/244], loss=55.7957
	step [48/244], loss=56.7977
	step [49/244], loss=57.3757
	step [50/244], loss=56.6953
	step [51/244], loss=56.0495
	step [52/244], loss=55.7442
	step [53/244], loss=55.5047
	step [54/244], loss=55.7606
	step [55/244], loss=56.1844
	step [56/244], loss=57.5304
	step [57/244], loss=57.2464
	step [58/244], loss=57.2716
	step [59/244], loss=55.4890
	step [60/244], loss=55.3866
	step [61/244], loss=58.7068
	step [62/244], loss=59.5872
	step [63/244], loss=55.5860
	step [64/244], loss=56.7061
	step [65/244], loss=54.6123
	step [66/244], loss=56.9682
	step [67/244], loss=55.4799
	step [68/244], loss=55.2951
	step [69/244], loss=54.7444
	step [70/244], loss=55.2170
	step [71/244], loss=53.3254
	step [72/244], loss=55.1040
	step [73/244], loss=53.8495
	step [74/244], loss=55.3969
	step [75/244], loss=56.8895
	step [76/244], loss=54.8465
	step [77/244], loss=55.0213
	step [78/244], loss=55.2442
	step [79/244], loss=54.1494
	step [80/244], loss=54.0870
	step [81/244], loss=55.4047
	step [82/244], loss=56.2498
	step [83/244], loss=54.6605
	step [84/244], loss=54.2685
	step [85/244], loss=54.7877
	step [86/244], loss=53.0727
	step [87/244], loss=54.6114
	step [88/244], loss=54.2237
	step [89/244], loss=54.3353
	step [90/244], loss=54.2120
	step [91/244], loss=53.0313
	step [92/244], loss=53.9443
	step [93/244], loss=55.4708
	step [94/244], loss=54.9380
	step [95/244], loss=53.5674
	step [96/244], loss=54.6685
	step [97/244], loss=51.6721
	step [98/244], loss=53.2705
	step [99/244], loss=53.3951
	step [100/244], loss=53.6939
	step [101/244], loss=52.7601
	step [102/244], loss=53.4908
	step [103/244], loss=53.3619
	step [104/244], loss=52.1867
	step [105/244], loss=53.7452
	step [106/244], loss=52.2940
	step [107/244], loss=51.8109
	step [108/244], loss=53.4521
	step [109/244], loss=51.1809
	step [110/244], loss=55.7029
	step [111/244], loss=52.1286
	step [112/244], loss=52.1164
	step [113/244], loss=52.5069
	step [114/244], loss=52.8288
	step [115/244], loss=54.8051
	step [116/244], loss=51.6572
	step [117/244], loss=51.1570
	step [118/244], loss=51.8168
	step [119/244], loss=53.7126
	step [120/244], loss=51.5574
	step [121/244], loss=52.2915
	step [122/244], loss=51.5936
	step [123/244], loss=51.2550
	step [124/244], loss=50.3269
	step [125/244], loss=50.1456
	step [126/244], loss=51.2515
	step [127/244], loss=50.1310
	step [128/244], loss=49.5964
	step [129/244], loss=50.7723
	step [130/244], loss=51.8454
	step [131/244], loss=50.1118
	step [132/244], loss=49.4157
	step [133/244], loss=49.6829
	step [134/244], loss=52.9208
	step [135/244], loss=51.9556
	step [136/244], loss=51.6388
	step [137/244], loss=50.6964
	step [138/244], loss=50.6559
	step [139/244], loss=49.5950
	step [140/244], loss=49.9938
	step [141/244], loss=50.3466
	step [142/244], loss=50.7937
	step [143/244], loss=50.2456
	step [144/244], loss=49.8521
	step [145/244], loss=50.8764
	step [146/244], loss=48.2730
	step [147/244], loss=50.3477
	step [148/244], loss=51.6476
	step [149/244], loss=52.4568
	step [150/244], loss=49.2387
	step [151/244], loss=50.7363
	step [152/244], loss=48.7778
	step [153/244], loss=46.8253
	step [154/244], loss=49.3819
	step [155/244], loss=49.2946
	step [156/244], loss=49.8426
	step [157/244], loss=48.8690
	step [158/244], loss=49.9926
	step [159/244], loss=49.6819
	step [160/244], loss=49.1664
	step [161/244], loss=48.7565
	step [162/244], loss=48.2285
	step [163/244], loss=50.3289
	step [164/244], loss=47.9646
	step [165/244], loss=47.7417
	step [166/244], loss=47.4205
	step [167/244], loss=47.7179
	step [168/244], loss=48.7929
	step [169/244], loss=47.6832
	step [170/244], loss=47.7718
	step [171/244], loss=50.3098
	step [172/244], loss=48.1686
	step [173/244], loss=49.7920
	step [174/244], loss=46.4406
	step [175/244], loss=47.9660
	step [176/244], loss=49.4002
	step [177/244], loss=48.6392
	step [178/244], loss=47.0316
	step [179/244], loss=47.9440
	step [180/244], loss=45.6129
	step [181/244], loss=47.3604
	step [182/244], loss=46.3195
	step [183/244], loss=49.3617
	step [184/244], loss=45.3075
	step [185/244], loss=47.0923
	step [186/244], loss=46.0324
	step [187/244], loss=47.5597
	step [188/244], loss=46.0287
	step [189/244], loss=46.1180
	step [190/244], loss=45.6521
	step [191/244], loss=47.6565
	step [192/244], loss=46.5678
	step [193/244], loss=47.1953
	step [194/244], loss=46.1121
	step [195/244], loss=48.8529
	step [196/244], loss=46.7400
	step [197/244], loss=46.4925
	step [198/244], loss=46.2129
	step [199/244], loss=46.0000
	step [200/244], loss=47.8488
	step [201/244], loss=45.7086
	step [202/244], loss=46.5354
	step [203/244], loss=45.2793
	step [204/244], loss=45.9452
	step [205/244], loss=45.6133
	step [206/244], loss=46.5995
	step [207/244], loss=46.0613
	step [208/244], loss=46.8393
	step [209/244], loss=46.4954
	step [210/244], loss=46.9088
	step [211/244], loss=46.6682
	step [212/244], loss=45.2812
	step [213/244], loss=46.1944
	step [214/244], loss=46.1678
	step [215/244], loss=46.4933
	step [216/244], loss=45.0018
	step [217/244], loss=45.5749
	step [218/244], loss=44.3938
	step [219/244], loss=44.5020
	step [220/244], loss=44.0365
	step [221/244], loss=43.9602
	step [222/244], loss=44.3109
	step [223/244], loss=45.8682
	step [224/244], loss=43.4551
	step [225/244], loss=43.8957
	step [226/244], loss=45.9962
	step [227/244], loss=46.4816
	step [228/244], loss=45.6204
	step [229/244], loss=45.0747
	step [230/244], loss=45.2048
	step [231/244], loss=44.7917
	step [232/244], loss=44.7363
	step [233/244], loss=42.9877
	step [234/244], loss=42.1242
	step [235/244], loss=43.9478
	step [236/244], loss=45.7352
	step [237/244], loss=45.3281
	step [238/244], loss=42.8980
	step [239/244], loss=43.4548
	step [240/244], loss=42.9496
	step [241/244], loss=47.3820
	step [242/244], loss=42.1911
	step [243/244], loss=44.6029
	step [244/244], loss=19.0771
	Evaluating
	loss=0.1604, precision=0.1546, recall=0.9960, f1=0.2677
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/244], loss=42.3403
	step [2/244], loss=43.8603
	step [3/244], loss=42.5559
	step [4/244], loss=42.6091
	step [5/244], loss=42.6941
	step [6/244], loss=42.8656
	step [7/244], loss=43.9735
	step [8/244], loss=42.6655
	step [9/244], loss=43.0550
	step [10/244], loss=44.2883
	step [11/244], loss=42.8659
	step [12/244], loss=44.4764
	step [13/244], loss=43.1330
	step [14/244], loss=42.8498
	step [15/244], loss=41.4513
	step [16/244], loss=41.7903
	step [17/244], loss=43.1598
	step [18/244], loss=41.0539
	step [19/244], loss=42.0407
	step [20/244], loss=43.4601
	step [21/244], loss=41.1698
	step [22/244], loss=43.3113
	step [23/244], loss=44.4210
	step [24/244], loss=41.9299
	step [25/244], loss=41.6374
	step [26/244], loss=41.7519
	step [27/244], loss=44.2770
	step [28/244], loss=42.2411
	step [29/244], loss=43.7933
	step [30/244], loss=41.2481
	step [31/244], loss=41.8504
	step [32/244], loss=40.6670
	step [33/244], loss=43.1600
	step [34/244], loss=41.5134
	step [35/244], loss=41.0159
	step [36/244], loss=40.6618
	step [37/244], loss=41.8414
	step [38/244], loss=42.2218
	step [39/244], loss=39.0223
	step [40/244], loss=39.9014
	step [41/244], loss=41.4002
	step [42/244], loss=41.4473
	step [43/244], loss=40.9961
	step [44/244], loss=41.1598
	step [45/244], loss=39.0658
	step [46/244], loss=40.5197
	step [47/244], loss=42.7544
	step [48/244], loss=39.0014
	step [49/244], loss=40.6562
	step [50/244], loss=39.9954
	step [51/244], loss=39.5290
	step [52/244], loss=40.1854
	step [53/244], loss=42.2579
	step [54/244], loss=41.2778
	step [55/244], loss=39.9536
	step [56/244], loss=41.6417
	step [57/244], loss=40.0927
	step [58/244], loss=41.8256
	step [59/244], loss=40.0675
	step [60/244], loss=39.7321
	step [61/244], loss=38.9004
	step [62/244], loss=38.2969
	step [63/244], loss=42.1226
	step [64/244], loss=40.6685
	step [65/244], loss=40.6522
	step [66/244], loss=40.9495
	step [67/244], loss=41.4797
	step [68/244], loss=38.4793
	step [69/244], loss=42.2485
	step [70/244], loss=40.2812
	step [71/244], loss=39.8779
	step [72/244], loss=39.5076
	step [73/244], loss=42.6108
	step [74/244], loss=40.8490
	step [75/244], loss=39.9400
	step [76/244], loss=38.5537
	step [77/244], loss=38.8663
	step [78/244], loss=40.4484
	step [79/244], loss=40.9234
	step [80/244], loss=40.7588
	step [81/244], loss=38.8886
	step [82/244], loss=38.6262
	step [83/244], loss=39.5982
	step [84/244], loss=39.4416
	step [85/244], loss=38.8668
	step [86/244], loss=38.0643
	step [87/244], loss=39.3993
	step [88/244], loss=38.9509
	step [89/244], loss=39.3677
	step [90/244], loss=40.1876
	step [91/244], loss=39.8879
	step [92/244], loss=37.8687
	step [93/244], loss=38.4816
	step [94/244], loss=38.8475
	step [95/244], loss=38.0159
	step [96/244], loss=37.2974
	step [97/244], loss=37.2415
	step [98/244], loss=37.8648
	step [99/244], loss=38.4004
	step [100/244], loss=40.2193
	step [101/244], loss=38.6964
	step [102/244], loss=38.2435
	step [103/244], loss=37.1593
	step [104/244], loss=38.7364
	step [105/244], loss=37.6238
	step [106/244], loss=38.1441
	step [107/244], loss=37.4529
	step [108/244], loss=38.5217
	step [109/244], loss=36.2397
	step [110/244], loss=38.6940
	step [111/244], loss=39.0209
	step [112/244], loss=37.8007
	step [113/244], loss=36.8989
	step [114/244], loss=36.4214
	step [115/244], loss=37.5122
	step [116/244], loss=37.7239
	step [117/244], loss=37.1347
	step [118/244], loss=36.6772
	step [119/244], loss=37.3280
	step [120/244], loss=38.2352
	step [121/244], loss=36.3227
	step [122/244], loss=36.3794
	step [123/244], loss=36.9243
	step [124/244], loss=37.1496
	step [125/244], loss=36.6075
	step [126/244], loss=36.5560
	step [127/244], loss=35.5971
	step [128/244], loss=35.1027
	step [129/244], loss=37.7333
	step [130/244], loss=37.5910
	step [131/244], loss=39.1632
	step [132/244], loss=35.3293
	step [133/244], loss=37.4279
	step [134/244], loss=36.8149
	step [135/244], loss=37.0862
	step [136/244], loss=38.8290
	step [137/244], loss=35.0940
	step [138/244], loss=35.7827
	step [139/244], loss=36.2868
	step [140/244], loss=35.3847
	step [141/244], loss=35.5029
	step [142/244], loss=36.7105
	step [143/244], loss=35.7273
	step [144/244], loss=36.2405
	step [145/244], loss=36.3037
	step [146/244], loss=35.7437
	step [147/244], loss=35.3714
	step [148/244], loss=36.4410
	step [149/244], loss=34.6284
	step [150/244], loss=35.0042
	step [151/244], loss=34.7649
	step [152/244], loss=34.8603
	step [153/244], loss=35.3017
	step [154/244], loss=35.5479
	step [155/244], loss=35.7448
	step [156/244], loss=34.4357
	step [157/244], loss=36.8515
	step [158/244], loss=36.1211
	step [159/244], loss=35.8685
	step [160/244], loss=35.8113
	step [161/244], loss=36.0358
	step [162/244], loss=34.9126
	step [163/244], loss=34.8378
	step [164/244], loss=36.0741
	step [165/244], loss=35.6453
	step [166/244], loss=33.5242
	step [167/244], loss=34.2642
	step [168/244], loss=34.3852
	step [169/244], loss=35.1439
	step [170/244], loss=36.4688
	step [171/244], loss=35.7368
	step [172/244], loss=34.5115
	step [173/244], loss=35.1344
	step [174/244], loss=35.6758
	step [175/244], loss=34.2188
	step [176/244], loss=33.9709
	step [177/244], loss=34.2018
	step [178/244], loss=35.1599
	step [179/244], loss=34.1030
	step [180/244], loss=35.2928
	step [181/244], loss=34.9169
	step [182/244], loss=35.0449
	step [183/244], loss=32.4544
	step [184/244], loss=33.5217
	step [185/244], loss=35.4278
	step [186/244], loss=32.8343
	step [187/244], loss=33.9599
	step [188/244], loss=34.0840
	step [189/244], loss=32.2592
	step [190/244], loss=33.3584
	step [191/244], loss=34.9589
	step [192/244], loss=32.0078
	step [193/244], loss=35.2936
	step [194/244], loss=32.8374
	step [195/244], loss=35.3957
	step [196/244], loss=34.4600
	step [197/244], loss=33.8631
	step [198/244], loss=32.7239
	step [199/244], loss=33.0713
	step [200/244], loss=32.9308
	step [201/244], loss=31.8716
	step [202/244], loss=34.7463
	step [203/244], loss=33.5554
	step [204/244], loss=34.4685
	step [205/244], loss=32.0151
	step [206/244], loss=35.0153
	step [207/244], loss=34.3218
	step [208/244], loss=32.0808
	step [209/244], loss=33.6897
	step [210/244], loss=31.9176
	step [211/244], loss=32.9094
	step [212/244], loss=32.1838
	step [213/244], loss=33.0002
	step [214/244], loss=36.0315
	step [215/244], loss=33.8388
	step [216/244], loss=32.8174
	step [217/244], loss=33.8897
	step [218/244], loss=32.1997
	step [219/244], loss=34.0605
	step [220/244], loss=34.3661
	step [221/244], loss=32.0003
	step [222/244], loss=32.2599
	step [223/244], loss=33.9030
	step [224/244], loss=33.3510
	step [225/244], loss=32.1179
	step [226/244], loss=33.3665
	step [227/244], loss=36.4514
	step [228/244], loss=31.7563
	step [229/244], loss=32.5301
	step [230/244], loss=32.3636
	step [231/244], loss=34.1010
	step [232/244], loss=33.6543
	step [233/244], loss=34.4859
	step [234/244], loss=31.3834
	step [235/244], loss=31.9543
	step [236/244], loss=32.6708
	step [237/244], loss=32.6845
	step [238/244], loss=32.4770
	step [239/244], loss=31.6325
	step [240/244], loss=31.5477
	step [241/244], loss=29.9593
	step [242/244], loss=32.2427
	step [243/244], loss=29.5430
	step [244/244], loss=14.5886
	Evaluating
	loss=0.1104, precision=0.2134, recall=0.9944, f1=0.3514
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/244], loss=30.8711
	step [2/244], loss=31.4678
	step [3/244], loss=31.1689
	step [4/244], loss=31.8424
	step [5/244], loss=31.5194
	step [6/244], loss=31.1619
	step [7/244], loss=31.2267
	step [8/244], loss=31.7225
	step [9/244], loss=32.8172
	step [10/244], loss=30.5247
	step [11/244], loss=30.2675
	step [12/244], loss=30.9567
	step [13/244], loss=32.1546
	step [14/244], loss=29.1232
	step [15/244], loss=31.5014
	step [16/244], loss=30.2829
	step [17/244], loss=32.6676
	step [18/244], loss=30.2102
	step [19/244], loss=30.5768
	step [20/244], loss=30.7784
	step [21/244], loss=31.9106
	step [22/244], loss=29.9613
	step [23/244], loss=32.8889
	step [24/244], loss=31.1548
	step [25/244], loss=29.9095
	step [26/244], loss=31.2281
	step [27/244], loss=29.6556
	step [28/244], loss=30.1796
	step [29/244], loss=31.8895
	step [30/244], loss=32.5834
	step [31/244], loss=30.9627
	step [32/244], loss=30.4076
	step [33/244], loss=30.3290
	step [34/244], loss=31.8265
	step [35/244], loss=29.6236
	step [36/244], loss=28.3043
	step [37/244], loss=30.3055
	step [38/244], loss=29.9419
	step [39/244], loss=30.8040
	step [40/244], loss=31.4154
	step [41/244], loss=32.7886
	step [42/244], loss=30.1515
	step [43/244], loss=30.7216
	step [44/244], loss=30.4573
	step [45/244], loss=29.6135
	step [46/244], loss=31.2339
	step [47/244], loss=31.9626
	step [48/244], loss=29.1835
	step [49/244], loss=29.9681
	step [50/244], loss=30.0646
	step [51/244], loss=29.6244
	step [52/244], loss=30.5151
	step [53/244], loss=28.5474
	step [54/244], loss=29.9011
	step [55/244], loss=28.6267
	step [56/244], loss=28.9176
	step [57/244], loss=30.8570
	step [58/244], loss=32.3432
	step [59/244], loss=31.8202
	step [60/244], loss=30.1473
	step [61/244], loss=30.4559
	step [62/244], loss=29.8613
	step [63/244], loss=30.9751
	step [64/244], loss=30.2556
	step [65/244], loss=30.4455
	step [66/244], loss=29.6571
	step [67/244], loss=29.7581
	step [68/244], loss=29.7835
	step [69/244], loss=29.6818
	step [70/244], loss=30.3500
	step [71/244], loss=29.1113
	step [72/244], loss=26.9479
	step [73/244], loss=31.0458
	step [74/244], loss=28.8315
	step [75/244], loss=29.2019
	step [76/244], loss=28.1076
	step [77/244], loss=29.8642
	step [78/244], loss=28.1636
	step [79/244], loss=31.7115
	step [80/244], loss=29.3891
	step [81/244], loss=29.6038
	step [82/244], loss=27.6137
	step [83/244], loss=29.8346
	step [84/244], loss=30.8773
	step [85/244], loss=27.9663
	step [86/244], loss=28.3627
	step [87/244], loss=27.8716
	step [88/244], loss=28.1245
	step [89/244], loss=30.1007
	step [90/244], loss=30.3581
	step [91/244], loss=31.4056
	step [92/244], loss=27.6491
	step [93/244], loss=27.4787
	step [94/244], loss=27.8330
	step [95/244], loss=29.3997
	step [96/244], loss=27.6509
	step [97/244], loss=28.0538
	step [98/244], loss=28.8932
	step [99/244], loss=28.2813
	step [100/244], loss=28.2367
	step [101/244], loss=28.2942
	step [102/244], loss=28.5410
	step [103/244], loss=27.7189
	step [104/244], loss=28.7214
	step [105/244], loss=27.9041
	step [106/244], loss=28.8424
	step [107/244], loss=27.8182
	step [108/244], loss=28.0992
	step [109/244], loss=28.6496
	step [110/244], loss=27.1257
	step [111/244], loss=28.9076
	step [112/244], loss=28.6247
	step [113/244], loss=29.8871
	step [114/244], loss=28.6176
	step [115/244], loss=27.4422
	step [116/244], loss=26.6076
	step [117/244], loss=27.8611
	step [118/244], loss=27.1613
	step [119/244], loss=27.9134
	step [120/244], loss=28.7353
	step [121/244], loss=28.3447
	step [122/244], loss=27.4797
	step [123/244], loss=27.6444
	step [124/244], loss=27.7794
	step [125/244], loss=27.4763
	step [126/244], loss=28.2351
	step [127/244], loss=27.1125
	step [128/244], loss=32.4940
	step [129/244], loss=26.3053
	step [130/244], loss=27.4143
	step [131/244], loss=28.1314
	step [132/244], loss=26.8316
	step [133/244], loss=26.3229
	step [134/244], loss=27.9099
	step [135/244], loss=26.2867
	step [136/244], loss=27.1253
	step [137/244], loss=26.8505
	step [138/244], loss=26.1879
	step [139/244], loss=27.5073
	step [140/244], loss=30.4653
	step [141/244], loss=29.7586
	step [142/244], loss=28.0804
	step [143/244], loss=29.6699
	step [144/244], loss=26.5134
	step [145/244], loss=27.8489
	step [146/244], loss=25.9277
	step [147/244], loss=27.6286
	step [148/244], loss=27.6771
	step [149/244], loss=26.4304
	step [150/244], loss=26.2356
	step [151/244], loss=27.9808
	step [152/244], loss=26.5563
	step [153/244], loss=26.2736
	step [154/244], loss=29.1367
	step [155/244], loss=26.5602
	step [156/244], loss=30.2305
	step [157/244], loss=27.8907
	step [158/244], loss=27.1256
	step [159/244], loss=25.8578
	step [160/244], loss=27.8273
	step [161/244], loss=28.3917
	step [162/244], loss=27.6054
	step [163/244], loss=26.1427
	step [164/244], loss=26.3812
	step [165/244], loss=25.3758
	step [166/244], loss=27.6853
	step [167/244], loss=25.9730
	step [168/244], loss=26.1744
	step [169/244], loss=25.2559
	step [170/244], loss=26.4236
	step [171/244], loss=26.0493
	step [172/244], loss=27.6907
	step [173/244], loss=27.8260
	step [174/244], loss=26.6704
	step [175/244], loss=27.3158
	step [176/244], loss=25.8129
	step [177/244], loss=26.1363
	step [178/244], loss=24.4650
	step [179/244], loss=24.7530
	step [180/244], loss=25.9306
	step [181/244], loss=28.3626
	step [182/244], loss=26.9514
	step [183/244], loss=26.9609
	step [184/244], loss=26.7008
	step [185/244], loss=28.9146
	step [186/244], loss=24.5495
	step [187/244], loss=26.7625
	step [188/244], loss=27.0866
	step [189/244], loss=28.5977
	step [190/244], loss=26.6128
	step [191/244], loss=25.5264
	step [192/244], loss=27.1095
	step [193/244], loss=24.6003
	step [194/244], loss=27.3780
	step [195/244], loss=25.7365
	step [196/244], loss=25.7115
	step [197/244], loss=26.5338
	step [198/244], loss=26.1159
	step [199/244], loss=27.4518
	step [200/244], loss=25.3817
	step [201/244], loss=26.4780
	step [202/244], loss=23.2818
	step [203/244], loss=25.8128
	step [204/244], loss=25.8852
	step [205/244], loss=25.2363
	step [206/244], loss=26.9255
	step [207/244], loss=28.5224
	step [208/244], loss=24.2207
	step [209/244], loss=24.5522
	step [210/244], loss=26.1883
	step [211/244], loss=26.3232
	step [212/244], loss=24.9963
	step [213/244], loss=25.4546
	step [214/244], loss=26.8318
	step [215/244], loss=26.5990
	step [216/244], loss=28.6570
	step [217/244], loss=26.0257
	step [218/244], loss=25.3241
	step [219/244], loss=24.8610
	step [220/244], loss=25.7676
	step [221/244], loss=23.7269
	step [222/244], loss=26.5999
	step [223/244], loss=25.8065
	step [224/244], loss=27.2675
	step [225/244], loss=26.4575
	step [226/244], loss=23.9592
	step [227/244], loss=26.6659
	step [228/244], loss=24.6541
	step [229/244], loss=26.2078
	step [230/244], loss=25.9627
	step [231/244], loss=25.6817
	step [232/244], loss=24.4259
	step [233/244], loss=27.5447
	step [234/244], loss=23.9521
	step [235/244], loss=27.3250
	step [236/244], loss=23.3135
	step [237/244], loss=25.2647
	step [238/244], loss=26.6857
	step [239/244], loss=23.9161
	step [240/244], loss=25.8492
	step [241/244], loss=23.6719
	step [242/244], loss=23.7188
	step [243/244], loss=24.0814
	step [244/244], loss=10.0583
	Evaluating
	loss=0.0852, precision=0.1893, recall=0.9952, f1=0.3181
Training epoch 5
	step [1/244], loss=23.6862
	step [2/244], loss=25.0268
	step [3/244], loss=23.6173
	step [4/244], loss=23.0960
	step [5/244], loss=22.0639
	step [6/244], loss=24.5868
	step [7/244], loss=24.2102
	step [8/244], loss=24.3243
	step [9/244], loss=27.1529
	step [10/244], loss=24.5020
	step [11/244], loss=24.2886
	step [12/244], loss=25.4260
	step [13/244], loss=22.9955
	step [14/244], loss=27.3929
	step [15/244], loss=24.3570
	step [16/244], loss=24.0777
	step [17/244], loss=24.6309
	step [18/244], loss=22.8825
	step [19/244], loss=23.8043
	step [20/244], loss=24.3681
	step [21/244], loss=23.7708
	step [22/244], loss=24.1572
	step [23/244], loss=28.3862
	step [24/244], loss=24.0904
	step [25/244], loss=23.0784
	step [26/244], loss=23.4832
	step [27/244], loss=24.8618
	step [28/244], loss=23.2643
	step [29/244], loss=24.7727
	step [30/244], loss=24.9885
	step [31/244], loss=24.2160
	step [32/244], loss=23.0830
	step [33/244], loss=21.8796
	step [34/244], loss=21.7452
	step [35/244], loss=22.3719
	step [36/244], loss=24.1869
	step [37/244], loss=24.6919
	step [38/244], loss=23.4700
	step [39/244], loss=22.7753
	step [40/244], loss=24.5437
	step [41/244], loss=23.6972
	step [42/244], loss=23.6910
	step [43/244], loss=23.1914
	step [44/244], loss=24.5282
	step [45/244], loss=22.5184
	step [46/244], loss=23.4782
	step [47/244], loss=24.4851
	step [48/244], loss=23.0134
	step [49/244], loss=24.6646
	step [50/244], loss=23.3927
	step [51/244], loss=22.2987
	step [52/244], loss=22.1851
	step [53/244], loss=22.7573
	step [54/244], loss=25.1991
	step [55/244], loss=22.5244
	step [56/244], loss=22.3132
	step [57/244], loss=22.7987
	step [58/244], loss=23.1756
	step [59/244], loss=22.2020
	step [60/244], loss=22.7546
	step [61/244], loss=21.3206
	step [62/244], loss=24.7924
	step [63/244], loss=24.3951
	step [64/244], loss=23.2371
	step [65/244], loss=25.7391
	step [66/244], loss=22.5546
	step [67/244], loss=24.7740
	step [68/244], loss=22.9626
	step [69/244], loss=22.8376
	step [70/244], loss=21.6415
	step [71/244], loss=22.1956
	step [72/244], loss=22.9206
	step [73/244], loss=24.0148
	step [74/244], loss=22.4701
	step [75/244], loss=24.3278
	step [76/244], loss=23.8817
	step [77/244], loss=22.2518
	step [78/244], loss=22.6389
	step [79/244], loss=24.5068
	step [80/244], loss=23.0093
	step [81/244], loss=23.4256
	step [82/244], loss=23.5807
	step [83/244], loss=27.2366
	step [84/244], loss=25.4585
	step [85/244], loss=21.7465
	step [86/244], loss=23.2024
	step [87/244], loss=24.0152
	step [88/244], loss=22.6644
	step [89/244], loss=23.0841
	step [90/244], loss=23.0964
	step [91/244], loss=24.0313
	step [92/244], loss=22.5112
	step [93/244], loss=22.0819
	step [94/244], loss=22.5568
	step [95/244], loss=20.5992
	step [96/244], loss=23.0546
	step [97/244], loss=22.3795
	step [98/244], loss=21.4275
	step [99/244], loss=24.9966
	step [100/244], loss=22.4079
	step [101/244], loss=23.4116
	step [102/244], loss=20.7400
	step [103/244], loss=22.3420
	step [104/244], loss=22.0194
	step [105/244], loss=20.9769
	step [106/244], loss=24.0442
	step [107/244], loss=22.1304
	step [108/244], loss=23.6286
	step [109/244], loss=22.1803
	step [110/244], loss=22.9122
	step [111/244], loss=22.2041
	step [112/244], loss=22.2331
	step [113/244], loss=21.4212
	step [114/244], loss=24.9652
	step [115/244], loss=22.3797
	step [116/244], loss=21.5646
	step [117/244], loss=23.8260
	step [118/244], loss=21.0762
	step [119/244], loss=23.0703
	step [120/244], loss=21.6436
	step [121/244], loss=21.3764
	step [122/244], loss=22.6119
	step [123/244], loss=22.3899
	step [124/244], loss=23.6123
	step [125/244], loss=21.0067
	step [126/244], loss=22.5513
	step [127/244], loss=21.9434
	step [128/244], loss=21.8401
	step [129/244], loss=22.1258
	step [130/244], loss=22.5025
	step [131/244], loss=21.8658
	step [132/244], loss=20.1203
	step [133/244], loss=22.0927
	step [134/244], loss=21.1898
	step [135/244], loss=22.0252
	step [136/244], loss=23.2044
	step [137/244], loss=20.9455
	step [138/244], loss=22.6539
	step [139/244], loss=22.2289
	step [140/244], loss=20.6983
	step [141/244], loss=23.2768
	step [142/244], loss=20.4620
	step [143/244], loss=24.5549
	step [144/244], loss=21.9787
	step [145/244], loss=23.2690
	step [146/244], loss=21.2388
	step [147/244], loss=23.3528
	step [148/244], loss=23.2695
	step [149/244], loss=22.5201
	step [150/244], loss=20.9457
	step [151/244], loss=20.7705
	step [152/244], loss=21.3000
	step [153/244], loss=21.3358
	step [154/244], loss=22.2918
	step [155/244], loss=19.7404
	step [156/244], loss=19.8398
	step [157/244], loss=21.2957
	step [158/244], loss=20.2497
	step [159/244], loss=21.1065
	step [160/244], loss=19.9320
	step [161/244], loss=20.0850
	step [162/244], loss=22.7554
	step [163/244], loss=20.8324
	step [164/244], loss=21.5880
	step [165/244], loss=20.5864
	step [166/244], loss=22.3903
	step [167/244], loss=21.5996
	step [168/244], loss=20.1758
	step [169/244], loss=21.6102
	step [170/244], loss=22.9713
	step [171/244], loss=20.8109
	step [172/244], loss=21.8274
	step [173/244], loss=20.1274
	step [174/244], loss=21.5124
	step [175/244], loss=21.5099
	step [176/244], loss=22.4768
	step [177/244], loss=20.3179
	step [178/244], loss=21.1757
	step [179/244], loss=20.9701
	step [180/244], loss=21.9179
	step [181/244], loss=24.4666
	step [182/244], loss=19.8646
	step [183/244], loss=21.5089
	step [184/244], loss=20.5380
	step [185/244], loss=19.7823
	step [186/244], loss=22.5244
	step [187/244], loss=19.9989
	step [188/244], loss=20.6039
	step [189/244], loss=20.8386
	step [190/244], loss=22.4102
	step [191/244], loss=20.5584
	step [192/244], loss=20.6565
	step [193/244], loss=18.5853
	step [194/244], loss=21.2669
	step [195/244], loss=21.4316
	step [196/244], loss=21.9230
	step [197/244], loss=19.8848
	step [198/244], loss=21.5634
	step [199/244], loss=19.8762
	step [200/244], loss=22.0609
	step [201/244], loss=20.1541
	step [202/244], loss=19.4279
	step [203/244], loss=21.3665
	step [204/244], loss=20.9953
	step [205/244], loss=21.4107
	step [206/244], loss=19.0872
	step [207/244], loss=20.4330
	step [208/244], loss=19.5680
	step [209/244], loss=19.0587
	step [210/244], loss=19.9249
	step [211/244], loss=22.7002
	step [212/244], loss=20.4270
	step [213/244], loss=19.3037
	step [214/244], loss=18.7399
	step [215/244], loss=20.5255
	step [216/244], loss=19.5092
	step [217/244], loss=20.6534
	step [218/244], loss=19.7750
	step [219/244], loss=18.8907
	step [220/244], loss=18.1347
	step [221/244], loss=18.7477
	step [222/244], loss=18.7179
	step [223/244], loss=19.0184
	step [224/244], loss=23.0651
	step [225/244], loss=18.9132
	step [226/244], loss=20.1342
	step [227/244], loss=19.1294
	step [228/244], loss=21.2633
	step [229/244], loss=21.1026
	step [230/244], loss=19.4532
	step [231/244], loss=19.1053
	step [232/244], loss=20.5146
	step [233/244], loss=19.8643
	step [234/244], loss=18.3392
	step [235/244], loss=19.6049
	step [236/244], loss=20.3381
	step [237/244], loss=19.3362
	step [238/244], loss=19.1960
	step [239/244], loss=20.2479
	step [240/244], loss=20.1916
	step [241/244], loss=21.6339
	step [242/244], loss=19.9321
	step [243/244], loss=18.9426
	step [244/244], loss=7.3825
	Evaluating
	loss=0.0728, precision=0.1422, recall=0.9966, f1=0.2489
Training epoch 6
	step [1/244], loss=18.1869
	step [2/244], loss=20.4093
	step [3/244], loss=19.1772
	step [4/244], loss=21.5260
	step [5/244], loss=18.2473
	step [6/244], loss=18.9616
	step [7/244], loss=19.3455
	step [8/244], loss=18.5417
	step [9/244], loss=19.5384
	step [10/244], loss=18.5586
	step [11/244], loss=19.1986
	step [12/244], loss=18.6280
	step [13/244], loss=20.9996
	step [14/244], loss=19.6334
	step [15/244], loss=18.2557
	step [16/244], loss=18.6289
	step [17/244], loss=19.7400
	step [18/244], loss=19.2701
	step [19/244], loss=21.0315
	step [20/244], loss=18.1137
	step [21/244], loss=18.6842
	step [22/244], loss=18.6755
	step [23/244], loss=19.9939
	step [24/244], loss=19.8130
	step [25/244], loss=21.0605
	step [26/244], loss=20.6665
	step [27/244], loss=19.4638
	step [28/244], loss=18.6067
	step [29/244], loss=19.3854
	step [30/244], loss=20.2243
	step [31/244], loss=19.1659
	step [32/244], loss=18.3168
	step [33/244], loss=18.5445
	step [34/244], loss=19.4946
	step [35/244], loss=19.1376
	step [36/244], loss=19.9416
	step [37/244], loss=19.6898
	step [38/244], loss=19.3707
	step [39/244], loss=19.7860
	step [40/244], loss=18.6165
	step [41/244], loss=21.6329
	step [42/244], loss=18.9950
	step [43/244], loss=18.2722
	step [44/244], loss=20.7072
	step [45/244], loss=20.3458
	step [46/244], loss=19.1918
	step [47/244], loss=18.9439
	step [48/244], loss=19.8305
	step [49/244], loss=18.4163
	step [50/244], loss=20.4584
	step [51/244], loss=20.3203
	step [52/244], loss=19.2991
	step [53/244], loss=20.0359
	step [54/244], loss=19.6775
	step [55/244], loss=20.5739
	step [56/244], loss=20.2934
	step [57/244], loss=18.5131
	step [58/244], loss=19.2933
	step [59/244], loss=18.4273
	step [60/244], loss=20.1577
	step [61/244], loss=19.3599
	step [62/244], loss=20.5848
	step [63/244], loss=20.1465
	step [64/244], loss=19.5002
	step [65/244], loss=17.8595
	step [66/244], loss=19.6802
	step [67/244], loss=18.3723
	step [68/244], loss=17.7534
	step [69/244], loss=19.9318
	step [70/244], loss=19.3161
	step [71/244], loss=19.7917
	step [72/244], loss=18.4250
	step [73/244], loss=17.9683
	step [74/244], loss=19.4261
	step [75/244], loss=17.1760
	step [76/244], loss=21.1719
	step [77/244], loss=19.5336
	step [78/244], loss=16.5875
	step [79/244], loss=19.1955
	step [80/244], loss=18.3882
	step [81/244], loss=19.0128
	step [82/244], loss=18.7736
	step [83/244], loss=17.9610
	step [84/244], loss=19.1735
	step [85/244], loss=18.6661
	step [86/244], loss=17.0415
	step [87/244], loss=18.4370
	step [88/244], loss=17.5690
	step [89/244], loss=21.8683
	step [90/244], loss=19.3918
	step [91/244], loss=20.0696
	step [92/244], loss=17.6696
	step [93/244], loss=18.9095
	step [94/244], loss=20.3289
	step [95/244], loss=17.8253
	step [96/244], loss=18.3648
	step [97/244], loss=20.0216
	step [98/244], loss=19.8776
	step [99/244], loss=21.8651
	step [100/244], loss=20.3642
	step [101/244], loss=18.2659
	step [102/244], loss=18.5976
	step [103/244], loss=18.1039
	step [104/244], loss=18.1077
	step [105/244], loss=16.7677
	step [106/244], loss=21.2078
	step [107/244], loss=17.2635
	step [108/244], loss=18.2298
	step [109/244], loss=19.7874
	step [110/244], loss=17.9670
	step [111/244], loss=19.1014
	step [112/244], loss=17.3166
	step [113/244], loss=17.9316
	step [114/244], loss=18.4877
	step [115/244], loss=17.6967
	step [116/244], loss=16.8058
	step [117/244], loss=18.2725
	step [118/244], loss=17.6794
	step [119/244], loss=17.5810
	step [120/244], loss=16.6748
	step [121/244], loss=18.4278
	step [122/244], loss=18.1384
	step [123/244], loss=19.1378
	step [124/244], loss=17.8723
	step [125/244], loss=16.2631
	step [126/244], loss=17.7797
	step [127/244], loss=17.7469
	step [128/244], loss=19.0054
	step [129/244], loss=16.4865
	step [130/244], loss=18.7185
	step [131/244], loss=17.3734
	step [132/244], loss=17.1806
	step [133/244], loss=18.0039
	step [134/244], loss=20.5231
	step [135/244], loss=19.4537
	step [136/244], loss=17.9196
	step [137/244], loss=18.1589
	step [138/244], loss=18.0266
	step [139/244], loss=18.8685
	step [140/244], loss=18.9739
	step [141/244], loss=17.1125
	step [142/244], loss=17.8366
	step [143/244], loss=19.3875
	step [144/244], loss=18.3639
	step [145/244], loss=18.5733
	step [146/244], loss=15.3970
	step [147/244], loss=15.9174
	step [148/244], loss=17.1755
	step [149/244], loss=18.3960
	step [150/244], loss=17.7325
	step [151/244], loss=17.8399
	step [152/244], loss=17.8069
	step [153/244], loss=18.6509
	step [154/244], loss=19.6694
	step [155/244], loss=17.8846
	step [156/244], loss=19.3097
	step [157/244], loss=17.6072
	step [158/244], loss=18.3809
	step [159/244], loss=19.8927
	step [160/244], loss=16.2448
	step [161/244], loss=18.1137
	step [162/244], loss=17.0570
	step [163/244], loss=16.1227
	step [164/244], loss=17.6509
	step [165/244], loss=17.7504
	step [166/244], loss=18.1606
	step [167/244], loss=17.0494
	step [168/244], loss=18.5488
	step [169/244], loss=17.2497
	step [170/244], loss=20.1365
	step [171/244], loss=17.8373
	step [172/244], loss=16.9072
	step [173/244], loss=17.9771
	step [174/244], loss=17.6032
	step [175/244], loss=18.0472
	step [176/244], loss=17.2994
	step [177/244], loss=16.3912
	step [178/244], loss=20.2432
	step [179/244], loss=16.4806
	step [180/244], loss=20.6617
	step [181/244], loss=15.7738
	step [182/244], loss=17.7765
	step [183/244], loss=17.5976
	step [184/244], loss=18.5214
	step [185/244], loss=17.0044
	step [186/244], loss=16.8705
	step [187/244], loss=17.0960
	step [188/244], loss=17.5020
	step [189/244], loss=15.0646
	step [190/244], loss=16.1414
	step [191/244], loss=17.7469
	step [192/244], loss=17.6328
	step [193/244], loss=15.4112
	step [194/244], loss=15.3248
	step [195/244], loss=17.4585
	step [196/244], loss=17.2613
	step [197/244], loss=16.3812
	step [198/244], loss=17.7052
	step [199/244], loss=17.7572
	step [200/244], loss=17.6723
	step [201/244], loss=15.9560
	step [202/244], loss=16.5581
	step [203/244], loss=17.0706
	step [204/244], loss=17.0869
	step [205/244], loss=17.1050
	step [206/244], loss=16.6251
	step [207/244], loss=18.3036
	step [208/244], loss=16.5360
	step [209/244], loss=16.5841
	step [210/244], loss=17.3797
	step [211/244], loss=17.5187
	step [212/244], loss=19.4866
	step [213/244], loss=16.3313
	step [214/244], loss=17.0983
	step [215/244], loss=17.0420
	step [216/244], loss=19.4722
	step [217/244], loss=17.1571
	step [218/244], loss=17.6205
	step [219/244], loss=15.4043
	step [220/244], loss=15.4936
	step [221/244], loss=15.1717
	step [222/244], loss=17.4906
	step [223/244], loss=16.7943
	step [224/244], loss=18.5313
	step [225/244], loss=16.0963
	step [226/244], loss=17.1393
	step [227/244], loss=16.7823
	step [228/244], loss=16.3194
	step [229/244], loss=19.0334
	step [230/244], loss=14.9140
	step [231/244], loss=15.1254
	step [232/244], loss=15.7892
	step [233/244], loss=16.8043
	step [234/244], loss=15.2465
	step [235/244], loss=15.5059
	step [236/244], loss=17.4015
	step [237/244], loss=16.3118
	step [238/244], loss=16.0567
	step [239/244], loss=16.5435
	step [240/244], loss=15.7893
	step [241/244], loss=16.0268
	step [242/244], loss=15.8847
	step [243/244], loss=17.2966
	step [244/244], loss=7.2845
	Evaluating
	loss=0.0512, precision=0.2170, recall=0.9941, f1=0.3562
saving model as: 1_saved_model.pth
Training epoch 7
	step [1/244], loss=15.7141
	step [2/244], loss=17.1757
	step [3/244], loss=15.8721
	step [4/244], loss=19.8092
	step [5/244], loss=15.7939
	step [6/244], loss=18.3656
	step [7/244], loss=16.4993
	step [8/244], loss=17.5578
	step [9/244], loss=15.2055
	step [10/244], loss=19.3872
	step [11/244], loss=15.6419
	step [12/244], loss=16.3047
	step [13/244], loss=17.2344
	step [14/244], loss=15.8705
	step [15/244], loss=17.6040
	step [16/244], loss=16.0656
	step [17/244], loss=17.7273
	step [18/244], loss=17.1866
	step [19/244], loss=15.8537
	step [20/244], loss=17.1740
	step [21/244], loss=17.0707
	step [22/244], loss=17.1441
	step [23/244], loss=17.0114
	step [24/244], loss=16.9034
	step [25/244], loss=20.2524
	step [26/244], loss=16.6104
	step [27/244], loss=17.7286
	step [28/244], loss=16.5647
	step [29/244], loss=16.1884
	step [30/244], loss=17.4029
	step [31/244], loss=17.0667
	step [32/244], loss=16.0216
	step [33/244], loss=16.3498
	step [34/244], loss=15.6803
	step [35/244], loss=17.4963
	step [36/244], loss=14.4511
	step [37/244], loss=15.0427
	step [38/244], loss=15.1402
	step [39/244], loss=14.7342
	step [40/244], loss=15.0422
	step [41/244], loss=14.6005
	step [42/244], loss=16.4896
	step [43/244], loss=17.7105
	step [44/244], loss=16.0702
	step [45/244], loss=15.3277
	step [46/244], loss=17.1981
	step [47/244], loss=19.9517
	step [48/244], loss=15.3161
	step [49/244], loss=16.6760
	step [50/244], loss=15.6549
	step [51/244], loss=15.5123
	step [52/244], loss=15.6674
	step [53/244], loss=15.4289
	step [54/244], loss=17.1238
	step [55/244], loss=16.2251
	step [56/244], loss=18.8584
	step [57/244], loss=14.9641
	step [58/244], loss=19.3006
	step [59/244], loss=16.2235
	step [60/244], loss=14.8237
	step [61/244], loss=14.9124
	step [62/244], loss=16.6554
	step [63/244], loss=14.0939
	step [64/244], loss=14.5938
	step [65/244], loss=15.6679
	step [66/244], loss=14.5839
	step [67/244], loss=14.1088
	step [68/244], loss=15.0570
	step [69/244], loss=15.2093
	step [70/244], loss=17.2776
	step [71/244], loss=19.1280
	step [72/244], loss=17.1205
	step [73/244], loss=16.2449
	step [74/244], loss=14.7084
	step [75/244], loss=15.8674
	step [76/244], loss=14.6858
	step [77/244], loss=16.7905
	step [78/244], loss=16.0094
	step [79/244], loss=16.2357
	step [80/244], loss=17.1494
	step [81/244], loss=15.8584
	step [82/244], loss=16.2415
	step [83/244], loss=16.3124
	step [84/244], loss=16.2774
	step [85/244], loss=16.8771
	step [86/244], loss=18.2431
	step [87/244], loss=15.9054
	step [88/244], loss=15.1832
	step [89/244], loss=14.9382
	step [90/244], loss=15.4446
	step [91/244], loss=17.1616
	step [92/244], loss=16.1587
	step [93/244], loss=14.9200
	step [94/244], loss=14.7544
	step [95/244], loss=16.8609
	step [96/244], loss=16.9828
	step [97/244], loss=14.3359
	step [98/244], loss=18.0033
	step [99/244], loss=15.6180
	step [100/244], loss=15.5378
	step [101/244], loss=14.8009
	step [102/244], loss=15.2978
	step [103/244], loss=15.3014
	step [104/244], loss=15.1269
	step [105/244], loss=13.6852
	step [106/244], loss=17.1586
	step [107/244], loss=15.4572
	step [108/244], loss=13.3948
	step [109/244], loss=14.4684
	step [110/244], loss=15.3475
	step [111/244], loss=16.2203
	step [112/244], loss=14.8669
	step [113/244], loss=18.3235
	step [114/244], loss=15.6704
	step [115/244], loss=16.7791
	step [116/244], loss=15.1013
	step [117/244], loss=14.4720
	step [118/244], loss=16.7269
	step [119/244], loss=15.6678
	step [120/244], loss=14.7371
	step [121/244], loss=15.8854
	step [122/244], loss=13.3588
	step [123/244], loss=15.4618
	step [124/244], loss=15.0896
	step [125/244], loss=17.6020
	step [126/244], loss=14.7105
	step [127/244], loss=15.0995
	step [128/244], loss=16.9017
	step [129/244], loss=16.5512
	step [130/244], loss=15.3687
	step [131/244], loss=16.9453
	step [132/244], loss=15.8555
	step [133/244], loss=14.9674
	step [134/244], loss=15.4658
	step [135/244], loss=14.6160
	step [136/244], loss=14.4969
	step [137/244], loss=16.0052
	step [138/244], loss=15.8202
	step [139/244], loss=16.0024
	step [140/244], loss=15.8565
	step [141/244], loss=15.0203
	step [142/244], loss=15.7085
	step [143/244], loss=15.2632
	step [144/244], loss=16.0019
	step [145/244], loss=15.5084
	step [146/244], loss=14.7290
	step [147/244], loss=13.7821
	step [148/244], loss=15.2040
	step [149/244], loss=14.4434
	step [150/244], loss=14.6266
	step [151/244], loss=16.2321
	step [152/244], loss=14.4831
	step [153/244], loss=13.7989
	step [154/244], loss=14.3144
	step [155/244], loss=15.0509
	step [156/244], loss=14.0655
	step [157/244], loss=17.6056
	step [158/244], loss=13.7380
	step [159/244], loss=13.8110
	step [160/244], loss=15.1507
	step [161/244], loss=15.0873
	step [162/244], loss=15.5769
	step [163/244], loss=15.7942
	step [164/244], loss=15.1397
	step [165/244], loss=15.4778
	step [166/244], loss=15.2548
	step [167/244], loss=14.4737
	step [168/244], loss=15.6557
	step [169/244], loss=15.5863
	step [170/244], loss=15.9207
	step [171/244], loss=14.1612
	step [172/244], loss=13.7090
	step [173/244], loss=14.0225
	step [174/244], loss=14.9321
	step [175/244], loss=14.2501
	step [176/244], loss=16.1788
	step [177/244], loss=16.4844
	step [178/244], loss=15.6110
	step [179/244], loss=14.5121
	step [180/244], loss=15.4771
	step [181/244], loss=14.1301
	step [182/244], loss=13.3890
	step [183/244], loss=15.0653
	step [184/244], loss=15.6041
	step [185/244], loss=14.6943
	step [186/244], loss=14.3537
	step [187/244], loss=13.5600
	step [188/244], loss=13.6249
	step [189/244], loss=14.6485
	step [190/244], loss=15.9706
	step [191/244], loss=14.0952
	step [192/244], loss=14.0940
	step [193/244], loss=16.4120
	step [194/244], loss=14.8795
	step [195/244], loss=15.0414
	step [196/244], loss=16.1823
	step [197/244], loss=15.6362
	step [198/244], loss=15.6401
	step [199/244], loss=14.1297
	step [200/244], loss=14.3550
	step [201/244], loss=14.3260
	step [202/244], loss=13.9821
	step [203/244], loss=13.9129
	step [204/244], loss=14.4747
	step [205/244], loss=15.4248
	step [206/244], loss=16.4067
	step [207/244], loss=14.7778
	step [208/244], loss=15.4737
	step [209/244], loss=14.3423
	step [210/244], loss=15.4075
	step [211/244], loss=14.8889
	step [212/244], loss=13.7913
	step [213/244], loss=15.5949
	step [214/244], loss=16.0892
	step [215/244], loss=16.1707
	step [216/244], loss=15.2727
	step [217/244], loss=15.4446
	step [218/244], loss=14.4512
	step [219/244], loss=15.3530
	step [220/244], loss=15.7227
	step [221/244], loss=14.1512
	step [222/244], loss=14.3910
	step [223/244], loss=16.6336
	step [224/244], loss=13.9898
	step [225/244], loss=13.2464
	step [226/244], loss=15.9277
	step [227/244], loss=13.0423
	step [228/244], loss=15.1364
	step [229/244], loss=16.2593
	step [230/244], loss=13.7400
	step [231/244], loss=14.9480
	step [232/244], loss=14.4484
	step [233/244], loss=13.6092
	step [234/244], loss=14.9398
	step [235/244], loss=13.8354
	step [236/244], loss=13.9234
	step [237/244], loss=13.7584
	step [238/244], loss=14.0797
	step [239/244], loss=13.8990
	step [240/244], loss=13.1844
	step [241/244], loss=16.9437
	step [242/244], loss=14.2281
	step [243/244], loss=14.7171
	step [244/244], loss=7.3210
	Evaluating
	loss=0.0524, precision=0.1282, recall=0.9970, f1=0.2273
Training epoch 8
	step [1/244], loss=16.7392
	step [2/244], loss=13.9276
	step [3/244], loss=14.6253
	step [4/244], loss=14.6872
	step [5/244], loss=14.1923
	step [6/244], loss=13.5619
	step [7/244], loss=14.5236
	step [8/244], loss=14.6559
	step [9/244], loss=14.9214
	step [10/244], loss=13.9405
	step [11/244], loss=15.6056
	step [12/244], loss=14.1180
	step [13/244], loss=12.5248
	step [14/244], loss=14.6670
	step [15/244], loss=13.6038
	step [16/244], loss=13.7204
	step [17/244], loss=14.0194
	step [18/244], loss=13.2403
	step [19/244], loss=15.2579
	step [20/244], loss=15.4832
	step [21/244], loss=12.7167
	step [22/244], loss=15.0681
	step [23/244], loss=14.1955
	step [24/244], loss=15.0568
	step [25/244], loss=12.4155
	step [26/244], loss=14.0112
	step [27/244], loss=14.7539
	step [28/244], loss=13.7290
	step [29/244], loss=16.2548
	step [30/244], loss=15.4013
	step [31/244], loss=13.6860
	step [32/244], loss=14.4503
	step [33/244], loss=14.2068
	step [34/244], loss=14.2293
	step [35/244], loss=13.8207
	step [36/244], loss=12.3721
	step [37/244], loss=14.6871
	step [38/244], loss=12.8376
	step [39/244], loss=14.4500
	step [40/244], loss=13.1952
	step [41/244], loss=12.5863
	step [42/244], loss=16.4955
	step [43/244], loss=12.5649
	step [44/244], loss=12.8659
	step [45/244], loss=13.3837
	step [46/244], loss=12.8238
	step [47/244], loss=17.1329
	step [48/244], loss=14.8662
	step [49/244], loss=14.2778
	step [50/244], loss=14.6280
	step [51/244], loss=14.9498
	step [52/244], loss=14.5479
	step [53/244], loss=13.2452
	step [54/244], loss=13.2707
	step [55/244], loss=13.7181
	step [56/244], loss=15.7069
	step [57/244], loss=13.3698
	step [58/244], loss=13.6675
	step [59/244], loss=12.6747
	step [60/244], loss=14.2296
	step [61/244], loss=12.7045
	step [62/244], loss=15.2312
	step [63/244], loss=12.9306
	step [64/244], loss=15.0970
	step [65/244], loss=11.9785
	step [66/244], loss=13.6643
	step [67/244], loss=12.7663
	step [68/244], loss=12.5846
	step [69/244], loss=14.7355
	step [70/244], loss=15.1855
	step [71/244], loss=14.1337
	step [72/244], loss=14.7195
	step [73/244], loss=14.7134
	step [74/244], loss=13.4612
	step [75/244], loss=12.9780
	step [76/244], loss=13.4908
	step [77/244], loss=13.5662
	step [78/244], loss=13.5451
	step [79/244], loss=15.1845
	step [80/244], loss=17.2435
	step [81/244], loss=12.1037
	step [82/244], loss=14.3914
	step [83/244], loss=13.0466
	step [84/244], loss=13.5599
	step [85/244], loss=14.7565
	step [86/244], loss=13.6102
	step [87/244], loss=15.0394
	step [88/244], loss=14.6247
	step [89/244], loss=14.4569
	step [90/244], loss=14.3855
	step [91/244], loss=13.9907
	step [92/244], loss=13.5796
	step [93/244], loss=14.0836
	step [94/244], loss=12.7105
	step [95/244], loss=15.0673
	step [96/244], loss=13.8215
	step [97/244], loss=15.6618
	step [98/244], loss=14.2478
	step [99/244], loss=14.6748
	step [100/244], loss=14.5245
	step [101/244], loss=14.0145
	step [102/244], loss=14.8644
	step [103/244], loss=15.1575
	step [104/244], loss=15.2283
	step [105/244], loss=12.7186
	step [106/244], loss=14.9569
	step [107/244], loss=15.1561
	step [108/244], loss=12.7625
	step [109/244], loss=12.9762
	step [110/244], loss=13.4607
	step [111/244], loss=13.8023
	step [112/244], loss=14.6781
	step [113/244], loss=13.8474
	step [114/244], loss=13.0300
	step [115/244], loss=12.5142
	step [116/244], loss=12.2229
	step [117/244], loss=14.3308
	step [118/244], loss=16.3565
	step [119/244], loss=15.4522
	step [120/244], loss=12.9071
	step [121/244], loss=14.6483
	step [122/244], loss=12.8900
	step [123/244], loss=13.4105
	step [124/244], loss=12.9134
	step [125/244], loss=12.4846
	step [126/244], loss=15.2491
	step [127/244], loss=13.2783
	step [128/244], loss=14.8311
	step [129/244], loss=12.7616
	step [130/244], loss=14.4758
	step [131/244], loss=13.2958
	step [132/244], loss=13.9146
	step [133/244], loss=13.3546
	step [134/244], loss=14.1262
	step [135/244], loss=13.9416
	step [136/244], loss=11.8058
	step [137/244], loss=14.5112
	step [138/244], loss=17.7010
	step [139/244], loss=13.0646
	step [140/244], loss=13.0309
	step [141/244], loss=13.0596
	step [142/244], loss=13.3247
	step [143/244], loss=13.7743
	step [144/244], loss=14.7458
	step [145/244], loss=14.2995
	step [146/244], loss=12.4928
	step [147/244], loss=13.0545
	step [148/244], loss=14.5175
	step [149/244], loss=13.9782
	step [150/244], loss=12.7166
	step [151/244], loss=12.9828
	step [152/244], loss=12.7242
	step [153/244], loss=13.2309
	step [154/244], loss=13.0207
	step [155/244], loss=11.8576
	step [156/244], loss=12.7843
	step [157/244], loss=13.7052
	step [158/244], loss=13.0229
	step [159/244], loss=12.0600
	step [160/244], loss=14.1592
	step [161/244], loss=14.1150
	step [162/244], loss=13.9239
	step [163/244], loss=13.6390
	step [164/244], loss=13.1475
	step [165/244], loss=14.4119
	step [166/244], loss=14.9110
	step [167/244], loss=12.5790
	step [168/244], loss=11.3517
	step [169/244], loss=12.8986
	step [170/244], loss=12.7193
	step [171/244], loss=13.0821
	step [172/244], loss=14.2550
	step [173/244], loss=13.5527
	step [174/244], loss=13.1058
	step [175/244], loss=14.7198
	step [176/244], loss=13.7498
	step [177/244], loss=13.8745
	step [178/244], loss=12.1693
	step [179/244], loss=14.2981
	step [180/244], loss=13.4963
	step [181/244], loss=13.1118
	step [182/244], loss=14.4542
	step [183/244], loss=14.0415
	step [184/244], loss=13.3919
	step [185/244], loss=13.3630
	step [186/244], loss=11.0400
	step [187/244], loss=13.2581
	step [188/244], loss=11.8486
	step [189/244], loss=12.8450
	step [190/244], loss=14.6167
	step [191/244], loss=12.9905
	step [192/244], loss=13.3648
	step [193/244], loss=13.1368
	step [194/244], loss=13.6452
	step [195/244], loss=13.2542
	step [196/244], loss=14.4833
	step [197/244], loss=13.2665
	step [198/244], loss=13.4898
	step [199/244], loss=15.2181
	step [200/244], loss=12.7800
	step [201/244], loss=12.8198
	step [202/244], loss=12.1270
	step [203/244], loss=12.3159
	step [204/244], loss=11.1103
	step [205/244], loss=11.4979
	step [206/244], loss=11.2501
	step [207/244], loss=12.4188
	step [208/244], loss=12.0468
	step [209/244], loss=15.1324
	step [210/244], loss=13.1417
	step [211/244], loss=11.8102
	step [212/244], loss=12.2634
	step [213/244], loss=14.4423
	step [214/244], loss=14.2060
	step [215/244], loss=14.0794
	step [216/244], loss=12.0049
	step [217/244], loss=13.0495
	step [218/244], loss=14.0675
	step [219/244], loss=13.9378
	step [220/244], loss=11.6989
	step [221/244], loss=12.3439
	step [222/244], loss=12.9723
	step [223/244], loss=14.3692
	step [224/244], loss=13.0898
	step [225/244], loss=11.9696
	step [226/244], loss=10.8469
	step [227/244], loss=15.0293
	step [228/244], loss=13.9306
	step [229/244], loss=11.8181
	step [230/244], loss=11.3010
	step [231/244], loss=13.9327
	step [232/244], loss=13.3152
	step [233/244], loss=12.5684
	step [234/244], loss=13.4968
	step [235/244], loss=11.6383
	step [236/244], loss=11.2473
	step [237/244], loss=13.6788
	step [238/244], loss=15.3434
	step [239/244], loss=12.1198
	step [240/244], loss=13.8131
	step [241/244], loss=11.8161
	step [242/244], loss=11.0289
	step [243/244], loss=12.0624
	step [244/244], loss=8.0109
	Evaluating
	loss=0.0401, precision=0.1825, recall=0.9954, f1=0.3084
Training epoch 9
	step [1/244], loss=11.9770
	step [2/244], loss=11.7703
	step [3/244], loss=12.8736
	step [4/244], loss=12.0483
	step [5/244], loss=14.6258
	step [6/244], loss=13.1274
	step [7/244], loss=13.5678
	step [8/244], loss=14.6987
	step [9/244], loss=12.5952
	step [10/244], loss=13.0812
	step [11/244], loss=12.7513
	step [12/244], loss=13.6686
	step [13/244], loss=13.5323
	step [14/244], loss=12.5290
	step [15/244], loss=11.7894
	step [16/244], loss=10.4299
	step [17/244], loss=11.2591
	step [18/244], loss=11.1999
	step [19/244], loss=11.9000
	step [20/244], loss=12.1813
	step [21/244], loss=13.2712
	step [22/244], loss=11.7922
	step [23/244], loss=13.7234
	step [24/244], loss=12.8399
	step [25/244], loss=13.9576
	step [26/244], loss=12.6432
	step [27/244], loss=15.2876
	step [28/244], loss=14.0406
	step [29/244], loss=13.2420
	step [30/244], loss=12.8437
	step [31/244], loss=12.1876
	step [32/244], loss=12.1496
	step [33/244], loss=13.3158
	step [34/244], loss=11.4887
	step [35/244], loss=12.7143
	step [36/244], loss=13.4714
	step [37/244], loss=11.0312
	step [38/244], loss=13.8098
	step [39/244], loss=13.3317
	step [40/244], loss=13.9049
	step [41/244], loss=11.6699
	step [42/244], loss=11.7424
	step [43/244], loss=11.4069
	step [44/244], loss=11.9656
	step [45/244], loss=12.3895
	step [46/244], loss=14.7324
	step [47/244], loss=12.2045
	step [48/244], loss=13.6122
	step [49/244], loss=13.1999
	step [50/244], loss=13.8035
	step [51/244], loss=12.8111
	step [52/244], loss=11.6075
	step [53/244], loss=12.2777
	step [54/244], loss=11.0684
	step [55/244], loss=11.4532
	step [56/244], loss=11.9428
	step [57/244], loss=13.1217
	step [58/244], loss=13.2537
	step [59/244], loss=11.8823
	step [60/244], loss=12.2009
	step [61/244], loss=11.4287
	step [62/244], loss=12.8542
	step [63/244], loss=11.4298
	step [64/244], loss=13.9287
	step [65/244], loss=11.6777
	step [66/244], loss=15.5498
	step [67/244], loss=11.9544
	step [68/244], loss=11.1966
	step [69/244], loss=11.5989
	step [70/244], loss=13.9343
	step [71/244], loss=13.4036
	step [72/244], loss=14.5460
	step [73/244], loss=11.7402
	step [74/244], loss=12.0233
	step [75/244], loss=11.4227
	step [76/244], loss=12.1292
	step [77/244], loss=12.3338
	step [78/244], loss=12.9936
	step [79/244], loss=11.9247
	step [80/244], loss=13.6481
	step [81/244], loss=12.2562
	step [82/244], loss=12.1998
	step [83/244], loss=11.7279
	step [84/244], loss=13.3916
	step [85/244], loss=14.2943
	step [86/244], loss=13.0874
	step [87/244], loss=12.8797
	step [88/244], loss=12.6948
	step [89/244], loss=11.7543
	step [90/244], loss=15.9412
	step [91/244], loss=12.2366
	step [92/244], loss=11.9481
	step [93/244], loss=13.0939
	step [94/244], loss=12.7053
	step [95/244], loss=11.4071
	step [96/244], loss=13.0437
	step [97/244], loss=11.0115
	step [98/244], loss=13.4254
	step [99/244], loss=11.8574
	step [100/244], loss=13.2578
	step [101/244], loss=13.6410
	step [102/244], loss=10.9811
	step [103/244], loss=12.7049
	step [104/244], loss=10.8035
	step [105/244], loss=12.0805
	step [106/244], loss=14.8630
	step [107/244], loss=11.3891
	step [108/244], loss=13.2992
	step [109/244], loss=12.0994
	step [110/244], loss=12.2041
	step [111/244], loss=13.5431
	step [112/244], loss=11.6461
	step [113/244], loss=12.9212
	step [114/244], loss=10.8597
	step [115/244], loss=11.6556
	step [116/244], loss=14.5224
	step [117/244], loss=15.9947
	step [118/244], loss=11.1012
	step [119/244], loss=10.7747
	step [120/244], loss=13.0245
	step [121/244], loss=12.6897
	step [122/244], loss=11.4668
	step [123/244], loss=12.7794
	step [124/244], loss=12.3425
	step [125/244], loss=11.7415
	step [126/244], loss=14.5986
	step [127/244], loss=13.5841
	step [128/244], loss=11.7732
	step [129/244], loss=12.1712
	step [130/244], loss=12.1314
	step [131/244], loss=12.4996
	step [132/244], loss=13.3648
	step [133/244], loss=12.2499
	step [134/244], loss=13.0103
	step [135/244], loss=10.5706
	step [136/244], loss=12.5345
	step [137/244], loss=11.9798
	step [138/244], loss=14.0324
	step [139/244], loss=10.6774
	step [140/244], loss=12.0849
	step [141/244], loss=13.6835
	step [142/244], loss=12.1391
	step [143/244], loss=12.1888
	step [144/244], loss=11.1872
	step [145/244], loss=10.6777
	step [146/244], loss=14.0024
	step [147/244], loss=12.7219
	step [148/244], loss=12.6851
	step [149/244], loss=15.4871
	step [150/244], loss=11.6455
	step [151/244], loss=12.2365
	step [152/244], loss=11.0120
	step [153/244], loss=11.9316
	step [154/244], loss=13.4152
	step [155/244], loss=11.2032
	step [156/244], loss=11.9382
	step [157/244], loss=12.0350
	step [158/244], loss=12.1179
	step [159/244], loss=12.4773
	step [160/244], loss=12.5325
	step [161/244], loss=12.4596
	step [162/244], loss=11.4358
	step [163/244], loss=10.8393
	step [164/244], loss=10.9715
	step [165/244], loss=11.2687
	step [166/244], loss=10.7884
	step [167/244], loss=10.2005
	step [168/244], loss=10.6701
	step [169/244], loss=10.9680
	step [170/244], loss=10.8219
	step [171/244], loss=12.2199
	step [172/244], loss=11.0021
	step [173/244], loss=13.3724
	step [174/244], loss=10.8276
	step [175/244], loss=12.7505
	step [176/244], loss=12.7667
	step [177/244], loss=11.7253
	step [178/244], loss=11.2863
	step [179/244], loss=12.4173
	step [180/244], loss=13.0547
	step [181/244], loss=11.2653
	step [182/244], loss=10.5402
	step [183/244], loss=10.8851
	step [184/244], loss=12.9272
	step [185/244], loss=13.2252
	step [186/244], loss=12.3090
	step [187/244], loss=11.4819
	step [188/244], loss=11.4003
	step [189/244], loss=13.2869
	step [190/244], loss=11.3951
	step [191/244], loss=12.5008
	step [192/244], loss=10.2836
	step [193/244], loss=11.6997
	step [194/244], loss=12.0655
	step [195/244], loss=12.3637
	step [196/244], loss=11.5849
	step [197/244], loss=12.3022
	step [198/244], loss=11.7334
	step [199/244], loss=10.6270
	step [200/244], loss=12.4018
	step [201/244], loss=11.6045
	step [202/244], loss=10.7959
	step [203/244], loss=13.1556
	step [204/244], loss=12.6986
	step [205/244], loss=12.0516
	step [206/244], loss=12.4930
	step [207/244], loss=12.3439
	step [208/244], loss=12.5548
	step [209/244], loss=12.9335
	step [210/244], loss=11.6861
	step [211/244], loss=12.2840
	step [212/244], loss=11.5737
	step [213/244], loss=11.2670
	step [214/244], loss=13.1054
	step [215/244], loss=12.1597
	step [216/244], loss=12.5767
	step [217/244], loss=11.6076
	step [218/244], loss=10.5133
	step [219/244], loss=11.3703
	step [220/244], loss=11.3788
	step [221/244], loss=9.5986
	step [222/244], loss=13.5109
	step [223/244], loss=10.1247
	step [224/244], loss=10.6238
	step [225/244], loss=11.8741
	step [226/244], loss=10.2829
	step [227/244], loss=11.8787
	step [228/244], loss=11.0490
	step [229/244], loss=11.5623
	step [230/244], loss=11.1324
	step [231/244], loss=13.3852
	step [232/244], loss=11.7978
	step [233/244], loss=12.0609
	step [234/244], loss=12.9213
	step [235/244], loss=12.9248
	step [236/244], loss=11.5292
	step [237/244], loss=11.5105
	step [238/244], loss=12.2593
	step [239/244], loss=12.6820
	step [240/244], loss=12.2593
	step [241/244], loss=12.4295
	step [242/244], loss=11.8875
	step [243/244], loss=10.7258
	step [244/244], loss=4.4252
	Evaluating
	loss=0.0389, precision=0.1652, recall=0.9962, f1=0.2833
Training epoch 10
	step [1/244], loss=10.4453
	step [2/244], loss=12.9806
	step [3/244], loss=12.2960
	step [4/244], loss=10.6683
	step [5/244], loss=11.1236
	step [6/244], loss=11.8057
	step [7/244], loss=10.6876
	step [8/244], loss=12.4752
	step [9/244], loss=11.6461
	step [10/244], loss=10.7929
	step [11/244], loss=13.6530
	step [12/244], loss=10.0795
	step [13/244], loss=11.2438
	step [14/244], loss=10.4062
	step [15/244], loss=10.7683
	step [16/244], loss=11.3132
	step [17/244], loss=10.9620
	step [18/244], loss=11.4885
	step [19/244], loss=10.5341
	step [20/244], loss=12.2608
	step [21/244], loss=11.8440
	step [22/244], loss=11.6840
	step [23/244], loss=10.6566
	step [24/244], loss=12.4381
	step [25/244], loss=12.3243
	step [26/244], loss=11.6522
	step [27/244], loss=11.3042
	step [28/244], loss=11.6573
	step [29/244], loss=10.9867
	step [30/244], loss=10.9837
	step [31/244], loss=10.9384
	step [32/244], loss=10.9166
	step [33/244], loss=11.2287
	step [34/244], loss=11.8848
	step [35/244], loss=13.3574
	step [36/244], loss=10.5851
	step [37/244], loss=12.1373
	step [38/244], loss=10.6121
	step [39/244], loss=12.8081
	step [40/244], loss=10.6131
	step [41/244], loss=12.4218
	step [42/244], loss=11.4971
	step [43/244], loss=11.9657
	step [44/244], loss=10.6895
	step [45/244], loss=11.5488
	step [46/244], loss=9.9690
	step [47/244], loss=11.4302
	step [48/244], loss=11.1849
	step [49/244], loss=10.9368
	step [50/244], loss=11.6288
	step [51/244], loss=11.0764
	step [52/244], loss=10.1803
	step [53/244], loss=9.5699
	step [54/244], loss=11.5033
	step [55/244], loss=10.2139
	step [56/244], loss=10.6621
	step [57/244], loss=10.8686
	step [58/244], loss=12.6064
	step [59/244], loss=13.5525
	step [60/244], loss=9.6379
	step [61/244], loss=13.2147
	step [62/244], loss=11.0951
	step [63/244], loss=10.6720
	step [64/244], loss=10.0770
	step [65/244], loss=11.6049
	step [66/244], loss=10.0937
	step [67/244], loss=9.4169
	step [68/244], loss=11.0238
	step [69/244], loss=11.5198
	step [70/244], loss=9.7686
	step [71/244], loss=11.7756
	step [72/244], loss=11.0136
	step [73/244], loss=10.9513
	step [74/244], loss=12.0751
	step [75/244], loss=12.0157
	step [76/244], loss=11.1139
	step [77/244], loss=10.5124
	step [78/244], loss=11.6282
	step [79/244], loss=11.1923
	step [80/244], loss=12.0920
	step [81/244], loss=11.5773
	step [82/244], loss=12.2129
	step [83/244], loss=11.2310
	step [84/244], loss=12.1058
	step [85/244], loss=11.7369
	step [86/244], loss=11.5334
	step [87/244], loss=11.4298
	step [88/244], loss=10.4168
	step [89/244], loss=11.1565
	step [90/244], loss=10.6759
	step [91/244], loss=10.6048
	step [92/244], loss=13.8759
	step [93/244], loss=12.3476
	step [94/244], loss=11.1471
	step [95/244], loss=11.9992
	step [96/244], loss=10.3514
	step [97/244], loss=12.5663
	step [98/244], loss=11.2084
	step [99/244], loss=11.2666
	step [100/244], loss=10.3283
	step [101/244], loss=11.6847
	step [102/244], loss=13.5925
	step [103/244], loss=10.7075
	step [104/244], loss=12.3481
	step [105/244], loss=11.2040
	step [106/244], loss=10.9440
	step [107/244], loss=13.0753
	step [108/244], loss=9.6650
	step [109/244], loss=10.4513
	step [110/244], loss=10.1830
	step [111/244], loss=11.1359
	step [112/244], loss=10.7969
	step [113/244], loss=12.4058
	step [114/244], loss=12.6254
	step [115/244], loss=11.5401
	step [116/244], loss=11.2747
	step [117/244], loss=10.1592
	step [118/244], loss=11.3010
	step [119/244], loss=10.2410
	step [120/244], loss=11.5987
	step [121/244], loss=11.0533
	step [122/244], loss=11.2760
	step [123/244], loss=11.0024
	step [124/244], loss=11.2003
	step [125/244], loss=10.9096
	step [126/244], loss=11.1552
	step [127/244], loss=10.9259
	step [128/244], loss=12.1907
	step [129/244], loss=11.4829
	step [130/244], loss=9.5833
	step [131/244], loss=10.7955
	step [132/244], loss=11.2341
	step [133/244], loss=10.2196
	step [134/244], loss=9.8807
	step [135/244], loss=10.2137
	step [136/244], loss=12.2916
	step [137/244], loss=11.4799
	step [138/244], loss=10.3046
	step [139/244], loss=9.8149
	step [140/244], loss=10.1814
	step [141/244], loss=11.5187
	step [142/244], loss=12.5397
	step [143/244], loss=10.5820
	step [144/244], loss=13.5094
	step [145/244], loss=12.4105
	step [146/244], loss=11.1547
	step [147/244], loss=11.2555
	step [148/244], loss=10.2064
	step [149/244], loss=11.8716
	step [150/244], loss=9.5521
	step [151/244], loss=9.8809
	step [152/244], loss=11.0038
	step [153/244], loss=10.0090
	step [154/244], loss=11.0559
	step [155/244], loss=10.1786
	step [156/244], loss=11.8627
	step [157/244], loss=11.1647
	step [158/244], loss=11.3364
	step [159/244], loss=10.3143
	step [160/244], loss=11.0213
	step [161/244], loss=11.3618
	step [162/244], loss=12.7220
	step [163/244], loss=11.3128
	step [164/244], loss=10.3067
	step [165/244], loss=12.8752
	step [166/244], loss=10.2546
	step [167/244], loss=10.8216
	step [168/244], loss=11.2606
	step [169/244], loss=11.7902
	step [170/244], loss=10.4213
	step [171/244], loss=11.5847
	step [172/244], loss=11.8756
	step [173/244], loss=11.1663
	step [174/244], loss=10.0576
	step [175/244], loss=10.7010
	step [176/244], loss=11.8524
	step [177/244], loss=11.6452
	step [178/244], loss=10.1894
	step [179/244], loss=9.2481
	step [180/244], loss=11.6868
	step [181/244], loss=11.2627
	step [182/244], loss=10.9501
	step [183/244], loss=12.8340
	step [184/244], loss=11.7237
	step [185/244], loss=10.2443
	step [186/244], loss=10.6782
	step [187/244], loss=10.8198
	step [188/244], loss=12.5244
	step [189/244], loss=11.1227
	step [190/244], loss=13.0519
	step [191/244], loss=13.0309
	step [192/244], loss=10.7091
	step [193/244], loss=12.2852
	step [194/244], loss=12.9536
	step [195/244], loss=10.4733
	step [196/244], loss=11.3780
	step [197/244], loss=11.6887
	step [198/244], loss=10.6143
	step [199/244], loss=12.4129
	step [200/244], loss=9.9745
	step [201/244], loss=9.6735
	step [202/244], loss=10.0286
	step [203/244], loss=12.7052
	step [204/244], loss=14.0129
	step [205/244], loss=13.9647
	step [206/244], loss=10.7548
	step [207/244], loss=11.2194
	step [208/244], loss=10.0181
	step [209/244], loss=11.9391
	step [210/244], loss=12.2804
	step [211/244], loss=10.0666
	step [212/244], loss=10.8088
	step [213/244], loss=10.8699
	step [214/244], loss=9.9338
	step [215/244], loss=10.5568
	step [216/244], loss=9.7087
	step [217/244], loss=10.1868
	step [218/244], loss=10.5504
	step [219/244], loss=11.0387
	step [220/244], loss=10.3423
	step [221/244], loss=11.0386
	step [222/244], loss=9.3764
	step [223/244], loss=9.3576
	step [224/244], loss=11.6693
	step [225/244], loss=10.1034
	step [226/244], loss=9.9975
	step [227/244], loss=9.1055
	step [228/244], loss=9.4976
	step [229/244], loss=10.6528
	step [230/244], loss=10.4302
	step [231/244], loss=10.9195
	step [232/244], loss=10.6239
	step [233/244], loss=11.1339
	step [234/244], loss=11.5929
	step [235/244], loss=12.8277
	step [236/244], loss=13.9847
	step [237/244], loss=10.5493
	step [238/244], loss=9.6580
	step [239/244], loss=12.2180
	step [240/244], loss=9.3971
	step [241/244], loss=10.9228
	step [242/244], loss=8.4502
	step [243/244], loss=11.0602
	step [244/244], loss=4.0558
	Evaluating
	loss=0.0294, precision=0.2240, recall=0.9936, f1=0.3655
saving model as: 1_saved_model.pth
Training epoch 11
	step [1/244], loss=11.1074
	step [2/244], loss=10.9467
	step [3/244], loss=10.7329
	step [4/244], loss=11.4293
	step [5/244], loss=9.7913
	step [6/244], loss=11.3950
	step [7/244], loss=9.4167
	step [8/244], loss=9.9324
	step [9/244], loss=10.9703
	step [10/244], loss=9.9441
	step [11/244], loss=11.8951
	step [12/244], loss=10.0526
	step [13/244], loss=10.9669
	step [14/244], loss=9.1039
	step [15/244], loss=9.6990
	step [16/244], loss=11.6085
	step [17/244], loss=12.1570
	step [18/244], loss=10.5435
	step [19/244], loss=10.4426
	step [20/244], loss=9.5445
	step [21/244], loss=10.9158
	step [22/244], loss=11.7426
	step [23/244], loss=12.8950
	step [24/244], loss=9.8883
	step [25/244], loss=10.6376
	step [26/244], loss=11.2067
	step [27/244], loss=10.7328
	step [28/244], loss=9.8556
	step [29/244], loss=10.9904
	step [30/244], loss=10.6645
	step [31/244], loss=10.2209
	step [32/244], loss=10.8258
	step [33/244], loss=10.6067
	step [34/244], loss=10.1572
	step [35/244], loss=12.4709
	step [36/244], loss=12.7041
	step [37/244], loss=8.9093
	step [38/244], loss=10.1712
	step [39/244], loss=10.7780
	step [40/244], loss=10.5962
	step [41/244], loss=8.7253
	step [42/244], loss=10.3560
	step [43/244], loss=10.5779
	step [44/244], loss=9.5185
	step [45/244], loss=11.3538
	step [46/244], loss=11.7549
	step [47/244], loss=9.7729
	step [48/244], loss=10.9111
	step [49/244], loss=11.6779
	step [50/244], loss=10.7181
	step [51/244], loss=9.1504
	step [52/244], loss=12.0092
	step [53/244], loss=11.2225
	step [54/244], loss=11.0690
	step [55/244], loss=9.4763
	step [56/244], loss=10.7049
	step [57/244], loss=11.6727
	step [58/244], loss=9.2756
	step [59/244], loss=9.4139
	step [60/244], loss=9.9646
	step [61/244], loss=11.5249
	step [62/244], loss=10.2039
	step [63/244], loss=9.7320
	step [64/244], loss=11.5730
	step [65/244], loss=9.6669
	step [66/244], loss=10.9407
	step [67/244], loss=9.6728
	step [68/244], loss=11.0565
	step [69/244], loss=10.2588
	step [70/244], loss=12.0876
	step [71/244], loss=9.3557
	step [72/244], loss=9.5579
	step [73/244], loss=11.4644
	step [74/244], loss=11.9024
	step [75/244], loss=9.0085
	step [76/244], loss=11.2742
	step [77/244], loss=11.0400
	step [78/244], loss=9.8844
	step [79/244], loss=11.5260
	step [80/244], loss=10.9587
	step [81/244], loss=10.8099
	step [82/244], loss=11.5267
	step [83/244], loss=11.8124
	step [84/244], loss=8.5258
	step [85/244], loss=9.5207
	step [86/244], loss=10.9482
	step [87/244], loss=9.1526
	step [88/244], loss=9.5246
	step [89/244], loss=11.1620
	step [90/244], loss=10.6314
	step [91/244], loss=12.7947
	step [92/244], loss=10.9061
	step [93/244], loss=9.0947
	step [94/244], loss=11.0091
	step [95/244], loss=8.6288
	step [96/244], loss=9.7199
	step [97/244], loss=11.5033
	step [98/244], loss=9.7916
	step [99/244], loss=11.1840
	step [100/244], loss=10.6559
	step [101/244], loss=11.1666
	step [102/244], loss=10.1728
	step [103/244], loss=10.3749
	step [104/244], loss=9.4570
	step [105/244], loss=9.1563
	step [106/244], loss=10.9454
	step [107/244], loss=10.6191
	step [108/244], loss=10.3722
	step [109/244], loss=10.8596
	step [110/244], loss=9.6724
	step [111/244], loss=9.9238
	step [112/244], loss=9.5560
	step [113/244], loss=9.5313
	step [114/244], loss=11.8528
	step [115/244], loss=10.0652
	step [116/244], loss=10.9120
	step [117/244], loss=9.5186
	step [118/244], loss=10.0911
	step [119/244], loss=10.0152
	step [120/244], loss=10.6287
	step [121/244], loss=9.9996
	step [122/244], loss=11.3152
	step [123/244], loss=9.1632
	step [124/244], loss=9.3402
	step [125/244], loss=10.0064
	step [126/244], loss=10.2274
	step [127/244], loss=10.0702
	step [128/244], loss=9.8354
	step [129/244], loss=11.0277
	step [130/244], loss=11.6203
	step [131/244], loss=10.2090
	step [132/244], loss=10.3956
	step [133/244], loss=11.4985
	step [134/244], loss=9.5231
	step [135/244], loss=8.6746
	step [136/244], loss=10.0064
	step [137/244], loss=9.9566
	step [138/244], loss=10.0941
	step [139/244], loss=9.8168
	step [140/244], loss=9.4211
	step [141/244], loss=11.8374
	step [142/244], loss=10.6746
	step [143/244], loss=9.0240
	step [144/244], loss=9.2847
	step [145/244], loss=9.9622
	step [146/244], loss=13.1270
	step [147/244], loss=9.3909
	step [148/244], loss=9.3640
	step [149/244], loss=10.3234
	step [150/244], loss=10.6760
	step [151/244], loss=9.7562
	step [152/244], loss=11.7538
	step [153/244], loss=10.7018
	step [154/244], loss=9.0629
	step [155/244], loss=10.6688
	step [156/244], loss=10.9961
	step [157/244], loss=9.8755
	step [158/244], loss=9.5410
	step [159/244], loss=10.5736
	step [160/244], loss=9.0814
	step [161/244], loss=8.8936
	step [162/244], loss=9.0218
	step [163/244], loss=8.7545
	step [164/244], loss=9.9479
	step [165/244], loss=9.3540
	step [166/244], loss=10.3696
	step [167/244], loss=9.2842
	step [168/244], loss=10.4337
	step [169/244], loss=10.9148
	step [170/244], loss=10.0518
	step [171/244], loss=10.6388
	step [172/244], loss=9.5333
	step [173/244], loss=11.9450
	step [174/244], loss=10.6559
	step [175/244], loss=9.8302
	step [176/244], loss=9.1753
	step [177/244], loss=12.0364
	step [178/244], loss=9.1135
	step [179/244], loss=10.0405
	step [180/244], loss=8.9166
	step [181/244], loss=8.7766
	step [182/244], loss=11.6528
	step [183/244], loss=8.5822
	step [184/244], loss=12.9777
	step [185/244], loss=10.5785
	step [186/244], loss=9.6501
	step [187/244], loss=9.9863
	step [188/244], loss=11.7547
	step [189/244], loss=12.0583
	step [190/244], loss=9.8889
	step [191/244], loss=8.8019
	step [192/244], loss=9.1161
	step [193/244], loss=11.1785
	step [194/244], loss=11.0217
	step [195/244], loss=9.2337
	step [196/244], loss=8.1994
	step [197/244], loss=9.4890
	step [198/244], loss=9.2004
	step [199/244], loss=10.6592
	step [200/244], loss=9.0385
	step [201/244], loss=10.5980
	step [202/244], loss=9.6070
	step [203/244], loss=10.1605
	step [204/244], loss=10.3819
	step [205/244], loss=9.5157
	step [206/244], loss=10.8394
	step [207/244], loss=13.0622
	step [208/244], loss=9.8954
	step [209/244], loss=8.4628
	step [210/244], loss=11.0954
	step [211/244], loss=10.7695
	step [212/244], loss=7.8587
	step [213/244], loss=8.6944
	step [214/244], loss=10.2142
	step [215/244], loss=10.8476
	step [216/244], loss=9.8485
	step [217/244], loss=9.5171
	step [218/244], loss=9.6152
	step [219/244], loss=10.0584
	step [220/244], loss=10.0163
	step [221/244], loss=10.2559
	step [222/244], loss=11.1359
	step [223/244], loss=11.8213
	step [224/244], loss=10.3655
	step [225/244], loss=9.7654
	step [226/244], loss=10.3016
	step [227/244], loss=8.6442
	step [228/244], loss=10.8981
	step [229/244], loss=10.0429
	step [230/244], loss=10.6694
	step [231/244], loss=11.3282
	step [232/244], loss=10.1704
	step [233/244], loss=8.6546
	step [234/244], loss=10.0805
	step [235/244], loss=9.9638
	step [236/244], loss=9.8521
	step [237/244], loss=9.9005
	step [238/244], loss=10.4503
	step [239/244], loss=10.3151
	step [240/244], loss=8.3081
	step [241/244], loss=8.8862
	step [242/244], loss=10.2252
	step [243/244], loss=8.5241
	step [244/244], loss=4.0755
	Evaluating
	loss=0.0285, precision=0.2168, recall=0.9943, f1=0.3560
Training epoch 12
	step [1/244], loss=10.5126
	step [2/244], loss=8.7072
	step [3/244], loss=9.9684
	step [4/244], loss=11.2689
	step [5/244], loss=8.3479
	step [6/244], loss=11.0646
	step [7/244], loss=10.7068
	step [8/244], loss=12.6456
	step [9/244], loss=10.7001
	step [10/244], loss=9.7960
	step [11/244], loss=9.3389
	step [12/244], loss=10.0468
	step [13/244], loss=9.7871
	step [14/244], loss=11.1802
	step [15/244], loss=10.9278
	step [16/244], loss=9.7824
	step [17/244], loss=10.4105
	step [18/244], loss=9.9756
	step [19/244], loss=9.1698
	step [20/244], loss=8.8147
	step [21/244], loss=9.1108
	step [22/244], loss=9.3768
	step [23/244], loss=10.4999
	step [24/244], loss=10.6189
	step [25/244], loss=10.8297
	step [26/244], loss=10.3620
	step [27/244], loss=10.1463
	step [28/244], loss=9.3783
	step [29/244], loss=10.0183
	step [30/244], loss=8.7729
	step [31/244], loss=13.0481
	step [32/244], loss=9.9600
	step [33/244], loss=9.3481
	step [34/244], loss=9.5332
	step [35/244], loss=11.2297
	step [36/244], loss=8.6115
	step [37/244], loss=10.4365
	step [38/244], loss=8.9208
	step [39/244], loss=9.9256
	step [40/244], loss=9.9335
	step [41/244], loss=10.2440
	step [42/244], loss=10.0391
	step [43/244], loss=9.9428
	step [44/244], loss=8.9581
	step [45/244], loss=8.8006
	step [46/244], loss=10.5892
	step [47/244], loss=11.5263
	step [48/244], loss=9.8342
	step [49/244], loss=9.4573
	step [50/244], loss=9.7852
	step [51/244], loss=10.4770
	step [52/244], loss=8.2765
	step [53/244], loss=9.5732
	step [54/244], loss=9.6009
	step [55/244], loss=8.4908
	step [56/244], loss=9.8576
	step [57/244], loss=10.7820
	step [58/244], loss=8.5514
	step [59/244], loss=10.0088
	step [60/244], loss=11.8542
	step [61/244], loss=9.3840
	step [62/244], loss=10.9560
	step [63/244], loss=10.0787
	step [64/244], loss=10.4331
	step [65/244], loss=8.2649
	step [66/244], loss=9.6756
	step [67/244], loss=10.8917
	step [68/244], loss=8.9727
	step [69/244], loss=8.5275
	step [70/244], loss=11.9585
	step [71/244], loss=9.5560
	step [72/244], loss=8.9330
	step [73/244], loss=9.1600
	step [74/244], loss=8.6647
	step [75/244], loss=8.4890
	step [76/244], loss=9.2589
	step [77/244], loss=9.6645
	step [78/244], loss=8.8380
	step [79/244], loss=9.0798
	step [80/244], loss=8.4688
	step [81/244], loss=11.8505
	step [82/244], loss=10.3080
	step [83/244], loss=8.6804
	step [84/244], loss=10.4506
	step [85/244], loss=9.9909
	step [86/244], loss=9.7084
	step [87/244], loss=10.7467
	step [88/244], loss=9.3036
	step [89/244], loss=8.1735
	step [90/244], loss=8.9720
	step [91/244], loss=10.5548
	step [92/244], loss=11.5621
	step [93/244], loss=8.5462
	step [94/244], loss=12.6723
	step [95/244], loss=9.3772
	step [96/244], loss=11.6936
	step [97/244], loss=10.1253
	step [98/244], loss=9.1586
	step [99/244], loss=9.6397
	step [100/244], loss=9.6642
	step [101/244], loss=10.3322
	step [102/244], loss=12.1172
	step [103/244], loss=10.3047
	step [104/244], loss=8.7456
	step [105/244], loss=10.3592
	step [106/244], loss=9.7291
	step [107/244], loss=9.4769
	step [108/244], loss=8.5889
	step [109/244], loss=8.2760
	step [110/244], loss=8.6411
	step [111/244], loss=9.7611
	step [112/244], loss=9.6213
	step [113/244], loss=9.4407
	step [114/244], loss=10.8549
	step [115/244], loss=7.6786
	step [116/244], loss=9.4116
	step [117/244], loss=7.9698
	step [118/244], loss=10.9831
	step [119/244], loss=8.6389
	step [120/244], loss=8.8607
	step [121/244], loss=11.6062
	step [122/244], loss=8.8240
	step [123/244], loss=10.1465
	step [124/244], loss=8.7142
	step [125/244], loss=8.2133
	step [126/244], loss=9.7726
	step [127/244], loss=8.4885
	step [128/244], loss=7.4734
	step [129/244], loss=10.5927
	step [130/244], loss=7.9608
	step [131/244], loss=10.0405
	step [132/244], loss=11.1259
	step [133/244], loss=8.9097
	step [134/244], loss=10.0883
	step [135/244], loss=10.3390
	step [136/244], loss=10.0032
	step [137/244], loss=9.3326
	step [138/244], loss=9.5086
	step [139/244], loss=8.3520
	step [140/244], loss=8.9562
	step [141/244], loss=9.5555
	step [142/244], loss=11.5473
	step [143/244], loss=9.6306
	step [144/244], loss=8.3226
	step [145/244], loss=10.4297
	step [146/244], loss=10.9113
	step [147/244], loss=10.6891
	step [148/244], loss=9.7598
	step [149/244], loss=8.7117
	step [150/244], loss=10.5006
	step [151/244], loss=10.0881
	step [152/244], loss=9.4364
	step [153/244], loss=8.4298
	step [154/244], loss=8.4292
	step [155/244], loss=10.3934
	step [156/244], loss=10.4871
	step [157/244], loss=9.2627
	step [158/244], loss=9.2988
	step [159/244], loss=9.2323
	step [160/244], loss=9.4310
	step [161/244], loss=9.6354
	step [162/244], loss=10.6377
	step [163/244], loss=9.8190
	step [164/244], loss=8.5489
	step [165/244], loss=8.3825
	step [166/244], loss=9.0195
	step [167/244], loss=8.5667
	step [168/244], loss=9.6589
	step [169/244], loss=8.0030
	step [170/244], loss=9.5950
	step [171/244], loss=8.7171
	step [172/244], loss=10.5796
	step [173/244], loss=9.6080
	step [174/244], loss=7.8723
	step [175/244], loss=9.0803
	step [176/244], loss=10.9504
	step [177/244], loss=8.3535
	step [178/244], loss=11.5372
	step [179/244], loss=9.2638
	step [180/244], loss=11.2742
	step [181/244], loss=11.6256
	step [182/244], loss=9.8505
	step [183/244], loss=10.5978
	step [184/244], loss=9.1623
	step [185/244], loss=9.1944
	step [186/244], loss=10.7530
	step [187/244], loss=10.7548
	step [188/244], loss=9.2789
	step [189/244], loss=10.0238
	step [190/244], loss=10.0067
	step [191/244], loss=9.2611
	step [192/244], loss=9.4482
	step [193/244], loss=9.3345
	step [194/244], loss=8.7761
	step [195/244], loss=8.3057
	step [196/244], loss=8.4184
	step [197/244], loss=10.1340
	step [198/244], loss=7.3286
	step [199/244], loss=9.1909
	step [200/244], loss=9.1769
	step [201/244], loss=8.3740
	step [202/244], loss=10.1057
	step [203/244], loss=9.8894
	step [204/244], loss=9.3559
	step [205/244], loss=9.0645
	step [206/244], loss=8.3890
	step [207/244], loss=11.0234
	step [208/244], loss=10.8284
	step [209/244], loss=8.5172
	step [210/244], loss=9.3138
	step [211/244], loss=8.5982
	step [212/244], loss=9.6327
	step [213/244], loss=8.7851
	step [214/244], loss=8.7407
	step [215/244], loss=10.5423
	step [216/244], loss=10.5783
	step [217/244], loss=8.4105
	step [218/244], loss=8.4182
	step [219/244], loss=10.0288
	step [220/244], loss=9.6054
	step [221/244], loss=9.2181
	step [222/244], loss=9.3235
	step [223/244], loss=8.5913
	step [224/244], loss=9.6546
	step [225/244], loss=9.1757
	step [226/244], loss=9.8013
	step [227/244], loss=9.0396
	step [228/244], loss=9.0819
	step [229/244], loss=9.6635
	step [230/244], loss=8.5499
	step [231/244], loss=7.7753
	step [232/244], loss=9.6377
	step [233/244], loss=9.2718
	step [234/244], loss=8.9955
	step [235/244], loss=10.1934
	step [236/244], loss=9.0814
	step [237/244], loss=7.9227
	step [238/244], loss=7.1729
	step [239/244], loss=8.5341
	step [240/244], loss=8.7156
	step [241/244], loss=9.5662
	step [242/244], loss=9.6956
	step [243/244], loss=8.7404
	step [244/244], loss=3.8454
	Evaluating
	loss=0.0235, precision=0.2351, recall=0.9933, f1=0.3801
saving model as: 1_saved_model.pth
Training epoch 13
	step [1/244], loss=8.6797
	step [2/244], loss=10.5793
	step [3/244], loss=8.9917
	step [4/244], loss=8.0991
	step [5/244], loss=11.5749
	step [6/244], loss=8.7736
	step [7/244], loss=9.3006
	step [8/244], loss=9.5986
	step [9/244], loss=10.6624
	step [10/244], loss=8.9272
	step [11/244], loss=9.8594
	step [12/244], loss=9.0252
	step [13/244], loss=9.8368
	step [14/244], loss=10.5148
	step [15/244], loss=8.8393
	step [16/244], loss=9.6035
	step [17/244], loss=7.7292
	step [18/244], loss=9.9635
	step [19/244], loss=9.9747
	step [20/244], loss=9.7535
	step [21/244], loss=8.8833
	step [22/244], loss=9.1138
	step [23/244], loss=8.1787
	step [24/244], loss=7.8036
	step [25/244], loss=8.4166
	step [26/244], loss=8.9588
	step [27/244], loss=9.0434
	step [28/244], loss=11.2112
	step [29/244], loss=8.2033
	step [30/244], loss=10.4308
	step [31/244], loss=8.6571
	step [32/244], loss=10.3646
	step [33/244], loss=9.6376
	step [34/244], loss=9.1668
	step [35/244], loss=7.6862
	step [36/244], loss=9.7620
	step [37/244], loss=9.7482
	step [38/244], loss=9.5560
	step [39/244], loss=9.5892
	step [40/244], loss=7.8187
	step [41/244], loss=8.9955
	step [42/244], loss=10.7707
	step [43/244], loss=10.3803
	step [44/244], loss=9.7424
	step [45/244], loss=8.3828
	step [46/244], loss=7.9304
	step [47/244], loss=9.8894
	step [48/244], loss=9.8394
	step [49/244], loss=9.6704
	step [50/244], loss=8.9549
	step [51/244], loss=9.2168
	step [52/244], loss=9.7706
	step [53/244], loss=9.4664
	step [54/244], loss=9.9628
	step [55/244], loss=8.3394
	step [56/244], loss=8.8188
	step [57/244], loss=9.0314
	step [58/244], loss=8.0165
	step [59/244], loss=8.5754
	step [60/244], loss=11.3480
	step [61/244], loss=8.1716
	step [62/244], loss=8.5919
	step [63/244], loss=10.4829
	step [64/244], loss=7.6408
	step [65/244], loss=8.8165
	step [66/244], loss=9.3400
	step [67/244], loss=8.4725
	step [68/244], loss=8.8730
	step [69/244], loss=8.6495
	step [70/244], loss=10.2975
	step [71/244], loss=8.7080
	step [72/244], loss=9.3281
	step [73/244], loss=8.0101
	step [74/244], loss=9.8930
	step [75/244], loss=8.0075
	step [76/244], loss=8.2958
	step [77/244], loss=8.1704
	step [78/244], loss=9.7364
	step [79/244], loss=9.3992
	step [80/244], loss=8.4336
	step [81/244], loss=8.2334
	step [82/244], loss=8.3792
	step [83/244], loss=8.3401
	step [84/244], loss=10.4076
	step [85/244], loss=8.2377
	step [86/244], loss=8.7172
	step [87/244], loss=8.9652
	step [88/244], loss=9.8951
	step [89/244], loss=8.5843
	step [90/244], loss=9.1812
	step [91/244], loss=8.7567
	step [92/244], loss=8.2519
	step [93/244], loss=7.7661
	step [94/244], loss=7.6846
	step [95/244], loss=9.6061
	step [96/244], loss=9.0507
	step [97/244], loss=11.5851
	step [98/244], loss=8.6731
	step [99/244], loss=7.7624
	step [100/244], loss=9.3456
	step [101/244], loss=12.6305
	step [102/244], loss=10.2594
	step [103/244], loss=8.2266
	step [104/244], loss=8.7711
	step [105/244], loss=10.6165
	step [106/244], loss=8.9842
	step [107/244], loss=9.9490
	step [108/244], loss=9.2613
	step [109/244], loss=9.6295
	step [110/244], loss=9.5871
	step [111/244], loss=9.3330
	step [112/244], loss=9.4248
	step [113/244], loss=10.0002
	step [114/244], loss=8.2169
	step [115/244], loss=10.0699
	step [116/244], loss=8.2876
	step [117/244], loss=9.4394
	step [118/244], loss=8.7556
	step [119/244], loss=8.0801
	step [120/244], loss=7.3169
	step [121/244], loss=8.9782
	step [122/244], loss=9.1832
	step [123/244], loss=10.6850
	step [124/244], loss=11.3412
	step [125/244], loss=8.7351
	step [126/244], loss=8.6139
	step [127/244], loss=8.7332
	step [128/244], loss=9.6801
	step [129/244], loss=8.3610
	step [130/244], loss=8.2975
	step [131/244], loss=8.9049
	step [132/244], loss=6.9603
	step [133/244], loss=9.3980
	step [134/244], loss=8.4890
	step [135/244], loss=9.7351
	step [136/244], loss=7.6858
	step [137/244], loss=8.6696
	step [138/244], loss=8.2386
	step [139/244], loss=9.3016
	step [140/244], loss=9.8822
	step [141/244], loss=8.3027
	step [142/244], loss=8.5760
	step [143/244], loss=7.7248
	step [144/244], loss=8.9999
	step [145/244], loss=10.8951
	step [146/244], loss=9.3274
	step [147/244], loss=10.1986
	step [148/244], loss=9.7913
	step [149/244], loss=8.3993
	step [150/244], loss=9.2546
	step [151/244], loss=8.3610
	step [152/244], loss=9.2641
	step [153/244], loss=7.6811
	step [154/244], loss=9.3383
	step [155/244], loss=7.9866
	step [156/244], loss=8.0066
	step [157/244], loss=6.4113
	step [158/244], loss=9.4574
	step [159/244], loss=8.4138
	step [160/244], loss=9.7200
	step [161/244], loss=8.6368
	step [162/244], loss=9.8920
	step [163/244], loss=7.3384
	step [164/244], loss=8.2036
	step [165/244], loss=9.6441
	step [166/244], loss=8.0161
	step [167/244], loss=7.5632
	step [168/244], loss=9.1539
	step [169/244], loss=9.5252
	step [170/244], loss=10.2595
	step [171/244], loss=8.2662
	step [172/244], loss=8.4125
	step [173/244], loss=8.3612
	step [174/244], loss=9.8928
	step [175/244], loss=9.2049
	step [176/244], loss=8.1961
	step [177/244], loss=8.8106
	step [178/244], loss=7.1299
	step [179/244], loss=8.1756
	step [180/244], loss=8.7542
	step [181/244], loss=7.6113
	step [182/244], loss=10.3476
	step [183/244], loss=9.5758
	step [184/244], loss=8.4698
	step [185/244], loss=9.8455
	step [186/244], loss=8.2884
	step [187/244], loss=8.8998
	step [188/244], loss=9.2669
	step [189/244], loss=9.1254
	step [190/244], loss=7.6394
	step [191/244], loss=8.4667
	step [192/244], loss=8.7933
	step [193/244], loss=8.5563
	step [194/244], loss=8.2176
	step [195/244], loss=8.1485
	step [196/244], loss=8.2689
	step [197/244], loss=9.5604
	step [198/244], loss=9.0589
	step [199/244], loss=9.0190
	step [200/244], loss=9.2268
	step [201/244], loss=9.7213
	step [202/244], loss=10.7052
	step [203/244], loss=9.8213
	step [204/244], loss=10.1809
	step [205/244], loss=7.9822
	step [206/244], loss=7.8863
	step [207/244], loss=8.1007
	step [208/244], loss=8.6871
	step [209/244], loss=6.8943
	step [210/244], loss=6.6714
	step [211/244], loss=9.3795
	step [212/244], loss=10.7472
	step [213/244], loss=8.2067
	step [214/244], loss=9.6282
	step [215/244], loss=8.2690
	step [216/244], loss=8.0456
	step [217/244], loss=8.2198
	step [218/244], loss=11.0955
	step [219/244], loss=7.1850
	step [220/244], loss=9.2609
	step [221/244], loss=7.5475
	step [222/244], loss=7.5909
	step [223/244], loss=11.0339
	step [224/244], loss=9.5084
	step [225/244], loss=9.6510
	step [226/244], loss=8.7305
	step [227/244], loss=8.4915
	step [228/244], loss=10.6429
	step [229/244], loss=9.2656
	step [230/244], loss=8.2620
	step [231/244], loss=8.3128
	step [232/244], loss=8.1318
	step [233/244], loss=8.0720
	step [234/244], loss=10.4271
	step [235/244], loss=8.3857
	step [236/244], loss=9.1040
	step [237/244], loss=9.3600
	step [238/244], loss=8.1134
	step [239/244], loss=7.5222
	step [240/244], loss=9.2360
	step [241/244], loss=7.7810
	step [242/244], loss=8.4393
	step [243/244], loss=8.3381
	step [244/244], loss=2.5900
	Evaluating
	loss=0.0262, precision=0.1889, recall=0.9944, f1=0.3175
Training epoch 14
	step [1/244], loss=6.4803
	step [2/244], loss=7.6809
	step [3/244], loss=9.1380
	step [4/244], loss=8.7769
	step [5/244], loss=8.6875
	step [6/244], loss=8.2638
	step [7/244], loss=6.5486
	step [8/244], loss=9.0817
	step [9/244], loss=8.5398
	step [10/244], loss=8.6175
	step [11/244], loss=8.1402
	step [12/244], loss=8.2790
	step [13/244], loss=8.3484
	step [14/244], loss=8.0685
	step [15/244], loss=10.0312
	step [16/244], loss=8.2859
	step [17/244], loss=9.8039
	step [18/244], loss=8.8195
	step [19/244], loss=7.9369
	step [20/244], loss=8.9796
	step [21/244], loss=7.9041
	step [22/244], loss=8.2402
	step [23/244], loss=10.2440
	step [24/244], loss=7.4732
	step [25/244], loss=10.3069
	step [26/244], loss=8.4923
	step [27/244], loss=11.2794
	step [28/244], loss=7.7602
	step [29/244], loss=8.2419
	step [30/244], loss=9.4928
	step [31/244], loss=9.4703
	step [32/244], loss=7.4708
	step [33/244], loss=6.9096
	step [34/244], loss=9.0206
	step [35/244], loss=8.5826
	step [36/244], loss=7.7588
	step [37/244], loss=10.2228
	step [38/244], loss=8.2278
	step [39/244], loss=8.5223
	step [40/244], loss=9.3145
	step [41/244], loss=9.6648
	step [42/244], loss=8.2323
	step [43/244], loss=9.2044
	step [44/244], loss=7.8251
	step [45/244], loss=8.2414
	step [46/244], loss=8.8118
	step [47/244], loss=8.3185
	step [48/244], loss=7.2789
	step [49/244], loss=7.3583
	step [50/244], loss=9.8936
	step [51/244], loss=8.7902
	step [52/244], loss=8.3449
	step [53/244], loss=9.6525
	step [54/244], loss=9.7943
	step [55/244], loss=7.2491
	step [56/244], loss=9.2617
	step [57/244], loss=8.8804
	step [58/244], loss=9.3131
	step [59/244], loss=9.6416
	step [60/244], loss=8.2806
	step [61/244], loss=7.7034
	step [62/244], loss=8.1243
	step [63/244], loss=9.5983
	step [64/244], loss=9.7773
	step [65/244], loss=10.0296
	step [66/244], loss=10.3554
	step [67/244], loss=6.2962
	step [68/244], loss=9.0308
	step [69/244], loss=8.2311
	step [70/244], loss=7.7612
	step [71/244], loss=10.0360
	step [72/244], loss=8.0181
	step [73/244], loss=8.0528
	step [74/244], loss=8.2045
	step [75/244], loss=9.6154
	step [76/244], loss=8.3528
	step [77/244], loss=8.4539
	step [78/244], loss=10.3019
	step [79/244], loss=8.2467
	step [80/244], loss=8.6816
	step [81/244], loss=8.2810
	step [82/244], loss=9.0332
	step [83/244], loss=9.5189
	step [84/244], loss=7.9920
	step [85/244], loss=8.0310
	step [86/244], loss=8.9340
	step [87/244], loss=9.1904
	step [88/244], loss=9.5825
	step [89/244], loss=6.9367
	step [90/244], loss=8.8293
	step [91/244], loss=7.7767
	step [92/244], loss=8.8222
	step [93/244], loss=8.3338
	step [94/244], loss=8.8427
	step [95/244], loss=7.7832
	step [96/244], loss=8.1832
	step [97/244], loss=8.6895
	step [98/244], loss=7.3396
	step [99/244], loss=9.5739
	step [100/244], loss=9.2104
	step [101/244], loss=7.6592
	step [102/244], loss=8.8057
	step [103/244], loss=9.3098
	step [104/244], loss=9.6042
	step [105/244], loss=8.9611
	step [106/244], loss=8.2504
	step [107/244], loss=8.0063
	step [108/244], loss=8.6178
	step [109/244], loss=9.5485
	step [110/244], loss=8.1143
	step [111/244], loss=8.9872
	step [112/244], loss=8.9803
	step [113/244], loss=8.4196
	step [114/244], loss=8.2562
	step [115/244], loss=9.5957
	step [116/244], loss=8.0998
	step [117/244], loss=8.3222
	step [118/244], loss=7.8609
	step [119/244], loss=7.4937
	step [120/244], loss=8.3445
	step [121/244], loss=8.0225
	step [122/244], loss=8.2576
	step [123/244], loss=9.4678
	step [124/244], loss=6.9411
	step [125/244], loss=10.4101
	step [126/244], loss=7.7048
	step [127/244], loss=8.3913
	step [128/244], loss=9.4682
	step [129/244], loss=7.9196
	step [130/244], loss=8.7865
	step [131/244], loss=8.9698
	step [132/244], loss=7.8128
	step [133/244], loss=7.9295
	step [134/244], loss=8.8839
	step [135/244], loss=8.7729
	step [136/244], loss=7.9190
	step [137/244], loss=7.6618
	step [138/244], loss=9.1034
	step [139/244], loss=7.9863
	step [140/244], loss=10.1036
	step [141/244], loss=9.6899
	step [142/244], loss=7.3988
	step [143/244], loss=7.8486
	step [144/244], loss=7.7160
	step [145/244], loss=8.6158
	step [146/244], loss=7.8527
	step [147/244], loss=8.4009
	step [148/244], loss=9.1746
	step [149/244], loss=7.5710
	step [150/244], loss=7.1236
	step [151/244], loss=7.4673
	step [152/244], loss=7.1706
	step [153/244], loss=9.1297
	step [154/244], loss=7.4775
	step [155/244], loss=8.1054
	step [156/244], loss=7.5722
	step [157/244], loss=6.9523
	step [158/244], loss=8.5658
	step [159/244], loss=8.5890
	step [160/244], loss=7.9700
	step [161/244], loss=9.6730
	step [162/244], loss=8.2530
	step [163/244], loss=7.8551
	step [164/244], loss=8.4101
	step [165/244], loss=8.2199
	step [166/244], loss=7.5897
	step [167/244], loss=7.8247
	step [168/244], loss=7.2782
	step [169/244], loss=9.8852
	step [170/244], loss=9.5701
	step [171/244], loss=8.3195
	step [172/244], loss=8.6410
	step [173/244], loss=9.7405
	step [174/244], loss=9.5947
	step [175/244], loss=9.2559
	step [176/244], loss=8.2893
	step [177/244], loss=8.5622
	step [178/244], loss=9.8869
	step [179/244], loss=8.5463
	step [180/244], loss=7.6838
	step [181/244], loss=7.6836
	step [182/244], loss=10.1412
	step [183/244], loss=6.9295
	step [184/244], loss=8.0331
	step [185/244], loss=7.7023
	step [186/244], loss=8.1546
	step [187/244], loss=8.6338
	step [188/244], loss=9.4323
	step [189/244], loss=11.7362
	step [190/244], loss=8.6487
	step [191/244], loss=8.9449
	step [192/244], loss=8.4894
	step [193/244], loss=8.2550
	step [194/244], loss=8.5380
	step [195/244], loss=7.8170
	step [196/244], loss=8.9166
	step [197/244], loss=7.4147
	step [198/244], loss=9.4839
	step [199/244], loss=8.5833
	step [200/244], loss=9.0391
	step [201/244], loss=7.8074
	step [202/244], loss=9.0045
	step [203/244], loss=8.3606
	step [204/244], loss=6.7787
	step [205/244], loss=6.8830
	step [206/244], loss=8.4900
	step [207/244], loss=9.6547
	step [208/244], loss=8.8342
	step [209/244], loss=8.8942
	step [210/244], loss=7.7991
	step [211/244], loss=7.5033
	step [212/244], loss=10.1239
	step [213/244], loss=8.3760
	step [214/244], loss=10.9445
	step [215/244], loss=7.4876
	step [216/244], loss=10.2405
	step [217/244], loss=9.4527
	step [218/244], loss=7.2714
	step [219/244], loss=7.2905
	step [220/244], loss=8.5597
	step [221/244], loss=7.8373
	step [222/244], loss=8.7144
	step [223/244], loss=8.2880
	step [224/244], loss=8.2605
	step [225/244], loss=7.1728
	step [226/244], loss=7.7524
	step [227/244], loss=7.7129
	step [228/244], loss=7.3354
	step [229/244], loss=8.2020
	step [230/244], loss=9.4242
	step [231/244], loss=9.4517
	step [232/244], loss=7.8649
	step [233/244], loss=8.9094
	step [234/244], loss=7.9537
	step [235/244], loss=8.2055
	step [236/244], loss=9.4703
	step [237/244], loss=8.6978
	step [238/244], loss=8.0985
	step [239/244], loss=8.0462
	step [240/244], loss=8.1341
	step [241/244], loss=7.7862
	step [242/244], loss=7.9035
	step [243/244], loss=9.0358
	step [244/244], loss=4.7341
	Evaluating
	loss=0.0234, precision=0.2166, recall=0.9939, f1=0.3556
Training epoch 15
	step [1/244], loss=8.2884
	step [2/244], loss=8.8030
	step [3/244], loss=7.7500
	step [4/244], loss=6.9166
	step [5/244], loss=8.0470
	step [6/244], loss=7.9641
	step [7/244], loss=8.9375
	step [8/244], loss=6.8903
	step [9/244], loss=8.8796
	step [10/244], loss=7.5537
	step [11/244], loss=7.9875
	step [12/244], loss=9.9554
	step [13/244], loss=7.6473
	step [14/244], loss=8.4413
	step [15/244], loss=10.5721
	step [16/244], loss=7.7835
	step [17/244], loss=7.9290
	step [18/244], loss=7.4755
	step [19/244], loss=7.8906
	step [20/244], loss=7.9108
	step [21/244], loss=6.5354
	step [22/244], loss=14.6059
	step [23/244], loss=9.8271
	step [24/244], loss=8.0967
	step [25/244], loss=9.0411
	step [26/244], loss=9.5889
	step [27/244], loss=9.1836
	step [28/244], loss=8.3625
	step [29/244], loss=7.6074
	step [30/244], loss=8.1017
	step [31/244], loss=8.1435
	step [32/244], loss=7.7985
	step [33/244], loss=8.4450
	step [34/244], loss=8.6077
	step [35/244], loss=10.8823
	step [36/244], loss=7.5803
	step [37/244], loss=7.8898
	step [38/244], loss=8.5755
	step [39/244], loss=7.8297
	step [40/244], loss=5.8110
	step [41/244], loss=7.8684
	step [42/244], loss=7.2717
	step [43/244], loss=8.0763
	step [44/244], loss=8.2128
	step [45/244], loss=8.3564
	step [46/244], loss=10.1287
	step [47/244], loss=8.8933
	step [48/244], loss=7.9576
	step [49/244], loss=8.9835
	step [50/244], loss=8.3542
	step [51/244], loss=7.6122
	step [52/244], loss=8.2735
	step [53/244], loss=8.4038
	step [54/244], loss=6.7604
	step [55/244], loss=9.0205
	step [56/244], loss=7.4630
	step [57/244], loss=6.6636
	step [58/244], loss=7.8516
	step [59/244], loss=7.5110
	step [60/244], loss=8.9137
	step [61/244], loss=7.5318
	step [62/244], loss=8.7088
	step [63/244], loss=7.6882
	step [64/244], loss=8.4462
	step [65/244], loss=7.5529
	step [66/244], loss=7.7099
	step [67/244], loss=9.6240
	step [68/244], loss=8.0101
	step [69/244], loss=7.1769
	step [70/244], loss=7.9392
	step [71/244], loss=8.6005
	step [72/244], loss=8.5213
	step [73/244], loss=7.5598
	step [74/244], loss=8.5793
	step [75/244], loss=8.4196
	step [76/244], loss=8.0043
	step [77/244], loss=8.6267
	step [78/244], loss=6.9781
	step [79/244], loss=6.9809
	step [80/244], loss=8.5384
	step [81/244], loss=8.0216
	step [82/244], loss=8.3266
	step [83/244], loss=7.0696
	step [84/244], loss=8.2548
	step [85/244], loss=8.6468
	step [86/244], loss=7.4956
	step [87/244], loss=9.1203
	step [88/244], loss=9.8227
	step [89/244], loss=7.9197
	step [90/244], loss=8.5187
	step [91/244], loss=8.8879
	step [92/244], loss=8.1714
	step [93/244], loss=7.9281
	step [94/244], loss=7.7084
	step [95/244], loss=8.4220
	step [96/244], loss=9.0765
	step [97/244], loss=7.7108
	step [98/244], loss=8.2215
	step [99/244], loss=9.1531
	step [100/244], loss=8.3621
	step [101/244], loss=8.1195
	step [102/244], loss=7.4951
	step [103/244], loss=7.4185
	step [104/244], loss=7.7385
	step [105/244], loss=8.6571
	step [106/244], loss=8.0779
	step [107/244], loss=7.0261
	step [108/244], loss=7.4007
	step [109/244], loss=7.5474
	step [110/244], loss=8.6411
	step [111/244], loss=7.2030
	step [112/244], loss=8.3802
	step [113/244], loss=8.4697
	step [114/244], loss=6.9606
	step [115/244], loss=9.3360
	step [116/244], loss=6.6537
	step [117/244], loss=6.9194
	step [118/244], loss=8.9660
	step [119/244], loss=6.8414
	step [120/244], loss=8.3818
	step [121/244], loss=7.5535
	step [122/244], loss=8.7534
	step [123/244], loss=8.4446
	step [124/244], loss=7.5228
	step [125/244], loss=7.1178
	step [126/244], loss=7.3425
	step [127/244], loss=8.3854
	step [128/244], loss=7.9060
	step [129/244], loss=8.7111
	step [130/244], loss=7.3289
	step [131/244], loss=6.7016
	step [132/244], loss=7.1843
	step [133/244], loss=8.2854
	step [134/244], loss=7.4450
	step [135/244], loss=7.5370
	step [136/244], loss=8.4786
	step [137/244], loss=6.7537
	step [138/244], loss=8.4713
	step [139/244], loss=6.8973
	step [140/244], loss=9.7638
	step [141/244], loss=7.9488
	step [142/244], loss=7.1461
	step [143/244], loss=7.2397
	step [144/244], loss=7.4240
	step [145/244], loss=7.5675
	step [146/244], loss=7.4419
	step [147/244], loss=7.6386
	step [148/244], loss=8.3228
	step [149/244], loss=8.5220
	step [150/244], loss=6.6015
	step [151/244], loss=8.4877
	step [152/244], loss=7.1789
	step [153/244], loss=9.2660
	step [154/244], loss=7.9631
	step [155/244], loss=8.0293
	step [156/244], loss=7.6955
	step [157/244], loss=7.0899
	step [158/244], loss=7.3114
	step [159/244], loss=9.1481
	step [160/244], loss=7.5890
	step [161/244], loss=8.3355
	step [162/244], loss=8.7034
	step [163/244], loss=7.3271
	step [164/244], loss=6.4807
	step [165/244], loss=7.4925
	step [166/244], loss=7.9776
	step [167/244], loss=6.7985
	step [168/244], loss=7.5845
	step [169/244], loss=7.8904
	step [170/244], loss=7.7029
	step [171/244], loss=7.7297
	step [172/244], loss=7.9148
	step [173/244], loss=7.7727
	step [174/244], loss=9.0328
	step [175/244], loss=8.3426
	step [176/244], loss=8.7179
	step [177/244], loss=6.3466
	step [178/244], loss=8.2314
	step [179/244], loss=7.8541
	step [180/244], loss=6.2697
	step [181/244], loss=9.5252
	step [182/244], loss=6.6918
	step [183/244], loss=8.4807
	step [184/244], loss=7.3990
	step [185/244], loss=8.2122
	step [186/244], loss=7.3625
	step [187/244], loss=8.1275
	step [188/244], loss=7.7172
	step [189/244], loss=8.4545
	step [190/244], loss=8.5114
	step [191/244], loss=9.0601
	step [192/244], loss=8.1976
	step [193/244], loss=9.2048
	step [194/244], loss=7.9088
	step [195/244], loss=6.9026
	step [196/244], loss=8.9049
	step [197/244], loss=7.5337
	step [198/244], loss=7.6754
	step [199/244], loss=7.8464
	step [200/244], loss=8.1232
	step [201/244], loss=7.0556
	step [202/244], loss=8.2307
	step [203/244], loss=6.7699
	step [204/244], loss=7.5327
	step [205/244], loss=7.7487
	step [206/244], loss=7.5675
	step [207/244], loss=7.0668
	step [208/244], loss=8.0587
	step [209/244], loss=7.6804
	step [210/244], loss=9.3435
	step [211/244], loss=7.2027
	step [212/244], loss=7.5313
	step [213/244], loss=8.1403
	step [214/244], loss=8.5463
	step [215/244], loss=7.0124
	step [216/244], loss=6.9538
	step [217/244], loss=7.9553
	step [218/244], loss=9.1857
	step [219/244], loss=6.4287
	step [220/244], loss=8.6454
	step [221/244], loss=7.7421
	step [222/244], loss=7.7211
	step [223/244], loss=8.5017
	step [224/244], loss=7.6009
	step [225/244], loss=7.4997
	step [226/244], loss=8.6197
	step [227/244], loss=7.1980
	step [228/244], loss=6.2151
	step [229/244], loss=7.1183
	step [230/244], loss=6.9806
	step [231/244], loss=8.2153
	step [232/244], loss=8.3648
	step [233/244], loss=6.5572
	step [234/244], loss=7.4437
	step [235/244], loss=7.2799
	step [236/244], loss=8.0795
	step [237/244], loss=7.0701
	step [238/244], loss=7.4647
	step [239/244], loss=9.1192
	step [240/244], loss=6.8805
	step [241/244], loss=7.0208
	step [242/244], loss=8.6851
	step [243/244], loss=8.3665
	step [244/244], loss=3.8125
	Evaluating
	loss=0.0181, precision=0.2613, recall=0.9921, f1=0.4136
saving model as: 1_saved_model.pth
Training epoch 16
	step [1/244], loss=8.2186
	step [2/244], loss=7.5339
	step [3/244], loss=7.9789
	step [4/244], loss=7.3351
	step [5/244], loss=7.7832
	step [6/244], loss=7.4495
	step [7/244], loss=8.6326
	step [8/244], loss=7.6180
	step [9/244], loss=8.5019
	step [10/244], loss=7.9513
	step [11/244], loss=11.7583
	step [12/244], loss=7.4873
	step [13/244], loss=8.6860
	step [14/244], loss=7.7540
	step [15/244], loss=6.0128
	step [16/244], loss=6.6422
	step [17/244], loss=8.2826
	step [18/244], loss=7.3663
	step [19/244], loss=6.1739
	step [20/244], loss=7.7653
	step [21/244], loss=6.9799
	step [22/244], loss=6.2548
	step [23/244], loss=5.9284
	step [24/244], loss=7.8812
	step [25/244], loss=8.1666
	step [26/244], loss=6.6844
	step [27/244], loss=8.2814
	step [28/244], loss=6.6496
	step [29/244], loss=7.8427
	step [30/244], loss=7.5922
	step [31/244], loss=6.4531
	step [32/244], loss=9.9830
	step [33/244], loss=8.1109
	step [34/244], loss=7.3756
	step [35/244], loss=8.2486
	step [36/244], loss=7.5145
	step [37/244], loss=8.4728
	step [38/244], loss=8.3450
	step [39/244], loss=9.6940
	step [40/244], loss=7.0053
	step [41/244], loss=6.5468
	step [42/244], loss=8.0135
	step [43/244], loss=7.0426
	step [44/244], loss=8.1441
	step [45/244], loss=8.3382
	step [46/244], loss=8.9805
	step [47/244], loss=8.2661
	step [48/244], loss=8.5407
	step [49/244], loss=7.3749
	step [50/244], loss=7.8238
	step [51/244], loss=10.1228
	step [52/244], loss=7.3169
	step [53/244], loss=7.5793
	step [54/244], loss=7.2102
	step [55/244], loss=7.2351
	step [56/244], loss=8.1932
	step [57/244], loss=6.1019
	step [58/244], loss=7.2070
	step [59/244], loss=8.0407
	step [60/244], loss=6.8120
	step [61/244], loss=7.8574
	step [62/244], loss=6.3989
	step [63/244], loss=8.5958
	step [64/244], loss=6.9343
	step [65/244], loss=7.3605
	step [66/244], loss=8.6822
	step [67/244], loss=7.5325
	step [68/244], loss=6.9753
	step [69/244], loss=7.8111
	step [70/244], loss=8.7785
	step [71/244], loss=6.7972
	step [72/244], loss=8.0541
	step [73/244], loss=8.2729
	step [74/244], loss=7.1877
	step [75/244], loss=8.2207
	step [76/244], loss=7.7189
	step [77/244], loss=6.6167
	step [78/244], loss=7.8576
	step [79/244], loss=7.1650
	step [80/244], loss=7.5487
	step [81/244], loss=6.8403
	step [82/244], loss=7.9851
	step [83/244], loss=6.8909
	step [84/244], loss=8.2128
	step [85/244], loss=6.8694
	step [86/244], loss=8.6655
	step [87/244], loss=8.0107
	step [88/244], loss=7.8112
	step [89/244], loss=7.1427
	step [90/244], loss=7.0897
	step [91/244], loss=8.0437
	step [92/244], loss=5.8434
	step [93/244], loss=7.5466
	step [94/244], loss=8.7823
	step [95/244], loss=7.5025
	step [96/244], loss=7.2004
	step [97/244], loss=7.9088
	step [98/244], loss=6.8775
	step [99/244], loss=7.3343
	step [100/244], loss=9.7664
	step [101/244], loss=7.9679
	step [102/244], loss=6.9933
	step [103/244], loss=7.8922
	step [104/244], loss=9.2433
	step [105/244], loss=8.9319
	step [106/244], loss=8.6008
	step [107/244], loss=6.2745
	step [108/244], loss=7.5584
	step [109/244], loss=8.2494
	step [110/244], loss=7.5975
	step [111/244], loss=8.7389
	step [112/244], loss=7.2869
	step [113/244], loss=9.2046
	step [114/244], loss=7.3225
	step [115/244], loss=8.0966
	step [116/244], loss=8.9124
	step [117/244], loss=8.1378
	step [118/244], loss=6.9386
	step [119/244], loss=9.0522
	step [120/244], loss=7.8780
	step [121/244], loss=7.2013
	step [122/244], loss=7.8783
	step [123/244], loss=7.2353
	step [124/244], loss=7.2930
	step [125/244], loss=9.0230
	step [126/244], loss=8.4134
	step [127/244], loss=6.7084
	step [128/244], loss=5.4089
	step [129/244], loss=7.1070
	step [130/244], loss=8.1266
	step [131/244], loss=7.4068
	step [132/244], loss=6.8889
	step [133/244], loss=6.1611
	step [134/244], loss=7.3945
	step [135/244], loss=7.4547
	step [136/244], loss=7.2648
	step [137/244], loss=9.9979
	step [138/244], loss=7.4811
	step [139/244], loss=8.9597
	step [140/244], loss=7.0057
	step [141/244], loss=7.5296
	step [142/244], loss=7.5489
	step [143/244], loss=8.4192
	step [144/244], loss=7.0239
	step [145/244], loss=6.5480
	step [146/244], loss=7.1993
	step [147/244], loss=7.5928
	step [148/244], loss=9.1026
	step [149/244], loss=5.9888
	step [150/244], loss=7.8862
	step [151/244], loss=9.1534
	step [152/244], loss=7.0972
	step [153/244], loss=6.9233
	step [154/244], loss=7.8614
	step [155/244], loss=6.9818
	step [156/244], loss=7.6883
	step [157/244], loss=7.1530
	step [158/244], loss=8.5175
	step [159/244], loss=7.8282
	step [160/244], loss=7.5938
	step [161/244], loss=7.9126
	step [162/244], loss=8.6505
	step [163/244], loss=7.7355
	step [164/244], loss=6.6980
	step [165/244], loss=7.1936
	step [166/244], loss=6.7595
	step [167/244], loss=7.3133
	step [168/244], loss=8.8306
	step [169/244], loss=7.5880
	step [170/244], loss=8.3825
	step [171/244], loss=5.2978
	step [172/244], loss=8.7620
	step [173/244], loss=6.5770
	step [174/244], loss=7.8674
	step [175/244], loss=8.3612
	step [176/244], loss=8.4228
	step [177/244], loss=6.3625
	step [178/244], loss=7.1592
	step [179/244], loss=7.5414
	step [180/244], loss=7.0138
	step [181/244], loss=7.2537
	step [182/244], loss=6.5006
	step [183/244], loss=7.2140
	step [184/244], loss=7.8683
	step [185/244], loss=8.1475
	step [186/244], loss=6.6209
	step [187/244], loss=6.6866
	step [188/244], loss=6.9736
	step [189/244], loss=6.6073
	step [190/244], loss=6.6005
	step [191/244], loss=8.0148
	step [192/244], loss=6.5745
	step [193/244], loss=7.5918
	step [194/244], loss=6.6102
	step [195/244], loss=7.3441
	step [196/244], loss=7.6542
	step [197/244], loss=7.0982
	step [198/244], loss=7.2898
	step [199/244], loss=8.3152
	step [200/244], loss=7.3287
	step [201/244], loss=8.2013
	step [202/244], loss=8.9600
	step [203/244], loss=7.8607
	step [204/244], loss=6.8690
	step [205/244], loss=6.2238
	step [206/244], loss=7.4273
	step [207/244], loss=7.1664
	step [208/244], loss=7.4542
	step [209/244], loss=6.3533
	step [210/244], loss=8.1469
	step [211/244], loss=6.8761
	step [212/244], loss=6.6815
	step [213/244], loss=7.7500
	step [214/244], loss=6.2475
	step [215/244], loss=6.9250
	step [216/244], loss=7.3614
	step [217/244], loss=8.1938
	step [218/244], loss=9.5110
	step [219/244], loss=6.4645
	step [220/244], loss=6.4981
	step [221/244], loss=7.3895
	step [222/244], loss=7.8247
	step [223/244], loss=7.1056
	step [224/244], loss=6.7341
	step [225/244], loss=8.2331
	step [226/244], loss=7.6029
	step [227/244], loss=7.2031
	step [228/244], loss=7.3786
	step [229/244], loss=6.7041
	step [230/244], loss=8.3057
	step [231/244], loss=7.1403
	step [232/244], loss=9.5012
	step [233/244], loss=9.1524
	step [234/244], loss=7.9400
	step [235/244], loss=6.5007
	step [236/244], loss=6.7784
	step [237/244], loss=7.1801
	step [238/244], loss=8.2277
	step [239/244], loss=8.5533
	step [240/244], loss=6.2862
	step [241/244], loss=7.0634
	step [242/244], loss=6.9039
	step [243/244], loss=7.8916
	step [244/244], loss=3.1738
	Evaluating
	loss=0.0203, precision=0.2231, recall=0.9939, f1=0.3644
Training epoch 17
	step [1/244], loss=9.2537
	step [2/244], loss=6.2836
	step [3/244], loss=6.1464
	step [4/244], loss=7.9534
	step [5/244], loss=7.5196
	step [6/244], loss=5.5707
	step [7/244], loss=6.9310
	step [8/244], loss=7.8753
	step [9/244], loss=6.6067
	step [10/244], loss=6.0866
	step [11/244], loss=6.8007
	step [12/244], loss=7.1183
	step [13/244], loss=7.7171
	step [14/244], loss=7.0842
	step [15/244], loss=6.9843
	step [16/244], loss=6.5503
	step [17/244], loss=6.7607
	step [18/244], loss=7.0358
	step [19/244], loss=7.5372
	step [20/244], loss=8.0769
	step [21/244], loss=6.8664
	step [22/244], loss=7.3261
	step [23/244], loss=7.5912
	step [24/244], loss=7.0300
	step [25/244], loss=6.7781
	step [26/244], loss=7.5292
	step [27/244], loss=7.3188
	step [28/244], loss=6.5762
	step [29/244], loss=7.4791
	step [30/244], loss=7.8998
	step [31/244], loss=7.4400
	step [32/244], loss=8.1423
	step [33/244], loss=7.2049
	step [34/244], loss=7.5929
	step [35/244], loss=7.5661
	step [36/244], loss=6.5862
	step [37/244], loss=7.6950
	step [38/244], loss=7.2837
	step [39/244], loss=6.5326
	step [40/244], loss=7.9959
	step [41/244], loss=7.3554
	step [42/244], loss=6.6300
	step [43/244], loss=6.1812
	step [44/244], loss=6.8136
	step [45/244], loss=8.7557
	step [46/244], loss=6.3924
	step [47/244], loss=10.2320
	step [48/244], loss=7.7018
	step [49/244], loss=7.0483
	step [50/244], loss=6.2306
	step [51/244], loss=6.8756
	step [52/244], loss=9.0028
	step [53/244], loss=7.6685
	step [54/244], loss=7.0007
	step [55/244], loss=6.6931
	step [56/244], loss=6.8958
	step [57/244], loss=8.0029
	step [58/244], loss=7.4091
	step [59/244], loss=7.6667
	step [60/244], loss=6.3694
	step [61/244], loss=6.7498
	step [62/244], loss=8.1599
	step [63/244], loss=6.1988
	step [64/244], loss=6.8034
	step [65/244], loss=6.5176
	step [66/244], loss=7.7940
	step [67/244], loss=8.0049
	step [68/244], loss=7.3975
	step [69/244], loss=7.5520
	step [70/244], loss=7.9813
	step [71/244], loss=7.6327
	step [72/244], loss=6.8602
	step [73/244], loss=6.7419
	step [74/244], loss=6.4368
	step [75/244], loss=6.6103
	step [76/244], loss=9.5017
	step [77/244], loss=9.2319
	step [78/244], loss=7.2108
	step [79/244], loss=5.8257
	step [80/244], loss=7.5649
	step [81/244], loss=7.7407
	step [82/244], loss=6.7827
	step [83/244], loss=7.1485
	step [84/244], loss=7.0080
	step [85/244], loss=8.2496
	step [86/244], loss=6.4354
	step [87/244], loss=6.7250
	step [88/244], loss=8.5476
	step [89/244], loss=5.2755
	step [90/244], loss=7.7773
	step [91/244], loss=5.6806
	step [92/244], loss=7.7072
	step [93/244], loss=9.2118
	step [94/244], loss=7.7696
	step [95/244], loss=7.2516
	step [96/244], loss=6.7071
	step [97/244], loss=6.4711
	step [98/244], loss=6.5906
	step [99/244], loss=7.5010
	step [100/244], loss=5.8212
	step [101/244], loss=6.9961
	step [102/244], loss=6.9216
	step [103/244], loss=7.0591
	step [104/244], loss=7.0748
	step [105/244], loss=8.3365
	step [106/244], loss=7.7338
	step [107/244], loss=6.6904
	step [108/244], loss=7.2819
	step [109/244], loss=8.6739
	step [110/244], loss=7.6781
	step [111/244], loss=6.8979
	step [112/244], loss=6.3893
	step [113/244], loss=7.8152
	step [114/244], loss=6.7805
	step [115/244], loss=8.1015
	step [116/244], loss=7.5089
	step [117/244], loss=7.8749
	step [118/244], loss=7.0391
	step [119/244], loss=7.4150
	step [120/244], loss=7.3696
	step [121/244], loss=6.8693
	step [122/244], loss=6.0316
	step [123/244], loss=8.0896
	step [124/244], loss=7.9895
	step [125/244], loss=6.8438
	step [126/244], loss=8.1372
	step [127/244], loss=6.0772
	step [128/244], loss=7.2544
	step [129/244], loss=7.4522
	step [130/244], loss=7.1248
	step [131/244], loss=6.8252
	step [132/244], loss=6.5266
	step [133/244], loss=6.7384
	step [134/244], loss=6.6645
	step [135/244], loss=6.9910
	step [136/244], loss=7.0782
	step [137/244], loss=7.0023
	step [138/244], loss=6.5656
	step [139/244], loss=7.4677
	step [140/244], loss=6.4738
	step [141/244], loss=6.3793
	step [142/244], loss=6.9872
	step [143/244], loss=8.5812
	step [144/244], loss=8.5199
	step [145/244], loss=10.5015
	step [146/244], loss=7.2642
	step [147/244], loss=8.7179
	step [148/244], loss=7.2115
	step [149/244], loss=7.9629
	step [150/244], loss=8.2883
	step [151/244], loss=7.8341
	step [152/244], loss=6.6911
	step [153/244], loss=7.0350
	step [154/244], loss=6.4049
	step [155/244], loss=9.0894
	step [156/244], loss=8.7078
	step [157/244], loss=5.7604
	step [158/244], loss=6.6593
	step [159/244], loss=6.8144
	step [160/244], loss=8.9868
	step [161/244], loss=7.0838
	step [162/244], loss=6.7147
	step [163/244], loss=6.1357
	step [164/244], loss=9.3049
	step [165/244], loss=6.3151
	step [166/244], loss=7.3931
	step [167/244], loss=6.8852
	step [168/244], loss=7.0529
	step [169/244], loss=6.7731
	step [170/244], loss=6.3767
	step [171/244], loss=7.1955
	step [172/244], loss=7.1411
	step [173/244], loss=8.0249
	step [174/244], loss=7.2061
	step [175/244], loss=7.3210
	step [176/244], loss=7.1515
	step [177/244], loss=6.5472
	step [178/244], loss=7.4931
	step [179/244], loss=6.5132
	step [180/244], loss=6.6833
	step [181/244], loss=7.7009
	step [182/244], loss=5.7088
	step [183/244], loss=7.3870
	step [184/244], loss=6.4859
	step [185/244], loss=5.9758
	step [186/244], loss=6.8501
	step [187/244], loss=6.0563
	step [188/244], loss=7.2164
	step [189/244], loss=7.4916
	step [190/244], loss=5.5857
	step [191/244], loss=8.3022
	step [192/244], loss=6.8692
	step [193/244], loss=5.9250
	step [194/244], loss=5.6927
	step [195/244], loss=6.6630
	step [196/244], loss=7.1717
	step [197/244], loss=8.1060
	step [198/244], loss=7.9155
	step [199/244], loss=7.5237
	step [200/244], loss=7.0329
	step [201/244], loss=7.0030
	step [202/244], loss=5.8042
	step [203/244], loss=7.2139
	step [204/244], loss=7.7281
	step [205/244], loss=7.6880
	step [206/244], loss=7.8459
	step [207/244], loss=6.6501
	step [208/244], loss=8.0433
	step [209/244], loss=7.5561
	step [210/244], loss=7.7472
	step [211/244], loss=7.0828
	step [212/244], loss=7.8239
	step [213/244], loss=7.7139
	step [214/244], loss=6.8387
	step [215/244], loss=6.6690
	step [216/244], loss=6.3842
	step [217/244], loss=7.0404
	step [218/244], loss=7.9685
	step [219/244], loss=6.8112
	step [220/244], loss=5.9092
	step [221/244], loss=8.1598
	step [222/244], loss=7.3689
	step [223/244], loss=7.8770
	step [224/244], loss=7.1387
	step [225/244], loss=7.6835
	step [226/244], loss=7.8688
	step [227/244], loss=7.8543
	step [228/244], loss=6.8441
	step [229/244], loss=7.3497
	step [230/244], loss=7.4894
	step [231/244], loss=7.1595
	step [232/244], loss=8.5500
	step [233/244], loss=7.4599
	step [234/244], loss=7.0167
	step [235/244], loss=6.4233
	step [236/244], loss=6.2372
	step [237/244], loss=8.3073
	step [238/244], loss=6.7892
	step [239/244], loss=6.2930
	step [240/244], loss=6.6933
	step [241/244], loss=6.3399
	step [242/244], loss=7.5988
	step [243/244], loss=7.1604
	step [244/244], loss=2.9905
	Evaluating
	loss=0.0223, precision=0.1953, recall=0.9936, f1=0.3265
Training epoch 18
	step [1/244], loss=5.4123
	step [2/244], loss=6.8556
	step [3/244], loss=6.0191
	step [4/244], loss=7.4972
	step [5/244], loss=7.3542
	step [6/244], loss=6.2635
	step [7/244], loss=8.4374
	step [8/244], loss=7.5184
	step [9/244], loss=7.0169
	step [10/244], loss=6.2056
	step [11/244], loss=6.6907
	step [12/244], loss=6.5118
	step [13/244], loss=6.6824
	step [14/244], loss=7.3234
	step [15/244], loss=7.2186
	step [16/244], loss=6.2481
	step [17/244], loss=7.4173
	step [18/244], loss=6.2320
	step [19/244], loss=7.4870
	step [20/244], loss=6.8143
	step [21/244], loss=7.9574
	step [22/244], loss=6.5173
	step [23/244], loss=6.3711
	step [24/244], loss=9.6028
	step [25/244], loss=8.2606
	step [26/244], loss=6.4553
	step [27/244], loss=6.8793
	step [28/244], loss=6.7787
	step [29/244], loss=6.1400
	step [30/244], loss=8.2922
	step [31/244], loss=6.2398
	step [32/244], loss=6.8064
	step [33/244], loss=6.1589
	step [34/244], loss=6.4893
	step [35/244], loss=9.4933
	step [36/244], loss=7.1437
	step [37/244], loss=6.3220
	step [38/244], loss=8.0487
	step [39/244], loss=6.6114
	step [40/244], loss=7.1544
	step [41/244], loss=6.8123
	step [42/244], loss=7.5149
	step [43/244], loss=7.2830
	step [44/244], loss=8.0083
	step [45/244], loss=6.6647
	step [46/244], loss=6.3531
	step [47/244], loss=7.0807
	step [48/244], loss=7.0883
	step [49/244], loss=6.0796
	step [50/244], loss=7.1444
	step [51/244], loss=7.1764
	step [52/244], loss=7.3794
	step [53/244], loss=7.8536
	step [54/244], loss=7.4493
	step [55/244], loss=8.3046
	step [56/244], loss=6.0259
	step [57/244], loss=6.0664
	step [58/244], loss=7.0723
	step [59/244], loss=7.5324
	step [60/244], loss=7.4185
	step [61/244], loss=5.5330
	step [62/244], loss=6.7062
	step [63/244], loss=5.9053
	step [64/244], loss=5.7710
	step [65/244], loss=8.7488
	step [66/244], loss=6.1421
	step [67/244], loss=8.0435
	step [68/244], loss=8.1267
	step [69/244], loss=6.0201
	step [70/244], loss=6.8245
	step [71/244], loss=6.6073
	step [72/244], loss=7.2200
	step [73/244], loss=6.9542
	step [74/244], loss=7.7666
	step [75/244], loss=7.1173
	step [76/244], loss=6.4345
	step [77/244], loss=7.5446
	step [78/244], loss=6.9979
	step [79/244], loss=7.3780
	step [80/244], loss=6.4544
	step [81/244], loss=5.9986
	step [82/244], loss=8.6580
	step [83/244], loss=5.7465
	step [84/244], loss=7.7504
	step [85/244], loss=6.4726
	step [86/244], loss=6.8057
	step [87/244], loss=7.5276
	step [88/244], loss=8.2394
	step [89/244], loss=6.3676
	step [90/244], loss=6.0030
	step [91/244], loss=7.0654
	step [92/244], loss=7.5648
	step [93/244], loss=7.9285
	step [94/244], loss=6.3454
	step [95/244], loss=7.4060
	step [96/244], loss=6.8211
	step [97/244], loss=5.4916
	step [98/244], loss=6.9993
	step [99/244], loss=8.0226
	step [100/244], loss=6.3671
	step [101/244], loss=6.5794
	step [102/244], loss=7.1064
	step [103/244], loss=8.1264
	step [104/244], loss=7.9152
	step [105/244], loss=7.1056
	step [106/244], loss=7.6716
	step [107/244], loss=7.0740
	step [108/244], loss=8.2556
	step [109/244], loss=6.6885
	step [110/244], loss=5.7815
	step [111/244], loss=7.6832
	step [112/244], loss=7.0195
	step [113/244], loss=6.5719
	step [114/244], loss=8.4528
	step [115/244], loss=5.8162
	step [116/244], loss=8.9085
	step [117/244], loss=9.1594
	step [118/244], loss=7.1489
	step [119/244], loss=6.5873
	step [120/244], loss=6.4618
	step [121/244], loss=7.8822
	step [122/244], loss=6.5825
	step [123/244], loss=7.4101
	step [124/244], loss=5.7168
	step [125/244], loss=6.9555
	step [126/244], loss=6.8489
	step [127/244], loss=7.3500
	step [128/244], loss=7.3656
	step [129/244], loss=8.0311
	step [130/244], loss=6.1875
	step [131/244], loss=6.6655
	step [132/244], loss=7.4559
	step [133/244], loss=6.5664
	step [134/244], loss=6.8216
	step [135/244], loss=6.5955
	step [136/244], loss=6.8430
	step [137/244], loss=6.2316
	step [138/244], loss=7.7413
	step [139/244], loss=6.7663
	step [140/244], loss=7.4741
	step [141/244], loss=5.4534
	step [142/244], loss=6.5101
	step [143/244], loss=5.9933
	step [144/244], loss=6.8432
	step [145/244], loss=5.7549
	step [146/244], loss=6.4468
	step [147/244], loss=7.4942
	step [148/244], loss=6.0699
	step [149/244], loss=6.6006
	step [150/244], loss=7.2000
	step [151/244], loss=7.3217
	step [152/244], loss=9.0997
	step [153/244], loss=7.9465
	step [154/244], loss=6.1663
	step [155/244], loss=6.6659
	step [156/244], loss=6.3175
	step [157/244], loss=6.7046
	step [158/244], loss=8.7692
	step [159/244], loss=5.4980
	step [160/244], loss=6.6142
	step [161/244], loss=7.3702
	step [162/244], loss=5.7673
	step [163/244], loss=7.2134
	step [164/244], loss=8.1985
	step [165/244], loss=6.0163
	step [166/244], loss=7.6272
	step [167/244], loss=7.2454
	step [168/244], loss=5.9016
	step [169/244], loss=6.9934
	step [170/244], loss=6.2168
	step [171/244], loss=6.7470
	step [172/244], loss=7.6068
	step [173/244], loss=7.3691
	step [174/244], loss=6.7710
	step [175/244], loss=5.8257
	step [176/244], loss=5.7838
	step [177/244], loss=7.5630
	step [178/244], loss=6.9092
	step [179/244], loss=5.1144
	step [180/244], loss=7.5630
	step [181/244], loss=6.4874
	step [182/244], loss=6.7433
	step [183/244], loss=6.5620
	step [184/244], loss=5.1261
	step [185/244], loss=5.8805
	step [186/244], loss=7.7842
	step [187/244], loss=7.8012
	step [188/244], loss=6.7600
	step [189/244], loss=6.6302
	step [190/244], loss=6.8648
	step [191/244], loss=6.6406
	step [192/244], loss=7.1554
	step [193/244], loss=7.7400
	step [194/244], loss=6.9821
	step [195/244], loss=5.3014
	step [196/244], loss=6.5645
	step [197/244], loss=7.8959
	step [198/244], loss=7.3442
	step [199/244], loss=5.8914
	step [200/244], loss=6.0553
	step [201/244], loss=6.7138
	step [202/244], loss=7.0141
	step [203/244], loss=6.5611
	step [204/244], loss=5.8594
	step [205/244], loss=5.7440
	step [206/244], loss=5.7366
	step [207/244], loss=6.9022
	step [208/244], loss=6.7951
	step [209/244], loss=6.2431
	step [210/244], loss=6.6009
	step [211/244], loss=6.5670
	step [212/244], loss=6.7982
	step [213/244], loss=6.4156
	step [214/244], loss=6.4641
	step [215/244], loss=6.1494
	step [216/244], loss=6.9069
	step [217/244], loss=5.8498
	step [218/244], loss=6.1464
	step [219/244], loss=6.7086
	step [220/244], loss=6.8768
	step [221/244], loss=6.5314
	step [222/244], loss=6.0111
	step [223/244], loss=5.6243
	step [224/244], loss=7.4918
	step [225/244], loss=6.4757
	step [226/244], loss=6.5828
	step [227/244], loss=6.5331
	step [228/244], loss=6.5395
	step [229/244], loss=6.0425
	step [230/244], loss=6.9123
	step [231/244], loss=7.9637
	step [232/244], loss=5.9932
	step [233/244], loss=6.4185
	step [234/244], loss=7.2511
	step [235/244], loss=6.4770
	step [236/244], loss=6.2694
	step [237/244], loss=5.7704
	step [238/244], loss=7.7919
	step [239/244], loss=7.5798
	step [240/244], loss=7.7323
	step [241/244], loss=7.9469
	step [242/244], loss=6.7135
	step [243/244], loss=7.5008
	step [244/244], loss=2.5856
	Evaluating
	loss=0.0207, precision=0.2094, recall=0.9940, f1=0.3459
Training epoch 19
	step [1/244], loss=6.9277
	step [2/244], loss=6.3422
	step [3/244], loss=4.5559
	step [4/244], loss=7.4093
	step [5/244], loss=7.1309
	step [6/244], loss=6.9929
	step [7/244], loss=6.3690
	step [8/244], loss=6.8154
	step [9/244], loss=6.7141
	step [10/244], loss=6.9397
	step [11/244], loss=6.8301
	step [12/244], loss=7.3411
	step [13/244], loss=7.2451
	step [14/244], loss=6.7282
	step [15/244], loss=5.3252
	step [16/244], loss=5.9767
	step [17/244], loss=6.4157
	step [18/244], loss=6.9817
	step [19/244], loss=7.4606
	step [20/244], loss=6.7243
	step [21/244], loss=6.0267
	step [22/244], loss=6.3371
	step [23/244], loss=7.2320
	step [24/244], loss=5.8903
	step [25/244], loss=6.2877
	step [26/244], loss=6.6126
	step [27/244], loss=7.0761
	step [28/244], loss=6.6824
	step [29/244], loss=6.8905
	step [30/244], loss=5.8083
	step [31/244], loss=8.0073
	step [32/244], loss=6.4801
	step [33/244], loss=6.1294
	step [34/244], loss=7.6735
	step [35/244], loss=5.5509
	step [36/244], loss=6.4401
	step [37/244], loss=6.2443
	step [38/244], loss=6.6023
	step [39/244], loss=7.0451
	step [40/244], loss=7.0255
	step [41/244], loss=6.5459
	step [42/244], loss=5.9298
	step [43/244], loss=7.0788
	step [44/244], loss=6.7230
	step [45/244], loss=6.7777
	step [46/244], loss=6.8399
	step [47/244], loss=8.4012
	step [48/244], loss=5.9114
	step [49/244], loss=7.6194
	step [50/244], loss=6.4133
	step [51/244], loss=7.3251
	step [52/244], loss=6.6626
	step [53/244], loss=6.4812
	step [54/244], loss=7.1484
	step [55/244], loss=5.8989
	step [56/244], loss=5.9609
	step [57/244], loss=6.1369
	step [58/244], loss=6.4742
	step [59/244], loss=5.9843
	step [60/244], loss=6.4286
	step [61/244], loss=5.6740
	step [62/244], loss=6.3766
	step [63/244], loss=7.1076
	step [64/244], loss=6.8761
	step [65/244], loss=6.0685
	step [66/244], loss=6.9862
	step [67/244], loss=5.8817
	step [68/244], loss=5.8983
	step [69/244], loss=5.5254
	step [70/244], loss=5.6158
	step [71/244], loss=5.7249
	step [72/244], loss=7.4369
	step [73/244], loss=6.5959
	step [74/244], loss=5.7130
	step [75/244], loss=6.3770
	step [76/244], loss=6.6997
	step [77/244], loss=6.2265
	step [78/244], loss=6.2109
	step [79/244], loss=6.5974
	step [80/244], loss=6.7191
	step [81/244], loss=6.1707
	step [82/244], loss=6.4955
	step [83/244], loss=5.4389
	step [84/244], loss=7.2140
	step [85/244], loss=5.6418
	step [86/244], loss=8.1331
	step [87/244], loss=5.9086
	step [88/244], loss=6.6843
	step [89/244], loss=7.9720
	step [90/244], loss=7.1006
	step [91/244], loss=6.1025
	step [92/244], loss=6.1097
	step [93/244], loss=7.8223
	step [94/244], loss=6.4196
	step [95/244], loss=8.3340
	step [96/244], loss=7.6295
	step [97/244], loss=7.8697
	step [98/244], loss=7.4900
	step [99/244], loss=7.0664
	step [100/244], loss=6.5553
	step [101/244], loss=7.6965
	step [102/244], loss=8.0844
	step [103/244], loss=7.5851
	step [104/244], loss=6.6298
	step [105/244], loss=6.8141
	step [106/244], loss=6.2868
	step [107/244], loss=7.1845
	step [108/244], loss=7.4945
	step [109/244], loss=7.9554
	step [110/244], loss=7.0389
	step [111/244], loss=6.9750
	step [112/244], loss=7.0712
	step [113/244], loss=6.6581
	step [114/244], loss=6.4969
	step [115/244], loss=6.5977
	step [116/244], loss=6.9805
	step [117/244], loss=6.7110
	step [118/244], loss=6.0767
	step [119/244], loss=5.9737
	step [120/244], loss=8.1027
	step [121/244], loss=5.4471
	step [122/244], loss=7.4004
	step [123/244], loss=5.9706
	step [124/244], loss=6.5037
	step [125/244], loss=6.1067
	step [126/244], loss=6.6911
	step [127/244], loss=7.3739
	step [128/244], loss=5.1124
	step [129/244], loss=6.1937
	step [130/244], loss=7.3575
	step [131/244], loss=8.1573
	step [132/244], loss=10.4433
	step [133/244], loss=6.7290
	step [134/244], loss=8.1656
	step [135/244], loss=7.3265
	step [136/244], loss=7.4417
	step [137/244], loss=6.1415
	step [138/244], loss=7.0042
	step [139/244], loss=7.7135
	step [140/244], loss=7.0707
	step [141/244], loss=6.0016
	step [142/244], loss=4.6500
	step [143/244], loss=7.6322
	step [144/244], loss=7.2571
	step [145/244], loss=5.9373
	step [146/244], loss=6.5153
	step [147/244], loss=6.9748
	step [148/244], loss=6.9878
	step [149/244], loss=5.1716
	step [150/244], loss=6.3361
	step [151/244], loss=7.6232
	step [152/244], loss=6.5878
	step [153/244], loss=8.9731
	step [154/244], loss=6.0600
	step [155/244], loss=6.8212
	step [156/244], loss=6.2921
	step [157/244], loss=6.4068
	step [158/244], loss=6.3133
	step [159/244], loss=6.9441
	step [160/244], loss=6.6370
	step [161/244], loss=6.5658
	step [162/244], loss=6.8200
	step [163/244], loss=6.3238
	step [164/244], loss=6.5605
	step [165/244], loss=6.0431
	step [166/244], loss=5.9438
	step [167/244], loss=7.2365
	step [168/244], loss=5.2678
	step [169/244], loss=6.3951
	step [170/244], loss=6.5367
	step [171/244], loss=6.7291
	step [172/244], loss=5.6534
	step [173/244], loss=7.1241
	step [174/244], loss=7.0039
	step [175/244], loss=6.4621
	step [176/244], loss=7.5542
	step [177/244], loss=6.1998
	step [178/244], loss=6.3303
	step [179/244], loss=6.4362
	step [180/244], loss=6.0842
	step [181/244], loss=6.7388
	step [182/244], loss=6.7903
	step [183/244], loss=6.1395
	step [184/244], loss=6.7750
	step [185/244], loss=6.5590
	step [186/244], loss=6.4970
	step [187/244], loss=6.6719
	step [188/244], loss=6.4188
	step [189/244], loss=7.0354
	step [190/244], loss=6.8319
	step [191/244], loss=6.2171
	step [192/244], loss=5.2617
	step [193/244], loss=6.7801
	step [194/244], loss=7.8295
	step [195/244], loss=5.0304
	step [196/244], loss=7.3006
	step [197/244], loss=6.7872
	step [198/244], loss=6.6907
	step [199/244], loss=6.6715
	step [200/244], loss=9.2171
	step [201/244], loss=5.9804
	step [202/244], loss=6.7448
	step [203/244], loss=6.6738
	step [204/244], loss=6.3407
	step [205/244], loss=5.6691
	step [206/244], loss=6.7429
	step [207/244], loss=6.7431
	step [208/244], loss=5.1262
	step [209/244], loss=5.9298
	step [210/244], loss=6.6753
	step [211/244], loss=6.6474
	step [212/244], loss=6.9870
	step [213/244], loss=6.8637
	step [214/244], loss=6.9680
	step [215/244], loss=6.3033
	step [216/244], loss=5.5892
	step [217/244], loss=6.1192
	step [218/244], loss=6.2342
	step [219/244], loss=5.6266
	step [220/244], loss=7.0018
	step [221/244], loss=5.7581
	step [222/244], loss=6.4169
	step [223/244], loss=6.4347
	step [224/244], loss=6.1363
	step [225/244], loss=5.7358
	step [226/244], loss=6.6297
	step [227/244], loss=6.2870
	step [228/244], loss=7.6844
	step [229/244], loss=6.4500
	step [230/244], loss=7.7393
	step [231/244], loss=5.8834
	step [232/244], loss=6.8571
	step [233/244], loss=6.6127
	step [234/244], loss=7.2097
	step [235/244], loss=6.1359
	step [236/244], loss=9.1767
	step [237/244], loss=6.8665
	step [238/244], loss=8.8292
	step [239/244], loss=5.3021
	step [240/244], loss=7.4726
	step [241/244], loss=6.1356
	step [242/244], loss=8.6452
	step [243/244], loss=5.8616
	step [244/244], loss=2.4054
	Evaluating
	loss=0.0190, precision=0.2259, recall=0.9937, f1=0.3681
Training epoch 20
	step [1/244], loss=6.6057
	step [2/244], loss=7.4482
	step [3/244], loss=6.9277
	step [4/244], loss=6.1817
	step [5/244], loss=7.0060
	step [6/244], loss=5.8342
	step [7/244], loss=6.6468
	step [8/244], loss=5.9102
	step [9/244], loss=7.4707
	step [10/244], loss=5.6816
	step [11/244], loss=6.6090
	step [12/244], loss=7.2426
	step [13/244], loss=6.4287
	step [14/244], loss=6.6504
	step [15/244], loss=7.2338
	step [16/244], loss=5.0971
	step [17/244], loss=6.6283
	step [18/244], loss=5.8579
	step [19/244], loss=5.7461
	step [20/244], loss=7.4677
	step [21/244], loss=6.6589
	step [22/244], loss=6.6187
	step [23/244], loss=6.3364
	step [24/244], loss=5.0367
	step [25/244], loss=7.0030
	step [26/244], loss=5.8203
	step [27/244], loss=5.8768
	step [28/244], loss=5.4691
	step [29/244], loss=5.6894
	step [30/244], loss=5.8646
	step [31/244], loss=6.3750
	step [32/244], loss=5.6399
	step [33/244], loss=7.2707
	step [34/244], loss=5.8706
	step [35/244], loss=7.2938
	step [36/244], loss=6.0582
	step [37/244], loss=6.7320
	step [38/244], loss=6.4139
	step [39/244], loss=7.9828
	step [40/244], loss=6.7682
	step [41/244], loss=5.5731
	step [42/244], loss=5.7976
	step [43/244], loss=6.5279
	step [44/244], loss=6.3646
	step [45/244], loss=7.0935
	step [46/244], loss=6.8305
	step [47/244], loss=7.1026
	step [48/244], loss=5.7124
	step [49/244], loss=5.7223
	step [50/244], loss=6.9415
	step [51/244], loss=5.5769
	step [52/244], loss=5.3762
	step [53/244], loss=6.4398
	step [54/244], loss=7.0374
	step [55/244], loss=7.0334
	step [56/244], loss=7.6338
	step [57/244], loss=7.0139
	step [58/244], loss=6.8264
	step [59/244], loss=5.0312
	step [60/244], loss=5.9123
	step [61/244], loss=7.4784
	step [62/244], loss=6.1224
	step [63/244], loss=6.1410
	step [64/244], loss=5.6678
	step [65/244], loss=5.7930
	step [66/244], loss=6.3734
	step [67/244], loss=4.7054
	step [68/244], loss=5.9359
	step [69/244], loss=6.0397
	step [70/244], loss=7.0076
	step [71/244], loss=5.4766
	step [72/244], loss=6.1198
	step [73/244], loss=5.4845
	step [74/244], loss=4.9625
	step [75/244], loss=6.7890
	step [76/244], loss=6.1637
	step [77/244], loss=7.6946
	step [78/244], loss=5.3949
	step [79/244], loss=6.3282
	step [80/244], loss=6.3070
	step [81/244], loss=6.3050
	step [82/244], loss=7.0268
	step [83/244], loss=5.7519
	step [84/244], loss=6.2159
	step [85/244], loss=6.9729
	step [86/244], loss=5.3823
	step [87/244], loss=7.1468
	step [88/244], loss=6.9682
	step [89/244], loss=6.3541
	step [90/244], loss=4.8038
	step [91/244], loss=7.5367
	step [92/244], loss=6.5480
	step [93/244], loss=5.1729
	step [94/244], loss=5.8697
	step [95/244], loss=6.8493
	step [96/244], loss=5.5619
	step [97/244], loss=5.7007
	step [98/244], loss=6.8599
	step [99/244], loss=6.6530
	step [100/244], loss=6.8583
	step [101/244], loss=6.1940
	step [102/244], loss=6.3738
	step [103/244], loss=7.7124
	step [104/244], loss=5.6416
	step [105/244], loss=4.6263
	step [106/244], loss=6.4164
	step [107/244], loss=7.3098
	step [108/244], loss=5.6007
	step [109/244], loss=6.9632
	step [110/244], loss=7.9089
	step [111/244], loss=6.5383
	step [112/244], loss=6.0914
	step [113/244], loss=6.1440
	step [114/244], loss=6.8979
	step [115/244], loss=5.9098
	step [116/244], loss=4.9062
	step [117/244], loss=5.7891
	step [118/244], loss=6.1146
	step [119/244], loss=5.1704
	step [120/244], loss=7.0345
	step [121/244], loss=8.6954
	step [122/244], loss=5.7200
	step [123/244], loss=5.6661
	step [124/244], loss=5.1221
	step [125/244], loss=5.3312
	step [126/244], loss=5.7703
	step [127/244], loss=6.3045
	step [128/244], loss=6.3245
	step [129/244], loss=5.3885
	step [130/244], loss=7.3253
	step [131/244], loss=6.6004
	step [132/244], loss=6.8717
	step [133/244], loss=6.8287
	step [134/244], loss=5.7120
	step [135/244], loss=6.8280
	step [136/244], loss=6.1545
	step [137/244], loss=6.0330
	step [138/244], loss=6.2905
	step [139/244], loss=6.4693
	step [140/244], loss=6.1394
	step [141/244], loss=5.8784
	step [142/244], loss=6.3969
	step [143/244], loss=5.8997
	step [144/244], loss=6.2783
	step [145/244], loss=5.8199
	step [146/244], loss=5.6351
	step [147/244], loss=6.7907
	step [148/244], loss=5.0149
	step [149/244], loss=6.7042
	step [150/244], loss=6.4620
	step [151/244], loss=5.3745
	step [152/244], loss=6.1539
	step [153/244], loss=8.0024
	step [154/244], loss=5.9679
	step [155/244], loss=5.9045
	step [156/244], loss=5.8130
	step [157/244], loss=5.5147
	step [158/244], loss=6.6986
	step [159/244], loss=5.8908
	step [160/244], loss=5.6362
	step [161/244], loss=7.3710
	step [162/244], loss=7.3603
	step [163/244], loss=5.8040
	step [164/244], loss=6.0175
	step [165/244], loss=5.1405
	step [166/244], loss=6.1452
	step [167/244], loss=5.8247
	step [168/244], loss=5.5189
	step [169/244], loss=5.7029
	step [170/244], loss=7.4888
	step [171/244], loss=5.7528
	step [172/244], loss=6.0760
	step [173/244], loss=7.9617
	step [174/244], loss=6.5240
	step [175/244], loss=6.6067
	step [176/244], loss=6.2503
	step [177/244], loss=5.3941
	step [178/244], loss=5.7738
	step [179/244], loss=6.8917
	step [180/244], loss=6.5816
	step [181/244], loss=6.1333
	step [182/244], loss=6.9416
	step [183/244], loss=5.1986
	step [184/244], loss=6.7186
	step [185/244], loss=5.8575
	step [186/244], loss=6.4043
	step [187/244], loss=6.7088
	step [188/244], loss=5.8365
	step [189/244], loss=7.5693
	step [190/244], loss=6.0475
	step [191/244], loss=6.8386
	step [192/244], loss=7.1764
	step [193/244], loss=6.4315
	step [194/244], loss=6.9426
	step [195/244], loss=7.2866
	step [196/244], loss=7.2908
	step [197/244], loss=6.5943
	step [198/244], loss=5.6135
	step [199/244], loss=7.3813
	step [200/244], loss=7.2694
	step [201/244], loss=6.2164
	step [202/244], loss=5.4774
	step [203/244], loss=7.7550
	step [204/244], loss=7.6238
	step [205/244], loss=6.6064
	step [206/244], loss=6.4976
	step [207/244], loss=6.7961
	step [208/244], loss=6.6907
	step [209/244], loss=5.9424
	step [210/244], loss=6.0840
	step [211/244], loss=5.8090
	step [212/244], loss=5.7731
	step [213/244], loss=6.8088
	step [214/244], loss=5.8585
	step [215/244], loss=7.2692
	step [216/244], loss=7.2311
	step [217/244], loss=9.2126
	step [218/244], loss=6.6631
	step [219/244], loss=5.3775
	step [220/244], loss=6.6326
	step [221/244], loss=7.8663
	step [222/244], loss=6.5654
	step [223/244], loss=6.2053
	step [224/244], loss=6.5889
	step [225/244], loss=6.9263
	step [226/244], loss=7.3140
	step [227/244], loss=6.8883
	step [228/244], loss=8.5625
	step [229/244], loss=6.2968
	step [230/244], loss=7.0781
	step [231/244], loss=6.8531
	step [232/244], loss=5.4337
	step [233/244], loss=6.4144
	step [234/244], loss=6.6109
	step [235/244], loss=5.9657
	step [236/244], loss=6.6297
	step [237/244], loss=6.2371
	step [238/244], loss=6.8988
	step [239/244], loss=6.4662
	step [240/244], loss=6.2621
	step [241/244], loss=6.1723
	step [242/244], loss=6.9122
	step [243/244], loss=6.3087
	step [244/244], loss=2.5353
	Evaluating
	loss=0.0195, precision=0.2228, recall=0.9938, f1=0.3639
Training epoch 21
	step [1/244], loss=6.5826
	step [2/244], loss=6.2102
	step [3/244], loss=5.8394
	step [4/244], loss=7.5482
	step [5/244], loss=6.7015
	step [6/244], loss=5.6465
	step [7/244], loss=6.3550
	step [8/244], loss=6.8020
	step [9/244], loss=7.3525
	step [10/244], loss=5.4727
	step [11/244], loss=6.0749
	step [12/244], loss=6.3279
	step [13/244], loss=6.5847
	step [14/244], loss=6.2033
	step [15/244], loss=5.9835
	step [16/244], loss=7.1919
	step [17/244], loss=7.4226
	step [18/244], loss=6.5605
	step [19/244], loss=6.8003
	step [20/244], loss=4.8520
	step [21/244], loss=6.6625
	step [22/244], loss=5.6440
	step [23/244], loss=7.1399
	step [24/244], loss=6.6108
	step [25/244], loss=5.1688
	step [26/244], loss=5.6040
	step [27/244], loss=6.4208
	step [28/244], loss=6.2823
	step [29/244], loss=7.9548
	step [30/244], loss=7.3842
	step [31/244], loss=5.7823
	step [32/244], loss=7.1503
	step [33/244], loss=6.6203
	step [34/244], loss=6.2313
	step [35/244], loss=6.5456
	step [36/244], loss=5.5546
	step [37/244], loss=5.0872
	step [38/244], loss=5.3062
	step [39/244], loss=6.4315
	step [40/244], loss=6.2333
	step [41/244], loss=6.0365
	step [42/244], loss=5.0230
	step [43/244], loss=7.8451
	step [44/244], loss=5.7178
	step [45/244], loss=6.0738
	step [46/244], loss=6.1441
	step [47/244], loss=6.2353
	step [48/244], loss=7.7170
	step [49/244], loss=6.1713
	step [50/244], loss=6.8023
	step [51/244], loss=6.6343
	step [52/244], loss=7.2151
	step [53/244], loss=6.9257
	step [54/244], loss=5.2440
	step [55/244], loss=5.7891
	step [56/244], loss=5.7030
	step [57/244], loss=5.4254
	step [58/244], loss=4.8890
	step [59/244], loss=5.1595
	step [60/244], loss=6.7518
	step [61/244], loss=7.0092
	step [62/244], loss=5.9128
	step [63/244], loss=4.9988
	step [64/244], loss=6.2862
	step [65/244], loss=6.2531
	step [66/244], loss=6.7332
	step [67/244], loss=5.7809
	step [68/244], loss=5.7384
	step [69/244], loss=6.1428
	step [70/244], loss=5.3833
	step [71/244], loss=5.0007
	step [72/244], loss=5.7752
	step [73/244], loss=6.6977
	step [74/244], loss=6.5242
	step [75/244], loss=7.0918
	step [76/244], loss=5.7127
	step [77/244], loss=7.0411
	step [78/244], loss=5.3863
	step [79/244], loss=5.6843
	step [80/244], loss=5.2212
	step [81/244], loss=7.4944
	step [82/244], loss=5.6845
	step [83/244], loss=6.3348
	step [84/244], loss=5.8345
	step [85/244], loss=5.8273
	step [86/244], loss=6.5561
	step [87/244], loss=6.6994
	step [88/244], loss=5.8318
	step [89/244], loss=5.3067
	step [90/244], loss=5.5851
	step [91/244], loss=6.1306
	step [92/244], loss=5.3882
	step [93/244], loss=5.5237
	step [94/244], loss=6.7472
	step [95/244], loss=6.0978
	step [96/244], loss=5.9655
	step [97/244], loss=5.8322
	step [98/244], loss=4.9720
	step [99/244], loss=6.1499
	step [100/244], loss=5.5466
	step [101/244], loss=6.3050
	step [102/244], loss=5.9221
	step [103/244], loss=5.9646
	step [104/244], loss=4.9345
	step [105/244], loss=7.0152
	step [106/244], loss=5.8546
	step [107/244], loss=5.7294
	step [108/244], loss=6.2960
	step [109/244], loss=5.5583
	step [110/244], loss=6.1346
	step [111/244], loss=6.3407
	step [112/244], loss=5.9658
	step [113/244], loss=5.2560
	step [114/244], loss=6.3697
	step [115/244], loss=5.1885
	step [116/244], loss=5.9177
	step [117/244], loss=5.8694
	step [118/244], loss=5.8740
	step [119/244], loss=4.8623
	step [120/244], loss=8.0784
	step [121/244], loss=5.1996
	step [122/244], loss=7.5341
	step [123/244], loss=7.1008
	step [124/244], loss=7.4319
	step [125/244], loss=5.9560
	step [126/244], loss=6.7989
	step [127/244], loss=6.6895
	step [128/244], loss=5.8327
	step [129/244], loss=4.9696
	step [130/244], loss=5.2522
	step [131/244], loss=6.9978
	step [132/244], loss=5.6099
	step [133/244], loss=5.2622
	step [134/244], loss=6.2483
	step [135/244], loss=6.0326
	step [136/244], loss=6.1333
	step [137/244], loss=6.8833
	step [138/244], loss=6.7698
	step [139/244], loss=5.0245
	step [140/244], loss=6.0085
	step [141/244], loss=6.0570
	step [142/244], loss=5.6254
	step [143/244], loss=7.6488
	step [144/244], loss=5.6393
	step [145/244], loss=5.7174
	step [146/244], loss=5.0534
	step [147/244], loss=7.2788
	step [148/244], loss=7.3294
	step [149/244], loss=7.4815
	step [150/244], loss=5.9126
	step [151/244], loss=6.2256
	step [152/244], loss=7.0436
	step [153/244], loss=5.6170
	step [154/244], loss=6.1911
	step [155/244], loss=6.4245
	step [156/244], loss=5.1345
	step [157/244], loss=6.4512
	step [158/244], loss=5.8435
	step [159/244], loss=5.1662
	step [160/244], loss=5.8861
	step [161/244], loss=5.4850
	step [162/244], loss=6.0415
	step [163/244], loss=6.7458
	step [164/244], loss=5.5498
	step [165/244], loss=6.0863
	step [166/244], loss=13.9887
	step [167/244], loss=6.0382
	step [168/244], loss=6.0423
	step [169/244], loss=5.8013
	step [170/244], loss=6.0775
	step [171/244], loss=6.4365
	step [172/244], loss=5.6285
	step [173/244], loss=5.5833
	step [174/244], loss=7.0900
	step [175/244], loss=5.9543
	step [176/244], loss=4.9729
	step [177/244], loss=6.8254
	step [178/244], loss=7.2873
	step [179/244], loss=6.1543
	step [180/244], loss=5.8728
	step [181/244], loss=5.1123
	step [182/244], loss=5.1734
	step [183/244], loss=5.6641
	step [184/244], loss=6.5486
	step [185/244], loss=5.9849
	step [186/244], loss=5.7070
	step [187/244], loss=6.3068
	step [188/244], loss=6.1064
	step [189/244], loss=5.1691
	step [190/244], loss=5.5232
	step [191/244], loss=6.0702
	step [192/244], loss=5.5106
	step [193/244], loss=5.8961
	step [194/244], loss=7.1340
	step [195/244], loss=6.9382
	step [196/244], loss=5.3858
	step [197/244], loss=6.5053
	step [198/244], loss=5.9949
	step [199/244], loss=5.9409
	step [200/244], loss=5.3389
	step [201/244], loss=5.9062
	step [202/244], loss=5.3112
	step [203/244], loss=6.1299
	step [204/244], loss=5.1790
	step [205/244], loss=6.0816
	step [206/244], loss=6.6305
	step [207/244], loss=6.1740
	step [208/244], loss=5.0096
	step [209/244], loss=8.2966
	step [210/244], loss=5.0283
	step [211/244], loss=5.2280
	step [212/244], loss=5.4415
	step [213/244], loss=4.6783
	step [214/244], loss=6.2195
	step [215/244], loss=6.6702
	step [216/244], loss=5.1433
	step [217/244], loss=5.6453
	step [218/244], loss=4.8819
	step [219/244], loss=5.3882
	step [220/244], loss=6.5337
	step [221/244], loss=6.3915
	step [222/244], loss=6.6313
	step [223/244], loss=5.2665
	step [224/244], loss=5.1771
	step [225/244], loss=5.0223
	step [226/244], loss=6.6983
	step [227/244], loss=5.5698
	step [228/244], loss=5.2629
	step [229/244], loss=6.1058
	step [230/244], loss=6.6248
	step [231/244], loss=5.0961
	step [232/244], loss=5.5681
	step [233/244], loss=6.3851
	step [234/244], loss=6.2130
	step [235/244], loss=6.5550
	step [236/244], loss=6.6190
	step [237/244], loss=5.9096
	step [238/244], loss=6.1077
	step [239/244], loss=4.9238
	step [240/244], loss=6.1867
	step [241/244], loss=5.4755
	step [242/244], loss=6.1581
	step [243/244], loss=6.9488
	step [244/244], loss=2.1945
	Evaluating
	loss=0.0146, precision=0.2689, recall=0.9911, f1=0.4230
saving model as: 1_saved_model.pth
Training epoch 22
	step [1/244], loss=5.7433
	step [2/244], loss=5.4086
	step [3/244], loss=6.4405
	step [4/244], loss=6.0682
	step [5/244], loss=5.6411
	step [6/244], loss=6.6364
	step [7/244], loss=5.0317
	step [8/244], loss=6.1964
	step [9/244], loss=5.4050
	step [10/244], loss=6.8056
	step [11/244], loss=6.9059
	step [12/244], loss=6.3190
	step [13/244], loss=6.0628
	step [14/244], loss=5.1284
	step [15/244], loss=8.1738
	step [16/244], loss=4.7710
	step [17/244], loss=5.2606
	step [18/244], loss=5.7245
	step [19/244], loss=5.4103
	step [20/244], loss=5.5566
	step [21/244], loss=4.5150
	step [22/244], loss=5.4333
	step [23/244], loss=5.8773
	step [24/244], loss=6.8280
	step [25/244], loss=5.8221
	step [26/244], loss=7.1065
	step [27/244], loss=6.4166
	step [28/244], loss=6.5416
	step [29/244], loss=5.1299
	step [30/244], loss=5.1879
	step [31/244], loss=6.0238
	step [32/244], loss=6.8230
	step [33/244], loss=6.4286
	step [34/244], loss=5.4020
	step [35/244], loss=6.2919
	step [36/244], loss=5.9705
	step [37/244], loss=6.3104
	step [38/244], loss=6.1063
	step [39/244], loss=6.0467
	step [40/244], loss=4.7585
	step [41/244], loss=5.1338
	step [42/244], loss=5.2912
	step [43/244], loss=6.1947
	step [44/244], loss=5.5799
	step [45/244], loss=5.2132
	step [46/244], loss=6.7085
	step [47/244], loss=6.3658
	step [48/244], loss=6.2898
	step [49/244], loss=6.5235
	step [50/244], loss=5.1541
	step [51/244], loss=5.0337
	step [52/244], loss=6.2220
	step [53/244], loss=5.7752
	step [54/244], loss=5.6125
	step [55/244], loss=5.9575
	step [56/244], loss=7.5186
	step [57/244], loss=5.7400
	step [58/244], loss=5.7204
	step [59/244], loss=6.3541
	step [60/244], loss=6.1341
	step [61/244], loss=5.6106
	step [62/244], loss=5.3148
	step [63/244], loss=4.7983
	step [64/244], loss=5.3627
	step [65/244], loss=5.8593
	step [66/244], loss=5.3518
	step [67/244], loss=5.8074
	step [68/244], loss=6.5050
	step [69/244], loss=5.6048
	step [70/244], loss=5.7191
	step [71/244], loss=6.3514
	step [72/244], loss=6.2178
	step [73/244], loss=6.2266
	step [74/244], loss=5.5969
	step [75/244], loss=5.1096
	step [76/244], loss=5.4611
	step [77/244], loss=5.6433
	step [78/244], loss=6.5392
	step [79/244], loss=5.7733
	step [80/244], loss=5.5243
	step [81/244], loss=5.6314
	step [82/244], loss=5.6200
	step [83/244], loss=5.9253
	step [84/244], loss=5.9430
	step [85/244], loss=6.5201
	step [86/244], loss=5.9561
	step [87/244], loss=5.4756
	step [88/244], loss=5.3809
	step [89/244], loss=7.0117
	step [90/244], loss=6.0836
	step [91/244], loss=5.6974
	step [92/244], loss=5.9091
	step [93/244], loss=4.3542
	step [94/244], loss=5.0252
	step [95/244], loss=6.2904
	step [96/244], loss=5.5403
	step [97/244], loss=5.1409
	step [98/244], loss=7.2761
	step [99/244], loss=5.5268
	step [100/244], loss=6.5740
	step [101/244], loss=5.2526
	step [102/244], loss=7.1446
	step [103/244], loss=7.9244
	step [104/244], loss=7.9027
	step [105/244], loss=6.4949
	step [106/244], loss=4.6933
	step [107/244], loss=6.8417
	step [108/244], loss=5.5762
	step [109/244], loss=5.5607
	step [110/244], loss=4.6748
	step [111/244], loss=6.3187
	step [112/244], loss=6.9473
	step [113/244], loss=6.6362
	step [114/244], loss=6.4262
	step [115/244], loss=5.7025
	step [116/244], loss=6.0712
	step [117/244], loss=5.5869
	step [118/244], loss=5.9036
	step [119/244], loss=5.2888
	step [120/244], loss=5.9005
	step [121/244], loss=5.3722
	step [122/244], loss=5.3741
	step [123/244], loss=6.7829
	step [124/244], loss=7.0157
	step [125/244], loss=7.0140
	step [126/244], loss=6.8933
	step [127/244], loss=6.9727
	step [128/244], loss=6.1205
	step [129/244], loss=5.8459
	step [130/244], loss=6.4644
	step [131/244], loss=5.9061
	step [132/244], loss=6.4786
	step [133/244], loss=5.9041
	step [134/244], loss=4.8899
	step [135/244], loss=5.8132
	step [136/244], loss=7.2367
	step [137/244], loss=6.4831
	step [138/244], loss=5.8316
	step [139/244], loss=6.0429
	step [140/244], loss=5.2935
	step [141/244], loss=5.9092
	step [142/244], loss=6.0356
	step [143/244], loss=5.2555
	step [144/244], loss=7.0545
	step [145/244], loss=7.0209
	step [146/244], loss=5.5489
	step [147/244], loss=5.8612
	step [148/244], loss=5.5001
	step [149/244], loss=6.0304
	step [150/244], loss=6.5083
	step [151/244], loss=6.4599
	step [152/244], loss=6.2670
	step [153/244], loss=6.4994
	step [154/244], loss=5.6217
	step [155/244], loss=5.4094
	step [156/244], loss=6.3345
	step [157/244], loss=4.2975
	step [158/244], loss=6.5246
	step [159/244], loss=6.7027
	step [160/244], loss=6.5382
	step [161/244], loss=4.9680
	step [162/244], loss=5.5693
	step [163/244], loss=6.7637
	step [164/244], loss=5.5751
	step [165/244], loss=5.4792
	step [166/244], loss=5.6491
	step [167/244], loss=6.2442
	step [168/244], loss=6.7135
	step [169/244], loss=5.0917
	step [170/244], loss=5.4758
	step [171/244], loss=5.4235
	step [172/244], loss=6.2456
	step [173/244], loss=5.1421
	step [174/244], loss=6.0368
	step [175/244], loss=6.6246
	step [176/244], loss=5.7743
	step [177/244], loss=5.9390
	step [178/244], loss=6.6325
	step [179/244], loss=4.9810
	step [180/244], loss=5.9896
	step [181/244], loss=6.1010
	step [182/244], loss=6.4127
	step [183/244], loss=5.4915
	step [184/244], loss=6.1901
	step [185/244], loss=5.7767
	step [186/244], loss=6.0534
	step [187/244], loss=6.1349
	step [188/244], loss=5.4441
	step [189/244], loss=4.9806
	step [190/244], loss=6.1909
	step [191/244], loss=5.0025
	step [192/244], loss=5.5073
	step [193/244], loss=5.5106
	step [194/244], loss=6.7219
	step [195/244], loss=6.5572
	step [196/244], loss=5.6735
	step [197/244], loss=6.6200
	step [198/244], loss=6.0221
	step [199/244], loss=5.9235
	step [200/244], loss=7.8563
	step [201/244], loss=5.2344
	step [202/244], loss=7.9499
	step [203/244], loss=6.2634
	step [204/244], loss=7.1136
	step [205/244], loss=7.8443
	step [206/244], loss=5.8512
	step [207/244], loss=6.2552
	step [208/244], loss=5.5110
	step [209/244], loss=5.7458
	step [210/244], loss=5.1149
	step [211/244], loss=5.5317
	step [212/244], loss=5.7470
	step [213/244], loss=6.1640
	step [214/244], loss=5.1378
	step [215/244], loss=6.3869
	step [216/244], loss=6.4743
	step [217/244], loss=5.3776
	step [218/244], loss=5.7804
	step [219/244], loss=5.7085
	step [220/244], loss=7.4598
	step [221/244], loss=5.3497
	step [222/244], loss=5.3999
	step [223/244], loss=5.3605
	step [224/244], loss=5.8321
	step [225/244], loss=6.2903
	step [226/244], loss=6.0385
	step [227/244], loss=7.2388
	step [228/244], loss=5.1808
	step [229/244], loss=6.4344
	step [230/244], loss=5.5405
	step [231/244], loss=5.6060
	step [232/244], loss=5.2496
	step [233/244], loss=7.8984
	step [234/244], loss=7.0297
	step [235/244], loss=4.8951
	step [236/244], loss=5.8340
	step [237/244], loss=5.3243
	step [238/244], loss=6.4183
	step [239/244], loss=4.5969
	step [240/244], loss=5.3599
	step [241/244], loss=6.3080
	step [242/244], loss=6.3708
	step [243/244], loss=5.7851
	step [244/244], loss=2.2258
	Evaluating
	loss=0.0175, precision=0.2266, recall=0.9928, f1=0.3690
Training epoch 23
	step [1/244], loss=5.3287
	step [2/244], loss=5.6636
	step [3/244], loss=5.1986
	step [4/244], loss=6.5446
	step [5/244], loss=6.1635
	step [6/244], loss=5.5434
	step [7/244], loss=5.9456
	step [8/244], loss=5.4064
	step [9/244], loss=5.8123
	step [10/244], loss=6.3815
	step [11/244], loss=6.1387
	step [12/244], loss=6.6842
	step [13/244], loss=6.6999
	step [14/244], loss=5.6101
	step [15/244], loss=5.7545
	step [16/244], loss=6.0816
	step [17/244], loss=5.3181
	step [18/244], loss=6.5677
	step [19/244], loss=6.1327
	step [20/244], loss=5.6635
	step [21/244], loss=5.7241
	step [22/244], loss=6.1374
	step [23/244], loss=5.1181
	step [24/244], loss=5.7516
	step [25/244], loss=5.6773
	step [26/244], loss=6.0130
	step [27/244], loss=5.5703
	step [28/244], loss=7.8432
	step [29/244], loss=5.8540
	step [30/244], loss=5.5142
	step [31/244], loss=6.1367
	step [32/244], loss=5.7289
	step [33/244], loss=6.4739
	step [34/244], loss=5.3510
	step [35/244], loss=5.2593
	step [36/244], loss=7.0517
	step [37/244], loss=6.4687
	step [38/244], loss=5.2457
	step [39/244], loss=6.0301
	step [40/244], loss=4.7490
	step [41/244], loss=4.5395
	step [42/244], loss=5.7296
	step [43/244], loss=6.0193
	step [44/244], loss=5.2379
	step [45/244], loss=6.1228
	step [46/244], loss=6.8042
	step [47/244], loss=6.9319
	step [48/244], loss=5.9943
	step [49/244], loss=6.7356
	step [50/244], loss=5.6672
	step [51/244], loss=5.2857
	step [52/244], loss=6.2719
	step [53/244], loss=5.2463
	step [54/244], loss=5.5032
	step [55/244], loss=4.4221
	step [56/244], loss=6.0497
	step [57/244], loss=5.9233
	step [58/244], loss=5.4509
	step [59/244], loss=5.3415
	step [60/244], loss=5.7945
	step [61/244], loss=5.6178
	step [62/244], loss=6.1462
	step [63/244], loss=6.3101
	step [64/244], loss=4.9094
	step [65/244], loss=5.9682
	step [66/244], loss=5.6326
	step [67/244], loss=5.9170
	step [68/244], loss=4.8750
	step [69/244], loss=4.9512
	step [70/244], loss=5.7348
	step [71/244], loss=5.5166
	step [72/244], loss=5.7228
	step [73/244], loss=4.9762
	step [74/244], loss=5.4488
	step [75/244], loss=5.4517
	step [76/244], loss=4.7677
	step [77/244], loss=7.0694
	step [78/244], loss=5.9699
	step [79/244], loss=4.5362
	step [80/244], loss=6.5521
	step [81/244], loss=5.7092
	step [82/244], loss=5.2506
	step [83/244], loss=4.6373
	step [84/244], loss=5.4841
	step [85/244], loss=5.5585
	step [86/244], loss=5.4287
	step [87/244], loss=6.4000
	step [88/244], loss=5.4276
	step [89/244], loss=6.4198
	step [90/244], loss=5.8174
	step [91/244], loss=5.6815
	step [92/244], loss=5.8435
	step [93/244], loss=5.1394
	step [94/244], loss=5.1888
	step [95/244], loss=5.4419
	step [96/244], loss=5.8547
	step [97/244], loss=6.0579
	step [98/244], loss=4.7246
	step [99/244], loss=6.0310
	step [100/244], loss=5.7824
	step [101/244], loss=6.6878
	step [102/244], loss=5.0553
	step [103/244], loss=5.0320
	step [104/244], loss=5.9487
	step [105/244], loss=4.5746
	step [106/244], loss=6.9408
	step [107/244], loss=5.5943
	step [108/244], loss=6.3282
	step [109/244], loss=5.3181
	step [110/244], loss=6.7409
	step [111/244], loss=5.5314
	step [112/244], loss=6.0663
	step [113/244], loss=5.5625
	step [114/244], loss=5.1404
	step [115/244], loss=4.4952
	step [116/244], loss=5.9873
	step [117/244], loss=6.6595
	step [118/244], loss=5.4782
	step [119/244], loss=6.2552
	step [120/244], loss=5.1869
	step [121/244], loss=4.9054
	step [122/244], loss=5.9860
	step [123/244], loss=4.9910
	step [124/244], loss=6.5098
	step [125/244], loss=5.3543
	step [126/244], loss=4.5906
	step [127/244], loss=6.6491
	step [128/244], loss=5.6857
	step [129/244], loss=5.9268
	step [130/244], loss=4.9873
	step [131/244], loss=6.9439
	step [132/244], loss=5.8257
	step [133/244], loss=5.4602
	step [134/244], loss=5.8234
	step [135/244], loss=6.9388
	step [136/244], loss=7.3844
	step [137/244], loss=6.5756
	step [138/244], loss=5.3236
	step [139/244], loss=5.6034
	step [140/244], loss=5.9160
	step [141/244], loss=5.6451
	step [142/244], loss=5.6427
	step [143/244], loss=5.4893
	step [144/244], loss=6.3799
	step [145/244], loss=6.0842
	step [146/244], loss=4.9863
	step [147/244], loss=5.4130
	step [148/244], loss=6.3462
	step [149/244], loss=5.8787
	step [150/244], loss=5.9184
	step [151/244], loss=6.3191
	step [152/244], loss=5.6752
	step [153/244], loss=6.5124
	step [154/244], loss=5.4978
	step [155/244], loss=4.6797
	step [156/244], loss=5.4661
	step [157/244], loss=5.4427
	step [158/244], loss=5.6683
	step [159/244], loss=5.7038
	step [160/244], loss=5.8482
	step [161/244], loss=5.8667
	step [162/244], loss=5.6010
	step [163/244], loss=6.0543
	step [164/244], loss=5.0139
	step [165/244], loss=6.0615
	step [166/244], loss=4.9749
	step [167/244], loss=5.5885
	step [168/244], loss=4.8494
	step [169/244], loss=6.2776
	step [170/244], loss=6.3471
	step [171/244], loss=5.7939
	step [172/244], loss=5.7311
	step [173/244], loss=6.2254
	step [174/244], loss=5.0271
	step [175/244], loss=6.1231
	step [176/244], loss=5.4404
	step [177/244], loss=5.3443
	step [178/244], loss=5.1646
	step [179/244], loss=6.1128
	step [180/244], loss=7.1506
	step [181/244], loss=6.5684
	step [182/244], loss=5.7732
	step [183/244], loss=4.9977
	step [184/244], loss=5.5746
	step [185/244], loss=4.9700
	step [186/244], loss=5.1676
	step [187/244], loss=5.2827
	step [188/244], loss=4.8253
	step [189/244], loss=5.6968
	step [190/244], loss=5.3177
	step [191/244], loss=4.9734
	step [192/244], loss=6.0225
	step [193/244], loss=6.2744
	step [194/244], loss=5.5199
	step [195/244], loss=5.8241
	step [196/244], loss=6.2756
	step [197/244], loss=5.4984
	step [198/244], loss=5.1152
	step [199/244], loss=4.6154
	step [200/244], loss=7.0473
	step [201/244], loss=4.9718
	step [202/244], loss=4.9821
	step [203/244], loss=6.0502
	step [204/244], loss=5.1119
	step [205/244], loss=5.3789
	step [206/244], loss=6.3986
	step [207/244], loss=6.1774
	step [208/244], loss=5.5693
	step [209/244], loss=5.6200
	step [210/244], loss=5.3617
	step [211/244], loss=5.8801
	step [212/244], loss=5.5415
	step [213/244], loss=6.3519
	step [214/244], loss=7.2354
	step [215/244], loss=4.8597
	step [216/244], loss=6.8185
	step [217/244], loss=5.2447
	step [218/244], loss=6.6982
	step [219/244], loss=4.6717
	step [220/244], loss=5.5169
	step [221/244], loss=5.4317
	step [222/244], loss=7.9429
	step [223/244], loss=4.2465
	step [224/244], loss=5.4217
	step [225/244], loss=4.4476
	step [226/244], loss=4.8329
	step [227/244], loss=5.2088
	step [228/244], loss=5.7760
	step [229/244], loss=5.3487
	step [230/244], loss=7.4533
	step [231/244], loss=5.6167
	step [232/244], loss=5.5756
	step [233/244], loss=6.2086
	step [234/244], loss=6.0230
	step [235/244], loss=5.5899
	step [236/244], loss=6.2583
	step [237/244], loss=5.4221
	step [238/244], loss=5.5010
	step [239/244], loss=5.6846
	step [240/244], loss=5.4044
	step [241/244], loss=6.5188
	step [242/244], loss=4.4941
	step [243/244], loss=5.4946
	step [244/244], loss=2.9665
	Evaluating
	loss=0.0147, precision=0.2631, recall=0.9908, f1=0.4157
Training epoch 24
	step [1/244], loss=5.7713
	step [2/244], loss=5.9160
	step [3/244], loss=7.0091
	step [4/244], loss=5.5860
	step [5/244], loss=5.3776
	step [6/244], loss=5.2271
	step [7/244], loss=5.2791
	step [8/244], loss=6.0634
	step [9/244], loss=5.8701
	step [10/244], loss=5.2597
	step [11/244], loss=6.4110
	step [12/244], loss=5.6195
	step [13/244], loss=4.2581
	step [14/244], loss=5.7795
	step [15/244], loss=7.4446
	step [16/244], loss=6.0064
	step [17/244], loss=5.2449
	step [18/244], loss=5.6324
	step [19/244], loss=5.2850
	step [20/244], loss=6.2510
	step [21/244], loss=5.0310
	step [22/244], loss=7.2647
	step [23/244], loss=5.9755
	step [24/244], loss=5.5832
	step [25/244], loss=6.5490
	step [26/244], loss=6.0115
	step [27/244], loss=5.0054
	step [28/244], loss=5.3547
	step [29/244], loss=5.9141
	step [30/244], loss=4.5583
	step [31/244], loss=6.1451
	step [32/244], loss=6.0894
	step [33/244], loss=5.6073
	step [34/244], loss=5.9305
	step [35/244], loss=5.6756
	step [36/244], loss=6.1804
	step [37/244], loss=5.1287
	step [38/244], loss=5.5592
	step [39/244], loss=5.7927
	step [40/244], loss=4.3748
	step [41/244], loss=5.2985
	step [42/244], loss=5.6455
	step [43/244], loss=6.3599
	step [44/244], loss=5.1472
	step [45/244], loss=5.7978
	step [46/244], loss=6.3740
	step [47/244], loss=5.5274
	step [48/244], loss=5.9143
	step [49/244], loss=5.6933
	step [50/244], loss=5.9359
	step [51/244], loss=6.1819
	step [52/244], loss=5.7844
	step [53/244], loss=5.8619
	step [54/244], loss=6.1722
	step [55/244], loss=5.3030
	step [56/244], loss=4.1711
	step [57/244], loss=5.6565
	step [58/244], loss=7.3728
	step [59/244], loss=5.6061
	step [60/244], loss=4.8510
	step [61/244], loss=5.1087
	step [62/244], loss=5.1574
	step [63/244], loss=7.1989
	step [64/244], loss=5.1507
	step [65/244], loss=6.1267
	step [66/244], loss=5.8875
	step [67/244], loss=6.2706
	step [68/244], loss=5.4661
	step [69/244], loss=5.4679
	step [70/244], loss=4.5506
	step [71/244], loss=5.9193
	step [72/244], loss=4.7152
	step [73/244], loss=5.2887
	step [74/244], loss=5.7476
	step [75/244], loss=5.3576
	step [76/244], loss=5.8582
	step [77/244], loss=5.4586
	step [78/244], loss=3.9300
	step [79/244], loss=4.9464
	step [80/244], loss=5.9700
	step [81/244], loss=5.3732
	step [82/244], loss=5.4201
	step [83/244], loss=5.6695
	step [84/244], loss=5.1530
	step [85/244], loss=5.9043
	step [86/244], loss=6.1272
	step [87/244], loss=5.6791
	step [88/244], loss=4.1499
	step [89/244], loss=5.2171
	step [90/244], loss=5.6903
	step [91/244], loss=5.1262
	step [92/244], loss=6.1376
	step [93/244], loss=6.4188
	step [94/244], loss=5.7980
	step [95/244], loss=5.1200
	step [96/244], loss=4.4484
	step [97/244], loss=5.1945
	step [98/244], loss=4.3598
	step [99/244], loss=5.6105
	step [100/244], loss=5.7793
	step [101/244], loss=6.6460
	step [102/244], loss=5.7911
	step [103/244], loss=6.1960
	step [104/244], loss=6.8336
	step [105/244], loss=5.6124
	step [106/244], loss=6.2272
	step [107/244], loss=4.9602
	step [108/244], loss=5.5413
	step [109/244], loss=5.2441
	step [110/244], loss=4.8980
	step [111/244], loss=6.3788
	step [112/244], loss=6.4713
	step [113/244], loss=5.7894
	step [114/244], loss=5.2297
	step [115/244], loss=4.4146
	step [116/244], loss=4.3554
	step [117/244], loss=6.4731
	step [118/244], loss=5.4299
	step [119/244], loss=5.1015
	step [120/244], loss=7.1133
	step [121/244], loss=4.4565
	step [122/244], loss=5.7282
	step [123/244], loss=5.3849
	step [124/244], loss=5.3442
	step [125/244], loss=5.7706
	step [126/244], loss=6.0057
	step [127/244], loss=5.2865
	step [128/244], loss=4.6304
	step [129/244], loss=5.8212
	step [130/244], loss=5.0597
	step [131/244], loss=5.7288
	step [132/244], loss=5.2365
	step [133/244], loss=5.5593
	step [134/244], loss=6.0114
	step [135/244], loss=7.0603
	step [136/244], loss=5.1028
	step [137/244], loss=5.7013
	step [138/244], loss=4.7647
	step [139/244], loss=6.0527
	step [140/244], loss=4.6233
	step [141/244], loss=5.8229
	step [142/244], loss=6.1550
	step [143/244], loss=6.6627
	step [144/244], loss=6.3773
	step [145/244], loss=5.4179
	step [146/244], loss=6.0393
	step [147/244], loss=6.3861
	step [148/244], loss=5.9093
	step [149/244], loss=5.7319
	step [150/244], loss=6.5530
	step [151/244], loss=5.7283
	step [152/244], loss=6.0139
	step [153/244], loss=5.4541
	step [154/244], loss=5.1598
	step [155/244], loss=4.3749
	step [156/244], loss=5.7921
	step [157/244], loss=4.9440
	step [158/244], loss=7.0759
	step [159/244], loss=4.6796
	step [160/244], loss=5.5498
	step [161/244], loss=5.3911
	step [162/244], loss=5.3285
	step [163/244], loss=6.2856
	step [164/244], loss=5.3168
	step [165/244], loss=5.8526
	step [166/244], loss=5.8732
	step [167/244], loss=5.9274
	step [168/244], loss=4.8658
	step [169/244], loss=4.8763
	step [170/244], loss=5.0104
	step [171/244], loss=6.6898
	step [172/244], loss=4.6599
	step [173/244], loss=5.9372
	step [174/244], loss=5.6897
	step [175/244], loss=5.2234
	step [176/244], loss=5.1083
	step [177/244], loss=4.6803
	step [178/244], loss=5.5541
	step [179/244], loss=5.8954
	step [180/244], loss=6.0183
	step [181/244], loss=5.4304
	step [182/244], loss=5.5959
	step [183/244], loss=5.5630
	step [184/244], loss=5.1595
	step [185/244], loss=5.9404
	step [186/244], loss=5.8579
	step [187/244], loss=5.0646
	step [188/244], loss=5.2794
	step [189/244], loss=5.2787
	step [190/244], loss=5.6651
	step [191/244], loss=5.5365
	step [192/244], loss=5.1043
	step [193/244], loss=3.9520
	step [194/244], loss=6.2716
	step [195/244], loss=4.7013
	step [196/244], loss=4.9991
	step [197/244], loss=5.0884
	step [198/244], loss=5.4509
	step [199/244], loss=5.6978
	step [200/244], loss=6.2613
	step [201/244], loss=4.9841
	step [202/244], loss=5.6370
	step [203/244], loss=5.8077
	step [204/244], loss=5.4676
	step [205/244], loss=5.0778
	step [206/244], loss=5.4181
	step [207/244], loss=6.0285
	step [208/244], loss=5.6471
	step [209/244], loss=5.7789
	step [210/244], loss=5.3318
	step [211/244], loss=5.1112
	step [212/244], loss=6.0399
	step [213/244], loss=5.0328
	step [214/244], loss=4.6852
	step [215/244], loss=5.6805
	step [216/244], loss=5.0570
	step [217/244], loss=4.9900
	step [218/244], loss=5.4331
	step [219/244], loss=4.4472
	step [220/244], loss=6.6101
	step [221/244], loss=4.9645
	step [222/244], loss=5.2866
	step [223/244], loss=6.1219
	step [224/244], loss=4.0214
	step [225/244], loss=5.7640
	step [226/244], loss=5.8517
	step [227/244], loss=5.9930
	step [228/244], loss=4.9586
	step [229/244], loss=5.7868
	step [230/244], loss=8.7518
	step [231/244], loss=6.7326
	step [232/244], loss=4.2829
	step [233/244], loss=5.6634
	step [234/244], loss=5.0906
	step [235/244], loss=5.3835
	step [236/244], loss=4.3752
	step [237/244], loss=5.9988
	step [238/244], loss=5.1743
	step [239/244], loss=5.2000
	step [240/244], loss=5.3106
	step [241/244], loss=5.8011
	step [242/244], loss=5.2148
	step [243/244], loss=6.7549
	step [244/244], loss=2.6240
	Evaluating
	loss=0.0151, precision=0.2550, recall=0.9921, f1=0.4058
Training epoch 25
	step [1/244], loss=4.6199
	step [2/244], loss=4.2515
	step [3/244], loss=5.3791
	step [4/244], loss=5.2757
	step [5/244], loss=4.7630
	step [6/244], loss=6.2378
	step [7/244], loss=6.0287
	step [8/244], loss=5.7873
	step [9/244], loss=4.3532
	step [10/244], loss=4.9739
	step [11/244], loss=6.1519
	step [12/244], loss=6.0692
	step [13/244], loss=5.4760
	step [14/244], loss=4.9799
	step [15/244], loss=5.9939
	step [16/244], loss=6.5857
	step [17/244], loss=5.9521
	step [18/244], loss=6.2165
	step [19/244], loss=4.6480
	step [20/244], loss=6.0912
	step [21/244], loss=5.7656
	step [22/244], loss=4.7786
	step [23/244], loss=5.9012
	step [24/244], loss=5.4949
	step [25/244], loss=5.5547
	step [26/244], loss=5.2824
	step [27/244], loss=5.4135
	step [28/244], loss=5.4237
	step [29/244], loss=4.5292
	step [30/244], loss=5.7293
	step [31/244], loss=5.1336
	step [32/244], loss=4.9278
	step [33/244], loss=5.1928
	step [34/244], loss=5.2786
	step [35/244], loss=5.2970
	step [36/244], loss=5.0285
	step [37/244], loss=6.1096
	step [38/244], loss=4.0799
	step [39/244], loss=4.7198
	step [40/244], loss=4.9405
	step [41/244], loss=4.6017
	step [42/244], loss=5.5495
	step [43/244], loss=6.2006
	step [44/244], loss=5.2339
	step [45/244], loss=5.1213
	step [46/244], loss=5.1260
	step [47/244], loss=4.2692
	step [48/244], loss=5.2357
	step [49/244], loss=5.6135
	step [50/244], loss=5.4669
	step [51/244], loss=4.7419
	step [52/244], loss=4.1786
	step [53/244], loss=4.2306
	step [54/244], loss=4.8052
	step [55/244], loss=7.8954
	step [56/244], loss=5.7182
	step [57/244], loss=5.3328
	step [58/244], loss=5.3149
	step [59/244], loss=6.2566
	step [60/244], loss=5.3357
	step [61/244], loss=5.1733
	step [62/244], loss=6.4691
	step [63/244], loss=5.4192
	step [64/244], loss=5.8961
	step [65/244], loss=5.5534
	step [66/244], loss=4.8566
	step [67/244], loss=5.6806
	step [68/244], loss=5.2852
	step [69/244], loss=4.9992
	step [70/244], loss=5.1716
	step [71/244], loss=5.4509
	step [72/244], loss=5.6876
	step [73/244], loss=4.9910
	step [74/244], loss=5.1192
	step [75/244], loss=5.7905
	step [76/244], loss=4.9702
	step [77/244], loss=5.2163
	step [78/244], loss=6.4561
	step [79/244], loss=4.5648
	step [80/244], loss=4.6489
	step [81/244], loss=5.0675
	step [82/244], loss=5.0650
	step [83/244], loss=4.8958
	step [84/244], loss=5.2624
	step [85/244], loss=5.7456
	step [86/244], loss=5.9291
	step [87/244], loss=6.1544
	step [88/244], loss=4.4075
	step [89/244], loss=5.2844
	step [90/244], loss=5.6481
	step [91/244], loss=5.8036
	step [92/244], loss=4.7667
	step [93/244], loss=5.8995
	step [94/244], loss=5.8170
	step [95/244], loss=5.5252
	step [96/244], loss=5.5473
	step [97/244], loss=5.4396
	step [98/244], loss=5.6524
	step [99/244], loss=5.6569
	step [100/244], loss=4.9624
	step [101/244], loss=4.5789
	step [102/244], loss=5.5947
	step [103/244], loss=4.8237
	step [104/244], loss=5.8624
	step [105/244], loss=6.2280
	step [106/244], loss=4.7436
	step [107/244], loss=5.7525
	step [108/244], loss=5.5152
	step [109/244], loss=4.9576
	step [110/244], loss=4.9361
	step [111/244], loss=5.2793
	step [112/244], loss=4.4131
	step [113/244], loss=5.0155
	step [114/244], loss=6.2612
	step [115/244], loss=6.0899
	step [116/244], loss=6.0733
	step [117/244], loss=6.1270
	step [118/244], loss=5.2725
	step [119/244], loss=5.8521
	step [120/244], loss=4.9208
	step [121/244], loss=5.5282
	step [122/244], loss=6.2339
	step [123/244], loss=4.7372
	step [124/244], loss=4.5930
	step [125/244], loss=5.6883
	step [126/244], loss=5.2531
	step [127/244], loss=6.0269
	step [128/244], loss=4.6616
	step [129/244], loss=4.7832
	step [130/244], loss=5.7652
	step [131/244], loss=5.2924
	step [132/244], loss=6.6451
	step [133/244], loss=4.9885
	step [134/244], loss=6.3172
	step [135/244], loss=4.3911
	step [136/244], loss=3.9483
	step [137/244], loss=5.6073
	step [138/244], loss=6.2868
	step [139/244], loss=6.1869
	step [140/244], loss=6.3882
	step [141/244], loss=5.4146
	step [142/244], loss=4.9691
	step [143/244], loss=5.4932
	step [144/244], loss=5.9218
	step [145/244], loss=5.9994
	step [146/244], loss=4.9533
	step [147/244], loss=6.0601
	step [148/244], loss=5.1629
	step [149/244], loss=4.9378
	step [150/244], loss=5.8953
	step [151/244], loss=4.9159
	step [152/244], loss=5.2498
	step [153/244], loss=4.9363
	step [154/244], loss=4.2997
	step [155/244], loss=5.6126
	step [156/244], loss=5.2471
	step [157/244], loss=5.3257
	step [158/244], loss=5.7309
	step [159/244], loss=6.1575
	step [160/244], loss=5.6575
	step [161/244], loss=4.4391
	step [162/244], loss=6.1852
	step [163/244], loss=5.2113
	step [164/244], loss=6.4971
	step [165/244], loss=6.4027
	step [166/244], loss=5.4470
	step [167/244], loss=4.7921
	step [168/244], loss=4.9508
	step [169/244], loss=6.6753
	step [170/244], loss=5.7896
	step [171/244], loss=5.0005
	step [172/244], loss=5.4421
	step [173/244], loss=6.7832
	step [174/244], loss=4.8152
	step [175/244], loss=5.6186
	step [176/244], loss=4.6746
	step [177/244], loss=5.0239
	step [178/244], loss=5.5051
	step [179/244], loss=4.5769
	step [180/244], loss=5.3097
	step [181/244], loss=4.8786
	step [182/244], loss=6.6378
	step [183/244], loss=5.7180
	step [184/244], loss=4.8409
	step [185/244], loss=4.5370
	step [186/244], loss=4.8194
	step [187/244], loss=5.4820
	step [188/244], loss=6.0540
	step [189/244], loss=5.5238
	step [190/244], loss=5.4665
	step [191/244], loss=4.8644
	step [192/244], loss=5.2665
	step [193/244], loss=5.2858
	step [194/244], loss=4.2966
	step [195/244], loss=5.1047
	step [196/244], loss=5.2132
	step [197/244], loss=5.5572
	step [198/244], loss=5.1265
	step [199/244], loss=4.8263
	step [200/244], loss=5.0593
	step [201/244], loss=3.9755
	step [202/244], loss=6.0500
	step [203/244], loss=4.8594
	step [204/244], loss=4.7959
	step [205/244], loss=4.8047
	step [206/244], loss=6.7830
	step [207/244], loss=4.7097
	step [208/244], loss=6.0817
	step [209/244], loss=5.7834
	step [210/244], loss=5.6405
	step [211/244], loss=4.7877
	step [212/244], loss=5.6539
	step [213/244], loss=4.6604
	step [214/244], loss=5.5517
	step [215/244], loss=5.4378
	step [216/244], loss=7.8600
	step [217/244], loss=4.0087
	step [218/244], loss=4.7122
	step [219/244], loss=5.4370
	step [220/244], loss=4.9520
	step [221/244], loss=5.3649
	step [222/244], loss=4.5610
	step [223/244], loss=4.9384
	step [224/244], loss=5.2179
	step [225/244], loss=6.1634
	step [226/244], loss=5.4696
	step [227/244], loss=5.4734
	step [228/244], loss=5.9857
	step [229/244], loss=5.0065
	step [230/244], loss=4.5256
	step [231/244], loss=5.2281
	step [232/244], loss=4.7998
	step [233/244], loss=4.7190
	step [234/244], loss=5.0660
	step [235/244], loss=5.8094
	step [236/244], loss=4.3865
	step [237/244], loss=5.2476
	step [238/244], loss=6.1704
	step [239/244], loss=4.8353
	step [240/244], loss=5.5098
	step [241/244], loss=5.3472
	step [242/244], loss=5.2583
	step [243/244], loss=4.8217
	step [244/244], loss=2.0808
	Evaluating
	loss=0.0145, precision=0.2584, recall=0.9918, f1=0.4100
Training epoch 26
	step [1/244], loss=4.6155
	step [2/244], loss=5.2075
	step [3/244], loss=4.7557
	step [4/244], loss=5.4768
	step [5/244], loss=4.1784
	step [6/244], loss=5.4420
	step [7/244], loss=5.9010
	step [8/244], loss=5.1175
	step [9/244], loss=5.1532
	step [10/244], loss=5.3074
	step [11/244], loss=5.2809
	step [12/244], loss=6.0330
	step [13/244], loss=5.6731
	step [14/244], loss=4.5624
	step [15/244], loss=5.2787
	step [16/244], loss=6.3151
	step [17/244], loss=6.8987
	step [18/244], loss=5.2790
	step [19/244], loss=4.8698
	step [20/244], loss=6.3179
	step [21/244], loss=5.4373
	step [22/244], loss=6.5780
	step [23/244], loss=6.2286
	step [24/244], loss=5.5947
	step [25/244], loss=5.2525
	step [26/244], loss=4.4030
	step [27/244], loss=5.1087
	step [28/244], loss=5.3657
	step [29/244], loss=5.1755
	step [30/244], loss=4.9291
	step [31/244], loss=5.5369
	step [32/244], loss=4.9366
	step [33/244], loss=4.4529
	step [34/244], loss=6.6346
	step [35/244], loss=5.5701
	step [36/244], loss=4.3617
	step [37/244], loss=5.0242
	step [38/244], loss=5.0917
	step [39/244], loss=4.7157
	step [40/244], loss=5.2938
	step [41/244], loss=5.5898
	step [42/244], loss=5.5745
	step [43/244], loss=4.7339
	step [44/244], loss=5.7789
	step [45/244], loss=5.0662
	step [46/244], loss=5.6310
	step [47/244], loss=5.3641
	step [48/244], loss=4.7816
	step [49/244], loss=4.5484
	step [50/244], loss=4.6659
	step [51/244], loss=4.2830
	step [52/244], loss=4.6419
	step [53/244], loss=5.3798
	step [54/244], loss=4.2820
	step [55/244], loss=5.3839
	step [56/244], loss=4.5866
	step [57/244], loss=5.5905
	step [58/244], loss=4.8778
	step [59/244], loss=4.9546
	step [60/244], loss=4.5653
	step [61/244], loss=5.2190
	step [62/244], loss=5.0669
	step [63/244], loss=6.0569
	step [64/244], loss=4.3164
	step [65/244], loss=4.7880
	step [66/244], loss=6.5266
	step [67/244], loss=4.5413
	step [68/244], loss=4.9526
	step [69/244], loss=5.5301
	step [70/244], loss=4.9894
	step [71/244], loss=5.1557
	step [72/244], loss=5.2480
	step [73/244], loss=5.6393
	step [74/244], loss=5.2343
	step [75/244], loss=5.2262
	step [76/244], loss=4.0729
	step [77/244], loss=4.7316
	step [78/244], loss=5.0825
	step [79/244], loss=5.8352
	step [80/244], loss=8.2456
	step [81/244], loss=4.4589
	step [82/244], loss=4.3206
	step [83/244], loss=5.9229
	step [84/244], loss=4.2256
	step [85/244], loss=4.7931
	step [86/244], loss=4.4581
	step [87/244], loss=4.9314
	step [88/244], loss=6.2468
	step [89/244], loss=5.6081
	step [90/244], loss=5.4793
	step [91/244], loss=5.2682
	step [92/244], loss=4.7566
	step [93/244], loss=4.6416
	step [94/244], loss=5.0900
	step [95/244], loss=6.1387
	step [96/244], loss=4.9824
	step [97/244], loss=4.5716
	step [98/244], loss=5.3234
	step [99/244], loss=4.8810
	step [100/244], loss=4.6298
	step [101/244], loss=4.7940
	step [102/244], loss=8.2089
	step [103/244], loss=4.6622
	step [104/244], loss=4.7576
	step [105/244], loss=4.9761
	step [106/244], loss=5.9527
	step [107/244], loss=5.5502
	step [108/244], loss=4.8547
	step [109/244], loss=4.6726
	step [110/244], loss=5.1529
	step [111/244], loss=4.2137
	step [112/244], loss=5.3479
	step [113/244], loss=5.6788
	step [114/244], loss=5.2966
	step [115/244], loss=5.8613
	step [116/244], loss=5.1696
	step [117/244], loss=4.9461
	step [118/244], loss=6.1424
	step [119/244], loss=4.5681
	step [120/244], loss=4.1943
	step [121/244], loss=4.9894
	step [122/244], loss=5.0277
	step [123/244], loss=5.0284
	step [124/244], loss=4.9201
	step [125/244], loss=5.7024
	step [126/244], loss=4.7647
	step [127/244], loss=6.0654
	step [128/244], loss=5.9040
	step [129/244], loss=4.7307
	step [130/244], loss=4.5165
	step [131/244], loss=4.6554
	step [132/244], loss=4.8212
	step [133/244], loss=5.8620
	step [134/244], loss=5.2055
	step [135/244], loss=4.6334
	step [136/244], loss=5.9904
	step [137/244], loss=4.8877
	step [138/244], loss=6.2014
	step [139/244], loss=5.6414
	step [140/244], loss=5.4399
	step [141/244], loss=5.6354
	step [142/244], loss=5.5342
	step [143/244], loss=4.4915
	step [144/244], loss=5.5605
	step [145/244], loss=5.5106
	step [146/244], loss=5.3217
	step [147/244], loss=5.2878
	step [148/244], loss=5.9349
	step [149/244], loss=5.4669
	step [150/244], loss=5.9775
	step [151/244], loss=3.9589
	step [152/244], loss=5.5450
	step [153/244], loss=5.1067
	step [154/244], loss=6.0319
	step [155/244], loss=5.8960
	step [156/244], loss=6.3655
	step [157/244], loss=4.7431
	step [158/244], loss=5.7123
	step [159/244], loss=5.3562
	step [160/244], loss=5.6464
	step [161/244], loss=5.2359
	step [162/244], loss=4.7349
	step [163/244], loss=5.7293
	step [164/244], loss=4.9630
	step [165/244], loss=5.2856
	step [166/244], loss=7.1356
	step [167/244], loss=6.5922
	step [168/244], loss=5.2523
	step [169/244], loss=5.0993
	step [170/244], loss=4.9763
	step [171/244], loss=4.9920
	step [172/244], loss=5.4180
	step [173/244], loss=5.9942
	step [174/244], loss=5.5123
	step [175/244], loss=5.4184
	step [176/244], loss=5.7610
	step [177/244], loss=5.1395
	step [178/244], loss=4.7215
	step [179/244], loss=5.1574
	step [180/244], loss=5.0119
	step [181/244], loss=4.6271
	step [182/244], loss=5.5166
	step [183/244], loss=5.3307
	step [184/244], loss=4.6672
	step [185/244], loss=5.4229
	step [186/244], loss=4.5536
	step [187/244], loss=5.1513
	step [188/244], loss=4.7669
	step [189/244], loss=5.4372
	step [190/244], loss=5.4892
	step [191/244], loss=5.0531
	step [192/244], loss=5.0124
	step [193/244], loss=5.7555
	step [194/244], loss=5.1242
	step [195/244], loss=5.8160
	step [196/244], loss=4.7153
	step [197/244], loss=6.0138
	step [198/244], loss=5.0152
	step [199/244], loss=6.4004
	step [200/244], loss=5.2347
	step [201/244], loss=4.6430
	step [202/244], loss=3.8749
	step [203/244], loss=5.5133
	step [204/244], loss=6.3378
	step [205/244], loss=4.5348
	step [206/244], loss=4.8334
	step [207/244], loss=4.8023
	step [208/244], loss=5.5805
	step [209/244], loss=5.2340
	step [210/244], loss=5.7436
	step [211/244], loss=5.2370
	step [212/244], loss=5.0558
	step [213/244], loss=4.6669
	step [214/244], loss=8.0124
	step [215/244], loss=5.4055
	step [216/244], loss=5.5787
	step [217/244], loss=4.8243
	step [218/244], loss=5.9918
	step [219/244], loss=5.1609
	step [220/244], loss=4.3418
	step [221/244], loss=4.8589
	step [222/244], loss=5.4415
	step [223/244], loss=4.8786
	step [224/244], loss=5.0189
	step [225/244], loss=5.5169
	step [226/244], loss=4.4774
	step [227/244], loss=4.9235
	step [228/244], loss=4.2544
	step [229/244], loss=4.6112
	step [230/244], loss=4.7011
	step [231/244], loss=6.1539
	step [232/244], loss=5.0165
	step [233/244], loss=5.3564
	step [234/244], loss=6.1580
	step [235/244], loss=5.0756
	step [236/244], loss=5.0026
	step [237/244], loss=4.2220
	step [238/244], loss=4.8506
	step [239/244], loss=4.3307
	step [240/244], loss=5.6558
	step [241/244], loss=4.6057
	step [242/244], loss=5.4830
	step [243/244], loss=5.0084
	step [244/244], loss=1.5439
	Evaluating
	loss=0.0151, precision=0.2504, recall=0.9909, f1=0.3998
Training epoch 27
	step [1/244], loss=4.6578
	step [2/244], loss=6.0983
	step [3/244], loss=4.3696
	step [4/244], loss=6.0274
	step [5/244], loss=6.3933
	step [6/244], loss=5.6404
	step [7/244], loss=4.8121
	step [8/244], loss=5.7982
	step [9/244], loss=5.0817
	step [10/244], loss=6.0675
	step [11/244], loss=4.8089
	step [12/244], loss=4.5501
	step [13/244], loss=4.5327
	step [14/244], loss=5.1327
	step [15/244], loss=4.3816
	step [16/244], loss=5.4020
	step [17/244], loss=5.0656
	step [18/244], loss=7.4191
	step [19/244], loss=4.7996
	step [20/244], loss=4.2537
	step [21/244], loss=5.0772
	step [22/244], loss=4.7588
	step [23/244], loss=5.3522
	step [24/244], loss=4.1576
	step [25/244], loss=4.9833
	step [26/244], loss=5.0636
	step [27/244], loss=5.5217
	step [28/244], loss=5.1224
	step [29/244], loss=5.0750
	step [30/244], loss=5.6175
	step [31/244], loss=5.6956
	step [32/244], loss=4.9449
	step [33/244], loss=4.5509
	step [34/244], loss=4.8675
	step [35/244], loss=5.6877
	step [36/244], loss=5.0500
	step [37/244], loss=3.9809
	step [38/244], loss=5.0620
	step [39/244], loss=4.5000
	step [40/244], loss=5.3502
	step [41/244], loss=3.8501
	step [42/244], loss=4.5988
	step [43/244], loss=4.9444
	step [44/244], loss=4.9478
	step [45/244], loss=5.8037
	step [46/244], loss=5.2252
	step [47/244], loss=5.9622
	step [48/244], loss=5.1024
	step [49/244], loss=5.1721
	step [50/244], loss=4.5119
	step [51/244], loss=4.9438
	step [52/244], loss=5.5526
	step [53/244], loss=5.5300
	step [54/244], loss=5.0221
	step [55/244], loss=4.3224
	step [56/244], loss=4.5250
	step [57/244], loss=5.5266
	step [58/244], loss=6.6420
	step [59/244], loss=4.5985
	step [60/244], loss=4.8764
	step [61/244], loss=5.0878
	step [62/244], loss=4.4258
	step [63/244], loss=4.1693
	step [64/244], loss=5.3966
	step [65/244], loss=5.0196
	step [66/244], loss=5.8004
	step [67/244], loss=4.6157
	step [68/244], loss=4.6339
	step [69/244], loss=4.8120
	step [70/244], loss=5.1257
	step [71/244], loss=5.3599
	step [72/244], loss=5.0804
	step [73/244], loss=4.4780
	step [74/244], loss=5.5105
	step [75/244], loss=4.0960
	step [76/244], loss=5.3000
	step [77/244], loss=5.8456
	step [78/244], loss=4.0020
	step [79/244], loss=5.5162
	step [80/244], loss=4.9152
	step [81/244], loss=4.5979
	step [82/244], loss=4.4344
	step [83/244], loss=5.3504
	step [84/244], loss=4.8154
	step [85/244], loss=3.7991
	step [86/244], loss=5.3662
	step [87/244], loss=5.0411
	step [88/244], loss=4.8914
	step [89/244], loss=5.3562
	step [90/244], loss=5.2689
	step [91/244], loss=4.6874
	step [92/244], loss=5.4002
	step [93/244], loss=6.3982
	step [94/244], loss=4.7471
	step [95/244], loss=3.9377
	step [96/244], loss=4.7536
	step [97/244], loss=5.1910
	step [98/244], loss=4.6962
	step [99/244], loss=5.4044
	step [100/244], loss=5.3323
	step [101/244], loss=4.9177
	step [102/244], loss=5.1729
	step [103/244], loss=4.8467
	step [104/244], loss=5.8074
	step [105/244], loss=6.1680
	step [106/244], loss=4.5136
	step [107/244], loss=4.5524
	step [108/244], loss=4.6707
	step [109/244], loss=5.1463
	step [110/244], loss=4.5412
	step [111/244], loss=6.0481
	step [112/244], loss=5.7473
	step [113/244], loss=5.2156
	step [114/244], loss=4.4861
	step [115/244], loss=5.4886
	step [116/244], loss=4.6754
	step [117/244], loss=4.4263
	step [118/244], loss=4.3342
	step [119/244], loss=4.5401
	step [120/244], loss=3.9135
	step [121/244], loss=5.7654
	step [122/244], loss=6.4851
	step [123/244], loss=5.2736
	step [124/244], loss=5.3506
	step [125/244], loss=5.7978
	step [126/244], loss=5.3146
	step [127/244], loss=4.3940
	step [128/244], loss=5.1596
	step [129/244], loss=4.4363
	step [130/244], loss=4.3449
	step [131/244], loss=4.5082
	step [132/244], loss=5.1181
	step [133/244], loss=6.1029
	step [134/244], loss=4.8407
	step [135/244], loss=4.9339
	step [136/244], loss=4.9112
	step [137/244], loss=4.7031
	step [138/244], loss=4.6140
	step [139/244], loss=5.9526
	step [140/244], loss=6.2729
	step [141/244], loss=4.8598
	step [142/244], loss=5.5565
	step [143/244], loss=4.4510
	step [144/244], loss=4.9084
	step [145/244], loss=4.7426
	step [146/244], loss=4.4098
	step [147/244], loss=4.4906
	step [148/244], loss=4.0149
	step [149/244], loss=6.2433
	step [150/244], loss=4.4730
	step [151/244], loss=4.4317
	step [152/244], loss=4.1172
	step [153/244], loss=4.2509
	step [154/244], loss=4.5458
	step [155/244], loss=5.9581
	step [156/244], loss=4.4760
	step [157/244], loss=4.6294
	step [158/244], loss=5.0653
	step [159/244], loss=4.8953
	step [160/244], loss=5.6834
	step [161/244], loss=4.5537
	step [162/244], loss=4.8263
	step [163/244], loss=5.4431
	step [164/244], loss=5.6414
	step [165/244], loss=4.4502
	step [166/244], loss=5.2214
	step [167/244], loss=4.7083
	step [168/244], loss=5.3299
	step [169/244], loss=5.4335
	step [170/244], loss=6.8229
	step [171/244], loss=5.1669
	step [172/244], loss=4.6871
	step [173/244], loss=4.5589
	step [174/244], loss=4.5474
	step [175/244], loss=5.5548
	step [176/244], loss=5.2009
	step [177/244], loss=6.2960
	step [178/244], loss=4.2908
	step [179/244], loss=6.0563
	step [180/244], loss=6.2462
	step [181/244], loss=4.9043
	step [182/244], loss=6.1712
	step [183/244], loss=5.8392
	step [184/244], loss=5.8002
	step [185/244], loss=4.8680
	step [186/244], loss=5.4849
	step [187/244], loss=4.4301
	step [188/244], loss=5.1583
	step [189/244], loss=5.5114
	step [190/244], loss=4.5262
	step [191/244], loss=5.1331
	step [192/244], loss=5.1779
	step [193/244], loss=5.8438
	step [194/244], loss=5.0083
	step [195/244], loss=5.2114
	step [196/244], loss=5.2611
	step [197/244], loss=5.4461
	step [198/244], loss=4.3434
	step [199/244], loss=5.7791
	step [200/244], loss=4.5474
	step [201/244], loss=5.1210
	step [202/244], loss=5.4890
	step [203/244], loss=5.1254
	step [204/244], loss=5.0046
	step [205/244], loss=5.7695
	step [206/244], loss=4.6265
	step [207/244], loss=6.1349
	step [208/244], loss=5.8562
	step [209/244], loss=4.4407
	step [210/244], loss=5.8888
	step [211/244], loss=5.4078
	step [212/244], loss=5.1093
	step [213/244], loss=6.5160
	step [214/244], loss=4.1625
	step [215/244], loss=6.2179
	step [216/244], loss=4.7156
	step [217/244], loss=5.7624
	step [218/244], loss=5.9424
	step [219/244], loss=5.0886
	step [220/244], loss=5.9549
	step [221/244], loss=4.8778
	step [222/244], loss=5.7275
	step [223/244], loss=6.2370
	step [224/244], loss=5.0283
	step [225/244], loss=5.5387
	step [226/244], loss=5.1134
	step [227/244], loss=4.7740
	step [228/244], loss=4.5434
	step [229/244], loss=4.3170
	step [230/244], loss=6.2068
	step [231/244], loss=4.6138
	step [232/244], loss=4.3850
	step [233/244], loss=5.3324
	step [234/244], loss=5.1451
	step [235/244], loss=4.6631
	step [236/244], loss=4.4858
	step [237/244], loss=4.6587
	step [238/244], loss=5.2653
	step [239/244], loss=5.7310
	step [240/244], loss=5.9646
	step [241/244], loss=4.5157
	step [242/244], loss=4.7472
	step [243/244], loss=4.7290
	step [244/244], loss=2.0748
	Evaluating
	loss=0.0137, precision=0.2770, recall=0.9904, f1=0.4329
saving model as: 1_saved_model.pth
Training epoch 28
	step [1/244], loss=4.0660
	step [2/244], loss=4.8502
	step [3/244], loss=4.7581
	step [4/244], loss=4.4077
	step [5/244], loss=4.1976
	step [6/244], loss=5.6355
	step [7/244], loss=4.9575
	step [8/244], loss=5.0218
	step [9/244], loss=4.8307
	step [10/244], loss=4.8331
	step [11/244], loss=4.4492
	step [12/244], loss=5.5215
	step [13/244], loss=3.9546
	step [14/244], loss=5.5909
	step [15/244], loss=4.4468
	step [16/244], loss=6.2476
	step [17/244], loss=6.3358
	step [18/244], loss=4.2645
	step [19/244], loss=4.7120
	step [20/244], loss=4.9744
	step [21/244], loss=4.6795
	step [22/244], loss=4.8613
	step [23/244], loss=4.8245
	step [24/244], loss=4.6077
	step [25/244], loss=4.4730
	step [26/244], loss=4.2986
	step [27/244], loss=4.9525
	step [28/244], loss=4.9727
	step [29/244], loss=4.0461
	step [30/244], loss=3.8270
	step [31/244], loss=5.7193
	step [32/244], loss=4.8261
	step [33/244], loss=4.8337
	step [34/244], loss=4.7329
	step [35/244], loss=5.7033
	step [36/244], loss=4.5524
	step [37/244], loss=4.1202
	step [38/244], loss=5.2934
	step [39/244], loss=5.3570
	step [40/244], loss=4.7942
	step [41/244], loss=4.9909
	step [42/244], loss=5.7995
	step [43/244], loss=4.3492
	step [44/244], loss=5.9161
	step [45/244], loss=4.7405
	step [46/244], loss=5.0991
	step [47/244], loss=4.4040
	step [48/244], loss=5.7617
	step [49/244], loss=5.6551
	step [50/244], loss=3.8053
	step [51/244], loss=5.0259
	step [52/244], loss=5.5535
	step [53/244], loss=5.9307
	step [54/244], loss=4.7785
	step [55/244], loss=4.6760
	step [56/244], loss=4.8525
	step [57/244], loss=5.8068
	step [58/244], loss=4.5917
	step [59/244], loss=5.3098
	step [60/244], loss=4.6412
	step [61/244], loss=5.4160
	step [62/244], loss=4.1165
	step [63/244], loss=5.8868
	step [64/244], loss=4.7354
	step [65/244], loss=5.0918
	step [66/244], loss=5.2050
	step [67/244], loss=5.0804
	step [68/244], loss=4.7124
	step [69/244], loss=5.3233
	step [70/244], loss=5.0940
	step [71/244], loss=5.2658
	step [72/244], loss=4.3407
	step [73/244], loss=5.9310
	step [74/244], loss=5.0197
	step [75/244], loss=4.7923
	step [76/244], loss=5.6124
	step [77/244], loss=4.7157
	step [78/244], loss=5.6562
	step [79/244], loss=5.1513
	step [80/244], loss=3.9755
	step [81/244], loss=5.3645
	step [82/244], loss=4.5813
	step [83/244], loss=4.9457
	step [84/244], loss=5.3962
	step [85/244], loss=5.3027
	step [86/244], loss=4.5574
	step [87/244], loss=4.5112
	step [88/244], loss=4.7048
	step [89/244], loss=4.0196
	step [90/244], loss=5.0600
	step [91/244], loss=4.9604
	step [92/244], loss=4.7252
	step [93/244], loss=5.4519
	step [94/244], loss=5.3174
	step [95/244], loss=5.1904
	step [96/244], loss=5.2555
	step [97/244], loss=4.0283
	step [98/244], loss=4.2736
	step [99/244], loss=4.9981
	step [100/244], loss=5.7351
	step [101/244], loss=5.2135
	step [102/244], loss=5.5428
	step [103/244], loss=5.0913
	step [104/244], loss=5.2859
	step [105/244], loss=5.6964
	step [106/244], loss=5.6194
	step [107/244], loss=3.6713
	step [108/244], loss=6.2091
	step [109/244], loss=5.1688
	step [110/244], loss=4.2870
	step [111/244], loss=3.9255
	step [112/244], loss=4.1920
	step [113/244], loss=4.4171
	step [114/244], loss=5.3899
	step [115/244], loss=4.8649
	step [116/244], loss=3.8738
	step [117/244], loss=4.9112
	step [118/244], loss=4.4396
	step [119/244], loss=4.5862
	step [120/244], loss=4.7519
	step [121/244], loss=5.1822
	step [122/244], loss=5.1355
	step [123/244], loss=3.9797
	step [124/244], loss=3.5479
	step [125/244], loss=5.1441
	step [126/244], loss=5.6435
	step [127/244], loss=4.9544
	step [128/244], loss=5.1631
	step [129/244], loss=4.8390
	step [130/244], loss=4.9958
	step [131/244], loss=5.8303
	step [132/244], loss=4.8037
	step [133/244], loss=5.8726
	step [134/244], loss=5.1791
	step [135/244], loss=4.2368
	step [136/244], loss=6.3164
	step [137/244], loss=5.1839
	step [138/244], loss=6.6506
	step [139/244], loss=3.5471
	step [140/244], loss=5.6433
	step [141/244], loss=4.1420
	step [142/244], loss=5.1678
	step [143/244], loss=4.5116
	step [144/244], loss=4.0665
	step [145/244], loss=4.5968
	step [146/244], loss=5.7591
	step [147/244], loss=4.4945
	step [148/244], loss=5.1113
	step [149/244], loss=5.3945
	step [150/244], loss=4.9926
	step [151/244], loss=4.3503
	step [152/244], loss=5.0879
	step [153/244], loss=4.6340
	step [154/244], loss=5.9648
	step [155/244], loss=5.1853
	step [156/244], loss=5.4449
	step [157/244], loss=3.9683
	step [158/244], loss=4.6231
	step [159/244], loss=4.9703
	step [160/244], loss=4.5148
	step [161/244], loss=5.9232
	step [162/244], loss=5.1301
	step [163/244], loss=5.3985
	step [164/244], loss=3.9368
	step [165/244], loss=6.4260
	step [166/244], loss=5.4675
	step [167/244], loss=4.5610
	step [168/244], loss=6.1981
	step [169/244], loss=5.0711
	step [170/244], loss=4.8891
	step [171/244], loss=5.7905
	step [172/244], loss=5.3450
	step [173/244], loss=4.4994
	step [174/244], loss=4.6303
	step [175/244], loss=5.3691
	step [176/244], loss=5.7075
	step [177/244], loss=5.0460
	step [178/244], loss=4.6357
	step [179/244], loss=4.2647
	step [180/244], loss=3.9392
	step [181/244], loss=4.5614
	step [182/244], loss=4.9123
	step [183/244], loss=4.4788
	step [184/244], loss=4.2123
	step [185/244], loss=4.5663
	step [186/244], loss=5.3375
	step [187/244], loss=5.4482
	step [188/244], loss=4.4345
	step [189/244], loss=5.1941
	step [190/244], loss=6.1911
	step [191/244], loss=4.6277
	step [192/244], loss=4.6661
	step [193/244], loss=4.8820
	step [194/244], loss=4.7051
	step [195/244], loss=5.8012
	step [196/244], loss=5.1246
	step [197/244], loss=4.4242
	step [198/244], loss=4.8236
	step [199/244], loss=5.1732
	step [200/244], loss=5.2992
	step [201/244], loss=6.5665
	step [202/244], loss=4.4887
	step [203/244], loss=4.9908
	step [204/244], loss=4.7424
	step [205/244], loss=5.1014
	step [206/244], loss=6.3190
	step [207/244], loss=6.2056
	step [208/244], loss=5.4436
	step [209/244], loss=4.9555
	step [210/244], loss=5.6722
	step [211/244], loss=4.6394
	step [212/244], loss=4.9404
	step [213/244], loss=5.1986
	step [214/244], loss=5.0949
	step [215/244], loss=5.5825
	step [216/244], loss=5.7243
	step [217/244], loss=5.2775
	step [218/244], loss=5.2667
	step [219/244], loss=4.5291
	step [220/244], loss=4.9222
	step [221/244], loss=4.6520
	step [222/244], loss=4.7391
	step [223/244], loss=5.2946
	step [224/244], loss=4.9052
	step [225/244], loss=4.2967
	step [226/244], loss=4.4627
	step [227/244], loss=4.4835
	step [228/244], loss=4.2910
	step [229/244], loss=5.3030
	step [230/244], loss=3.9798
	step [231/244], loss=5.1999
	step [232/244], loss=4.8406
	step [233/244], loss=4.7023
	step [234/244], loss=4.1744
	step [235/244], loss=5.6928
	step [236/244], loss=5.1393
	step [237/244], loss=4.7335
	step [238/244], loss=5.1599
	step [239/244], loss=4.7238
	step [240/244], loss=5.0892
	step [241/244], loss=5.7749
	step [242/244], loss=5.3419
	step [243/244], loss=4.4303
	step [244/244], loss=2.9046
	Evaluating
	loss=0.0155, precision=0.2569, recall=0.9905, f1=0.4079
Training epoch 29
	step [1/244], loss=4.4368
	step [2/244], loss=4.9866
	step [3/244], loss=5.4590
	step [4/244], loss=4.4598
	step [5/244], loss=4.6133
	step [6/244], loss=5.2328
	step [7/244], loss=4.9520
	step [8/244], loss=4.8571
	step [9/244], loss=4.3712
	step [10/244], loss=5.6319
	step [11/244], loss=4.3570
	step [12/244], loss=5.2773
	step [13/244], loss=4.7138
	step [14/244], loss=5.0457
	step [15/244], loss=4.3846
	step [16/244], loss=4.8795
	step [17/244], loss=4.7760
	step [18/244], loss=5.0062
	step [19/244], loss=4.4520
	step [20/244], loss=5.8296
	step [21/244], loss=4.6329
	step [22/244], loss=4.0556
	step [23/244], loss=5.4465
	step [24/244], loss=4.9339
	step [25/244], loss=5.9284
	step [26/244], loss=5.2857
	step [27/244], loss=4.4068
	step [28/244], loss=4.8861
	step [29/244], loss=4.4911
	step [30/244], loss=5.0163
	step [31/244], loss=4.8890
	step [32/244], loss=5.1206
	step [33/244], loss=5.3944
	step [34/244], loss=5.2851
	step [35/244], loss=4.6837
	step [36/244], loss=5.7667
	step [37/244], loss=5.1357
	step [38/244], loss=4.7188
	step [39/244], loss=5.3105
	step [40/244], loss=4.8656
	step [41/244], loss=5.7776
	step [42/244], loss=4.6213
	step [43/244], loss=4.9848
	step [44/244], loss=5.5344
	step [45/244], loss=4.8160
	step [46/244], loss=5.1178
	step [47/244], loss=4.5091
	step [48/244], loss=5.5005
	step [49/244], loss=4.9166
	step [50/244], loss=4.4719
	step [51/244], loss=4.8934
	step [52/244], loss=4.5646
	step [53/244], loss=4.8238
	step [54/244], loss=5.2739
	step [55/244], loss=5.0995
	step [56/244], loss=4.3706
	step [57/244], loss=5.3632
	step [58/244], loss=4.9595
	step [59/244], loss=5.7253
	step [60/244], loss=4.4718
	step [61/244], loss=4.3319
	step [62/244], loss=4.9837
	step [63/244], loss=5.7840
	step [64/244], loss=5.7741
	step [65/244], loss=4.3226
	step [66/244], loss=5.7073
	step [67/244], loss=4.5270
	step [68/244], loss=4.2839
	step [69/244], loss=4.5922
	step [70/244], loss=5.1840
	step [71/244], loss=4.4610
	step [72/244], loss=5.3819
	step [73/244], loss=5.2726
	step [74/244], loss=4.6468
	step [75/244], loss=5.1593
	step [76/244], loss=5.3721
	step [77/244], loss=4.7247
	step [78/244], loss=5.8560
	step [79/244], loss=4.7674
	step [80/244], loss=4.1869
	step [81/244], loss=4.1136
	step [82/244], loss=4.2816
	step [83/244], loss=4.5188
	step [84/244], loss=4.5109
	step [85/244], loss=4.2231
	step [86/244], loss=6.2336
	step [87/244], loss=6.1971
	step [88/244], loss=4.2119
	step [89/244], loss=3.8837
	step [90/244], loss=5.1514
	step [91/244], loss=4.3439
	step [92/244], loss=4.5827
	step [93/244], loss=5.3875
	step [94/244], loss=5.2724
	step [95/244], loss=4.4711
	step [96/244], loss=5.8886
	step [97/244], loss=5.0610
	step [98/244], loss=4.9491
	step [99/244], loss=5.6159
	step [100/244], loss=4.5326
	step [101/244], loss=4.0236
	step [102/244], loss=4.3742
	step [103/244], loss=4.0885
	step [104/244], loss=4.6354
	step [105/244], loss=5.0702
	step [106/244], loss=3.5050
	step [107/244], loss=4.8424
	step [108/244], loss=5.1657
	step [109/244], loss=5.2398
	step [110/244], loss=4.6943
	step [111/244], loss=4.5783
	step [112/244], loss=4.8838
	step [113/244], loss=5.3271
	step [114/244], loss=4.2954
	step [115/244], loss=4.9361
	step [116/244], loss=4.5375
	step [117/244], loss=5.2468
	step [118/244], loss=5.2270
	step [119/244], loss=4.1354
	step [120/244], loss=4.4541
	step [121/244], loss=3.8983
	step [122/244], loss=5.2280
	step [123/244], loss=5.4549
	step [124/244], loss=4.5188
	step [125/244], loss=4.4413
	step [126/244], loss=4.7323
	step [127/244], loss=5.4577
	step [128/244], loss=3.6378
	step [129/244], loss=4.2827
	step [130/244], loss=5.0715
	step [131/244], loss=5.2084
	step [132/244], loss=3.8190
	step [133/244], loss=4.7560
	step [134/244], loss=4.3319
	step [135/244], loss=5.0288
	step [136/244], loss=5.0670
	step [137/244], loss=5.5791
	step [138/244], loss=4.8164
	step [139/244], loss=5.4554
	step [140/244], loss=4.0317
	step [141/244], loss=5.4605
	step [142/244], loss=4.1814
	step [143/244], loss=4.8816
	step [144/244], loss=4.1675
	step [145/244], loss=4.6926
	step [146/244], loss=4.6933
	step [147/244], loss=4.1897
	step [148/244], loss=4.4033
	step [149/244], loss=4.9425
	step [150/244], loss=4.2203
	step [151/244], loss=5.2653
	step [152/244], loss=4.6351
	step [153/244], loss=5.1331
	step [154/244], loss=4.7768
	step [155/244], loss=4.6551
	step [156/244], loss=4.8405
	step [157/244], loss=4.6628
	step [158/244], loss=5.1020
	step [159/244], loss=3.9103
	step [160/244], loss=3.9098
	step [161/244], loss=5.3609
	step [162/244], loss=4.9819
	step [163/244], loss=5.2307
	step [164/244], loss=4.7358
	step [165/244], loss=4.5453
	step [166/244], loss=3.9467
	step [167/244], loss=4.3192
	step [168/244], loss=4.2355
	step [169/244], loss=5.1727
	step [170/244], loss=4.6547
	step [171/244], loss=5.5109
	step [172/244], loss=4.1817
	step [173/244], loss=5.8053
	step [174/244], loss=5.3121
	step [175/244], loss=5.1437
	step [176/244], loss=5.1392
	step [177/244], loss=4.2337
	step [178/244], loss=4.1540
	step [179/244], loss=4.5656
	step [180/244], loss=5.1116
	step [181/244], loss=4.2571
	step [182/244], loss=5.0445
	step [183/244], loss=4.6262
	step [184/244], loss=5.2806
	step [185/244], loss=5.1221
	step [186/244], loss=5.3130
	step [187/244], loss=5.1838
	step [188/244], loss=5.0683
	step [189/244], loss=5.8797
	step [190/244], loss=5.7944
	step [191/244], loss=4.4763
	step [192/244], loss=4.8033
	step [193/244], loss=3.8914
	step [194/244], loss=4.0342
	step [195/244], loss=4.4112
	step [196/244], loss=4.5334
	step [197/244], loss=3.7828
	step [198/244], loss=4.2041
	step [199/244], loss=4.7584
	step [200/244], loss=4.3494
	step [201/244], loss=5.6928
	step [202/244], loss=5.7929
	step [203/244], loss=5.8678
	step [204/244], loss=4.9856
	step [205/244], loss=4.7142
	step [206/244], loss=5.2306
	step [207/244], loss=4.8713
	step [208/244], loss=4.3717
	step [209/244], loss=5.1763
	step [210/244], loss=4.9758
	step [211/244], loss=4.2395
	step [212/244], loss=5.0248
	step [213/244], loss=4.9951
	step [214/244], loss=6.1314
	step [215/244], loss=5.1722
	step [216/244], loss=4.7470
	step [217/244], loss=4.2490
	step [218/244], loss=4.6860
	step [219/244], loss=3.9625
	step [220/244], loss=4.2858
	step [221/244], loss=5.6408
	step [222/244], loss=6.0089
	step [223/244], loss=4.6799
	step [224/244], loss=5.3957
	step [225/244], loss=5.2703
	step [226/244], loss=5.0767
	step [227/244], loss=5.5039
	step [228/244], loss=5.0056
	step [229/244], loss=4.3620
	step [230/244], loss=4.1745
	step [231/244], loss=4.4743
	step [232/244], loss=3.9485
	step [233/244], loss=4.9348
	step [234/244], loss=5.4873
	step [235/244], loss=5.8852
	step [236/244], loss=4.9331
	step [237/244], loss=4.5286
	step [238/244], loss=4.6302
	step [239/244], loss=5.7940
	step [240/244], loss=5.9553
	step [241/244], loss=4.7955
	step [242/244], loss=4.6445
	step [243/244], loss=5.5186
	step [244/244], loss=1.6065
	Evaluating
	loss=0.0130, precision=0.2821, recall=0.9893, f1=0.4390
saving model as: 1_saved_model.pth
Training epoch 30
	step [1/244], loss=4.5694
	step [2/244], loss=4.9834
	step [3/244], loss=4.3769
	step [4/244], loss=4.8009
	step [5/244], loss=4.8919
	step [6/244], loss=4.5855
	step [7/244], loss=4.4251
	step [8/244], loss=3.6893
	step [9/244], loss=3.5725
	step [10/244], loss=4.0097
	step [11/244], loss=4.0332
	step [12/244], loss=4.0653
	step [13/244], loss=4.3632
	step [14/244], loss=4.1491
	step [15/244], loss=4.7969
	step [16/244], loss=4.3341
	step [17/244], loss=4.5020
	step [18/244], loss=4.9818
	step [19/244], loss=3.8721
	step [20/244], loss=4.2710
	step [21/244], loss=4.9521
	step [22/244], loss=4.5211
	step [23/244], loss=4.7344
	step [24/244], loss=4.9098
	step [25/244], loss=4.0352
	step [26/244], loss=4.0990
	step [27/244], loss=4.2442
	step [28/244], loss=3.4206
	step [29/244], loss=4.3557
	step [30/244], loss=5.3948
	step [31/244], loss=3.6327
	step [32/244], loss=4.2351
	step [33/244], loss=4.8633
	step [34/244], loss=4.1421
	step [35/244], loss=4.9723
	step [36/244], loss=4.7270
	step [37/244], loss=5.4053
	step [38/244], loss=5.3438
	step [39/244], loss=4.7846
	step [40/244], loss=5.3278
	step [41/244], loss=5.3411
	step [42/244], loss=4.1588
	step [43/244], loss=4.1749
	step [44/244], loss=5.2983
	step [45/244], loss=4.0413
	step [46/244], loss=4.2681
	step [47/244], loss=4.7326
	step [48/244], loss=4.8705
	step [49/244], loss=4.0040
	step [50/244], loss=4.9370
	step [51/244], loss=4.4628
	step [52/244], loss=4.0612
	step [53/244], loss=4.8925
	step [54/244], loss=4.6008
	step [55/244], loss=4.0394
	step [56/244], loss=4.9319
	step [57/244], loss=5.3176
	step [58/244], loss=4.7499
	step [59/244], loss=5.1281
	step [60/244], loss=4.3721
	step [61/244], loss=4.8378
	step [62/244], loss=5.0778
	step [63/244], loss=4.8751
	step [64/244], loss=4.4740
	step [65/244], loss=4.3359
	step [66/244], loss=4.4053
	step [67/244], loss=3.8651
	step [68/244], loss=5.1987
	step [69/244], loss=6.1673
	step [70/244], loss=4.9748
	step [71/244], loss=4.3724
	step [72/244], loss=4.4314
	step [73/244], loss=4.1241
	step [74/244], loss=5.2899
	step [75/244], loss=5.8970
	step [76/244], loss=5.5622
	step [77/244], loss=4.8129
	step [78/244], loss=5.4141
	step [79/244], loss=4.4575
	step [80/244], loss=5.0668
	step [81/244], loss=4.5646
	step [82/244], loss=4.7997
	step [83/244], loss=4.9425
	step [84/244], loss=4.3485
	step [85/244], loss=5.9112
	step [86/244], loss=4.5173
	step [87/244], loss=4.9668
	step [88/244], loss=4.5318
	step [89/244], loss=5.7079
	step [90/244], loss=4.9544
	step [91/244], loss=6.1275
	step [92/244], loss=5.5571
	step [93/244], loss=3.7870
	step [94/244], loss=3.8968
	step [95/244], loss=4.7274
	step [96/244], loss=5.4952
	step [97/244], loss=5.4742
	step [98/244], loss=5.1434
	step [99/244], loss=4.7880
	step [100/244], loss=5.2500
	step [101/244], loss=5.3508
	step [102/244], loss=4.3777
	step [103/244], loss=6.1052
	step [104/244], loss=4.9142
	step [105/244], loss=4.3634
	step [106/244], loss=5.6211
	step [107/244], loss=4.6437
	step [108/244], loss=5.1329
	step [109/244], loss=4.8770
	step [110/244], loss=4.7063
	step [111/244], loss=5.7171
	step [112/244], loss=5.3252
	step [113/244], loss=4.1890
	step [114/244], loss=6.1789
	step [115/244], loss=4.6506
	step [116/244], loss=4.4988
	step [117/244], loss=4.8572
	step [118/244], loss=4.7034
	step [119/244], loss=4.7492
	step [120/244], loss=4.7307
	step [121/244], loss=5.2146
	step [122/244], loss=4.5146
	step [123/244], loss=4.7628
	step [124/244], loss=3.5774
	step [125/244], loss=4.8833
	step [126/244], loss=4.9185
	step [127/244], loss=5.0889
	step [128/244], loss=4.1015
	step [129/244], loss=4.7135
	step [130/244], loss=4.1727
	step [131/244], loss=4.3602
	step [132/244], loss=4.1368
	step [133/244], loss=5.0308
	step [134/244], loss=4.4710
	step [135/244], loss=4.4113
	step [136/244], loss=4.1862
	step [137/244], loss=5.3039
	step [138/244], loss=5.1515
	step [139/244], loss=4.7008
	step [140/244], loss=5.2930
	step [141/244], loss=4.6672
	step [142/244], loss=5.2030
	step [143/244], loss=4.7000
	step [144/244], loss=5.2810
	step [145/244], loss=3.5915
	step [146/244], loss=5.7697
	step [147/244], loss=4.9810
	step [148/244], loss=4.8677
	step [149/244], loss=4.7387
	step [150/244], loss=4.8798
	step [151/244], loss=4.9772
	step [152/244], loss=4.9364
	step [153/244], loss=4.5596
	step [154/244], loss=5.5361
	step [155/244], loss=4.4123
	step [156/244], loss=4.3165
	step [157/244], loss=4.5218
	step [158/244], loss=5.0262
	step [159/244], loss=5.3672
	step [160/244], loss=5.4309
	step [161/244], loss=4.3736
	step [162/244], loss=5.7457
	step [163/244], loss=4.9982
	step [164/244], loss=4.1376
	step [165/244], loss=5.8863
	step [166/244], loss=5.0061
	step [167/244], loss=4.5888
	step [168/244], loss=4.5827
	step [169/244], loss=4.7924
	step [170/244], loss=4.1210
	step [171/244], loss=4.4819
	step [172/244], loss=4.3656
	step [173/244], loss=3.7903
	step [174/244], loss=4.9095
	step [175/244], loss=3.6690
	step [176/244], loss=5.4821
	step [177/244], loss=4.7136
	step [178/244], loss=5.0498
	step [179/244], loss=4.4691
	step [180/244], loss=6.3457
	step [181/244], loss=3.8678
	step [182/244], loss=4.8391
	step [183/244], loss=5.8118
	step [184/244], loss=5.7371
	step [185/244], loss=4.4397
	step [186/244], loss=4.8009
	step [187/244], loss=5.0991
	step [188/244], loss=5.1794
	step [189/244], loss=3.8843
	step [190/244], loss=4.6889
	step [191/244], loss=4.2509
	step [192/244], loss=4.2820
	step [193/244], loss=5.1792
	step [194/244], loss=4.7195
	step [195/244], loss=5.1657
	step [196/244], loss=5.1953
	step [197/244], loss=4.2189
	step [198/244], loss=5.1271
	step [199/244], loss=4.8130
	step [200/244], loss=4.4038
	step [201/244], loss=4.6492
	step [202/244], loss=5.4917
	step [203/244], loss=4.2062
	step [204/244], loss=5.3011
	step [205/244], loss=4.4053
	step [206/244], loss=4.7635
	step [207/244], loss=5.1908
	step [208/244], loss=5.2114
	step [209/244], loss=4.8693
	step [210/244], loss=3.6145
	step [211/244], loss=4.8289
	step [212/244], loss=5.4551
	step [213/244], loss=4.7377
	step [214/244], loss=5.6559
	step [215/244], loss=4.9081
	step [216/244], loss=5.0011
	step [217/244], loss=4.8239
	step [218/244], loss=4.0001
	step [219/244], loss=4.7850
	step [220/244], loss=4.6099
	step [221/244], loss=4.4493
	step [222/244], loss=4.9715
	step [223/244], loss=4.4848
	step [224/244], loss=4.8953
	step [225/244], loss=4.7191
	step [226/244], loss=5.3517
	step [227/244], loss=5.1216
	step [228/244], loss=4.8754
	step [229/244], loss=5.5592
	step [230/244], loss=4.5449
	step [231/244], loss=5.4444
	step [232/244], loss=4.1955
	step [233/244], loss=4.3341
	step [234/244], loss=5.2531
	step [235/244], loss=4.5733
	step [236/244], loss=5.7015
	step [237/244], loss=5.9762
	step [238/244], loss=3.9404
	step [239/244], loss=5.2219
	step [240/244], loss=4.4523
	step [241/244], loss=5.5602
	step [242/244], loss=5.3573
	step [243/244], loss=5.1454
	step [244/244], loss=2.0384
	Evaluating
	loss=0.0113, precision=0.3258, recall=0.9868, f1=0.4899
saving model as: 1_saved_model.pth
Training finished
best_f1: 0.48988483418569506
directing: Z rim_enhanced: False test_id 1
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12179 # image files with weight 12153
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3340 # image files with weight 3331
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Z 12153
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/190], loss=321.0726
	step [2/190], loss=175.9822
	step [3/190], loss=162.8999
	step [4/190], loss=151.2923
	step [5/190], loss=150.9838
	step [6/190], loss=148.6174
	step [7/190], loss=147.1568
	step [8/190], loss=147.2843
	step [9/190], loss=142.8863
	step [10/190], loss=144.0948
	step [11/190], loss=142.3192
	step [12/190], loss=141.8322
	step [13/190], loss=140.5538
	step [14/190], loss=137.6023
	step [15/190], loss=136.2638
	step [16/190], loss=134.5377
	step [17/190], loss=135.5068
	step [18/190], loss=131.7243
	step [19/190], loss=130.0433
	step [20/190], loss=127.6935
	step [21/190], loss=128.6648
	step [22/190], loss=126.8169
	step [23/190], loss=125.2282
	step [24/190], loss=123.6033
	step [25/190], loss=123.6989
	step [26/190], loss=121.8118
	step [27/190], loss=119.4818
	step [28/190], loss=119.4738
	step [29/190], loss=117.0189
	step [30/190], loss=116.9493
	step [31/190], loss=116.3986
	step [32/190], loss=113.6144
	step [33/190], loss=112.8613
	step [34/190], loss=111.6197
	step [35/190], loss=111.3730
	step [36/190], loss=109.5304
	step [37/190], loss=107.0327
	step [38/190], loss=108.2955
	step [39/190], loss=105.3532
	step [40/190], loss=107.2607
	step [41/190], loss=105.7209
	step [42/190], loss=106.8907
	step [43/190], loss=102.5645
	step [44/190], loss=103.5225
	step [45/190], loss=101.8474
	step [46/190], loss=101.2435
	step [47/190], loss=102.0538
	step [48/190], loss=100.3938
	step [49/190], loss=99.0716
	step [50/190], loss=97.2005
	step [51/190], loss=95.8446
	step [52/190], loss=97.5703
	step [53/190], loss=97.8283
	step [54/190], loss=95.4047
	step [55/190], loss=94.8845
	step [56/190], loss=94.3208
	step [57/190], loss=96.2997
	step [58/190], loss=95.1849
	step [59/190], loss=90.9322
	step [60/190], loss=90.7339
	step [61/190], loss=92.8914
	step [62/190], loss=90.5532
	step [63/190], loss=90.0618
	step [64/190], loss=89.8174
	step [65/190], loss=89.2555
	step [66/190], loss=88.0558
	step [67/190], loss=87.6892
	step [68/190], loss=88.3899
	step [69/190], loss=87.0364
	step [70/190], loss=85.8302
	step [71/190], loss=87.6458
	step [72/190], loss=86.1184
	step [73/190], loss=86.0008
	step [74/190], loss=84.5467
	step [75/190], loss=84.9467
	step [76/190], loss=85.2035
	step [77/190], loss=83.2991
	step [78/190], loss=83.2024
	step [79/190], loss=81.8932
	step [80/190], loss=82.4860
	step [81/190], loss=80.3628
	step [82/190], loss=81.0684
	step [83/190], loss=78.0876
	step [84/190], loss=78.9319
	step [85/190], loss=79.0544
	step [86/190], loss=81.2615
	step [87/190], loss=79.3583
	step [88/190], loss=78.3156
	step [89/190], loss=77.8009
	step [90/190], loss=78.1321
	step [91/190], loss=76.4648
	step [92/190], loss=76.4750
	step [93/190], loss=77.0670
	step [94/190], loss=77.4108
	step [95/190], loss=76.8000
	step [96/190], loss=74.5601
	step [97/190], loss=73.8921
	step [98/190], loss=73.0045
	step [99/190], loss=73.4664
	step [100/190], loss=71.8669
	step [101/190], loss=72.8079
	step [102/190], loss=74.1412
	step [103/190], loss=72.7842
	step [104/190], loss=72.0381
	step [105/190], loss=72.7718
	step [106/190], loss=72.5236
	step [107/190], loss=74.0297
	step [108/190], loss=71.8793
	step [109/190], loss=68.9493
	step [110/190], loss=72.5110
	step [111/190], loss=72.8924
	step [112/190], loss=70.9727
	step [113/190], loss=69.8812
	step [114/190], loss=66.6550
	step [115/190], loss=70.2291
	step [116/190], loss=69.4672
	step [117/190], loss=69.4251
	step [118/190], loss=67.2000
	step [119/190], loss=66.8679
	step [120/190], loss=68.9526
	step [121/190], loss=69.7136
	step [122/190], loss=67.6076
	step [123/190], loss=66.9306
	step [124/190], loss=67.5683
	step [125/190], loss=67.8872
	step [126/190], loss=66.4834
	step [127/190], loss=66.5681
	step [128/190], loss=65.5667
	step [129/190], loss=65.1114
	step [130/190], loss=67.9830
	step [131/190], loss=65.5788
	step [132/190], loss=65.7906
	step [133/190], loss=65.1698
	step [134/190], loss=65.0069
	step [135/190], loss=63.7639
	step [136/190], loss=66.0161
	step [137/190], loss=65.5981
	step [138/190], loss=65.1036
	step [139/190], loss=65.9444
	step [140/190], loss=63.6124
	step [141/190], loss=63.0084
	step [142/190], loss=61.1923
	step [143/190], loss=62.2312
	step [144/190], loss=64.2435
	step [145/190], loss=63.2430
	step [146/190], loss=62.0907
	step [147/190], loss=62.8734
	step [148/190], loss=62.5186
	step [149/190], loss=61.1711
	step [150/190], loss=61.2984
	step [151/190], loss=62.4920
	step [152/190], loss=59.7598
	step [153/190], loss=60.6969
	step [154/190], loss=62.7599
	step [155/190], loss=62.3403
	step [156/190], loss=62.3719
	step [157/190], loss=61.8884
	step [158/190], loss=61.6649
	step [159/190], loss=63.1293
	step [160/190], loss=60.5059
	step [161/190], loss=60.1116
	step [162/190], loss=61.4443
	step [163/190], loss=60.7192
	step [164/190], loss=60.6416
	step [165/190], loss=62.9559
	step [166/190], loss=62.2368
	step [167/190], loss=60.0610
	step [168/190], loss=59.9458
	step [169/190], loss=61.2430
	step [170/190], loss=63.4529
	step [171/190], loss=60.4073
	step [172/190], loss=59.7308
	step [173/190], loss=59.2137
	step [174/190], loss=61.6378
	step [175/190], loss=59.3273
	step [176/190], loss=57.0390
	step [177/190], loss=60.0723
	step [178/190], loss=59.6005
	step [179/190], loss=55.7729
	step [180/190], loss=58.8842
	step [181/190], loss=58.6934
	step [182/190], loss=57.8329
	step [183/190], loss=57.0451
	step [184/190], loss=57.3708
	step [185/190], loss=56.9366
	step [186/190], loss=59.1926
	step [187/190], loss=57.4435
	step [188/190], loss=59.6650
	step [189/190], loss=60.2527
	step [190/190], loss=53.3103
	Evaluating
	loss=0.2158, precision=0.1665, recall=0.9954, f1=0.2853
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/190], loss=58.8997
	step [2/190], loss=58.5151
	step [3/190], loss=58.8802
	step [4/190], loss=57.3562
	step [5/190], loss=55.3697
	step [6/190], loss=58.8931
	step [7/190], loss=56.3450
	step [8/190], loss=57.5148
	step [9/190], loss=57.1338
	step [10/190], loss=56.8795
	step [11/190], loss=56.6004
	step [12/190], loss=55.3533
	step [13/190], loss=55.3280
	step [14/190], loss=55.8182
	step [15/190], loss=57.7658
	step [16/190], loss=58.6033
	step [17/190], loss=56.7854
	step [18/190], loss=54.7944
	step [19/190], loss=56.2912
	step [20/190], loss=55.2954
	step [21/190], loss=54.4211
	step [22/190], loss=56.0210
	step [23/190], loss=54.5494
	step [24/190], loss=54.9883
	step [25/190], loss=53.9964
	step [26/190], loss=56.1810
	step [27/190], loss=53.1106
	step [28/190], loss=54.4465
	step [29/190], loss=55.5070
	step [30/190], loss=56.2370
	step [31/190], loss=54.0770
	step [32/190], loss=54.7506
	step [33/190], loss=53.4080
	step [34/190], loss=53.5374
	step [35/190], loss=53.9455
	step [36/190], loss=54.0145
	step [37/190], loss=53.7102
	step [38/190], loss=53.5597
	step [39/190], loss=53.6644
	step [40/190], loss=54.8021
	step [41/190], loss=51.2166
	step [42/190], loss=51.8134
	step [43/190], loss=52.8716
	step [44/190], loss=54.1231
	step [45/190], loss=52.1489
	step [46/190], loss=52.5470
	step [47/190], loss=52.8384
	step [48/190], loss=51.1608
	step [49/190], loss=52.7366
	step [50/190], loss=51.5586
	step [51/190], loss=53.5385
	step [52/190], loss=52.9946
	step [53/190], loss=53.2658
	step [54/190], loss=51.2617
	step [55/190], loss=53.0329
	step [56/190], loss=50.9754
	step [57/190], loss=50.9624
	step [58/190], loss=52.6483
	step [59/190], loss=54.0537
	step [60/190], loss=50.5398
	step [61/190], loss=51.7659
	step [62/190], loss=52.8763
	step [63/190], loss=51.3136
	step [64/190], loss=53.8007
	step [65/190], loss=51.4172
	step [66/190], loss=52.9995
	step [67/190], loss=50.8849
	step [68/190], loss=50.7025
	step [69/190], loss=52.2443
	step [70/190], loss=50.6481
	step [71/190], loss=52.2064
	step [72/190], loss=51.5951
	step [73/190], loss=49.2915
	step [74/190], loss=52.9940
	step [75/190], loss=51.3281
	step [76/190], loss=50.2974
	step [77/190], loss=50.1492
	step [78/190], loss=49.4327
	step [79/190], loss=49.9887
	step [80/190], loss=50.2820
	step [81/190], loss=50.6595
	step [82/190], loss=50.3344
	step [83/190], loss=51.6705
	step [84/190], loss=49.0820
	step [85/190], loss=51.0006
	step [86/190], loss=48.4215
	step [87/190], loss=50.7929
	step [88/190], loss=50.4216
	step [89/190], loss=49.2716
	step [90/190], loss=48.8225
	step [91/190], loss=49.1284
	step [92/190], loss=51.4721
	step [93/190], loss=47.7663
	step [94/190], loss=49.3889
	step [95/190], loss=48.8662
	step [96/190], loss=48.5639
	step [97/190], loss=49.5269
	step [98/190], loss=48.1304
	step [99/190], loss=47.9455
	step [100/190], loss=49.1990
	step [101/190], loss=48.6889
	step [102/190], loss=48.2574
	step [103/190], loss=45.9239
	step [104/190], loss=47.2791
	step [105/190], loss=49.3467
	step [106/190], loss=46.4416
	step [107/190], loss=47.2352
	step [108/190], loss=51.1348
	step [109/190], loss=46.3402
	step [110/190], loss=45.6328
	step [111/190], loss=48.2928
	step [112/190], loss=45.6964
	step [113/190], loss=47.8895
	step [114/190], loss=45.9895
	step [115/190], loss=48.3239
	step [116/190], loss=47.0937
	step [117/190], loss=50.1110
	step [118/190], loss=47.2600
	step [119/190], loss=46.3164
	step [120/190], loss=44.8602
	step [121/190], loss=47.0351
	step [122/190], loss=46.0816
	step [123/190], loss=48.9711
	step [124/190], loss=50.6568
	step [125/190], loss=46.5855
	step [126/190], loss=47.7109
	step [127/190], loss=45.9608
	step [128/190], loss=45.7327
	step [129/190], loss=47.4618
	step [130/190], loss=46.5218
	step [131/190], loss=47.1494
	step [132/190], loss=46.5958
	step [133/190], loss=46.2503
	step [134/190], loss=47.8563
	step [135/190], loss=45.8651
	step [136/190], loss=45.7965
	step [137/190], loss=45.6362
	step [138/190], loss=45.4304
	step [139/190], loss=45.3054
	step [140/190], loss=50.5540
	step [141/190], loss=45.3046
	step [142/190], loss=46.6689
	step [143/190], loss=45.7537
	step [144/190], loss=45.0814
	step [145/190], loss=46.0863
	step [146/190], loss=46.2647
	step [147/190], loss=44.9968
	step [148/190], loss=44.5016
	step [149/190], loss=44.0092
	step [150/190], loss=45.9255
	step [151/190], loss=45.3892
	step [152/190], loss=45.2743
	step [153/190], loss=44.9988
	step [154/190], loss=44.6284
	step [155/190], loss=44.9115
	step [156/190], loss=44.6501
	step [157/190], loss=44.6741
	step [158/190], loss=45.1190
	step [159/190], loss=44.3529
	step [160/190], loss=44.0300
	step [161/190], loss=43.8884
	step [162/190], loss=45.4103
	step [163/190], loss=45.9138
	step [164/190], loss=43.4762
	step [165/190], loss=45.9414
	step [166/190], loss=43.3766
	step [167/190], loss=44.5154
	step [168/190], loss=44.5061
	step [169/190], loss=44.2396
	step [170/190], loss=44.7918
	step [171/190], loss=44.1189
	step [172/190], loss=46.5114
	step [173/190], loss=44.5540
	step [174/190], loss=42.8143
	step [175/190], loss=41.6784
	step [176/190], loss=43.2610
	step [177/190], loss=43.0095
	step [178/190], loss=44.9894
	step [179/190], loss=42.7268
	step [180/190], loss=48.2173
	step [181/190], loss=42.9126
	step [182/190], loss=43.8624
	step [183/190], loss=42.9925
	step [184/190], loss=43.8706
	step [185/190], loss=40.9374
	step [186/190], loss=44.7120
	step [187/190], loss=42.6759
	step [188/190], loss=41.4780
	step [189/190], loss=44.0862
	step [190/190], loss=39.0928
	Evaluating
	loss=0.1508, precision=0.2386, recall=0.9922, f1=0.3847
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/190], loss=42.1856
	step [2/190], loss=44.1398
	step [3/190], loss=42.6735
	step [4/190], loss=44.2111
	step [5/190], loss=42.9697
	step [6/190], loss=41.7730
	step [7/190], loss=41.4894
	step [8/190], loss=43.7958
	step [9/190], loss=41.5407
	step [10/190], loss=41.1390
	step [11/190], loss=41.7159
	step [12/190], loss=41.3525
	step [13/190], loss=40.5196
	step [14/190], loss=42.1760
	step [15/190], loss=39.9890
	step [16/190], loss=42.1952
	step [17/190], loss=43.0849
	step [18/190], loss=41.2410
	step [19/190], loss=42.1718
	step [20/190], loss=42.9719
	step [21/190], loss=40.5525
	step [22/190], loss=40.0079
	step [23/190], loss=43.3644
	step [24/190], loss=40.3471
	step [25/190], loss=41.5724
	step [26/190], loss=41.0145
	step [27/190], loss=42.2832
	step [28/190], loss=40.7146
	step [29/190], loss=42.2338
	step [30/190], loss=40.2130
	step [31/190], loss=42.2640
	step [32/190], loss=41.8291
	step [33/190], loss=41.6018
	step [34/190], loss=41.8212
	step [35/190], loss=39.8909
	step [36/190], loss=41.1223
	step [37/190], loss=40.5753
	step [38/190], loss=39.3029
	step [39/190], loss=39.4627
	step [40/190], loss=40.6195
	step [41/190], loss=40.4547
	step [42/190], loss=41.3241
	step [43/190], loss=39.9264
	step [44/190], loss=42.1248
	step [45/190], loss=41.9187
	step [46/190], loss=40.6365
	step [47/190], loss=39.8376
	step [48/190], loss=40.9476
	step [49/190], loss=39.4232
	step [50/190], loss=40.8772
	step [51/190], loss=40.0585
	step [52/190], loss=40.0464
	step [53/190], loss=41.6725
	step [54/190], loss=37.4769
	step [55/190], loss=38.9018
	step [56/190], loss=40.5239
	step [57/190], loss=38.4821
	step [58/190], loss=39.6592
	step [59/190], loss=41.4799
	step [60/190], loss=37.8043
	step [61/190], loss=39.6836
	step [62/190], loss=37.5551
	step [63/190], loss=39.7545
	step [64/190], loss=37.7842
	step [65/190], loss=39.8726
	step [66/190], loss=38.4452
	step [67/190], loss=38.9516
	step [68/190], loss=38.2108
	step [69/190], loss=38.8382
	step [70/190], loss=40.1120
	step [71/190], loss=38.3397
	step [72/190], loss=38.3246
	step [73/190], loss=40.4448
	step [74/190], loss=37.4112
	step [75/190], loss=39.5234
	step [76/190], loss=39.4866
	step [77/190], loss=41.2653
	step [78/190], loss=40.5886
	step [79/190], loss=39.3408
	step [80/190], loss=37.8047
	step [81/190], loss=39.1357
	step [82/190], loss=37.1226
	step [83/190], loss=37.8810
	step [84/190], loss=37.1042
	step [85/190], loss=39.1225
	step [86/190], loss=39.1257
	step [87/190], loss=39.0902
	step [88/190], loss=39.6008
	step [89/190], loss=40.0515
	step [90/190], loss=38.0588
	step [91/190], loss=39.3541
	step [92/190], loss=36.5911
	step [93/190], loss=37.6613
	step [94/190], loss=36.9358
	step [95/190], loss=37.2817
	step [96/190], loss=37.1497
	step [97/190], loss=38.3730
	step [98/190], loss=39.5465
	step [99/190], loss=39.0162
	step [100/190], loss=36.2858
	step [101/190], loss=38.6027
	step [102/190], loss=36.7758
	step [103/190], loss=39.9908
	step [104/190], loss=36.3558
	step [105/190], loss=36.4520
	step [106/190], loss=38.8514
	step [107/190], loss=38.1128
	step [108/190], loss=38.8519
	step [109/190], loss=38.9592
	step [110/190], loss=37.5505
	step [111/190], loss=36.0211
	step [112/190], loss=38.6010
	step [113/190], loss=36.8675
	step [114/190], loss=38.4269
	step [115/190], loss=38.0429
	step [116/190], loss=36.4377
	step [117/190], loss=37.8696
	step [118/190], loss=35.0328
	step [119/190], loss=38.9039
	step [120/190], loss=36.7141
	step [121/190], loss=38.1579
	step [122/190], loss=36.2477
	step [123/190], loss=34.7904
	step [124/190], loss=37.3845
	step [125/190], loss=36.9517
	step [126/190], loss=38.3913
	step [127/190], loss=35.0081
	step [128/190], loss=35.2462
	step [129/190], loss=38.4684
	step [130/190], loss=38.2379
	step [131/190], loss=36.0340
	step [132/190], loss=35.0564
	step [133/190], loss=36.6720
	step [134/190], loss=34.2326
	step [135/190], loss=35.0217
	step [136/190], loss=35.7649
	step [137/190], loss=37.4902
	step [138/190], loss=38.1243
	step [139/190], loss=34.9554
	step [140/190], loss=35.9404
	step [141/190], loss=38.1929
	step [142/190], loss=38.5802
	step [143/190], loss=35.4346
	step [144/190], loss=37.8604
	step [145/190], loss=36.3266
	step [146/190], loss=37.9191
	step [147/190], loss=34.7170
	step [148/190], loss=36.5965
	step [149/190], loss=35.6720
	step [150/190], loss=35.3517
	step [151/190], loss=34.7382
	step [152/190], loss=34.8599
	step [153/190], loss=33.9266
	step [154/190], loss=35.5519
	step [155/190], loss=36.1403
	step [156/190], loss=35.8595
	step [157/190], loss=33.8904
	step [158/190], loss=35.9106
	step [159/190], loss=35.1803
	step [160/190], loss=35.3361
	step [161/190], loss=34.7309
	step [162/190], loss=34.0496
	step [163/190], loss=35.9935
	step [164/190], loss=36.4707
	step [165/190], loss=36.8537
	step [166/190], loss=36.3798
	step [167/190], loss=34.1558
	step [168/190], loss=34.7910
	step [169/190], loss=35.5478
	step [170/190], loss=35.4841
	step [171/190], loss=35.3606
	step [172/190], loss=33.8398
	step [173/190], loss=36.4172
	step [174/190], loss=34.1475
	step [175/190], loss=33.5554
	step [176/190], loss=35.8657
	step [177/190], loss=32.6959
	step [178/190], loss=34.2850
	step [179/190], loss=34.2727
	step [180/190], loss=35.3618
	step [181/190], loss=33.7099
	step [182/190], loss=35.3558
	step [183/190], loss=34.5036
	step [184/190], loss=35.0560
	step [185/190], loss=34.7761
	step [186/190], loss=35.3801
	step [187/190], loss=34.4995
	step [188/190], loss=32.3153
	step [189/190], loss=35.7998
	step [190/190], loss=29.2614
	Evaluating
	loss=0.1240, precision=0.2002, recall=0.9943, f1=0.3333
Training epoch 4
	step [1/190], loss=33.7321
	step [2/190], loss=33.3392
	step [3/190], loss=34.6108
	step [4/190], loss=32.5338
	step [5/190], loss=33.0840
	step [6/190], loss=34.7652
	step [7/190], loss=33.5988
	step [8/190], loss=33.6444
	step [9/190], loss=32.4303
	step [10/190], loss=31.6336
	step [11/190], loss=34.6548
	step [12/190], loss=32.2094
	step [13/190], loss=35.2948
	step [14/190], loss=32.6067
	step [15/190], loss=33.9267
	step [16/190], loss=33.7227
	step [17/190], loss=32.3780
	step [18/190], loss=33.1037
	step [19/190], loss=33.2426
	step [20/190], loss=33.6436
	step [21/190], loss=33.9255
	step [22/190], loss=33.6985
	step [23/190], loss=32.7833
	step [24/190], loss=34.0287
	step [25/190], loss=32.2852
	step [26/190], loss=33.0830
	step [27/190], loss=31.1717
	step [28/190], loss=34.6891
	step [29/190], loss=33.3485
	step [30/190], loss=34.1295
	step [31/190], loss=32.1126
	step [32/190], loss=33.3947
	step [33/190], loss=33.8282
	step [34/190], loss=36.7234
	step [35/190], loss=32.8868
	step [36/190], loss=33.5177
	step [37/190], loss=31.7176
	step [38/190], loss=34.4883
	step [39/190], loss=30.8714
	step [40/190], loss=33.0849
	step [41/190], loss=32.7569
	step [42/190], loss=31.5124
	step [43/190], loss=32.0776
	step [44/190], loss=29.7023
	step [45/190], loss=32.7778
	step [46/190], loss=32.9242
	step [47/190], loss=30.6122
	step [48/190], loss=32.9401
	step [49/190], loss=32.0418
	step [50/190], loss=31.6613
	step [51/190], loss=32.2117
	step [52/190], loss=34.1695
	step [53/190], loss=30.9364
	step [54/190], loss=31.9977
	step [55/190], loss=31.8360
	step [56/190], loss=31.0159
	step [57/190], loss=32.7286
	step [58/190], loss=31.8161
	step [59/190], loss=33.9995
	step [60/190], loss=32.7267
	step [61/190], loss=31.8743
	step [62/190], loss=30.9254
	step [63/190], loss=32.0034
	step [64/190], loss=34.1855
	step [65/190], loss=31.9453
	step [66/190], loss=31.8985
	step [67/190], loss=31.1110
	step [68/190], loss=29.3311
	step [69/190], loss=32.8557
	step [70/190], loss=31.1421
	step [71/190], loss=29.5611
	step [72/190], loss=30.8204
	step [73/190], loss=30.4989
	step [74/190], loss=31.7419
	step [75/190], loss=33.2737
	step [76/190], loss=31.3041
	step [77/190], loss=33.6990
	step [78/190], loss=34.1758
	step [79/190], loss=30.7424
	step [80/190], loss=30.8128
	step [81/190], loss=29.6125
	step [82/190], loss=31.5406
	step [83/190], loss=30.4047
	step [84/190], loss=31.3044
	step [85/190], loss=28.2430
	step [86/190], loss=30.9045
	step [87/190], loss=30.4868
	step [88/190], loss=30.1694
	step [89/190], loss=31.6134
	step [90/190], loss=30.8494
	step [91/190], loss=29.6329
	step [92/190], loss=33.0264
	step [93/190], loss=34.8881
	step [94/190], loss=30.4253
	step [95/190], loss=30.2281
	step [96/190], loss=29.5829
	step [97/190], loss=30.1380
	step [98/190], loss=29.3675
	step [99/190], loss=30.7152
	step [100/190], loss=32.4168
	step [101/190], loss=31.6165
	step [102/190], loss=29.7552
	step [103/190], loss=33.6068
	step [104/190], loss=29.9042
	step [105/190], loss=31.5317
	step [106/190], loss=30.2769
	step [107/190], loss=30.5138
	step [108/190], loss=30.2052
	step [109/190], loss=32.4270
	step [110/190], loss=30.0652
	step [111/190], loss=30.7626
	step [112/190], loss=31.8438
	step [113/190], loss=29.5155
	step [114/190], loss=31.0858
	step [115/190], loss=29.1639
	step [116/190], loss=29.4969
	step [117/190], loss=29.8998
	step [118/190], loss=31.4052
	step [119/190], loss=29.9380
	step [120/190], loss=31.2665
	step [121/190], loss=29.4528
	step [122/190], loss=29.1823
	step [123/190], loss=28.8392
	step [124/190], loss=30.9297
	step [125/190], loss=28.6263
	step [126/190], loss=31.2691
	step [127/190], loss=29.0469
	step [128/190], loss=29.3004
	step [129/190], loss=27.9221
	step [130/190], loss=30.7390
	step [131/190], loss=29.2944
	step [132/190], loss=28.2450
	step [133/190], loss=30.0162
	step [134/190], loss=29.2753
	step [135/190], loss=28.5691
	step [136/190], loss=30.1663
	step [137/190], loss=28.0715
	step [138/190], loss=29.1892
	step [139/190], loss=27.2360
	step [140/190], loss=31.0721
	step [141/190], loss=28.3983
	step [142/190], loss=30.0522
	step [143/190], loss=29.0311
	step [144/190], loss=29.0701
	step [145/190], loss=28.7298
	step [146/190], loss=28.3034
	step [147/190], loss=27.1667
	step [148/190], loss=29.3607
	step [149/190], loss=28.7352
	step [150/190], loss=29.6361
	step [151/190], loss=30.9664
	step [152/190], loss=27.7294
	step [153/190], loss=29.9956
	step [154/190], loss=28.6240
	step [155/190], loss=28.7593
	step [156/190], loss=28.9538
	step [157/190], loss=27.6921
	step [158/190], loss=28.2884
	step [159/190], loss=27.1455
	step [160/190], loss=27.7332
	step [161/190], loss=28.3593
	step [162/190], loss=29.1673
	step [163/190], loss=29.0093
	step [164/190], loss=29.8480
	step [165/190], loss=28.4739
	step [166/190], loss=27.0586
	step [167/190], loss=26.3638
	step [168/190], loss=28.8276
	step [169/190], loss=28.3707
	step [170/190], loss=29.8101
	step [171/190], loss=25.4926
	step [172/190], loss=29.4575
	step [173/190], loss=26.3838
	step [174/190], loss=28.7365
	step [175/190], loss=28.3694
	step [176/190], loss=27.1050
	step [177/190], loss=30.5389
	step [178/190], loss=27.6538
	step [179/190], loss=27.3555
	step [180/190], loss=28.1294
	step [181/190], loss=28.5706
	step [182/190], loss=26.7122
	step [183/190], loss=28.1946
	step [184/190], loss=27.9560
	step [185/190], loss=30.7537
	step [186/190], loss=27.8861
	step [187/190], loss=27.0652
	step [188/190], loss=25.5236
	step [189/190], loss=28.3954
	step [190/190], loss=24.7882
	Evaluating
	loss=0.0961, precision=0.2246, recall=0.9935, f1=0.3664
Training epoch 5
	step [1/190], loss=26.1812
	step [2/190], loss=28.2249
	step [3/190], loss=27.9235
	step [4/190], loss=27.4958
	step [5/190], loss=27.9268
	step [6/190], loss=27.3471
	step [7/190], loss=26.7869
	step [8/190], loss=29.2375
	step [9/190], loss=27.3176
	step [10/190], loss=29.5717
	step [11/190], loss=25.6943
	step [12/190], loss=26.3236
	step [13/190], loss=28.1503
	step [14/190], loss=25.3646
	step [15/190], loss=28.5537
	step [16/190], loss=26.6940
	step [17/190], loss=26.6728
	step [18/190], loss=26.3976
	step [19/190], loss=24.3402
	step [20/190], loss=28.7141
	step [21/190], loss=27.4283
	step [22/190], loss=24.1643
	step [23/190], loss=28.7642
	step [24/190], loss=27.7816
	step [25/190], loss=26.4979
	step [26/190], loss=27.4404
	step [27/190], loss=27.9453
	step [28/190], loss=26.2695
	step [29/190], loss=26.1694
	step [30/190], loss=29.1762
	step [31/190], loss=29.4979
	step [32/190], loss=27.2321
	step [33/190], loss=27.6928
	step [34/190], loss=26.2930
	step [35/190], loss=26.4360
	step [36/190], loss=26.2527
	step [37/190], loss=27.0724
	step [38/190], loss=27.3833
	step [39/190], loss=26.9249
	step [40/190], loss=27.0946
	step [41/190], loss=25.6843
	step [42/190], loss=28.4582
	step [43/190], loss=26.8577
	step [44/190], loss=27.9958
	step [45/190], loss=25.5010
	step [46/190], loss=27.7251
	step [47/190], loss=26.0327
	step [48/190], loss=25.4859
	step [49/190], loss=25.6205
	step [50/190], loss=26.6997
	step [51/190], loss=25.3591
	step [52/190], loss=27.8362
	step [53/190], loss=24.9900
	step [54/190], loss=26.4912
	step [55/190], loss=26.4963
	step [56/190], loss=28.6361
	step [57/190], loss=25.4235
	step [58/190], loss=26.4560
	step [59/190], loss=24.7761
	step [60/190], loss=26.0632
	step [61/190], loss=25.6146
	step [62/190], loss=25.2162
	step [63/190], loss=25.3177
	step [64/190], loss=27.7911
	step [65/190], loss=28.3547
	step [66/190], loss=26.1598
	step [67/190], loss=29.2789
	step [68/190], loss=26.4666
	step [69/190], loss=26.2726
	step [70/190], loss=27.3919
	step [71/190], loss=25.4036
	step [72/190], loss=28.6719
	step [73/190], loss=25.2829
	step [74/190], loss=25.4578
	step [75/190], loss=26.9528
	step [76/190], loss=25.0125
	step [77/190], loss=25.3985
	step [78/190], loss=24.6752
	step [79/190], loss=26.7927
	step [80/190], loss=24.5037
	step [81/190], loss=24.2938
	step [82/190], loss=26.1953
	step [83/190], loss=25.5808
	step [84/190], loss=27.4709
	step [85/190], loss=24.3652
	step [86/190], loss=24.6776
	step [87/190], loss=26.9887
	step [88/190], loss=26.6049
	step [89/190], loss=25.4359
	step [90/190], loss=27.3866
	step [91/190], loss=24.6644
	step [92/190], loss=26.1400
	step [93/190], loss=25.2738
	step [94/190], loss=25.0050
	step [95/190], loss=25.9388
	step [96/190], loss=23.3127
	step [97/190], loss=25.8095
	step [98/190], loss=24.6737
	step [99/190], loss=25.4958
	step [100/190], loss=25.2151
	step [101/190], loss=29.0660
	step [102/190], loss=25.6465
	step [103/190], loss=24.9205
	step [104/190], loss=27.4929
	step [105/190], loss=24.2930
	step [106/190], loss=27.9282
	step [107/190], loss=27.0084
	step [108/190], loss=23.9218
	step [109/190], loss=26.3384
	step [110/190], loss=27.1494
	step [111/190], loss=24.9900
	step [112/190], loss=23.4482
	step [113/190], loss=25.3627
	step [114/190], loss=24.4225
	step [115/190], loss=23.8435
	step [116/190], loss=24.9760
	step [117/190], loss=25.3267
	step [118/190], loss=24.9094
	step [119/190], loss=24.4718
	step [120/190], loss=23.4798
	step [121/190], loss=24.3905
	step [122/190], loss=23.7906
	step [123/190], loss=23.9673
	step [124/190], loss=25.3077
	step [125/190], loss=24.3239
	step [126/190], loss=24.4582
	step [127/190], loss=25.6688
	step [128/190], loss=23.8205
	step [129/190], loss=26.8548
	step [130/190], loss=23.3468
	step [131/190], loss=25.1995
	step [132/190], loss=27.7706
	step [133/190], loss=24.8982
	step [134/190], loss=25.5072
	step [135/190], loss=24.5166
	step [136/190], loss=23.9743
	step [137/190], loss=25.0390
	step [138/190], loss=23.6881
	step [139/190], loss=24.4956
	step [140/190], loss=23.8212
	step [141/190], loss=24.8317
	step [142/190], loss=25.6382
	step [143/190], loss=26.4272
	step [144/190], loss=23.6677
	step [145/190], loss=23.2329
	step [146/190], loss=23.8517
	step [147/190], loss=24.1955
	step [148/190], loss=24.5985
	step [149/190], loss=22.7720
	step [150/190], loss=24.8346
	step [151/190], loss=23.8862
	step [152/190], loss=21.9441
	step [153/190], loss=25.7732
	step [154/190], loss=23.7632
	step [155/190], loss=22.9730
	step [156/190], loss=28.4566
	step [157/190], loss=25.9782
	step [158/190], loss=24.3669
	step [159/190], loss=23.7953
	step [160/190], loss=25.6707
	step [161/190], loss=25.7169
	step [162/190], loss=24.5000
	step [163/190], loss=23.3261
	step [164/190], loss=26.0492
	step [165/190], loss=23.7601
	step [166/190], loss=22.6672
	step [167/190], loss=24.2525
	step [168/190], loss=22.9578
	step [169/190], loss=22.7001
	step [170/190], loss=22.4463
	step [171/190], loss=23.5907
	step [172/190], loss=22.8499
	step [173/190], loss=22.6059
	step [174/190], loss=21.5344
	step [175/190], loss=23.8963
	step [176/190], loss=23.7120
	step [177/190], loss=23.2523
	step [178/190], loss=23.4342
	step [179/190], loss=25.6935
	step [180/190], loss=23.9509
	step [181/190], loss=24.0218
	step [182/190], loss=25.0571
	step [183/190], loss=22.8876
	step [184/190], loss=24.1311
	step [185/190], loss=25.9711
	step [186/190], loss=25.6261
	step [187/190], loss=24.7705
	step [188/190], loss=23.7118
	step [189/190], loss=23.7959
	step [190/190], loss=22.0475
	Evaluating
	loss=0.0770, precision=0.2246, recall=0.9933, f1=0.3664
Training epoch 6
	step [1/190], loss=23.1422
	step [2/190], loss=25.5426
	step [3/190], loss=23.9119
	step [4/190], loss=23.6150
	step [5/190], loss=23.3389
	step [6/190], loss=22.2102
	step [7/190], loss=21.7920
	step [8/190], loss=25.0893
	step [9/190], loss=25.5149
	step [10/190], loss=22.3404
	step [11/190], loss=22.0721
	step [12/190], loss=23.6114
	step [13/190], loss=22.5843
	step [14/190], loss=21.6794
	step [15/190], loss=25.5415
	step [16/190], loss=23.8041
	step [17/190], loss=22.3452
	step [18/190], loss=24.1481
	step [19/190], loss=25.8895
	step [20/190], loss=23.6850
	step [21/190], loss=22.5475
	step [22/190], loss=21.2304
	step [23/190], loss=22.5387
	step [24/190], loss=23.1403
	step [25/190], loss=23.7191
	step [26/190], loss=22.1579
	step [27/190], loss=24.1127
	step [28/190], loss=20.6753
	step [29/190], loss=22.5883
	step [30/190], loss=24.9282
	step [31/190], loss=22.3741
	step [32/190], loss=24.8299
	step [33/190], loss=24.1784
	step [34/190], loss=24.4024
	step [35/190], loss=22.8375
	step [36/190], loss=22.2781
	step [37/190], loss=23.6231
	step [38/190], loss=23.9883
	step [39/190], loss=21.5920
	step [40/190], loss=22.1227
	step [41/190], loss=21.0904
	step [42/190], loss=22.1835
	step [43/190], loss=22.1453
	step [44/190], loss=23.5095
	step [45/190], loss=21.4013
	step [46/190], loss=23.3412
	step [47/190], loss=23.1944
	step [48/190], loss=21.7076
	step [49/190], loss=22.9174
	step [50/190], loss=22.5268
	step [51/190], loss=23.1436
	step [52/190], loss=24.1509
	step [53/190], loss=23.2417
	step [54/190], loss=24.5747
	step [55/190], loss=23.2585
	step [56/190], loss=23.7043
	step [57/190], loss=21.7847
	step [58/190], loss=21.4556
	step [59/190], loss=22.3087
	step [60/190], loss=23.3855
	step [61/190], loss=21.9559
	step [62/190], loss=23.2228
	step [63/190], loss=23.1010
	step [64/190], loss=22.5159
	step [65/190], loss=23.0438
	step [66/190], loss=21.4462
	step [67/190], loss=22.0345
	step [68/190], loss=22.3816
	step [69/190], loss=23.2651
	step [70/190], loss=22.1495
	step [71/190], loss=21.2302
	step [72/190], loss=22.0145
	step [73/190], loss=21.8919
	step [74/190], loss=21.7055
	step [75/190], loss=21.7527
	step [76/190], loss=22.3771
	step [77/190], loss=22.0015
	step [78/190], loss=20.8551
	step [79/190], loss=22.2068
	step [80/190], loss=19.7593
	step [81/190], loss=22.2445
	step [82/190], loss=19.2318
	step [83/190], loss=20.2375
	step [84/190], loss=21.1681
	step [85/190], loss=21.6689
	step [86/190], loss=20.7657
	step [87/190], loss=20.9420
	step [88/190], loss=21.6369
	step [89/190], loss=20.9990
	step [90/190], loss=21.0603
	step [91/190], loss=22.2518
	step [92/190], loss=19.8625
	step [93/190], loss=20.6491
	step [94/190], loss=22.0532
	step [95/190], loss=21.0904
	step [96/190], loss=21.4577
	step [97/190], loss=21.3312
	step [98/190], loss=22.0986
	step [99/190], loss=23.9148
	step [100/190], loss=21.7010
	step [101/190], loss=21.3322
	step [102/190], loss=19.9881
	step [103/190], loss=20.2351
	step [104/190], loss=20.8336
	step [105/190], loss=22.1805
	step [106/190], loss=21.6802
	step [107/190], loss=23.2035
	step [108/190], loss=25.0660
	step [109/190], loss=20.4591
	step [110/190], loss=21.8587
	step [111/190], loss=23.1048
	step [112/190], loss=22.9196
	step [113/190], loss=21.3520
	step [114/190], loss=22.1802
	step [115/190], loss=21.7744
	step [116/190], loss=21.1991
	step [117/190], loss=22.3462
	step [118/190], loss=20.7087
	step [119/190], loss=21.0454
	step [120/190], loss=20.1454
	step [121/190], loss=22.2675
	step [122/190], loss=20.0693
	step [123/190], loss=20.4753
	step [124/190], loss=20.6714
	step [125/190], loss=21.7195
	step [126/190], loss=22.6616
	step [127/190], loss=23.2538
	step [128/190], loss=22.4884
	step [129/190], loss=21.9380
	step [130/190], loss=20.2709
	step [131/190], loss=24.5724
	step [132/190], loss=22.3554
	step [133/190], loss=20.6650
	step [134/190], loss=21.6048
	step [135/190], loss=20.1413
	step [136/190], loss=22.0179
	step [137/190], loss=21.5050
	step [138/190], loss=22.4584
	step [139/190], loss=19.3572
	step [140/190], loss=20.2314
	step [141/190], loss=22.3638
	step [142/190], loss=19.0714
	step [143/190], loss=22.4942
	step [144/190], loss=19.1530
	step [145/190], loss=19.5162
	step [146/190], loss=21.5821
	step [147/190], loss=22.6506
	step [148/190], loss=21.1873
	step [149/190], loss=22.6806
	step [150/190], loss=20.8178
	step [151/190], loss=20.5459
	step [152/190], loss=23.3423
	step [153/190], loss=22.2362
	step [154/190], loss=21.4438
	step [155/190], loss=22.3391
	step [156/190], loss=20.7714
	step [157/190], loss=22.2259
	step [158/190], loss=19.8638
	step [159/190], loss=19.0649
	step [160/190], loss=20.6293
	step [161/190], loss=20.3486
	step [162/190], loss=20.4405
	step [163/190], loss=20.3921
	step [164/190], loss=18.8780
	step [165/190], loss=20.0467
	step [166/190], loss=20.1554
	step [167/190], loss=20.1206
	step [168/190], loss=22.9773
	step [169/190], loss=19.7895
	step [170/190], loss=19.0417
	step [171/190], loss=21.5671
	step [172/190], loss=20.8538
	step [173/190], loss=19.4418
	step [174/190], loss=22.2217
	step [175/190], loss=20.8430
	step [176/190], loss=19.9550
	step [177/190], loss=22.5070
	step [178/190], loss=20.9496
	step [179/190], loss=19.0966
	step [180/190], loss=19.2250
	step [181/190], loss=19.3076
	step [182/190], loss=20.9719
	step [183/190], loss=20.8129
	step [184/190], loss=18.8814
	step [185/190], loss=20.6406
	step [186/190], loss=19.8345
	step [187/190], loss=21.8362
	step [188/190], loss=20.4103
	step [189/190], loss=20.8917
	step [190/190], loss=19.2626
	Evaluating
	loss=0.0668, precision=0.2138, recall=0.9937, f1=0.3518
Training epoch 7
	step [1/190], loss=21.6131
	step [2/190], loss=19.7702
	step [3/190], loss=21.0449
	step [4/190], loss=21.2761
	step [5/190], loss=21.6649
	step [6/190], loss=19.5861
	step [7/190], loss=21.0129
	step [8/190], loss=22.3760
	step [9/190], loss=19.7379
	step [10/190], loss=21.2961
	step [11/190], loss=19.6468
	step [12/190], loss=18.5929
	step [13/190], loss=22.6720
	step [14/190], loss=21.4690
	step [15/190], loss=20.3446
	step [16/190], loss=20.0751
	step [17/190], loss=20.0120
	step [18/190], loss=19.3658
	step [19/190], loss=19.9970
	step [20/190], loss=21.7666
	step [21/190], loss=18.1910
	step [22/190], loss=20.0093
	step [23/190], loss=20.2572
	step [24/190], loss=19.8264
	step [25/190], loss=18.7321
	step [26/190], loss=19.1155
	step [27/190], loss=20.1462
	step [28/190], loss=19.5620
	step [29/190], loss=20.9114
	step [30/190], loss=19.9343
	step [31/190], loss=21.3592
	step [32/190], loss=19.8402
	step [33/190], loss=20.9349
	step [34/190], loss=18.8659
	step [35/190], loss=18.1883
	step [36/190], loss=21.4138
	step [37/190], loss=20.2222
	step [38/190], loss=18.8154
	step [39/190], loss=20.8995
	step [40/190], loss=18.8534
	step [41/190], loss=18.8920
	step [42/190], loss=18.4335
	step [43/190], loss=17.9835
	step [44/190], loss=20.3576
	step [45/190], loss=22.5702
	step [46/190], loss=17.7791
	step [47/190], loss=19.2615
	step [48/190], loss=20.6986
	step [49/190], loss=20.8388
	step [50/190], loss=18.6128
	step [51/190], loss=18.2889
	step [52/190], loss=19.3369
	step [53/190], loss=19.2981
	step [54/190], loss=17.7954
	step [55/190], loss=20.8974
	step [56/190], loss=20.2345
	step [57/190], loss=19.0126
	step [58/190], loss=20.5872
	step [59/190], loss=19.8830
	step [60/190], loss=19.5883
	step [61/190], loss=18.8448
	step [62/190], loss=18.3542
	step [63/190], loss=20.0545
	step [64/190], loss=22.6900
	step [65/190], loss=19.1294
	step [66/190], loss=20.2237
	step [67/190], loss=18.7601
	step [68/190], loss=18.8608
	step [69/190], loss=18.7290
	step [70/190], loss=17.7441
	step [71/190], loss=18.9053
	step [72/190], loss=20.0521
	step [73/190], loss=21.4958
	step [74/190], loss=18.8780
	step [75/190], loss=20.0140
	step [76/190], loss=18.6159
	step [77/190], loss=18.3875
	step [78/190], loss=19.8611
	step [79/190], loss=18.4959
	step [80/190], loss=19.9119
	step [81/190], loss=19.4559
	step [82/190], loss=18.7173
	step [83/190], loss=19.1411
	step [84/190], loss=18.5287
	step [85/190], loss=19.9381
	step [86/190], loss=21.9332
	step [87/190], loss=18.5640
	step [88/190], loss=19.2496
	step [89/190], loss=17.6533
	step [90/190], loss=18.2879
	step [91/190], loss=18.0936
	step [92/190], loss=20.7957
	step [93/190], loss=19.6801
	step [94/190], loss=18.3382
	step [95/190], loss=18.5589
	step [96/190], loss=19.4008
	step [97/190], loss=18.4899
	step [98/190], loss=20.8018
	step [99/190], loss=19.7608
	step [100/190], loss=21.0888
	step [101/190], loss=21.2776
	step [102/190], loss=19.7849
	step [103/190], loss=18.8278
	step [104/190], loss=18.0075
	step [105/190], loss=18.5646
	step [106/190], loss=19.1332
	step [107/190], loss=18.9372
	step [108/190], loss=19.1076
	step [109/190], loss=17.1477
	step [110/190], loss=18.9379
	step [111/190], loss=20.7085
	step [112/190], loss=19.5825
	step [113/190], loss=17.4912
	step [114/190], loss=17.6314
	step [115/190], loss=18.6718
	step [116/190], loss=16.7156
	step [117/190], loss=21.0756
	step [118/190], loss=18.9454
	step [119/190], loss=17.0597
	step [120/190], loss=19.0711
	step [121/190], loss=18.3773
	step [122/190], loss=17.7781
	step [123/190], loss=18.6952
	step [124/190], loss=19.1549
	step [125/190], loss=18.6085
	step [126/190], loss=20.0917
	step [127/190], loss=18.1773
	step [128/190], loss=18.8173
	step [129/190], loss=17.4578
	step [130/190], loss=16.1747
	step [131/190], loss=19.2567
	step [132/190], loss=19.0510
	step [133/190], loss=18.3216
	step [134/190], loss=18.5736
	step [135/190], loss=19.6328
	step [136/190], loss=16.6452
	step [137/190], loss=17.9722
	step [138/190], loss=18.1010
	step [139/190], loss=17.7953
	step [140/190], loss=19.3599
	step [141/190], loss=17.9445
	step [142/190], loss=17.9353
	step [143/190], loss=18.2865
	step [144/190], loss=18.3083
	step [145/190], loss=17.7622
	step [146/190], loss=20.2617
	step [147/190], loss=18.9802
	step [148/190], loss=18.9549
	step [149/190], loss=20.0896
	step [150/190], loss=15.9651
	step [151/190], loss=16.4227
	step [152/190], loss=21.0182
	step [153/190], loss=18.6037
	step [154/190], loss=20.9192
	step [155/190], loss=16.7979
	step [156/190], loss=17.7952
	step [157/190], loss=17.7392
	step [158/190], loss=17.5241
	step [159/190], loss=17.0203
	step [160/190], loss=19.6457
	step [161/190], loss=17.8672
	step [162/190], loss=18.8311
	step [163/190], loss=18.2374
	step [164/190], loss=18.7890
	step [165/190], loss=17.7297
	step [166/190], loss=18.3888
	step [167/190], loss=18.2084
	step [168/190], loss=16.7368
	step [169/190], loss=19.1823
	step [170/190], loss=19.4781
	step [171/190], loss=16.4158
	step [172/190], loss=17.6726
	step [173/190], loss=18.0304
	step [174/190], loss=17.1365
	step [175/190], loss=16.8639
	step [176/190], loss=17.7640
	step [177/190], loss=20.1623
	step [178/190], loss=16.4193
	step [179/190], loss=17.5272
	step [180/190], loss=17.6788
	step [181/190], loss=19.1130
	step [182/190], loss=16.6529
	step [183/190], loss=16.8961
	step [184/190], loss=17.7527
	step [185/190], loss=17.8947
	step [186/190], loss=18.1853
	step [187/190], loss=18.8951
	step [188/190], loss=17.5064
	step [189/190], loss=19.0443
	step [190/190], loss=14.9392
	Evaluating
	loss=0.0594, precision=0.1970, recall=0.9944, f1=0.3288
Training epoch 8
	step [1/190], loss=19.3698
	step [2/190], loss=16.6471
	step [3/190], loss=17.4517
	step [4/190], loss=16.4910
	step [5/190], loss=17.3907
	step [6/190], loss=18.7505
	step [7/190], loss=18.7032
	step [8/190], loss=15.5869
	step [9/190], loss=18.3854
	step [10/190], loss=18.4446
	step [11/190], loss=18.6853
	step [12/190], loss=17.6883
	step [13/190], loss=16.6908
	step [14/190], loss=20.8891
	step [15/190], loss=16.8324
	step [16/190], loss=17.7534
	step [17/190], loss=15.9755
	step [18/190], loss=15.9780
	step [19/190], loss=18.1226
	step [20/190], loss=18.0422
	step [21/190], loss=16.8391
	step [22/190], loss=16.9093
	step [23/190], loss=17.9859
	step [24/190], loss=19.8618
	step [25/190], loss=18.0922
	step [26/190], loss=16.9362
	step [27/190], loss=17.2976
	step [28/190], loss=15.1674
	step [29/190], loss=17.8381
	step [30/190], loss=19.4430
	step [31/190], loss=17.8539
	step [32/190], loss=18.4380
	step [33/190], loss=17.8655
	step [34/190], loss=17.7933
	step [35/190], loss=18.0011
	step [36/190], loss=18.7117
	step [37/190], loss=19.2130
	step [38/190], loss=17.4153
	step [39/190], loss=18.6378
	step [40/190], loss=18.0320
	step [41/190], loss=16.3800
	step [42/190], loss=17.1355
	step [43/190], loss=16.4274
	step [44/190], loss=20.7441
	step [45/190], loss=16.9195
	step [46/190], loss=17.2124
	step [47/190], loss=17.1191
	step [48/190], loss=18.4361
	step [49/190], loss=15.8302
	step [50/190], loss=17.6946
	step [51/190], loss=17.7634
	step [52/190], loss=16.4372
	step [53/190], loss=18.6837
	step [54/190], loss=18.7614
	step [55/190], loss=17.2741
	step [56/190], loss=17.4227
	step [57/190], loss=16.5906
	step [58/190], loss=18.8881
	step [59/190], loss=17.4483
	step [60/190], loss=16.9571
	step [61/190], loss=16.3807
	step [62/190], loss=16.2819
	step [63/190], loss=16.3622
	step [64/190], loss=18.3957
	step [65/190], loss=18.1503
	step [66/190], loss=17.3985
	step [67/190], loss=17.3964
	step [68/190], loss=14.4928
	step [69/190], loss=19.1738
	step [70/190], loss=15.3253
	step [71/190], loss=18.5033
	step [72/190], loss=17.1635
	step [73/190], loss=17.0524
	step [74/190], loss=18.6980
	step [75/190], loss=16.2359
	step [76/190], loss=18.5749
	step [77/190], loss=15.4722
	step [78/190], loss=14.6309
	step [79/190], loss=17.2171
	step [80/190], loss=16.3541
	step [81/190], loss=16.6062
	step [82/190], loss=16.4217
	step [83/190], loss=18.8692
	step [84/190], loss=18.2694
	step [85/190], loss=19.9429
	step [86/190], loss=17.0336
	step [87/190], loss=17.3599
	step [88/190], loss=17.3803
	step [89/190], loss=16.0540
	step [90/190], loss=17.3878
	step [91/190], loss=15.6379
	step [92/190], loss=15.6714
	step [93/190], loss=18.3856
	step [94/190], loss=14.7540
	step [95/190], loss=17.7153
	step [96/190], loss=18.6563
	step [97/190], loss=17.3601
	step [98/190], loss=15.1922
	step [99/190], loss=16.8598
	step [100/190], loss=18.1838
	step [101/190], loss=17.8594
	step [102/190], loss=18.4371
	step [103/190], loss=16.3518
	step [104/190], loss=16.4658
	step [105/190], loss=15.6716
	step [106/190], loss=15.2728
	step [107/190], loss=18.0306
	step [108/190], loss=18.6564
	step [109/190], loss=18.7299
	step [110/190], loss=16.0465
	step [111/190], loss=18.1109
	step [112/190], loss=17.1976
	step [113/190], loss=18.1735
	step [114/190], loss=16.1313
	step [115/190], loss=17.3015
	step [116/190], loss=16.6821
	step [117/190], loss=16.2175
	step [118/190], loss=15.3411
	step [119/190], loss=15.9403
	step [120/190], loss=16.4772
	step [121/190], loss=15.9722
	step [122/190], loss=15.7115
	step [123/190], loss=17.3119
	step [124/190], loss=16.3490
	step [125/190], loss=17.5998
	step [126/190], loss=17.0652
	step [127/190], loss=17.8753
	step [128/190], loss=15.8038
	step [129/190], loss=16.6659
	step [130/190], loss=18.4036
	step [131/190], loss=17.8797
	step [132/190], loss=17.5539
	step [133/190], loss=15.9446
	step [134/190], loss=17.0664
	step [135/190], loss=15.5916
	step [136/190], loss=16.3652
	step [137/190], loss=18.0284
	step [138/190], loss=17.0057
	step [139/190], loss=17.6376
	step [140/190], loss=15.9417
	step [141/190], loss=16.1657
	step [142/190], loss=14.7259
	step [143/190], loss=17.1057
	step [144/190], loss=15.1781
	step [145/190], loss=14.6347
	step [146/190], loss=16.3462
	step [147/190], loss=16.4266
	step [148/190], loss=17.7060
	step [149/190], loss=14.9565
	step [150/190], loss=14.3095
	step [151/190], loss=18.1099
	step [152/190], loss=15.1219
	step [153/190], loss=14.5803
	step [154/190], loss=15.8165
	step [155/190], loss=14.9949
	step [156/190], loss=17.8463
	step [157/190], loss=16.5185
	step [158/190], loss=16.8934
	step [159/190], loss=18.3518
	step [160/190], loss=15.2046
	step [161/190], loss=15.6303
	step [162/190], loss=17.0106
	step [163/190], loss=16.8115
	step [164/190], loss=15.3571
	step [165/190], loss=16.3650
	step [166/190], loss=16.6391
	step [167/190], loss=16.5868
	step [168/190], loss=15.9931
	step [169/190], loss=16.9781
	step [170/190], loss=14.0590
	step [171/190], loss=15.8078
	step [172/190], loss=18.3005
	step [173/190], loss=17.7825
	step [174/190], loss=16.7522
	step [175/190], loss=17.5843
	step [176/190], loss=16.8927
	step [177/190], loss=15.7215
	step [178/190], loss=16.7969
	step [179/190], loss=15.0437
	step [180/190], loss=16.4023
	step [181/190], loss=15.1141
	step [182/190], loss=16.8901
	step [183/190], loss=16.1153
	step [184/190], loss=19.1828
	step [185/190], loss=16.6430
	step [186/190], loss=15.5910
	step [187/190], loss=17.3458
	step [188/190], loss=16.3604
	step [189/190], loss=15.7361
	step [190/190], loss=13.6369
	Evaluating
	loss=0.0513, precision=0.2245, recall=0.9930, f1=0.3663
Training epoch 9
	step [1/190], loss=16.1707
	step [2/190], loss=15.2593
	step [3/190], loss=16.6004
	step [4/190], loss=14.8366
	step [5/190], loss=17.0199
	step [6/190], loss=15.5616
	step [7/190], loss=16.3419
	step [8/190], loss=16.9770
	step [9/190], loss=15.4553
	step [10/190], loss=16.5249
	step [11/190], loss=18.0632
	step [12/190], loss=15.5256
	step [13/190], loss=14.5124
	step [14/190], loss=17.3669
	step [15/190], loss=14.9272
	step [16/190], loss=14.5678
	step [17/190], loss=15.5036
	step [18/190], loss=15.3009
	step [19/190], loss=14.5281
	step [20/190], loss=15.8103
	step [21/190], loss=16.0897
	step [22/190], loss=16.8988
	step [23/190], loss=15.5167
	step [24/190], loss=16.4952
	step [25/190], loss=19.5500
	step [26/190], loss=15.9578
	step [27/190], loss=17.7456
	step [28/190], loss=16.4901
	step [29/190], loss=15.2108
	step [30/190], loss=17.2442
	step [31/190], loss=15.6994
	step [32/190], loss=13.8383
	step [33/190], loss=16.1889
	step [34/190], loss=15.4269
	step [35/190], loss=16.8295
	step [36/190], loss=13.6581
	step [37/190], loss=16.1565
	step [38/190], loss=15.4222
	step [39/190], loss=15.1886
	step [40/190], loss=14.6612
	step [41/190], loss=15.7498
	step [42/190], loss=15.4337
	step [43/190], loss=14.6717
	step [44/190], loss=15.8606
	step [45/190], loss=16.9493
	step [46/190], loss=14.0453
	step [47/190], loss=15.6730
	step [48/190], loss=14.3863
	step [49/190], loss=16.5604
	step [50/190], loss=15.6985
	step [51/190], loss=15.1195
	step [52/190], loss=16.3313
	step [53/190], loss=17.0481
	step [54/190], loss=19.4772
	step [55/190], loss=15.6855
	step [56/190], loss=16.2991
	step [57/190], loss=15.7666
	step [58/190], loss=15.6763
	step [59/190], loss=14.4705
	step [60/190], loss=14.7595
	step [61/190], loss=16.4149
	step [62/190], loss=15.3322
	step [63/190], loss=16.1357
	step [64/190], loss=14.4147
	step [65/190], loss=17.3063
	step [66/190], loss=17.3220
	step [67/190], loss=17.0070
	step [68/190], loss=14.7909
	step [69/190], loss=16.1225
	step [70/190], loss=17.5453
	step [71/190], loss=17.3607
	step [72/190], loss=14.4682
	step [73/190], loss=15.9956
	step [74/190], loss=13.8763
	step [75/190], loss=17.0336
	step [76/190], loss=14.5899
	step [77/190], loss=13.0301
	step [78/190], loss=14.5343
	step [79/190], loss=15.1184
	step [80/190], loss=14.4949
	step [81/190], loss=15.5257
	step [82/190], loss=15.2271
	step [83/190], loss=15.0236
	step [84/190], loss=16.4351
	step [85/190], loss=15.6247
	step [86/190], loss=16.0936
	step [87/190], loss=15.9811
	step [88/190], loss=16.3136
	step [89/190], loss=15.2112
	step [90/190], loss=14.8027
	step [91/190], loss=16.0901
	step [92/190], loss=14.1858
	step [93/190], loss=16.1613
	step [94/190], loss=14.1524
	step [95/190], loss=17.2009
	step [96/190], loss=16.2359
	step [97/190], loss=16.4831
	step [98/190], loss=12.8454
	step [99/190], loss=15.1092
	step [100/190], loss=14.2432
	step [101/190], loss=18.6826
	step [102/190], loss=14.9776
	step [103/190], loss=15.3744
	step [104/190], loss=16.3393
	step [105/190], loss=16.0418
	step [106/190], loss=14.5544
	step [107/190], loss=14.4064
	step [108/190], loss=14.5120
	step [109/190], loss=15.0869
	step [110/190], loss=15.6982
	step [111/190], loss=15.3568
	step [112/190], loss=13.7178
	step [113/190], loss=13.9553
	step [114/190], loss=14.7104
	step [115/190], loss=15.6788
	step [116/190], loss=14.4534
	step [117/190], loss=16.2901
	step [118/190], loss=17.3760
	step [119/190], loss=15.1057
	step [120/190], loss=14.8589
	step [121/190], loss=16.1170
	step [122/190], loss=15.5353
	step [123/190], loss=14.4391
	step [124/190], loss=14.9041
	step [125/190], loss=14.1427
	step [126/190], loss=15.5938
	step [127/190], loss=13.9169
	step [128/190], loss=16.2173
	step [129/190], loss=15.4400
	step [130/190], loss=15.0818
	step [131/190], loss=14.5164
	step [132/190], loss=14.9081
	step [133/190], loss=16.4161
	step [134/190], loss=15.2517
	step [135/190], loss=16.8464
	step [136/190], loss=14.3429
	step [137/190], loss=15.1091
	step [138/190], loss=16.6741
	step [139/190], loss=13.4535
	step [140/190], loss=15.2185
	step [141/190], loss=13.9646
	step [142/190], loss=15.4148
	step [143/190], loss=13.2903
	step [144/190], loss=15.6208
	step [145/190], loss=14.1929
	step [146/190], loss=15.7232
	step [147/190], loss=15.7687
	step [148/190], loss=14.6384
	step [149/190], loss=13.3414
	step [150/190], loss=13.8065
	step [151/190], loss=16.6234
	step [152/190], loss=13.7494
	step [153/190], loss=14.4021
	step [154/190], loss=15.4716
	step [155/190], loss=15.5664
	step [156/190], loss=14.5726
	step [157/190], loss=14.5214
	step [158/190], loss=15.0368
	step [159/190], loss=14.6471
	step [160/190], loss=15.8084
	step [161/190], loss=14.2720
	step [162/190], loss=15.1387
	step [163/190], loss=16.0446
	step [164/190], loss=14.8831
	step [165/190], loss=13.4633
	step [166/190], loss=14.8641
	step [167/190], loss=14.1166
	step [168/190], loss=13.8585
	step [169/190], loss=15.8712
	step [170/190], loss=14.1443
	step [171/190], loss=13.7360
	step [172/190], loss=17.5527
	step [173/190], loss=14.4357
	step [174/190], loss=14.3526
	step [175/190], loss=15.6077
	step [176/190], loss=13.4639
	step [177/190], loss=13.9471
	step [178/190], loss=16.3189
	step [179/190], loss=14.4936
	step [180/190], loss=15.7737
	step [181/190], loss=14.0384
	step [182/190], loss=16.9697
	step [183/190], loss=12.7931
	step [184/190], loss=14.1163
	step [185/190], loss=16.4369
	step [186/190], loss=13.6551
	step [187/190], loss=16.3860
	step [188/190], loss=15.6342
	step [189/190], loss=14.8665
	step [190/190], loss=12.8727
	Evaluating
	loss=0.0469, precision=0.2121, recall=0.9937, f1=0.3497
Training epoch 10
	step [1/190], loss=14.7454
	step [2/190], loss=13.7589
	step [3/190], loss=15.2851
	step [4/190], loss=14.4031
	step [5/190], loss=14.8828
	step [6/190], loss=16.2068
	step [7/190], loss=13.1964
	step [8/190], loss=14.0451
	step [9/190], loss=14.2150
	step [10/190], loss=17.6278
	step [11/190], loss=14.2035
	step [12/190], loss=15.3896
	step [13/190], loss=13.3809
	step [14/190], loss=14.9844
	step [15/190], loss=14.9892
	step [16/190], loss=14.4974
	step [17/190], loss=18.2637
	step [18/190], loss=13.9122
	step [19/190], loss=12.2360
	step [20/190], loss=15.1377
	step [21/190], loss=14.4455
	step [22/190], loss=14.3829
	step [23/190], loss=13.8330
	step [24/190], loss=13.8460
	step [25/190], loss=12.7777
	step [26/190], loss=16.0488
	step [27/190], loss=14.9423
	step [28/190], loss=14.4894
	step [29/190], loss=13.6774
	step [30/190], loss=14.7120
	step [31/190], loss=14.9138
	step [32/190], loss=13.7912
	step [33/190], loss=12.7475
	step [34/190], loss=13.0695
	step [35/190], loss=14.5929
	step [36/190], loss=12.4483
	step [37/190], loss=15.1203
	step [38/190], loss=14.9517
	step [39/190], loss=15.1616
	step [40/190], loss=13.3650
	step [41/190], loss=14.5358
	step [42/190], loss=14.7918
	step [43/190], loss=15.7125
	step [44/190], loss=12.5850
	step [45/190], loss=13.7991
	step [46/190], loss=16.0529
	step [47/190], loss=13.0322
	step [48/190], loss=12.5266
	step [49/190], loss=14.5708
	step [50/190], loss=12.0512
	step [51/190], loss=13.1810
	step [52/190], loss=16.1444
	step [53/190], loss=14.3424
	step [54/190], loss=17.8139
	step [55/190], loss=14.3731
	step [56/190], loss=14.9411
	step [57/190], loss=13.7011
	step [58/190], loss=13.7960
	step [59/190], loss=14.6363
	step [60/190], loss=13.1646
	step [61/190], loss=15.1060
	step [62/190], loss=15.0744
	step [63/190], loss=15.5863
	step [64/190], loss=13.2095
	step [65/190], loss=12.0867
	step [66/190], loss=13.9310
	step [67/190], loss=15.3744
	step [68/190], loss=16.5675
	step [69/190], loss=14.5495
	step [70/190], loss=12.2701
	step [71/190], loss=14.7190
	step [72/190], loss=12.6554
	step [73/190], loss=13.8419
	step [74/190], loss=12.7049
	step [75/190], loss=14.3593
	step [76/190], loss=11.4608
	step [77/190], loss=13.5252
	step [78/190], loss=13.4608
	step [79/190], loss=13.7483
	step [80/190], loss=15.2617
	step [81/190], loss=14.4404
	step [82/190], loss=12.6777
	step [83/190], loss=13.9856
	step [84/190], loss=16.5892
	step [85/190], loss=14.2625
	step [86/190], loss=16.0012
	step [87/190], loss=15.0473
	step [88/190], loss=14.5867
	step [89/190], loss=17.4228
	step [90/190], loss=12.5689
	step [91/190], loss=15.9849
	step [92/190], loss=14.8680
	step [93/190], loss=15.3750
	step [94/190], loss=15.1920
	step [95/190], loss=13.2088
	step [96/190], loss=15.9704
	step [97/190], loss=12.6851
	step [98/190], loss=14.9854
	step [99/190], loss=14.1191
	step [100/190], loss=15.4994
	step [101/190], loss=15.9025
	step [102/190], loss=16.2461
	step [103/190], loss=12.8295
	step [104/190], loss=14.7611
	step [105/190], loss=15.3592
	step [106/190], loss=12.8619
	step [107/190], loss=12.6532
	step [108/190], loss=12.9904
	step [109/190], loss=14.8676
	step [110/190], loss=13.6295
	step [111/190], loss=13.8504
	step [112/190], loss=13.6903
	step [113/190], loss=14.0042
	step [114/190], loss=13.2905
	step [115/190], loss=13.4174
	step [116/190], loss=12.5869
	step [117/190], loss=13.4369
	step [118/190], loss=12.6224
	step [119/190], loss=13.1790
	step [120/190], loss=14.7722
	step [121/190], loss=18.0298
	step [122/190], loss=15.1729
	step [123/190], loss=15.9388
	step [124/190], loss=14.4130
	step [125/190], loss=13.0269
	step [126/190], loss=14.6728
	step [127/190], loss=13.8263
	step [128/190], loss=15.2519
	step [129/190], loss=14.1437
	step [130/190], loss=11.8349
	step [131/190], loss=14.2886
	step [132/190], loss=15.4523
	step [133/190], loss=13.3362
	step [134/190], loss=13.7368
	step [135/190], loss=12.9586
	step [136/190], loss=12.4839
	step [137/190], loss=16.2328
	step [138/190], loss=14.1219
	step [139/190], loss=11.2649
	step [140/190], loss=13.5986
	step [141/190], loss=13.3317
	step [142/190], loss=13.2137
	step [143/190], loss=14.0666
	step [144/190], loss=14.0206
	step [145/190], loss=14.5418
	step [146/190], loss=15.0948
	step [147/190], loss=12.9193
	step [148/190], loss=14.0350
	step [149/190], loss=16.9902
	step [150/190], loss=16.0291
	step [151/190], loss=15.2241
	step [152/190], loss=13.6234
	step [153/190], loss=14.4281
	step [154/190], loss=12.8617
	step [155/190], loss=12.0818
	step [156/190], loss=12.8465
	step [157/190], loss=15.0400
	step [158/190], loss=13.5953
	step [159/190], loss=13.9240
	step [160/190], loss=13.9450
	step [161/190], loss=16.1644
	step [162/190], loss=14.1073
	step [163/190], loss=12.4010
	step [164/190], loss=14.4198
	step [165/190], loss=11.9319
	step [166/190], loss=13.6449
	step [167/190], loss=13.9501
	step [168/190], loss=15.6376
	step [169/190], loss=12.7251
	step [170/190], loss=13.3054
	step [171/190], loss=12.1382
	step [172/190], loss=13.6163
	step [173/190], loss=13.1325
	step [174/190], loss=14.7239
	step [175/190], loss=13.7890
	step [176/190], loss=12.5771
	step [177/190], loss=13.1692
	step [178/190], loss=14.7016
	step [179/190], loss=11.3991
	step [180/190], loss=11.9899
	step [181/190], loss=14.8532
	step [182/190], loss=13.6408
	step [183/190], loss=13.8447
	step [184/190], loss=13.1405
	step [185/190], loss=14.2480
	step [186/190], loss=13.9555
	step [187/190], loss=15.5860
	step [188/190], loss=14.5043
	step [189/190], loss=14.8641
	step [190/190], loss=12.0302
	Evaluating
	loss=0.0404, precision=0.2148, recall=0.9935, f1=0.3532
Training epoch 11
	step [1/190], loss=14.2763
	step [2/190], loss=12.7779
	step [3/190], loss=13.2394
	step [4/190], loss=12.7254
	step [5/190], loss=12.9541
	step [6/190], loss=13.1248
	step [7/190], loss=14.0097
	step [8/190], loss=12.5774
	step [9/190], loss=13.7504
	step [10/190], loss=15.2588
	step [11/190], loss=14.2740
	step [12/190], loss=15.5402
	step [13/190], loss=12.6195
	step [14/190], loss=15.4481
	step [15/190], loss=14.4982
	step [16/190], loss=14.7113
	step [17/190], loss=13.1711
	step [18/190], loss=14.0868
	step [19/190], loss=13.0480
	step [20/190], loss=14.1544
	step [21/190], loss=12.0676
	step [22/190], loss=14.8053
	step [23/190], loss=13.5362
	step [24/190], loss=12.6233
	step [25/190], loss=13.5244
	step [26/190], loss=14.0829
	step [27/190], loss=12.0323
	step [28/190], loss=12.8321
	step [29/190], loss=14.2086
	step [30/190], loss=12.1514
	step [31/190], loss=14.2906
	step [32/190], loss=14.4698
	step [33/190], loss=12.1824
	step [34/190], loss=12.6886
	step [35/190], loss=12.9729
	step [36/190], loss=13.7695
	step [37/190], loss=14.1156
	step [38/190], loss=12.2999
	step [39/190], loss=15.0348
	step [40/190], loss=14.2889
	step [41/190], loss=13.5088
	step [42/190], loss=12.5910
	step [43/190], loss=12.3454
	step [44/190], loss=11.3425
	step [45/190], loss=11.4010
	step [46/190], loss=15.5092
	step [47/190], loss=14.1004
	step [48/190], loss=13.9498
	step [49/190], loss=14.2448
	step [50/190], loss=12.5933
	step [51/190], loss=14.1514
	step [52/190], loss=12.5832
	step [53/190], loss=14.0797
	step [54/190], loss=12.9043
	step [55/190], loss=12.8243
	step [56/190], loss=15.1112
	step [57/190], loss=14.1238
	step [58/190], loss=12.6972
	step [59/190], loss=12.5666
	step [60/190], loss=13.8419
	step [61/190], loss=12.3625
	step [62/190], loss=12.7096
	step [63/190], loss=13.3291
	step [64/190], loss=14.1869
	step [65/190], loss=13.3020
	step [66/190], loss=12.7655
	step [67/190], loss=12.2934
	step [68/190], loss=19.2314
	step [69/190], loss=13.9371
	step [70/190], loss=14.9518
	step [71/190], loss=11.9610
	step [72/190], loss=12.7056
	step [73/190], loss=14.8064
	step [74/190], loss=11.6314
	step [75/190], loss=12.3968
	step [76/190], loss=13.3292
	step [77/190], loss=12.6868
	step [78/190], loss=13.4976
	step [79/190], loss=13.9202
	step [80/190], loss=17.2512
	step [81/190], loss=12.8943
	step [82/190], loss=15.2343
	step [83/190], loss=11.8823
	step [84/190], loss=13.3678
	step [85/190], loss=12.9732
	step [86/190], loss=14.2539
	step [87/190], loss=11.4269
	step [88/190], loss=11.3293
	step [89/190], loss=12.0678
	step [90/190], loss=11.0925
	step [91/190], loss=13.9289
	step [92/190], loss=15.3947
	step [93/190], loss=14.2889
	step [94/190], loss=12.7039
	step [95/190], loss=13.2356
	step [96/190], loss=14.2808
	step [97/190], loss=12.8293
	step [98/190], loss=13.1432
	step [99/190], loss=12.4764
	step [100/190], loss=12.0230
	step [101/190], loss=11.1582
	step [102/190], loss=12.6220
	step [103/190], loss=12.3973
	step [104/190], loss=13.9340
	step [105/190], loss=14.3501
	step [106/190], loss=13.8810
	step [107/190], loss=13.2672
	step [108/190], loss=14.9466
	step [109/190], loss=12.4926
	step [110/190], loss=12.3941
	step [111/190], loss=13.1911
	step [112/190], loss=12.2656
	step [113/190], loss=15.5196
	step [114/190], loss=11.0669
	step [115/190], loss=11.5877
	step [116/190], loss=12.5108
	step [117/190], loss=13.0281
	step [118/190], loss=12.2319
	step [119/190], loss=12.6842
	step [120/190], loss=12.0159
	step [121/190], loss=13.3001
	step [122/190], loss=11.4574
	step [123/190], loss=14.1974
	step [124/190], loss=11.8389
	step [125/190], loss=12.4437
	step [126/190], loss=13.7785
	step [127/190], loss=12.9258
	step [128/190], loss=12.3609
	step [129/190], loss=16.4506
	step [130/190], loss=12.4812
	step [131/190], loss=12.7726
	step [132/190], loss=11.8925
	step [133/190], loss=13.0731
	step [134/190], loss=13.2623
	step [135/190], loss=10.6968
	step [136/190], loss=12.1254
	step [137/190], loss=12.1532
	step [138/190], loss=11.9536
	step [139/190], loss=13.3684
	step [140/190], loss=11.2923
	step [141/190], loss=12.2273
	step [142/190], loss=12.6086
	step [143/190], loss=11.5388
	step [144/190], loss=15.2581
	step [145/190], loss=12.9807
	step [146/190], loss=11.8083
	step [147/190], loss=13.3488
	step [148/190], loss=11.8971
	step [149/190], loss=13.4788
	step [150/190], loss=11.7830
	step [151/190], loss=11.5912
	step [152/190], loss=13.6984
	step [153/190], loss=12.1613
	step [154/190], loss=13.6106
	step [155/190], loss=11.9794
	step [156/190], loss=14.1808
	step [157/190], loss=11.8249
	step [158/190], loss=13.8389
	step [159/190], loss=10.7339
	step [160/190], loss=12.6554
	step [161/190], loss=14.5096
	step [162/190], loss=12.0219
	step [163/190], loss=13.0551
	step [164/190], loss=13.5969
	step [165/190], loss=13.4758
	step [166/190], loss=11.2062
	step [167/190], loss=13.7861
	step [168/190], loss=11.4465
	step [169/190], loss=12.8433
	step [170/190], loss=15.3948
	step [171/190], loss=12.3160
	step [172/190], loss=12.2237
	step [173/190], loss=12.3704
	step [174/190], loss=13.7101
	step [175/190], loss=13.6826
	step [176/190], loss=12.4536
	step [177/190], loss=12.8266
	step [178/190], loss=15.1129
	step [179/190], loss=11.8003
	step [180/190], loss=12.7613
	step [181/190], loss=13.8096
	step [182/190], loss=12.8339
	step [183/190], loss=14.0395
	step [184/190], loss=12.9522
	step [185/190], loss=11.0514
	step [186/190], loss=15.5786
	step [187/190], loss=10.7712
	step [188/190], loss=12.2056
	step [189/190], loss=11.5157
	step [190/190], loss=11.3465
	Evaluating
	loss=0.0343, precision=0.2612, recall=0.9904, f1=0.4134
saving model as: 1_saved_model.pth
Training epoch 12
	step [1/190], loss=11.1634
	step [2/190], loss=12.6795
	step [3/190], loss=13.3184
	step [4/190], loss=11.6886
	step [5/190], loss=12.0204
	step [6/190], loss=14.9772
	step [7/190], loss=12.0132
	step [8/190], loss=13.3603
	step [9/190], loss=12.1442
	step [10/190], loss=12.6844
	step [11/190], loss=12.9725
	step [12/190], loss=13.3614
	step [13/190], loss=14.8088
	step [14/190], loss=11.8407
	step [15/190], loss=13.1469
	step [16/190], loss=12.4386
	step [17/190], loss=11.9425
	step [18/190], loss=12.4128
	step [19/190], loss=12.5348
	step [20/190], loss=12.3338
	step [21/190], loss=14.1315
	step [22/190], loss=11.7456
	step [23/190], loss=12.2085
	step [24/190], loss=12.8276
	step [25/190], loss=12.4380
	step [26/190], loss=13.4000
	step [27/190], loss=11.2187
	step [28/190], loss=12.4296
	step [29/190], loss=12.7878
	step [30/190], loss=12.3772
	step [31/190], loss=12.4914
	step [32/190], loss=14.1641
	step [33/190], loss=10.8447
	step [34/190], loss=12.5470
	step [35/190], loss=12.1447
	step [36/190], loss=10.8405
	step [37/190], loss=11.9656
	step [38/190], loss=11.0382
	step [39/190], loss=13.7110
	step [40/190], loss=12.2995
	step [41/190], loss=11.9559
	step [42/190], loss=13.6282
	step [43/190], loss=12.0859
	step [44/190], loss=11.3946
	step [45/190], loss=13.9432
	step [46/190], loss=12.3885
	step [47/190], loss=15.1334
	step [48/190], loss=11.7184
	step [49/190], loss=11.1084
	step [50/190], loss=13.0809
	step [51/190], loss=13.2559
	step [52/190], loss=13.1624
	step [53/190], loss=13.9108
	step [54/190], loss=9.8056
	step [55/190], loss=14.1091
	step [56/190], loss=13.1276
	step [57/190], loss=11.0767
	step [58/190], loss=10.5820
	step [59/190], loss=11.1046
	step [60/190], loss=11.4470
	step [61/190], loss=11.6247
	step [62/190], loss=11.6731
	step [63/190], loss=12.8364
	step [64/190], loss=12.2168
	step [65/190], loss=13.3655
	step [66/190], loss=10.6399
	step [67/190], loss=10.1904
	step [68/190], loss=13.5991
	step [69/190], loss=12.6952
	step [70/190], loss=14.0739
	step [71/190], loss=13.1732
	step [72/190], loss=10.8609
	step [73/190], loss=13.3542
	step [74/190], loss=11.2068
	step [75/190], loss=10.9002
	step [76/190], loss=12.9473
	step [77/190], loss=14.4659
	step [78/190], loss=12.4248
	step [79/190], loss=15.6865
	step [80/190], loss=12.0672
	step [81/190], loss=10.9985
	step [82/190], loss=12.5119
	step [83/190], loss=12.6950
	step [84/190], loss=14.3276
	step [85/190], loss=13.1827
	step [86/190], loss=13.8002
	step [87/190], loss=11.2354
	step [88/190], loss=11.0613
	step [89/190], loss=12.1957
	step [90/190], loss=12.4592
	step [91/190], loss=11.1112
	step [92/190], loss=12.5548
	step [93/190], loss=11.6155
	step [94/190], loss=11.6556
	step [95/190], loss=13.8389
	step [96/190], loss=11.2745
	step [97/190], loss=13.1664
	step [98/190], loss=14.0737
	step [99/190], loss=11.3040
	step [100/190], loss=13.0162
	step [101/190], loss=12.5995
	step [102/190], loss=13.4658
	step [103/190], loss=13.6300
	step [104/190], loss=12.0087
	step [105/190], loss=13.0364
	step [106/190], loss=12.3200
	step [107/190], loss=11.4127
	step [108/190], loss=12.4977
	step [109/190], loss=11.2381
	step [110/190], loss=12.3940
	step [111/190], loss=11.0974
	step [112/190], loss=10.7441
	step [113/190], loss=11.3307
	step [114/190], loss=13.3307
	step [115/190], loss=11.4932
	step [116/190], loss=11.9617
	step [117/190], loss=13.5129
	step [118/190], loss=12.7203
	step [119/190], loss=13.3135
	step [120/190], loss=11.3307
	step [121/190], loss=12.4504
	step [122/190], loss=12.5094
	step [123/190], loss=10.3792
	step [124/190], loss=11.7346
	step [125/190], loss=11.2192
	step [126/190], loss=11.2578
	step [127/190], loss=11.2050
	step [128/190], loss=11.3443
	step [129/190], loss=11.9735
	step [130/190], loss=11.6648
	step [131/190], loss=10.9590
	step [132/190], loss=12.5276
	step [133/190], loss=11.0581
	step [134/190], loss=12.3127
	step [135/190], loss=14.0354
	step [136/190], loss=12.4763
	step [137/190], loss=12.1769
	step [138/190], loss=14.7385
	step [139/190], loss=10.6375
	step [140/190], loss=12.6283
	step [141/190], loss=11.5401
	step [142/190], loss=12.4588
	step [143/190], loss=12.7418
	step [144/190], loss=13.8423
	step [145/190], loss=11.7410
	step [146/190], loss=12.3969
	step [147/190], loss=13.7589
	step [148/190], loss=13.0523
	step [149/190], loss=12.6114
	step [150/190], loss=12.2298
	step [151/190], loss=13.2002
	step [152/190], loss=14.7197
	step [153/190], loss=13.0582
	step [154/190], loss=12.3185
	step [155/190], loss=11.5110
	step [156/190], loss=12.7457
	step [157/190], loss=14.9000
	step [158/190], loss=13.2630
	step [159/190], loss=10.3760
	step [160/190], loss=11.5517
	step [161/190], loss=11.8385
	step [162/190], loss=12.0430
	step [163/190], loss=10.0135
	step [164/190], loss=10.9760
	step [165/190], loss=12.9234
	step [166/190], loss=14.0529
	step [167/190], loss=11.4640
	step [168/190], loss=12.3604
	step [169/190], loss=13.2225
	step [170/190], loss=10.6580
	step [171/190], loss=12.9700
	step [172/190], loss=11.2143
	step [173/190], loss=11.9277
	step [174/190], loss=14.2425
	step [175/190], loss=11.5626
	step [176/190], loss=11.0233
	step [177/190], loss=14.4447
	step [178/190], loss=11.4978
	step [179/190], loss=12.6513
	step [180/190], loss=11.2250
	step [181/190], loss=13.3140
	step [182/190], loss=14.4502
	step [183/190], loss=12.2707
	step [184/190], loss=9.7498
	step [185/190], loss=12.4196
	step [186/190], loss=10.2680
	step [187/190], loss=12.6967
	step [188/190], loss=10.3523
	step [189/190], loss=12.2859
	step [190/190], loss=10.3422
	Evaluating
	loss=0.0337, precision=0.2301, recall=0.9935, f1=0.3737
Training epoch 13
	step [1/190], loss=13.0193
	step [2/190], loss=9.7542
	step [3/190], loss=11.2060
	step [4/190], loss=11.1041
	step [5/190], loss=13.1064
	step [6/190], loss=12.7047
	step [7/190], loss=12.9859
	step [8/190], loss=12.7103
	step [9/190], loss=13.0221
	step [10/190], loss=12.4529
	step [11/190], loss=10.9881
	step [12/190], loss=12.0450
	step [13/190], loss=11.4382
	step [14/190], loss=11.6599
	step [15/190], loss=14.1133
	step [16/190], loss=11.9722
	step [17/190], loss=12.6524
	step [18/190], loss=10.5063
	step [19/190], loss=13.0098
	step [20/190], loss=11.5814
	step [21/190], loss=10.1311
	step [22/190], loss=11.0311
	step [23/190], loss=10.0997
	step [24/190], loss=13.6783
	step [25/190], loss=11.4814
	step [26/190], loss=13.5119
	step [27/190], loss=11.0309
	step [28/190], loss=13.1324
	step [29/190], loss=11.1350
	step [30/190], loss=12.2816
	step [31/190], loss=14.0952
	step [32/190], loss=11.4848
	step [33/190], loss=11.7597
	step [34/190], loss=13.7667
	step [35/190], loss=12.7358
	step [36/190], loss=11.9681
	step [37/190], loss=10.6351
	step [38/190], loss=11.9315
	step [39/190], loss=12.7635
	step [40/190], loss=10.5001
	step [41/190], loss=10.7700
	step [42/190], loss=12.3501
	step [43/190], loss=10.4854
	step [44/190], loss=11.4969
	step [45/190], loss=12.6664
	step [46/190], loss=13.8274
	step [47/190], loss=10.3987
	step [48/190], loss=14.0463
	step [49/190], loss=11.0993
	step [50/190], loss=11.9736
	step [51/190], loss=12.8538
	step [52/190], loss=10.3055
	step [53/190], loss=12.7905
	step [54/190], loss=10.2321
	step [55/190], loss=8.8226
	step [56/190], loss=11.4809
	step [57/190], loss=12.1163
	step [58/190], loss=10.9180
	step [59/190], loss=11.9012
	step [60/190], loss=11.0229
	step [61/190], loss=10.7833
	step [62/190], loss=13.4203
	step [63/190], loss=10.7948
	step [64/190], loss=11.5930
	step [65/190], loss=11.8175
	step [66/190], loss=13.3053
	step [67/190], loss=10.7908
	step [68/190], loss=12.0871
	step [69/190], loss=13.7449
	step [70/190], loss=12.0597
	step [71/190], loss=12.2837
	step [72/190], loss=11.7204
	step [73/190], loss=12.2712
	step [74/190], loss=10.2018
	step [75/190], loss=10.5641
	step [76/190], loss=11.1291
	step [77/190], loss=12.7886
	step [78/190], loss=13.2720
	step [79/190], loss=10.3873
	step [80/190], loss=12.2368
	step [81/190], loss=12.5744
	step [82/190], loss=10.9203
	step [83/190], loss=10.8005
	step [84/190], loss=10.4308
	step [85/190], loss=10.3622
	step [86/190], loss=10.7812
	step [87/190], loss=10.7922
	step [88/190], loss=12.4047
	step [89/190], loss=11.9069
	step [90/190], loss=10.9705
	step [91/190], loss=11.7718
	step [92/190], loss=10.9756
	step [93/190], loss=14.2208
	step [94/190], loss=11.8152
	step [95/190], loss=11.7782
	step [96/190], loss=13.0587
	step [97/190], loss=11.0235
	step [98/190], loss=12.0852
	step [99/190], loss=13.0146
	step [100/190], loss=11.5677
	step [101/190], loss=10.5164
	step [102/190], loss=11.5007
	step [103/190], loss=12.6592
	step [104/190], loss=12.6572
	step [105/190], loss=10.9076
	step [106/190], loss=12.9177
	step [107/190], loss=11.6056
	step [108/190], loss=13.1992
	step [109/190], loss=11.9615
	step [110/190], loss=11.3765
	step [111/190], loss=10.3896
	step [112/190], loss=10.6234
	step [113/190], loss=12.0169
	step [114/190], loss=12.0275
	step [115/190], loss=10.4010
	step [116/190], loss=13.8781
	step [117/190], loss=9.6984
	step [118/190], loss=10.3597
	step [119/190], loss=12.5033
	step [120/190], loss=10.3445
	step [121/190], loss=10.9015
	step [122/190], loss=10.7078
	step [123/190], loss=11.7931
	step [124/190], loss=11.9976
	step [125/190], loss=11.6426
	step [126/190], loss=10.6647
	step [127/190], loss=9.5750
	step [128/190], loss=9.9592
	step [129/190], loss=11.3519
	step [130/190], loss=9.4716
	step [131/190], loss=12.7136
	step [132/190], loss=11.2296
	step [133/190], loss=9.1713
	step [134/190], loss=10.8162
	step [135/190], loss=12.5645
	step [136/190], loss=11.1249
	step [137/190], loss=12.0180
	step [138/190], loss=13.1922
	step [139/190], loss=11.3951
	step [140/190], loss=12.8779
	step [141/190], loss=13.6644
	step [142/190], loss=10.7710
	step [143/190], loss=10.5925
	step [144/190], loss=10.2802
	step [145/190], loss=11.1524
	step [146/190], loss=12.8532
	step [147/190], loss=11.4084
	step [148/190], loss=10.2116
	step [149/190], loss=9.6627
	step [150/190], loss=12.2637
	step [151/190], loss=13.6446
	step [152/190], loss=11.2419
	step [153/190], loss=10.6915
	step [154/190], loss=11.2364
	step [155/190], loss=11.7171
	step [156/190], loss=11.4877
	step [157/190], loss=11.6953
	step [158/190], loss=12.3848
	step [159/190], loss=12.4938
	step [160/190], loss=11.5857
	step [161/190], loss=11.4961
	step [162/190], loss=10.6358
	step [163/190], loss=12.1011
	step [164/190], loss=11.5851
	step [165/190], loss=13.8425
	step [166/190], loss=10.8127
	step [167/190], loss=9.9431
	step [168/190], loss=13.4647
	step [169/190], loss=11.1478
	step [170/190], loss=11.5247
	step [171/190], loss=12.3594
	step [172/190], loss=10.6845
	step [173/190], loss=11.6394
	step [174/190], loss=10.7914
	step [175/190], loss=11.7807
	step [176/190], loss=12.2440
	step [177/190], loss=10.9166
	step [178/190], loss=12.3919
	step [179/190], loss=9.3726
	step [180/190], loss=12.1248
	step [181/190], loss=10.2904
	step [182/190], loss=13.6749
	step [183/190], loss=10.7692
	step [184/190], loss=12.2510
	step [185/190], loss=10.8433
	step [186/190], loss=11.1099
	step [187/190], loss=10.1660
	step [188/190], loss=9.5349
	step [189/190], loss=10.3702
	step [190/190], loss=8.5579
	Evaluating
	loss=0.0306, precision=0.2475, recall=0.9916, f1=0.3962
Training epoch 14
	step [1/190], loss=12.3856
	step [2/190], loss=8.6549
	step [3/190], loss=11.8700
	step [4/190], loss=12.7507
	step [5/190], loss=10.3922
	step [6/190], loss=12.2392
	step [7/190], loss=11.7649
	step [8/190], loss=10.4069
	step [9/190], loss=11.5605
	step [10/190], loss=11.1919
	step [11/190], loss=11.8568
	step [12/190], loss=10.9677
	step [13/190], loss=10.5895
	step [14/190], loss=12.8253
	step [15/190], loss=11.1767
	step [16/190], loss=9.8213
	step [17/190], loss=12.1064
	step [18/190], loss=11.4778
	step [19/190], loss=10.1746
	step [20/190], loss=12.4039
	step [21/190], loss=11.9281
	step [22/190], loss=10.0913
	step [23/190], loss=12.9990
	step [24/190], loss=12.7094
	step [25/190], loss=12.4814
	step [26/190], loss=8.6909
	step [27/190], loss=11.9712
	step [28/190], loss=10.8567
	step [29/190], loss=9.8469
	step [30/190], loss=8.6115
	step [31/190], loss=11.4148
	step [32/190], loss=13.2679
	step [33/190], loss=10.7023
	step [34/190], loss=12.3175
	step [35/190], loss=10.6263
	step [36/190], loss=11.1724
	step [37/190], loss=10.7112
	step [38/190], loss=11.1745
	step [39/190], loss=10.3004
	step [40/190], loss=8.1424
	step [41/190], loss=10.8121
	step [42/190], loss=12.5883
	step [43/190], loss=9.1965
	step [44/190], loss=13.3769
	step [45/190], loss=9.5162
	step [46/190], loss=11.7574
	step [47/190], loss=16.4445
	step [48/190], loss=12.1222
	step [49/190], loss=10.9339
	step [50/190], loss=11.1408
	step [51/190], loss=11.5347
	step [52/190], loss=12.5158
	step [53/190], loss=9.9828
	step [54/190], loss=10.8079
	step [55/190], loss=10.9708
	step [56/190], loss=12.2786
	step [57/190], loss=12.8269
	step [58/190], loss=10.1589
	step [59/190], loss=10.4414
	step [60/190], loss=11.9466
	step [61/190], loss=12.0351
	step [62/190], loss=10.9733
	step [63/190], loss=13.3387
	step [64/190], loss=10.4449
	step [65/190], loss=10.3211
	step [66/190], loss=10.6938
	step [67/190], loss=10.8206
	step [68/190], loss=13.4429
	step [69/190], loss=12.5251
	step [70/190], loss=10.5424
	step [71/190], loss=10.9690
	step [72/190], loss=12.7931
	step [73/190], loss=11.0311
	step [74/190], loss=11.4527
	step [75/190], loss=11.9242
	step [76/190], loss=9.7674
	step [77/190], loss=11.0871
	step [78/190], loss=10.2693
	step [79/190], loss=12.4119
	step [80/190], loss=11.0196
	step [81/190], loss=9.7562
	step [82/190], loss=13.1274
	step [83/190], loss=10.0740
	step [84/190], loss=11.9470
	step [85/190], loss=9.7671
	step [86/190], loss=10.2404
	step [87/190], loss=9.6262
	step [88/190], loss=10.8984
	step [89/190], loss=11.0867
	step [90/190], loss=9.9748
	step [91/190], loss=11.2020
	step [92/190], loss=10.0216
	step [93/190], loss=13.5201
	step [94/190], loss=13.5342
	step [95/190], loss=11.3225
	step [96/190], loss=9.9156
	step [97/190], loss=12.6121
	step [98/190], loss=12.0129
	step [99/190], loss=11.9465
	step [100/190], loss=8.5988
	step [101/190], loss=11.1348
	step [102/190], loss=11.7323
	step [103/190], loss=10.4824
	step [104/190], loss=13.0607
	step [105/190], loss=10.8329
	step [106/190], loss=11.4336
	step [107/190], loss=12.1120
	step [108/190], loss=10.6988
	step [109/190], loss=11.2637
	step [110/190], loss=11.6929
	step [111/190], loss=11.3375
	step [112/190], loss=11.4592
	step [113/190], loss=9.1181
	step [114/190], loss=9.1928
	step [115/190], loss=9.3028
	step [116/190], loss=10.1240
	step [117/190], loss=9.8094
	step [118/190], loss=11.9299
	step [119/190], loss=12.0231
	step [120/190], loss=10.4971
	step [121/190], loss=10.5422
	step [122/190], loss=11.2090
	step [123/190], loss=10.4151
	step [124/190], loss=9.7319
	step [125/190], loss=10.1386
	step [126/190], loss=11.7959
	step [127/190], loss=10.7152
	step [128/190], loss=11.1944
	step [129/190], loss=12.4555
	step [130/190], loss=9.6615
	step [131/190], loss=10.4916
	step [132/190], loss=10.5904
	step [133/190], loss=11.8641
	step [134/190], loss=9.7913
	step [135/190], loss=9.5534
	step [136/190], loss=12.0736
	step [137/190], loss=10.0218
	step [138/190], loss=10.7790
	step [139/190], loss=13.0371
	step [140/190], loss=10.1413
	step [141/190], loss=11.9033
	step [142/190], loss=10.0687
	step [143/190], loss=10.7618
	step [144/190], loss=9.7996
	step [145/190], loss=12.6106
	step [146/190], loss=10.3207
	step [147/190], loss=10.6331
	step [148/190], loss=11.8874
	step [149/190], loss=11.2688
	step [150/190], loss=10.1174
	step [151/190], loss=11.6712
	step [152/190], loss=9.9911
	step [153/190], loss=9.3418
	step [154/190], loss=9.9875
	step [155/190], loss=11.7959
	step [156/190], loss=11.2965
	step [157/190], loss=10.9054
	step [158/190], loss=11.5204
	step [159/190], loss=11.9966
	step [160/190], loss=12.4875
	step [161/190], loss=10.7344
	step [162/190], loss=11.3539
	step [163/190], loss=10.6786
	step [164/190], loss=9.7347
	step [165/190], loss=9.8270
	step [166/190], loss=9.9400
	step [167/190], loss=12.0308
	step [168/190], loss=12.8630
	step [169/190], loss=11.4363
	step [170/190], loss=10.0108
	step [171/190], loss=12.4779
	step [172/190], loss=10.1084
	step [173/190], loss=10.7535
	step [174/190], loss=9.0796
	step [175/190], loss=10.7750
	step [176/190], loss=8.5029
	step [177/190], loss=13.0848
	step [178/190], loss=11.6240
	step [179/190], loss=11.8425
	step [180/190], loss=11.4345
	step [181/190], loss=10.8902
	step [182/190], loss=10.8973
	step [183/190], loss=11.1571
	step [184/190], loss=10.1638
	step [185/190], loss=10.7006
	step [186/190], loss=11.6856
	step [187/190], loss=10.4512
	step [188/190], loss=12.0446
	step [189/190], loss=13.1014
	step [190/190], loss=10.0812
	Evaluating
	loss=0.0288, precision=0.2332, recall=0.9925, f1=0.3776
Training epoch 15
	step [1/190], loss=12.2822
	step [2/190], loss=12.4686
	step [3/190], loss=9.8841
	step [4/190], loss=9.8909
	step [5/190], loss=9.4787
	step [6/190], loss=9.5527
	step [7/190], loss=12.3053
	step [8/190], loss=10.1902
	step [9/190], loss=10.9136
	step [10/190], loss=11.9771
	step [11/190], loss=10.1688
	step [12/190], loss=11.2348
	step [13/190], loss=11.0859
	step [14/190], loss=9.9957
	step [15/190], loss=10.4088
	step [16/190], loss=9.7156
	step [17/190], loss=9.9869
	step [18/190], loss=12.0586
	step [19/190], loss=11.4937
	step [20/190], loss=11.1307
	step [21/190], loss=10.6049
	step [22/190], loss=10.4961
	step [23/190], loss=11.9847
	step [24/190], loss=9.7500
	step [25/190], loss=10.5265
	step [26/190], loss=10.8725
	step [27/190], loss=10.8101
	step [28/190], loss=10.3316
	step [29/190], loss=11.5451
	step [30/190], loss=10.3125
	step [31/190], loss=11.1624
	step [32/190], loss=10.0707
	step [33/190], loss=11.6244
	step [34/190], loss=10.8693
	step [35/190], loss=11.0273
	step [36/190], loss=9.6513
	step [37/190], loss=10.4004
	step [38/190], loss=10.4116
	step [39/190], loss=14.4158
	step [40/190], loss=9.7698
	step [41/190], loss=11.3527
	step [42/190], loss=10.3663
	step [43/190], loss=9.5617
	step [44/190], loss=11.2564
	step [45/190], loss=9.8069
	step [46/190], loss=10.8097
	step [47/190], loss=10.4850
	step [48/190], loss=10.0343
	step [49/190], loss=10.3484
	step [50/190], loss=10.1715
	step [51/190], loss=10.4996
	step [52/190], loss=10.8566
	step [53/190], loss=12.2422
	step [54/190], loss=10.8385
	step [55/190], loss=9.9874
	step [56/190], loss=11.2365
	step [57/190], loss=10.9153
	step [58/190], loss=9.1306
	step [59/190], loss=9.8328
	step [60/190], loss=10.4346
	step [61/190], loss=9.3169
	step [62/190], loss=10.5886
	step [63/190], loss=11.5389
	step [64/190], loss=10.1632
	step [65/190], loss=10.8185
	step [66/190], loss=11.5613
	step [67/190], loss=9.3441
	step [68/190], loss=9.7212
	step [69/190], loss=11.1648
	step [70/190], loss=9.1151
	step [71/190], loss=11.0101
	step [72/190], loss=10.0659
	step [73/190], loss=9.1609
	step [74/190], loss=10.2814
	step [75/190], loss=9.9049
	step [76/190], loss=11.6803
	step [77/190], loss=9.1833
	step [78/190], loss=12.4652
	step [79/190], loss=11.8547
	step [80/190], loss=11.5838
	step [81/190], loss=10.8834
	step [82/190], loss=9.2544
	step [83/190], loss=11.8921
	step [84/190], loss=8.4441
	step [85/190], loss=10.0718
	step [86/190], loss=9.8987
	step [87/190], loss=10.3191
	step [88/190], loss=9.5125
	step [89/190], loss=10.4317
	step [90/190], loss=9.9570
	step [91/190], loss=11.3412
	step [92/190], loss=9.4504
	step [93/190], loss=9.2572
	step [94/190], loss=9.4553
	step [95/190], loss=9.4608
	step [96/190], loss=9.6911
	step [97/190], loss=10.5218
	step [98/190], loss=12.0355
	step [99/190], loss=10.6287
	step [100/190], loss=10.7959
	step [101/190], loss=10.6259
	step [102/190], loss=11.1336
	step [103/190], loss=11.0701
	step [104/190], loss=9.5269
	step [105/190], loss=10.7502
	step [106/190], loss=10.7517
	step [107/190], loss=9.5627
	step [108/190], loss=10.5201
	step [109/190], loss=10.5862
	step [110/190], loss=9.2088
	step [111/190], loss=9.9991
	step [112/190], loss=9.4690
	step [113/190], loss=13.2775
	step [114/190], loss=10.2076
	step [115/190], loss=9.4184
	step [116/190], loss=10.8418
	step [117/190], loss=9.5621
	step [118/190], loss=11.9948
	step [119/190], loss=14.9203
	step [120/190], loss=11.1522
	step [121/190], loss=10.3431
	step [122/190], loss=9.5606
	step [123/190], loss=11.2920
	step [124/190], loss=8.7607
	step [125/190], loss=8.7204
	step [126/190], loss=12.4232
	step [127/190], loss=10.3425
	step [128/190], loss=10.8039
	step [129/190], loss=9.9860
	step [130/190], loss=8.9543
	step [131/190], loss=13.3378
	step [132/190], loss=8.6478
	step [133/190], loss=10.1427
	step [134/190], loss=10.7847
	step [135/190], loss=9.0936
	step [136/190], loss=11.6693
	step [137/190], loss=8.8017
	step [138/190], loss=10.0569
	step [139/190], loss=8.8793
	step [140/190], loss=10.6674
	step [141/190], loss=8.9689
	step [142/190], loss=9.4386
	step [143/190], loss=8.8436
	step [144/190], loss=8.9761
	step [145/190], loss=11.7687
	step [146/190], loss=9.0890
	step [147/190], loss=11.5583
	step [148/190], loss=8.8244
	step [149/190], loss=12.1963
	step [150/190], loss=10.2903
	step [151/190], loss=10.3901
	step [152/190], loss=12.0324
	step [153/190], loss=11.6789
	step [154/190], loss=10.1852
	step [155/190], loss=10.8543
	step [156/190], loss=10.5621
	step [157/190], loss=9.7422
	step [158/190], loss=8.3070
	step [159/190], loss=11.1433
	step [160/190], loss=9.4402
	step [161/190], loss=9.6134
	step [162/190], loss=10.1746
	step [163/190], loss=10.0728
	step [164/190], loss=9.0892
	step [165/190], loss=10.4516
	step [166/190], loss=10.9068
	step [167/190], loss=11.9011
	step [168/190], loss=10.7450
	step [169/190], loss=9.6050
	step [170/190], loss=9.0085
	step [171/190], loss=9.9009
	step [172/190], loss=10.2107
	step [173/190], loss=9.5603
	step [174/190], loss=10.2874
	step [175/190], loss=10.9384
	step [176/190], loss=10.3059
	step [177/190], loss=12.3623
	step [178/190], loss=9.1148
	step [179/190], loss=12.3493
	step [180/190], loss=10.1765
	step [181/190], loss=11.3824
	step [182/190], loss=12.3721
	step [183/190], loss=10.7405
	step [184/190], loss=10.5625
	step [185/190], loss=12.1227
	step [186/190], loss=12.7834
	step [187/190], loss=10.6977
	step [188/190], loss=10.3283
	step [189/190], loss=10.0000
	step [190/190], loss=10.8307
	Evaluating
	loss=0.0291, precision=0.2193, recall=0.9940, f1=0.3593
Training epoch 16
	step [1/190], loss=12.8319
	step [2/190], loss=10.4882
	step [3/190], loss=10.4975
	step [4/190], loss=9.5472
	step [5/190], loss=9.2294
	step [6/190], loss=8.9673
	step [7/190], loss=10.7942
	step [8/190], loss=10.7177
	step [9/190], loss=10.7661
	step [10/190], loss=9.1477
	step [11/190], loss=9.5455
	step [12/190], loss=10.2236
	step [13/190], loss=12.1536
	step [14/190], loss=9.4241
	step [15/190], loss=10.3688
	step [16/190], loss=11.6156
	step [17/190], loss=10.6690
	step [18/190], loss=10.2133
	step [19/190], loss=9.5665
	step [20/190], loss=10.1562
	step [21/190], loss=9.4041
	step [22/190], loss=10.8056
	step [23/190], loss=9.5757
	step [24/190], loss=10.6886
	step [25/190], loss=9.1242
	step [26/190], loss=10.9711
	step [27/190], loss=10.5891
	step [28/190], loss=8.7945
	step [29/190], loss=8.7667
	step [30/190], loss=9.2991
	step [31/190], loss=10.1085
	step [32/190], loss=10.2348
	step [33/190], loss=9.7009
	step [34/190], loss=9.8896
	step [35/190], loss=9.6101
	step [36/190], loss=8.7221
	step [37/190], loss=9.7276
	step [38/190], loss=9.6639
	step [39/190], loss=11.6200
	step [40/190], loss=10.9325
	step [41/190], loss=11.2861
	step [42/190], loss=10.1654
	step [43/190], loss=8.9555
	step [44/190], loss=8.9974
	step [45/190], loss=9.0425
	step [46/190], loss=9.1059
	step [47/190], loss=9.6858
	step [48/190], loss=10.2577
	step [49/190], loss=11.1134
	step [50/190], loss=9.7147
	step [51/190], loss=9.7223
	step [52/190], loss=10.0809
	step [53/190], loss=9.6342
	step [54/190], loss=8.9093
	step [55/190], loss=10.0256
	step [56/190], loss=7.8525
	step [57/190], loss=12.6988
	step [58/190], loss=9.9338
	step [59/190], loss=9.8135
	step [60/190], loss=9.7843
	step [61/190], loss=10.8248
	step [62/190], loss=10.0902
	step [63/190], loss=9.0700
	step [64/190], loss=10.5383
	step [65/190], loss=9.5351
	step [66/190], loss=10.2629
	step [67/190], loss=8.9799
	step [68/190], loss=10.3557
	step [69/190], loss=11.2851
	step [70/190], loss=10.7864
	step [71/190], loss=8.7508
	step [72/190], loss=10.5803
	step [73/190], loss=9.9197
	step [74/190], loss=14.5629
	step [75/190], loss=11.1313
	step [76/190], loss=9.1792
	step [77/190], loss=9.8532
	step [78/190], loss=11.4312
	step [79/190], loss=11.6266
	step [80/190], loss=11.8711
	step [81/190], loss=10.5853
	step [82/190], loss=9.3564
	step [83/190], loss=10.0764
	step [84/190], loss=9.2294
	step [85/190], loss=9.9430
	step [86/190], loss=12.6243
	step [87/190], loss=10.4727
	step [88/190], loss=10.4105
	step [89/190], loss=10.2122
	step [90/190], loss=10.1941
	step [91/190], loss=12.5637
	step [92/190], loss=11.8184
	step [93/190], loss=9.4286
	step [94/190], loss=9.7138
	step [95/190], loss=9.8845
	step [96/190], loss=8.8207
	step [97/190], loss=10.6793
	step [98/190], loss=9.3449
	step [99/190], loss=11.3365
	step [100/190], loss=11.7258
	step [101/190], loss=10.2975
	step [102/190], loss=9.1749
	step [103/190], loss=8.8800
	step [104/190], loss=11.7315
	step [105/190], loss=8.7426
	step [106/190], loss=11.0258
	step [107/190], loss=10.1949
	step [108/190], loss=11.2587
	step [109/190], loss=9.1180
	step [110/190], loss=12.6557
	step [111/190], loss=11.0097
	step [112/190], loss=10.8187
	step [113/190], loss=11.1563
	step [114/190], loss=10.1534
	step [115/190], loss=10.0740
	step [116/190], loss=8.9495
	step [117/190], loss=10.0457
	step [118/190], loss=9.1320
	step [119/190], loss=8.9595
	step [120/190], loss=8.2012
	step [121/190], loss=10.2933
	step [122/190], loss=11.1736
	step [123/190], loss=9.6084
	step [124/190], loss=9.6538
	step [125/190], loss=8.8361
	step [126/190], loss=10.6706
	step [127/190], loss=7.7759
	step [128/190], loss=12.0201
	step [129/190], loss=10.4647
	step [130/190], loss=9.0911
	step [131/190], loss=10.3196
	step [132/190], loss=7.7629
	step [133/190], loss=9.1985
	step [134/190], loss=9.5730
	step [135/190], loss=10.2996
	step [136/190], loss=10.1442
	step [137/190], loss=9.3460
	step [138/190], loss=10.0086
	step [139/190], loss=9.1664
	step [140/190], loss=11.2857
	step [141/190], loss=11.0247
	step [142/190], loss=11.5319
	step [143/190], loss=12.1194
	step [144/190], loss=9.4687
	step [145/190], loss=9.4134
	step [146/190], loss=10.0907
	step [147/190], loss=8.8938
	step [148/190], loss=10.3093
	step [149/190], loss=9.3964
	step [150/190], loss=10.0136
	step [151/190], loss=9.0572
	step [152/190], loss=9.1228
	step [153/190], loss=10.2171
	step [154/190], loss=10.2387
	step [155/190], loss=9.4496
	step [156/190], loss=10.6942
	step [157/190], loss=8.0091
	step [158/190], loss=10.6579
	step [159/190], loss=10.0279
	step [160/190], loss=8.7550
	step [161/190], loss=9.3223
	step [162/190], loss=8.6480
	step [163/190], loss=10.1128
	step [164/190], loss=12.8817
	step [165/190], loss=12.0963
	step [166/190], loss=9.1981
	step [167/190], loss=10.6891
	step [168/190], loss=9.7730
	step [169/190], loss=10.2958
	step [170/190], loss=9.3480
	step [171/190], loss=8.8647
	step [172/190], loss=8.6569
	step [173/190], loss=8.4386
	step [174/190], loss=12.1034
	step [175/190], loss=10.7174
	step [176/190], loss=11.1665
	step [177/190], loss=10.7104
	step [178/190], loss=10.9490
	step [179/190], loss=10.0392
	step [180/190], loss=10.1478
	step [181/190], loss=9.9212
	step [182/190], loss=9.3227
	step [183/190], loss=10.3080
	step [184/190], loss=10.4642
	step [185/190], loss=11.0923
	step [186/190], loss=9.9918
	step [187/190], loss=10.0794
	step [188/190], loss=9.0513
	step [189/190], loss=11.1954
	step [190/190], loss=8.7112
	Evaluating
	loss=0.0343, precision=0.1776, recall=0.9955, f1=0.3015
Training epoch 17
	step [1/190], loss=10.2503
	step [2/190], loss=9.3322
	step [3/190], loss=9.0031
	step [4/190], loss=10.2841
	step [5/190], loss=11.5397
	step [6/190], loss=8.2957
	step [7/190], loss=8.9967
	step [8/190], loss=11.1214
	step [9/190], loss=11.6517
	step [10/190], loss=8.1541
	step [11/190], loss=10.2316
	step [12/190], loss=9.2126
	step [13/190], loss=10.6135
	step [14/190], loss=9.9327
	step [15/190], loss=10.9932
	step [16/190], loss=9.6175
	step [17/190], loss=10.4648
	step [18/190], loss=8.4801
	step [19/190], loss=12.3850
	step [20/190], loss=8.6689
	step [21/190], loss=10.4883
	step [22/190], loss=10.5595
	step [23/190], loss=9.3423
	step [24/190], loss=9.6111
	step [25/190], loss=9.9276
	step [26/190], loss=9.9348
	step [27/190], loss=9.7020
	step [28/190], loss=9.6133
	step [29/190], loss=10.4533
	step [30/190], loss=11.4088
	step [31/190], loss=8.9136
	step [32/190], loss=10.0479
	step [33/190], loss=9.7847
	step [34/190], loss=9.1313
	step [35/190], loss=10.1903
	step [36/190], loss=10.4222
	step [37/190], loss=9.9380
	step [38/190], loss=8.9405
	step [39/190], loss=11.0209
	step [40/190], loss=9.4436
	step [41/190], loss=9.6568
	step [42/190], loss=11.2423
	step [43/190], loss=9.9030
	step [44/190], loss=8.0149
	step [45/190], loss=10.4112
	step [46/190], loss=9.1144
	step [47/190], loss=10.1948
	step [48/190], loss=9.0561
	step [49/190], loss=9.4716
	step [50/190], loss=10.9077
	step [51/190], loss=11.6827
	step [52/190], loss=8.6198
	step [53/190], loss=10.4877
	step [54/190], loss=9.0426
	step [55/190], loss=9.6303
	step [56/190], loss=9.4164
	step [57/190], loss=9.5778
	step [58/190], loss=9.5195
	step [59/190], loss=10.4685
	step [60/190], loss=8.6996
	step [61/190], loss=11.2049
	step [62/190], loss=10.3000
	step [63/190], loss=8.1374
	step [64/190], loss=9.2309
	step [65/190], loss=9.4878
	step [66/190], loss=10.8586
	step [67/190], loss=8.7193
	step [68/190], loss=8.4502
	step [69/190], loss=10.9313
	step [70/190], loss=8.7545
	step [71/190], loss=9.9158
	step [72/190], loss=10.0206
	step [73/190], loss=12.6788
	step [74/190], loss=9.0946
	step [75/190], loss=9.4864
	step [76/190], loss=11.2960
	step [77/190], loss=8.3278
	step [78/190], loss=9.7294
	step [79/190], loss=8.8697
	step [80/190], loss=9.0576
	step [81/190], loss=11.0699
	step [82/190], loss=11.2622
	step [83/190], loss=10.7220
	step [84/190], loss=9.4110
	step [85/190], loss=9.8902
	step [86/190], loss=9.3015
	step [87/190], loss=10.6036
	step [88/190], loss=9.2393
	step [89/190], loss=9.7477
	step [90/190], loss=9.7907
	step [91/190], loss=10.6032
	step [92/190], loss=8.9558
	step [93/190], loss=10.3553
	step [94/190], loss=9.4295
	step [95/190], loss=8.9465
	step [96/190], loss=8.8018
	step [97/190], loss=10.3855
	step [98/190], loss=8.7455
	step [99/190], loss=10.0995
	step [100/190], loss=10.7773
	step [101/190], loss=8.7417
	step [102/190], loss=9.5012
	step [103/190], loss=10.2810
	step [104/190], loss=10.0189
	step [105/190], loss=8.7724
	step [106/190], loss=10.2584
	step [107/190], loss=8.3390
	step [108/190], loss=8.9207
	step [109/190], loss=11.1710
	step [110/190], loss=9.4008
	step [111/190], loss=9.6181
	step [112/190], loss=9.5216
	step [113/190], loss=10.6577
	step [114/190], loss=9.9445
	step [115/190], loss=8.4130
	step [116/190], loss=11.0795
	step [117/190], loss=8.8418
	step [118/190], loss=9.8056
	step [119/190], loss=7.5268
	step [120/190], loss=9.8281
	step [121/190], loss=8.3344
	step [122/190], loss=11.7365
	step [123/190], loss=9.7468
	step [124/190], loss=10.1033
	step [125/190], loss=9.4818
	step [126/190], loss=9.5905
	step [127/190], loss=8.8410
	step [128/190], loss=9.9344
	step [129/190], loss=10.3324
	step [130/190], loss=10.0770
	step [131/190], loss=9.6652
	step [132/190], loss=10.8877
	step [133/190], loss=8.7916
	step [134/190], loss=11.2381
	step [135/190], loss=9.4790
	step [136/190], loss=10.9093
	step [137/190], loss=9.2110
	step [138/190], loss=10.5392
	step [139/190], loss=9.5725
	step [140/190], loss=10.5201
	step [141/190], loss=8.5247
	step [142/190], loss=9.5450
	step [143/190], loss=9.2892
	step [144/190], loss=9.7048
	step [145/190], loss=10.0032
	step [146/190], loss=9.3992
	step [147/190], loss=9.5031
	step [148/190], loss=8.7951
	step [149/190], loss=8.0039
	step [150/190], loss=7.7955
	step [151/190], loss=9.9141
	step [152/190], loss=10.2651
	step [153/190], loss=9.7862
	step [154/190], loss=9.7147
	step [155/190], loss=10.4347
	step [156/190], loss=8.3093
	step [157/190], loss=9.4513
	step [158/190], loss=9.6160
	step [159/190], loss=10.2888
	step [160/190], loss=9.7196
	step [161/190], loss=8.3080
	step [162/190], loss=7.4316
	step [163/190], loss=9.9153
	step [164/190], loss=8.3972
	step [165/190], loss=8.6364
	step [166/190], loss=10.3838
	step [167/190], loss=11.3962
	step [168/190], loss=9.3770
	step [169/190], loss=9.7535
	step [170/190], loss=10.2012
	step [171/190], loss=8.5419
	step [172/190], loss=11.1661
	step [173/190], loss=8.2560
	step [174/190], loss=8.9066
	step [175/190], loss=9.5088
	step [176/190], loss=9.3785
	step [177/190], loss=8.8720
	step [178/190], loss=10.4976
	step [179/190], loss=8.6401
	step [180/190], loss=8.7227
	step [181/190], loss=9.0463
	step [182/190], loss=9.2718
	step [183/190], loss=8.7056
	step [184/190], loss=8.3167
	step [185/190], loss=9.8005
	step [186/190], loss=9.1138
	step [187/190], loss=10.3017
	step [188/190], loss=8.6707
	step [189/190], loss=8.3416
	step [190/190], loss=8.0563
	Evaluating
	loss=0.0222, precision=0.2753, recall=0.9891, f1=0.4307
saving model as: 1_saved_model.pth
Training epoch 18
	step [1/190], loss=11.6666
	step [2/190], loss=10.7757
	step [3/190], loss=8.4579
	step [4/190], loss=8.8567
	step [5/190], loss=8.4255
	step [6/190], loss=7.3594
	step [7/190], loss=9.7008
	step [8/190], loss=8.1900
	step [9/190], loss=8.5813
	step [10/190], loss=10.5156
	step [11/190], loss=9.5925
	step [12/190], loss=11.4007
	step [13/190], loss=9.8584
	step [14/190], loss=7.8367
	step [15/190], loss=10.2107
	step [16/190], loss=8.4198
	step [17/190], loss=10.2944
	step [18/190], loss=9.2242
	step [19/190], loss=9.9363
	step [20/190], loss=8.6794
	step [21/190], loss=11.7282
	step [22/190], loss=7.2679
	step [23/190], loss=8.5271
	step [24/190], loss=8.5606
	step [25/190], loss=9.1060
	step [26/190], loss=9.6557
	step [27/190], loss=10.5536
	step [28/190], loss=9.6368
	step [29/190], loss=10.6647
	step [30/190], loss=9.4834
	step [31/190], loss=8.6084
	step [32/190], loss=9.2765
	step [33/190], loss=9.1324
	step [34/190], loss=8.7238
	step [35/190], loss=9.3084
	step [36/190], loss=12.5624
	step [37/190], loss=9.6371
	step [38/190], loss=8.2156
	step [39/190], loss=9.2092
	step [40/190], loss=10.3883
	step [41/190], loss=8.7099
	step [42/190], loss=9.2656
	step [43/190], loss=9.2261
	step [44/190], loss=9.5980
	step [45/190], loss=8.7069
	step [46/190], loss=9.5173
	step [47/190], loss=8.1748
	step [48/190], loss=7.5703
	step [49/190], loss=9.1524
	step [50/190], loss=10.0628
	step [51/190], loss=9.8709
	step [52/190], loss=9.3310
	step [53/190], loss=9.5595
	step [54/190], loss=8.5178
	step [55/190], loss=9.7818
	step [56/190], loss=10.3879
	step [57/190], loss=8.5303
	step [58/190], loss=8.2850
	step [59/190], loss=9.4738
	step [60/190], loss=9.5429
	step [61/190], loss=8.6243
	step [62/190], loss=8.2291
	step [63/190], loss=6.7597
	step [64/190], loss=10.3540
	step [65/190], loss=10.4993
	step [66/190], loss=10.8168
	step [67/190], loss=9.1426
	step [68/190], loss=9.4193
	step [69/190], loss=8.6848
	step [70/190], loss=9.5284
	step [71/190], loss=9.6738
	step [72/190], loss=9.7948
	step [73/190], loss=8.7599
	step [74/190], loss=8.5450
	step [75/190], loss=8.7397
	step [76/190], loss=9.7366
	step [77/190], loss=10.2011
	step [78/190], loss=9.7094
	step [79/190], loss=8.9321
	step [80/190], loss=10.9711
	step [81/190], loss=7.8201
	step [82/190], loss=8.2769
	step [83/190], loss=9.3558
	step [84/190], loss=9.5236
	step [85/190], loss=9.4280
	step [86/190], loss=8.4248
	step [87/190], loss=10.3608
	step [88/190], loss=8.9977
	step [89/190], loss=9.2164
	step [90/190], loss=10.3660
	step [91/190], loss=9.9095
	step [92/190], loss=9.7140
	step [93/190], loss=8.9650
	step [94/190], loss=8.8964
	step [95/190], loss=9.8280
	step [96/190], loss=9.5406
	step [97/190], loss=9.9145
	step [98/190], loss=9.8176
	step [99/190], loss=10.8722
	step [100/190], loss=8.2798
	step [101/190], loss=8.5122
	step [102/190], loss=9.0928
	step [103/190], loss=9.0761
	step [104/190], loss=9.5156
	step [105/190], loss=10.3338
	step [106/190], loss=8.0405
	step [107/190], loss=8.5319
	step [108/190], loss=10.1710
	step [109/190], loss=11.5866
	step [110/190], loss=8.0068
	step [111/190], loss=8.4561
	step [112/190], loss=10.6096
	step [113/190], loss=10.0396
	step [114/190], loss=8.9883
	step [115/190], loss=10.2905
	step [116/190], loss=9.3963
	step [117/190], loss=9.2135
	step [118/190], loss=9.1011
	step [119/190], loss=11.2874
	step [120/190], loss=10.0376
	step [121/190], loss=8.6605
	step [122/190], loss=9.0792
	step [123/190], loss=9.2771
	step [124/190], loss=8.9714
	step [125/190], loss=8.5497
	step [126/190], loss=8.2516
	step [127/190], loss=11.9939
	step [128/190], loss=9.4878
	step [129/190], loss=9.6639
	step [130/190], loss=8.1652
	step [131/190], loss=10.0856
	step [132/190], loss=9.5115
	step [133/190], loss=10.0468
	step [134/190], loss=9.9562
	step [135/190], loss=9.6976
	step [136/190], loss=9.3576
	step [137/190], loss=8.6486
	step [138/190], loss=8.9517
	step [139/190], loss=8.4727
	step [140/190], loss=8.2007
	step [141/190], loss=9.8563
	step [142/190], loss=10.7804
	step [143/190], loss=8.0142
	step [144/190], loss=7.6886
	step [145/190], loss=9.3999
	step [146/190], loss=9.5222
	step [147/190], loss=11.4418
	step [148/190], loss=10.7031
	step [149/190], loss=8.4109
	step [150/190], loss=8.7711
	step [151/190], loss=10.9933
	step [152/190], loss=9.3898
	step [153/190], loss=10.4128
	step [154/190], loss=7.9266
	step [155/190], loss=10.0498
	step [156/190], loss=10.5026
	step [157/190], loss=8.8758
	step [158/190], loss=8.1413
	step [159/190], loss=7.8885
	step [160/190], loss=8.6101
	step [161/190], loss=9.6001
	step [162/190], loss=10.3251
	step [163/190], loss=9.8430
	step [164/190], loss=9.7087
	step [165/190], loss=9.4513
	step [166/190], loss=9.6419
	step [167/190], loss=10.1324
	step [168/190], loss=9.0315
	step [169/190], loss=7.8205
	step [170/190], loss=8.5920
	step [171/190], loss=9.2784
	step [172/190], loss=9.3332
	step [173/190], loss=8.1319
	step [174/190], loss=8.8726
	step [175/190], loss=9.4625
	step [176/190], loss=9.6821
	step [177/190], loss=9.4656
	step [178/190], loss=8.9169
	step [179/190], loss=9.4144
	step [180/190], loss=8.3733
	step [181/190], loss=9.6035
	step [182/190], loss=9.2136
	step [183/190], loss=9.1128
	step [184/190], loss=9.9221
	step [185/190], loss=11.3649
	step [186/190], loss=8.5553
	step [187/190], loss=9.1879
	step [188/190], loss=8.3096
	step [189/190], loss=9.8475
	step [190/190], loss=9.0599
	Evaluating
	loss=0.0254, precision=0.2439, recall=0.9919, f1=0.3916
Training epoch 19
	step [1/190], loss=8.9694
	step [2/190], loss=8.6717
	step [3/190], loss=7.9423
	step [4/190], loss=7.6596
	step [5/190], loss=9.7734
	step [6/190], loss=9.2928
	step [7/190], loss=9.2476
	step [8/190], loss=9.7630
	step [9/190], loss=9.3178
	step [10/190], loss=11.0016
	step [11/190], loss=10.8517
	step [12/190], loss=9.2623
	step [13/190], loss=9.0249
	step [14/190], loss=10.1664
	step [15/190], loss=7.7327
	step [16/190], loss=9.0780
	step [17/190], loss=8.4449
	step [18/190], loss=8.1724
	step [19/190], loss=9.0703
	step [20/190], loss=8.3098
	step [21/190], loss=8.3732
	step [22/190], loss=9.2343
	step [23/190], loss=10.2226
	step [24/190], loss=10.8003
	step [25/190], loss=9.5284
	step [26/190], loss=7.5992
	step [27/190], loss=9.0975
	step [28/190], loss=8.4463
	step [29/190], loss=9.5203
	step [30/190], loss=8.4249
	step [31/190], loss=9.4971
	step [32/190], loss=9.0216
	step [33/190], loss=8.0776
	step [34/190], loss=9.6559
	step [35/190], loss=9.2390
	step [36/190], loss=8.0439
	step [37/190], loss=8.7257
	step [38/190], loss=8.6848
	step [39/190], loss=8.2357
	step [40/190], loss=9.4930
	step [41/190], loss=7.7146
	step [42/190], loss=9.7674
	step [43/190], loss=8.2801
	step [44/190], loss=11.3303
	step [45/190], loss=11.2792
	step [46/190], loss=8.6209
	step [47/190], loss=8.8675
	step [48/190], loss=8.1205
	step [49/190], loss=10.2020
	step [50/190], loss=10.0625
	step [51/190], loss=9.7744
	step [52/190], loss=9.2169
	step [53/190], loss=9.9940
	step [54/190], loss=8.9166
	step [55/190], loss=8.5481
	step [56/190], loss=8.0471
	step [57/190], loss=8.5181
	step [58/190], loss=9.8084
	step [59/190], loss=11.2797
	step [60/190], loss=8.9581
	step [61/190], loss=9.7614
	step [62/190], loss=11.6443
	step [63/190], loss=9.7515
	step [64/190], loss=8.6979
	step [65/190], loss=9.7567
	step [66/190], loss=8.8183
	step [67/190], loss=10.0650
	step [68/190], loss=9.9365
	step [69/190], loss=8.5751
	step [70/190], loss=10.3643
	step [71/190], loss=7.7333
	step [72/190], loss=9.2682
	step [73/190], loss=8.7786
	step [74/190], loss=8.9510
	step [75/190], loss=8.4352
	step [76/190], loss=9.2110
	step [77/190], loss=9.8856
	step [78/190], loss=8.2481
	step [79/190], loss=9.1713
	step [80/190], loss=9.7626
	step [81/190], loss=9.7008
	step [82/190], loss=7.9044
	step [83/190], loss=8.1120
	step [84/190], loss=10.2714
	step [85/190], loss=8.3546
	step [86/190], loss=7.8312
	step [87/190], loss=9.1831
	step [88/190], loss=8.0783
	step [89/190], loss=9.2914
	step [90/190], loss=10.5623
	step [91/190], loss=8.8958
	step [92/190], loss=8.2353
	step [93/190], loss=9.5419
	step [94/190], loss=12.2515
	step [95/190], loss=9.2005
	step [96/190], loss=7.3677
	step [97/190], loss=8.2390
	step [98/190], loss=10.8121
	step [99/190], loss=9.0520
	step [100/190], loss=9.7160
	step [101/190], loss=8.7922
	step [102/190], loss=8.8667
	step [103/190], loss=8.3386
	step [104/190], loss=7.5561
	step [105/190], loss=11.3313
	step [106/190], loss=8.3410
	step [107/190], loss=7.7645
	step [108/190], loss=11.1698
	step [109/190], loss=9.8387
	step [110/190], loss=10.6547
	step [111/190], loss=8.3780
	step [112/190], loss=9.1943
	step [113/190], loss=9.3116
	step [114/190], loss=10.6686
	step [115/190], loss=9.2262
	step [116/190], loss=8.3321
	step [117/190], loss=9.4519
	step [118/190], loss=7.9546
	step [119/190], loss=9.8770
	step [120/190], loss=7.6214
	step [121/190], loss=8.2673
	step [122/190], loss=8.5653
	step [123/190], loss=9.3459
	step [124/190], loss=8.5800
	step [125/190], loss=8.1820
	step [126/190], loss=9.0205
	step [127/190], loss=8.5968
	step [128/190], loss=7.6601
	step [129/190], loss=10.5833
	step [130/190], loss=9.6802
	step [131/190], loss=8.1787
	step [132/190], loss=8.9183
	step [133/190], loss=8.3246
	step [134/190], loss=9.8554
	step [135/190], loss=8.2379
	step [136/190], loss=8.1167
	step [137/190], loss=9.6685
	step [138/190], loss=8.4325
	step [139/190], loss=9.0003
	step [140/190], loss=8.0588
	step [141/190], loss=11.4409
	step [142/190], loss=9.9469
	step [143/190], loss=8.6321
	step [144/190], loss=8.1691
	step [145/190], loss=9.2092
	step [146/190], loss=8.3362
	step [147/190], loss=9.1411
	step [148/190], loss=8.7317
	step [149/190], loss=8.1939
	step [150/190], loss=8.5247
	step [151/190], loss=7.8786
	step [152/190], loss=8.6379
	step [153/190], loss=9.8771
	step [154/190], loss=8.8032
	step [155/190], loss=9.6296
	step [156/190], loss=8.3477
	step [157/190], loss=9.7609
	step [158/190], loss=7.4052
	step [159/190], loss=9.0778
	step [160/190], loss=8.5331
	step [161/190], loss=7.4664
	step [162/190], loss=10.5439
	step [163/190], loss=8.4397
	step [164/190], loss=9.8929
	step [165/190], loss=8.3735
	step [166/190], loss=9.4025
	step [167/190], loss=10.0841
	step [168/190], loss=9.5992
	step [169/190], loss=7.9774
	step [170/190], loss=9.1420
	step [171/190], loss=9.1655
	step [172/190], loss=8.5687
	step [173/190], loss=8.4753
	step [174/190], loss=10.7970
	step [175/190], loss=8.2973
	step [176/190], loss=7.1954
	step [177/190], loss=8.3670
	step [178/190], loss=9.5048
	step [179/190], loss=7.3615
	step [180/190], loss=8.4775
	step [181/190], loss=8.4017
	step [182/190], loss=8.9044
	step [183/190], loss=9.8397
	step [184/190], loss=8.4064
	step [185/190], loss=9.3494
	step [186/190], loss=9.7171
	step [187/190], loss=8.0741
	step [188/190], loss=10.1811
	step [189/190], loss=7.7674
	step [190/190], loss=8.7407
	Evaluating
	loss=0.0260, precision=0.2243, recall=0.9930, f1=0.3659
Training epoch 20
	step [1/190], loss=8.4423
	step [2/190], loss=7.0421
	step [3/190], loss=7.9212
	step [4/190], loss=10.2221
	step [5/190], loss=7.8404
	step [6/190], loss=10.6654
	step [7/190], loss=7.2367
	step [8/190], loss=7.1829
	step [9/190], loss=8.3836
	step [10/190], loss=8.9981
	step [11/190], loss=8.5882
	step [12/190], loss=7.9680
	step [13/190], loss=7.6915
	step [14/190], loss=10.5331
	step [15/190], loss=9.1959
	step [16/190], loss=7.0840
	step [17/190], loss=8.4213
	step [18/190], loss=7.7489
	step [19/190], loss=8.0206
	step [20/190], loss=7.9905
	step [21/190], loss=8.8444
	step [22/190], loss=7.3503
	step [23/190], loss=8.2393
	step [24/190], loss=10.3013
	step [25/190], loss=8.8047
	step [26/190], loss=7.2703
	step [27/190], loss=8.3224
	step [28/190], loss=8.3755
	step [29/190], loss=10.2049
	step [30/190], loss=9.3113
	step [31/190], loss=9.1144
	step [32/190], loss=7.5198
	step [33/190], loss=9.3336
	step [34/190], loss=9.7926
	step [35/190], loss=7.8885
	step [36/190], loss=9.1377
	step [37/190], loss=9.1426
	step [38/190], loss=7.7695
	step [39/190], loss=11.1174
	step [40/190], loss=8.1817
	step [41/190], loss=8.8323
	step [42/190], loss=9.7224
	step [43/190], loss=8.0555
	step [44/190], loss=8.8502
	step [45/190], loss=8.8098
	step [46/190], loss=8.2735
	step [47/190], loss=9.5786
	step [48/190], loss=10.0497
	step [49/190], loss=7.5566
	step [50/190], loss=8.7698
	step [51/190], loss=8.5047
	step [52/190], loss=8.5810
	step [53/190], loss=8.7867
	step [54/190], loss=9.2675
	step [55/190], loss=8.3140
	step [56/190], loss=8.3384
	step [57/190], loss=10.3290
	step [58/190], loss=7.7124
	step [59/190], loss=8.3369
	step [60/190], loss=10.3266
	step [61/190], loss=7.7647
	step [62/190], loss=9.9371
	step [63/190], loss=8.6923
	step [64/190], loss=8.4766
	step [65/190], loss=7.7027
	step [66/190], loss=7.8644
	step [67/190], loss=7.7316
	step [68/190], loss=8.9808
	step [69/190], loss=11.2416
	step [70/190], loss=8.6345
	step [71/190], loss=7.6293
	step [72/190], loss=7.6133
	step [73/190], loss=9.8993
	step [74/190], loss=7.8611
	step [75/190], loss=9.1688
	step [76/190], loss=7.2845
	step [77/190], loss=9.5241
	step [78/190], loss=9.6645
	step [79/190], loss=8.2597
	step [80/190], loss=9.8589
	step [81/190], loss=8.6725
	step [82/190], loss=8.8162
	step [83/190], loss=9.4624
	step [84/190], loss=8.8012
	step [85/190], loss=8.9224
	step [86/190], loss=9.0193
	step [87/190], loss=9.2377
	step [88/190], loss=9.1064
	step [89/190], loss=8.8260
	step [90/190], loss=7.7293
	step [91/190], loss=7.9520
	step [92/190], loss=11.2561
	step [93/190], loss=10.3307
	step [94/190], loss=9.3331
	step [95/190], loss=10.1250
	step [96/190], loss=9.0139
	step [97/190], loss=9.4242
	step [98/190], loss=9.4376
	step [99/190], loss=8.1201
	step [100/190], loss=8.9824
	step [101/190], loss=11.4227
	step [102/190], loss=10.3613
	step [103/190], loss=7.9769
	step [104/190], loss=8.7774
	step [105/190], loss=7.8637
	step [106/190], loss=8.3188
	step [107/190], loss=7.9218
	step [108/190], loss=9.8543
	step [109/190], loss=9.5171
	step [110/190], loss=9.5504
	step [111/190], loss=9.3076
	step [112/190], loss=9.4181
	step [113/190], loss=9.0827
	step [114/190], loss=7.8698
	step [115/190], loss=9.6404
	step [116/190], loss=6.9325
	step [117/190], loss=7.9124
	step [118/190], loss=8.1130
	step [119/190], loss=10.5641
	step [120/190], loss=11.4910
	step [121/190], loss=8.8324
	step [122/190], loss=6.9966
	step [123/190], loss=10.7367
	step [124/190], loss=8.4302
	step [125/190], loss=10.8814
	step [126/190], loss=9.1362
	step [127/190], loss=8.2829
	step [128/190], loss=8.4403
	step [129/190], loss=8.3349
	step [130/190], loss=8.3262
	step [131/190], loss=6.5166
	step [132/190], loss=12.6544
	step [133/190], loss=8.5989
	step [134/190], loss=9.8192
	step [135/190], loss=10.0958
	step [136/190], loss=8.6533
	step [137/190], loss=7.7619
	step [138/190], loss=9.0388
	step [139/190], loss=8.5929
	step [140/190], loss=9.8425
	step [141/190], loss=9.1676
	step [142/190], loss=8.5733
	step [143/190], loss=8.4942
	step [144/190], loss=7.5764
	step [145/190], loss=8.8389
	step [146/190], loss=9.6310
	step [147/190], loss=11.3950
	step [148/190], loss=7.9147
	step [149/190], loss=8.3135
	step [150/190], loss=8.3119
	step [151/190], loss=8.6991
	step [152/190], loss=7.8604
	step [153/190], loss=10.2569
	step [154/190], loss=8.6488
	step [155/190], loss=9.4466
	step [156/190], loss=8.8821
	step [157/190], loss=8.8765
	step [158/190], loss=7.4932
	step [159/190], loss=9.0675
	step [160/190], loss=9.1841
	step [161/190], loss=8.3907
	step [162/190], loss=9.2515
	step [163/190], loss=8.5856
	step [164/190], loss=8.2817
	step [165/190], loss=9.0827
	step [166/190], loss=11.0375
	step [167/190], loss=9.3488
	step [168/190], loss=9.3161
	step [169/190], loss=9.3793
	step [170/190], loss=7.7875
	step [171/190], loss=7.4048
	step [172/190], loss=9.2227
	step [173/190], loss=7.7170
	step [174/190], loss=10.7375
	step [175/190], loss=6.4502
	step [176/190], loss=9.9418
	step [177/190], loss=8.2254
	step [178/190], loss=6.4673
	step [179/190], loss=9.9118
	step [180/190], loss=10.3730
	step [181/190], loss=8.7787
	step [182/190], loss=8.8346
	step [183/190], loss=7.3823
	step [184/190], loss=7.7045
	step [185/190], loss=10.0610
	step [186/190], loss=10.2557
	step [187/190], loss=8.0938
	step [188/190], loss=8.8605
	step [189/190], loss=7.8937
	step [190/190], loss=6.8159
	Evaluating
	loss=0.0316, precision=0.1798, recall=0.9943, f1=0.3046
Training epoch 21
	step [1/190], loss=9.0191
	step [2/190], loss=7.8396
	step [3/190], loss=8.4685
	step [4/190], loss=6.1109
	step [5/190], loss=7.7788
	step [6/190], loss=7.9119
	step [7/190], loss=8.2749
	step [8/190], loss=8.8357
	step [9/190], loss=9.9128
	step [10/190], loss=7.9245
	step [11/190], loss=8.4448
	step [12/190], loss=6.9825
	step [13/190], loss=9.4917
	step [14/190], loss=8.1647
	step [15/190], loss=8.5520
	step [16/190], loss=8.0966
	step [17/190], loss=7.8358
	step [18/190], loss=7.3207
	step [19/190], loss=8.2863
	step [20/190], loss=8.3526
	step [21/190], loss=10.5238
	step [22/190], loss=7.4828
	step [23/190], loss=8.3296
	step [24/190], loss=8.2542
	step [25/190], loss=10.1060
	step [26/190], loss=9.7662
	step [27/190], loss=7.7905
	step [28/190], loss=8.5464
	step [29/190], loss=7.6380
	step [30/190], loss=8.9967
	step [31/190], loss=10.6690
	step [32/190], loss=8.8925
	step [33/190], loss=7.7923
	step [34/190], loss=7.2466
	step [35/190], loss=7.9803
	step [36/190], loss=8.6888
	step [37/190], loss=11.1680
	step [38/190], loss=8.2934
	step [39/190], loss=7.4107
	step [40/190], loss=8.8513
	step [41/190], loss=8.5879
	step [42/190], loss=10.0706
	step [43/190], loss=8.9769
	step [44/190], loss=6.8617
	step [45/190], loss=7.9633
	step [46/190], loss=9.1511
	step [47/190], loss=8.4750
	step [48/190], loss=7.7352
	step [49/190], loss=9.2514
	step [50/190], loss=9.8249
	step [51/190], loss=9.9495
	step [52/190], loss=10.4968
	step [53/190], loss=7.5172
	step [54/190], loss=8.2187
	step [55/190], loss=8.3591
	step [56/190], loss=9.4418
	step [57/190], loss=9.3599
	step [58/190], loss=8.1625
	step [59/190], loss=8.9967
	step [60/190], loss=9.0013
	step [61/190], loss=9.9818
	step [62/190], loss=8.8863
	step [63/190], loss=7.9660
	step [64/190], loss=6.9844
	step [65/190], loss=7.2882
	step [66/190], loss=7.5773
	step [67/190], loss=8.0936
	step [68/190], loss=8.8674
	step [69/190], loss=8.2618
	step [70/190], loss=6.3370
	step [71/190], loss=8.4779
	step [72/190], loss=8.1653
	step [73/190], loss=9.0536
	step [74/190], loss=7.9161
	step [75/190], loss=7.6308
	step [76/190], loss=9.4325
	step [77/190], loss=9.5390
	step [78/190], loss=8.2270
	step [79/190], loss=10.5812
	step [80/190], loss=8.2872
	step [81/190], loss=9.6787
	step [82/190], loss=8.4612
	step [83/190], loss=7.0196
	step [84/190], loss=7.8110
	step [85/190], loss=8.3356
	step [86/190], loss=8.7513
	step [87/190], loss=8.7543
	step [88/190], loss=8.6747
	step [89/190], loss=7.5758
	step [90/190], loss=9.7864
	step [91/190], loss=7.2326
	step [92/190], loss=7.5234
	step [93/190], loss=8.3601
	step [94/190], loss=7.6222
	step [95/190], loss=9.2989
	step [96/190], loss=8.1809
	step [97/190], loss=7.5117
	step [98/190], loss=9.1993
	step [99/190], loss=9.0517
	step [100/190], loss=7.3817
	step [101/190], loss=8.9272
	step [102/190], loss=7.7419
	step [103/190], loss=8.9412
	step [104/190], loss=7.8646
	step [105/190], loss=9.3575
	step [106/190], loss=7.9951
	step [107/190], loss=9.0130
	step [108/190], loss=8.0181
	step [109/190], loss=7.7959
	step [110/190], loss=7.7791
	step [111/190], loss=8.4776
	step [112/190], loss=8.7702
	step [113/190], loss=8.9491
	step [114/190], loss=7.4852
	step [115/190], loss=7.7847
	step [116/190], loss=12.0006
	step [117/190], loss=7.9565
	step [118/190], loss=8.3248
	step [119/190], loss=8.4378
	step [120/190], loss=8.2462
	step [121/190], loss=9.7046
	step [122/190], loss=8.9070
	step [123/190], loss=8.0969
	step [124/190], loss=8.0639
	step [125/190], loss=8.4486
	step [126/190], loss=7.1549
	step [127/190], loss=8.1032
	step [128/190], loss=12.0751
	step [129/190], loss=9.3312
	step [130/190], loss=8.6200
	step [131/190], loss=7.4022
	step [132/190], loss=10.1602
	step [133/190], loss=9.3900
	step [134/190], loss=8.6253
	step [135/190], loss=7.9812
	step [136/190], loss=7.4810
	step [137/190], loss=7.3535
	step [138/190], loss=8.9692
	step [139/190], loss=9.9385
	step [140/190], loss=8.7560
	step [141/190], loss=8.8881
	step [142/190], loss=6.8366
	step [143/190], loss=8.0708
	step [144/190], loss=8.5944
	step [145/190], loss=9.7398
	step [146/190], loss=9.1181
	step [147/190], loss=7.8754
	step [148/190], loss=7.9598
	step [149/190], loss=9.0000
	step [150/190], loss=7.5477
	step [151/190], loss=9.3245
	step [152/190], loss=7.5409
	step [153/190], loss=8.0630
	step [154/190], loss=8.3939
	step [155/190], loss=9.0732
	step [156/190], loss=9.0886
	step [157/190], loss=7.9229
	step [158/190], loss=10.3697
	step [159/190], loss=7.9891
	step [160/190], loss=7.1741
	step [161/190], loss=7.8071
	step [162/190], loss=9.1110
	step [163/190], loss=7.2583
	step [164/190], loss=7.3439
	step [165/190], loss=7.2074
	step [166/190], loss=9.4695
	step [167/190], loss=8.9527
	step [168/190], loss=7.7710
	step [169/190], loss=7.8956
	step [170/190], loss=7.1704
	step [171/190], loss=8.1324
	step [172/190], loss=8.2506
	step [173/190], loss=7.1887
	step [174/190], loss=8.5481
	step [175/190], loss=9.3289
	step [176/190], loss=7.3957
	step [177/190], loss=6.7064
	step [178/190], loss=8.8811
	step [179/190], loss=7.1422
	step [180/190], loss=6.7379
	step [181/190], loss=9.0598
	step [182/190], loss=9.7480
	step [183/190], loss=7.2295
	step [184/190], loss=9.6049
	step [185/190], loss=7.8810
	step [186/190], loss=7.4754
	step [187/190], loss=7.0088
	step [188/190], loss=9.7482
	step [189/190], loss=7.6921
	step [190/190], loss=7.4781
	Evaluating
	loss=0.0228, precision=0.2407, recall=0.9917, f1=0.3874
Training epoch 22
	step [1/190], loss=7.5952
	step [2/190], loss=7.6034
	step [3/190], loss=6.3028
	step [4/190], loss=7.5930
	step [5/190], loss=7.9348
	step [6/190], loss=7.0819
	step [7/190], loss=8.7757
	step [8/190], loss=10.8081
	step [9/190], loss=8.5470
	step [10/190], loss=8.4442
	step [11/190], loss=9.9655
	step [12/190], loss=8.9139
	step [13/190], loss=7.4728
	step [14/190], loss=8.1866
	step [15/190], loss=9.7796
	step [16/190], loss=8.0265
	step [17/190], loss=6.9335
	step [18/190], loss=7.3934
	step [19/190], loss=8.2109
	step [20/190], loss=9.7454
	step [21/190], loss=8.5901
	step [22/190], loss=8.5035
	step [23/190], loss=8.1256
	step [24/190], loss=7.8723
	step [25/190], loss=8.8065
	step [26/190], loss=8.4011
	step [27/190], loss=8.7993
	step [28/190], loss=7.5418
	step [29/190], loss=7.5073
	step [30/190], loss=7.6316
	step [31/190], loss=8.1971
	step [32/190], loss=7.8968
	step [33/190], loss=6.9309
	step [34/190], loss=7.7050
	step [35/190], loss=8.3227
	step [36/190], loss=9.0840
	step [37/190], loss=7.0424
	step [38/190], loss=7.9652
	step [39/190], loss=6.8886
	step [40/190], loss=8.2068
	step [41/190], loss=7.4397
	step [42/190], loss=7.7726
	step [43/190], loss=9.9507
	step [44/190], loss=7.9813
	step [45/190], loss=9.2406
	step [46/190], loss=8.3139
	step [47/190], loss=7.8266
	step [48/190], loss=10.1007
	step [49/190], loss=7.0803
	step [50/190], loss=9.6655
	step [51/190], loss=8.4135
	step [52/190], loss=7.0264
	step [53/190], loss=8.7772
	step [54/190], loss=9.0286
	step [55/190], loss=7.2295
	step [56/190], loss=7.7170
	step [57/190], loss=9.5213
	step [58/190], loss=7.6112
	step [59/190], loss=9.9365
	step [60/190], loss=6.5069
	step [61/190], loss=9.1551
	step [62/190], loss=8.9638
	step [63/190], loss=7.5631
	step [64/190], loss=6.0016
	step [65/190], loss=9.6799
	step [66/190], loss=8.1514
	step [67/190], loss=7.2248
	step [68/190], loss=8.2540
	step [69/190], loss=8.8022
	step [70/190], loss=8.6553
	step [71/190], loss=8.7469
	step [72/190], loss=8.4366
	step [73/190], loss=7.5443
	step [74/190], loss=9.7336
	step [75/190], loss=7.2013
	step [76/190], loss=8.4954
	step [77/190], loss=8.6883
	step [78/190], loss=7.8996
	step [79/190], loss=7.1351
	step [80/190], loss=8.2141
	step [81/190], loss=7.1047
	step [82/190], loss=7.4148
	step [83/190], loss=8.0713
	step [84/190], loss=8.7368
	step [85/190], loss=7.9523
	step [86/190], loss=8.4113
	step [87/190], loss=9.3538
	step [88/190], loss=8.8749
	step [89/190], loss=8.7438
	step [90/190], loss=9.9947
	step [91/190], loss=7.6460
	step [92/190], loss=6.9542
	step [93/190], loss=7.0867
	step [94/190], loss=7.4462
	step [95/190], loss=7.8866
	step [96/190], loss=7.8539
	step [97/190], loss=8.2584
	step [98/190], loss=8.2812
	step [99/190], loss=6.4533
	step [100/190], loss=7.5187
	step [101/190], loss=7.6524
	step [102/190], loss=7.4308
	step [103/190], loss=7.1712
	step [104/190], loss=7.2316
	step [105/190], loss=9.2642
	step [106/190], loss=7.0427
	step [107/190], loss=9.7629
	step [108/190], loss=8.6143
	step [109/190], loss=9.6839
	step [110/190], loss=8.7100
	step [111/190], loss=10.8563
	step [112/190], loss=10.9950
	step [113/190], loss=8.2867
	step [114/190], loss=7.8909
	step [115/190], loss=7.2825
	step [116/190], loss=8.5809
	step [117/190], loss=7.9027
	step [118/190], loss=7.7045
	step [119/190], loss=7.9927
	step [120/190], loss=7.3838
	step [121/190], loss=8.3895
	step [122/190], loss=7.9202
	step [123/190], loss=8.6909
	step [124/190], loss=8.7971
	step [125/190], loss=7.4652
	step [126/190], loss=7.8021
	step [127/190], loss=7.8338
	step [128/190], loss=8.6335
	step [129/190], loss=9.2923
	step [130/190], loss=8.7790
	step [131/190], loss=7.5676
	step [132/190], loss=8.8190
	step [133/190], loss=7.7768
	step [134/190], loss=8.1029
	step [135/190], loss=8.0685
	step [136/190], loss=6.3460
	step [137/190], loss=6.8243
	step [138/190], loss=7.6355
	step [139/190], loss=8.4520
	step [140/190], loss=9.3284
	step [141/190], loss=6.7448
	step [142/190], loss=8.2459
	step [143/190], loss=8.2558
	step [144/190], loss=7.4935
	step [145/190], loss=8.7164
	step [146/190], loss=6.8539
	step [147/190], loss=8.4988
	step [148/190], loss=8.3805
	step [149/190], loss=6.8836
	step [150/190], loss=7.0857
	step [151/190], loss=8.2588
	step [152/190], loss=8.9058
	step [153/190], loss=8.5761
	step [154/190], loss=9.0599
	step [155/190], loss=10.7445
	step [156/190], loss=8.5657
	step [157/190], loss=8.3949
	step [158/190], loss=6.9259
	step [159/190], loss=8.8867
	step [160/190], loss=9.5209
	step [161/190], loss=7.8391
	step [162/190], loss=7.5854
	step [163/190], loss=7.7759
	step [164/190], loss=8.0311
	step [165/190], loss=9.7435
	step [166/190], loss=8.1676
	step [167/190], loss=7.5232
	step [168/190], loss=8.0120
	step [169/190], loss=9.3965
	step [170/190], loss=7.1180
	step [171/190], loss=8.7810
	step [172/190], loss=7.2086
	step [173/190], loss=8.7253
	step [174/190], loss=8.2073
	step [175/190], loss=7.6109
	step [176/190], loss=7.4392
	step [177/190], loss=7.3381
	step [178/190], loss=9.5377
	step [179/190], loss=8.5881
	step [180/190], loss=8.9798
	step [181/190], loss=6.9651
	step [182/190], loss=7.8024
	step [183/190], loss=6.7201
	step [184/190], loss=8.8065
	step [185/190], loss=9.1586
	step [186/190], loss=9.9246
	step [187/190], loss=7.1945
	step [188/190], loss=8.4128
	step [189/190], loss=7.8692
	step [190/190], loss=8.5179
	Evaluating
	loss=0.0255, precision=0.2109, recall=0.9935, f1=0.3479
Training epoch 23
	step [1/190], loss=7.5837
	step [2/190], loss=8.7894
	step [3/190], loss=8.8030
	step [4/190], loss=8.2702
	step [5/190], loss=8.3050
	step [6/190], loss=8.4874
	step [7/190], loss=6.5760
	step [8/190], loss=8.4250
	step [9/190], loss=7.7648
	step [10/190], loss=8.1940
	step [11/190], loss=7.0850
	step [12/190], loss=7.4407
	step [13/190], loss=7.7898
	step [14/190], loss=7.9007
	step [15/190], loss=7.6887
	step [16/190], loss=6.5588
	step [17/190], loss=8.3313
	step [18/190], loss=8.9530
	step [19/190], loss=8.3945
	step [20/190], loss=7.0555
	step [21/190], loss=9.6826
	step [22/190], loss=5.7896
	step [23/190], loss=9.8423
	step [24/190], loss=7.4635
	step [25/190], loss=7.2701
	step [26/190], loss=9.0968
	step [27/190], loss=8.9231
	step [28/190], loss=7.7828
	step [29/190], loss=7.6615
	step [30/190], loss=7.5856
	step [31/190], loss=7.6188
	step [32/190], loss=8.0506
	step [33/190], loss=9.4086
	step [34/190], loss=7.0872
	step [35/190], loss=8.2365
	step [36/190], loss=9.2608
	step [37/190], loss=7.2391
	step [38/190], loss=8.2407
	step [39/190], loss=7.5294
	step [40/190], loss=7.2730
	step [41/190], loss=9.1397
	step [42/190], loss=7.2999
	step [43/190], loss=8.1357
	step [44/190], loss=8.0412
	step [45/190], loss=7.5237
	step [46/190], loss=8.1493
	step [47/190], loss=9.2657
	step [48/190], loss=7.4841
	step [49/190], loss=7.8450
	step [50/190], loss=7.8865
	step [51/190], loss=7.5132
	step [52/190], loss=8.2740
	step [53/190], loss=7.1993
	step [54/190], loss=6.3016
	step [55/190], loss=9.0142
	step [56/190], loss=7.5127
	step [57/190], loss=9.1242
	step [58/190], loss=7.9533
	step [59/190], loss=10.3095
	step [60/190], loss=7.2026
	step [61/190], loss=7.1923
	step [62/190], loss=9.3071
	step [63/190], loss=7.6868
	step [64/190], loss=8.6089
	step [65/190], loss=10.2473
	step [66/190], loss=7.3368
	step [67/190], loss=8.8673
	step [68/190], loss=7.0714
	step [69/190], loss=7.7847
	step [70/190], loss=7.0616
	step [71/190], loss=8.9674
	step [72/190], loss=7.3880
	step [73/190], loss=7.2161
	step [74/190], loss=7.0200
	step [75/190], loss=8.2349
	step [76/190], loss=8.2132
	step [77/190], loss=10.2864
	step [78/190], loss=9.4925
	step [79/190], loss=7.0802
	step [80/190], loss=8.8464
	step [81/190], loss=7.3358
	step [82/190], loss=7.3410
	step [83/190], loss=7.5850
	step [84/190], loss=9.1556
	step [85/190], loss=6.8636
	step [86/190], loss=7.8029
	step [87/190], loss=9.3411
	step [88/190], loss=8.2446
	step [89/190], loss=7.7409
	step [90/190], loss=9.3436
	step [91/190], loss=7.7232
	step [92/190], loss=10.1943
	step [93/190], loss=9.1135
	step [94/190], loss=7.3377
	step [95/190], loss=8.1257
	step [96/190], loss=9.0546
	step [97/190], loss=7.5845
	step [98/190], loss=7.3805
	step [99/190], loss=6.6932
	step [100/190], loss=7.9050
	step [101/190], loss=8.0546
	step [102/190], loss=8.5577
	step [103/190], loss=9.4752
	step [104/190], loss=8.0827
	step [105/190], loss=13.3958
	step [106/190], loss=6.0908
	step [107/190], loss=7.1626
	step [108/190], loss=7.2068
	step [109/190], loss=7.5085
	step [110/190], loss=7.9501
	step [111/190], loss=8.6523
	step [112/190], loss=8.1313
	step [113/190], loss=7.6431
	step [114/190], loss=8.3198
	step [115/190], loss=9.2535
	step [116/190], loss=8.5262
	step [117/190], loss=8.3060
	step [118/190], loss=6.6131
	step [119/190], loss=9.1093
	step [120/190], loss=7.2513
	step [121/190], loss=7.8262
	step [122/190], loss=7.2986
	step [123/190], loss=7.2249
	step [124/190], loss=9.7326
	step [125/190], loss=6.8599
	step [126/190], loss=7.9452
	step [127/190], loss=7.6072
	step [128/190], loss=8.0273
	step [129/190], loss=7.8380
	step [130/190], loss=8.7832
	step [131/190], loss=8.3561
	step [132/190], loss=7.8037
	step [133/190], loss=8.0631
	step [134/190], loss=7.1698
	step [135/190], loss=9.0275
	step [136/190], loss=6.6580
	step [137/190], loss=6.7834
	step [138/190], loss=6.4648
	step [139/190], loss=12.1500
	step [140/190], loss=6.4851
	step [141/190], loss=7.5309
	step [142/190], loss=8.0457
	step [143/190], loss=7.4103
	step [144/190], loss=9.2990
	step [145/190], loss=9.0389
	step [146/190], loss=8.3856
	step [147/190], loss=7.5774
	step [148/190], loss=7.5950
	step [149/190], loss=9.0509
	step [150/190], loss=6.3766
	step [151/190], loss=7.9272
	step [152/190], loss=8.2916
	step [153/190], loss=6.8962
	step [154/190], loss=8.3471
	step [155/190], loss=7.7933
	step [156/190], loss=9.2162
	step [157/190], loss=6.8946
	step [158/190], loss=7.2846
	step [159/190], loss=8.8514
	step [160/190], loss=7.4517
	step [161/190], loss=7.3197
	step [162/190], loss=9.2083
	step [163/190], loss=7.2183
	step [164/190], loss=7.6693
	step [165/190], loss=9.0569
	step [166/190], loss=7.7272
	step [167/190], loss=9.2275
	step [168/190], loss=7.3479
	step [169/190], loss=8.1739
	step [170/190], loss=7.6345
	step [171/190], loss=8.0168
	step [172/190], loss=8.0742
	step [173/190], loss=8.5628
	step [174/190], loss=8.2919
	step [175/190], loss=6.8785
	step [176/190], loss=8.7354
	step [177/190], loss=7.9427
	step [178/190], loss=7.2494
	step [179/190], loss=9.5515
	step [180/190], loss=7.2184
	step [181/190], loss=7.3894
	step [182/190], loss=6.6288
	step [183/190], loss=8.3853
	step [184/190], loss=8.8442
	step [185/190], loss=8.0447
	step [186/190], loss=8.6161
	step [187/190], loss=8.4600
	step [188/190], loss=7.4886
	step [189/190], loss=7.3330
	step [190/190], loss=7.9547
	Evaluating
	loss=0.0199, precision=0.2651, recall=0.9905, f1=0.4183
Training epoch 24
	step [1/190], loss=8.0492
	step [2/190], loss=8.4426
	step [3/190], loss=7.3743
	step [4/190], loss=7.5399
	step [5/190], loss=8.8191
	step [6/190], loss=8.0396
	step [7/190], loss=7.4845
	step [8/190], loss=7.6626
	step [9/190], loss=7.3829
	step [10/190], loss=7.5776
	step [11/190], loss=7.9482
	step [12/190], loss=9.0363
	step [13/190], loss=8.8638
	step [14/190], loss=8.2951
	step [15/190], loss=9.5265
	step [16/190], loss=7.6328
	step [17/190], loss=9.5961
	step [18/190], loss=7.5421
	step [19/190], loss=8.1746
	step [20/190], loss=7.0892
	step [21/190], loss=7.3099
	step [22/190], loss=8.0571
	step [23/190], loss=9.6280
	step [24/190], loss=6.5184
	step [25/190], loss=7.1877
	step [26/190], loss=6.8206
	step [27/190], loss=9.1796
	step [28/190], loss=8.5149
	step [29/190], loss=6.4518
	step [30/190], loss=7.4005
	step [31/190], loss=6.2236
	step [32/190], loss=9.5714
	step [33/190], loss=8.9321
	step [34/190], loss=8.5752
	step [35/190], loss=7.0923
	step [36/190], loss=7.6191
	step [37/190], loss=6.1134
	step [38/190], loss=8.0944
	step [39/190], loss=7.3218
	step [40/190], loss=7.8518
	step [41/190], loss=6.6771
	step [42/190], loss=8.3189
	step [43/190], loss=8.6472
	step [44/190], loss=7.5233
	step [45/190], loss=8.5364
	step [46/190], loss=8.1681
	step [47/190], loss=9.2173
	step [48/190], loss=6.9086
	step [49/190], loss=7.1298
	step [50/190], loss=6.7837
	step [51/190], loss=8.9680
	step [52/190], loss=9.2769
	step [53/190], loss=7.8007
	step [54/190], loss=8.7907
	step [55/190], loss=8.6665
	step [56/190], loss=7.7901
	step [57/190], loss=6.5712
	step [58/190], loss=7.1782
	step [59/190], loss=6.3641
	step [60/190], loss=6.8225
	step [61/190], loss=8.4507
	step [62/190], loss=8.2863
	step [63/190], loss=6.3109
	step [64/190], loss=6.4670
	step [65/190], loss=7.6176
	step [66/190], loss=9.0227
	step [67/190], loss=7.9960
	step [68/190], loss=6.7646
	step [69/190], loss=8.4841
	step [70/190], loss=6.1287
	step [71/190], loss=9.1368
	step [72/190], loss=7.3647
	step [73/190], loss=7.1236
	step [74/190], loss=8.0599
	step [75/190], loss=8.1426
	step [76/190], loss=8.4922
	step [77/190], loss=8.6977
	step [78/190], loss=6.3612
	step [79/190], loss=7.7392
	step [80/190], loss=9.0882
	step [81/190], loss=7.9235
	step [82/190], loss=8.6664
	step [83/190], loss=6.9662
	step [84/190], loss=8.2505
	step [85/190], loss=9.6467
	step [86/190], loss=8.0307
	step [87/190], loss=7.9406
	step [88/190], loss=7.4033
	step [89/190], loss=8.4816
	step [90/190], loss=6.8506
	step [91/190], loss=7.7845
	step [92/190], loss=7.3835
	step [93/190], loss=7.6401
	step [94/190], loss=8.2715
	step [95/190], loss=7.5260
	step [96/190], loss=6.2328
	step [97/190], loss=8.0467
	step [98/190], loss=7.2408
	step [99/190], loss=7.3758
	step [100/190], loss=6.9362
	step [101/190], loss=8.7795
	step [102/190], loss=9.2114
	step [103/190], loss=7.3310
	step [104/190], loss=7.1509
	step [105/190], loss=8.6163
	step [106/190], loss=8.4868
	step [107/190], loss=7.1383
	step [108/190], loss=9.6349
	step [109/190], loss=7.8915
	step [110/190], loss=7.7142
	step [111/190], loss=9.2334
	step [112/190], loss=7.3819
	step [113/190], loss=6.5022
	step [114/190], loss=7.2020
	step [115/190], loss=7.1739
	step [116/190], loss=6.7098
	step [117/190], loss=7.5272
	step [118/190], loss=7.6455
	step [119/190], loss=6.8163
	step [120/190], loss=7.6589
	step [121/190], loss=6.5058
	step [122/190], loss=6.0439
	step [123/190], loss=8.4963
	step [124/190], loss=8.2938
	step [125/190], loss=6.0285
	step [126/190], loss=9.2504
	step [127/190], loss=6.7597
	step [128/190], loss=6.3266
	step [129/190], loss=7.0815
	step [130/190], loss=7.0585
	step [131/190], loss=8.9783
	step [132/190], loss=7.9347
	step [133/190], loss=6.8775
	step [134/190], loss=6.7490
	step [135/190], loss=6.1210
	step [136/190], loss=7.3820
	step [137/190], loss=10.3686
	step [138/190], loss=8.5010
	step [139/190], loss=9.6383
	step [140/190], loss=9.9710
	step [141/190], loss=8.8657
	step [142/190], loss=8.4787
	step [143/190], loss=7.6168
	step [144/190], loss=7.4892
	step [145/190], loss=7.1836
	step [146/190], loss=7.4853
	step [147/190], loss=7.6009
	step [148/190], loss=7.9169
	step [149/190], loss=7.0259
	step [150/190], loss=7.3394
	step [151/190], loss=6.3562
	step [152/190], loss=7.5393
	step [153/190], loss=7.5918
	step [154/190], loss=7.0733
	step [155/190], loss=7.1487
	step [156/190], loss=8.2034
	step [157/190], loss=6.8167
	step [158/190], loss=8.3674
	step [159/190], loss=8.4841
	step [160/190], loss=7.3245
	step [161/190], loss=8.2281
	step [162/190], loss=6.3334
	step [163/190], loss=8.0508
	step [164/190], loss=8.6509
	step [165/190], loss=7.1646
	step [166/190], loss=7.2250
	step [167/190], loss=6.6403
	step [168/190], loss=7.4690
	step [169/190], loss=7.1056
	step [170/190], loss=7.5004
	step [171/190], loss=7.5122
	step [172/190], loss=8.3345
	step [173/190], loss=8.7887
	step [174/190], loss=8.2713
	step [175/190], loss=8.1221
	step [176/190], loss=7.8281
	step [177/190], loss=8.5977
	step [178/190], loss=8.3606
	step [179/190], loss=7.9204
	step [180/190], loss=9.0687
	step [181/190], loss=8.4821
	step [182/190], loss=8.7710
	step [183/190], loss=7.0317
	step [184/190], loss=7.3178
	step [185/190], loss=6.9147
	step [186/190], loss=7.6805
	step [187/190], loss=8.1763
	step [188/190], loss=7.7957
	step [189/190], loss=7.2373
	step [190/190], loss=8.5003
	Evaluating
	loss=0.0196, precision=0.2696, recall=0.9896, f1=0.4238
Training epoch 25
	step [1/190], loss=6.9332
	step [2/190], loss=7.4689
	step [3/190], loss=8.8547
	step [4/190], loss=9.0917
	step [5/190], loss=8.7378
	step [6/190], loss=8.3206
	step [7/190], loss=8.0803
	step [8/190], loss=8.3900
	step [9/190], loss=7.6345
	step [10/190], loss=8.9725
	step [11/190], loss=7.3987
	step [12/190], loss=7.9120
	step [13/190], loss=8.2938
	step [14/190], loss=7.2570
	step [15/190], loss=8.9747
	step [16/190], loss=7.0020
	step [17/190], loss=6.9293
	step [18/190], loss=6.6920
	step [19/190], loss=7.0628
	step [20/190], loss=8.0982
	step [21/190], loss=7.3443
	step [22/190], loss=7.6166
	step [23/190], loss=6.3541
	step [24/190], loss=5.6776
	step [25/190], loss=9.4245
	step [26/190], loss=7.1834
	step [27/190], loss=7.9565
	step [28/190], loss=6.8567
	step [29/190], loss=6.9344
	step [30/190], loss=6.8687
	step [31/190], loss=6.7884
	step [32/190], loss=7.2205
	step [33/190], loss=8.6722
	step [34/190], loss=5.9500
	step [35/190], loss=6.8258
	step [36/190], loss=6.8313
	step [37/190], loss=8.5259
	step [38/190], loss=7.6772
	step [39/190], loss=6.5051
	step [40/190], loss=7.2047
	step [41/190], loss=6.9402
	step [42/190], loss=8.3061
	step [43/190], loss=7.2353
	step [44/190], loss=8.4976
	step [45/190], loss=7.0924
	step [46/190], loss=8.4871
	step [47/190], loss=9.4473
	step [48/190], loss=8.1343
	step [49/190], loss=8.0599
	step [50/190], loss=7.3661
	step [51/190], loss=7.1884
	step [52/190], loss=7.5843
	step [53/190], loss=5.8246
	step [54/190], loss=9.5359
	step [55/190], loss=8.0523
	step [56/190], loss=7.6645
	step [57/190], loss=6.7448
	step [58/190], loss=7.3752
	step [59/190], loss=6.8685
	step [60/190], loss=7.8640
	step [61/190], loss=6.7550
	step [62/190], loss=7.4998
	step [63/190], loss=7.6190
	step [64/190], loss=9.1786
	step [65/190], loss=7.8105
	step [66/190], loss=7.5450
	step [67/190], loss=7.4101
	step [68/190], loss=8.1515
	step [69/190], loss=8.3730
	step [70/190], loss=8.1403
	step [71/190], loss=9.0255
	step [72/190], loss=7.1992
	step [73/190], loss=6.6658
	step [74/190], loss=7.1077
	step [75/190], loss=7.3237
	step [76/190], loss=7.8433
	step [77/190], loss=6.6440
	step [78/190], loss=7.3583
	step [79/190], loss=7.6751
	step [80/190], loss=6.3638
	step [81/190], loss=8.2752
	step [82/190], loss=7.4589
	step [83/190], loss=8.2372
	step [84/190], loss=7.2935
	step [85/190], loss=6.7871
	step [86/190], loss=7.8896
	step [87/190], loss=7.2760
	step [88/190], loss=8.1649
	step [89/190], loss=6.3397
	step [90/190], loss=7.0137
	step [91/190], loss=7.8805
	step [92/190], loss=7.4398
	step [93/190], loss=6.8202
	step [94/190], loss=9.3033
	step [95/190], loss=7.9153
	step [96/190], loss=7.2923
	step [97/190], loss=6.4296
	step [98/190], loss=7.9688
	step [99/190], loss=7.4686
	step [100/190], loss=8.1861
	step [101/190], loss=5.9223
	step [102/190], loss=8.3850
	step [103/190], loss=7.9861
	step [104/190], loss=10.2376
	step [105/190], loss=7.5837
	step [106/190], loss=6.8342
	step [107/190], loss=7.6416
	step [108/190], loss=7.1392
	step [109/190], loss=7.1963
	step [110/190], loss=8.2675
	step [111/190], loss=6.7792
	step [112/190], loss=8.3384
	step [113/190], loss=7.9913
	step [114/190], loss=8.8519
	step [115/190], loss=9.2611
	step [116/190], loss=8.2544
	step [117/190], loss=7.5415
	step [118/190], loss=8.0489
	step [119/190], loss=6.5252
	step [120/190], loss=7.1249
	step [121/190], loss=7.3231
	step [122/190], loss=8.3655
	step [123/190], loss=7.6365
	step [124/190], loss=8.0174
	step [125/190], loss=7.2551
	step [126/190], loss=8.2836
	step [127/190], loss=6.6813
	step [128/190], loss=7.7262
	step [129/190], loss=7.9619
	step [130/190], loss=6.9277
	step [131/190], loss=6.6904
	step [132/190], loss=7.6171
	step [133/190], loss=7.9894
	step [134/190], loss=8.0680
	step [135/190], loss=7.1385
	step [136/190], loss=6.8852
	step [137/190], loss=6.6200
	step [138/190], loss=7.9238
	step [139/190], loss=7.0387
	step [140/190], loss=7.2192
	step [141/190], loss=7.1937
	step [142/190], loss=7.8000
	step [143/190], loss=7.2762
	step [144/190], loss=5.3937
	step [145/190], loss=6.1677
	step [146/190], loss=7.2684
	step [147/190], loss=7.1482
	step [148/190], loss=7.6775
	step [149/190], loss=7.5797
	step [150/190], loss=7.8240
	step [151/190], loss=7.4415
	step [152/190], loss=6.6023
	step [153/190], loss=8.0024
	step [154/190], loss=8.2544
	step [155/190], loss=6.5476
	step [156/190], loss=8.3238
	step [157/190], loss=7.0735
	step [158/190], loss=8.9360
	step [159/190], loss=6.4464
	step [160/190], loss=7.6886
	step [161/190], loss=8.7104
	step [162/190], loss=6.3870
	step [163/190], loss=7.4045
	step [164/190], loss=8.5571
	step [165/190], loss=8.6102
	step [166/190], loss=8.5869
	step [167/190], loss=7.1038
	step [168/190], loss=7.3265
	step [169/190], loss=7.3923
	step [170/190], loss=7.4009
	step [171/190], loss=7.9320
	step [172/190], loss=8.1093
	step [173/190], loss=7.5237
	step [174/190], loss=8.1577
	step [175/190], loss=7.2867
	step [176/190], loss=6.8790
	step [177/190], loss=7.5614
	step [178/190], loss=8.2885
	step [179/190], loss=7.2015
	step [180/190], loss=7.2648
	step [181/190], loss=7.2695
	step [182/190], loss=7.2517
	step [183/190], loss=6.1295
	step [184/190], loss=7.3913
	step [185/190], loss=7.5720
	step [186/190], loss=7.4872
	step [187/190], loss=7.4214
	step [188/190], loss=7.2952
	step [189/190], loss=7.0408
	step [190/190], loss=5.6630
	Evaluating
	loss=0.0216, precision=0.2429, recall=0.9906, f1=0.3902
Training epoch 26
	step [1/190], loss=6.2108
	step [2/190], loss=5.7630
	step [3/190], loss=5.9848
	step [4/190], loss=6.9657
	step [5/190], loss=7.5580
	step [6/190], loss=7.7996
	step [7/190], loss=7.0119
	step [8/190], loss=8.0493
	step [9/190], loss=9.0922
	step [10/190], loss=6.4538
	step [11/190], loss=6.9252
	step [12/190], loss=6.4936
	step [13/190], loss=6.5733
	step [14/190], loss=7.8931
	step [15/190], loss=6.2236
	step [16/190], loss=8.4331
	step [17/190], loss=6.0027
	step [18/190], loss=7.0178
	step [19/190], loss=8.0592
	step [20/190], loss=7.9535
	step [21/190], loss=6.8459
	step [22/190], loss=7.3662
	step [23/190], loss=7.9870
	step [24/190], loss=7.5291
	step [25/190], loss=7.5205
	step [26/190], loss=7.1817
	step [27/190], loss=8.6496
	step [28/190], loss=9.0871
	step [29/190], loss=7.0362
	step [30/190], loss=8.1730
	step [31/190], loss=8.5072
	step [32/190], loss=7.9391
	step [33/190], loss=7.4633
	step [34/190], loss=6.1613
	step [35/190], loss=7.2820
	step [36/190], loss=9.2476
	step [37/190], loss=6.9884
	step [38/190], loss=8.7211
	step [39/190], loss=8.3233
	step [40/190], loss=7.5178
	step [41/190], loss=7.3900
	step [42/190], loss=7.4320
	step [43/190], loss=7.4725
	step [44/190], loss=7.6536
	step [45/190], loss=7.7957
	step [46/190], loss=8.8630
	step [47/190], loss=7.5140
	step [48/190], loss=7.5773
	step [49/190], loss=7.0778
	step [50/190], loss=7.7853
	step [51/190], loss=6.8887
	step [52/190], loss=7.4163
	step [53/190], loss=8.1285
	step [54/190], loss=5.9067
	step [55/190], loss=7.2094
	step [56/190], loss=7.6396
	step [57/190], loss=6.8482
	step [58/190], loss=7.2745
	step [59/190], loss=6.3007
	step [60/190], loss=6.6222
	step [61/190], loss=7.2959
	step [62/190], loss=7.1233
	step [63/190], loss=8.7580
	step [64/190], loss=6.1201
	step [65/190], loss=7.9430
	step [66/190], loss=7.6125
	step [67/190], loss=8.5701
	step [68/190], loss=8.0140
	step [69/190], loss=6.7240
	step [70/190], loss=6.9925
	step [71/190], loss=6.2924
	step [72/190], loss=7.8684
	step [73/190], loss=5.7466
	step [74/190], loss=7.1733
	step [75/190], loss=6.7762
	step [76/190], loss=7.8964
	step [77/190], loss=7.7559
	step [78/190], loss=7.7878
	step [79/190], loss=6.5308
	step [80/190], loss=7.3226
	step [81/190], loss=6.4513
	step [82/190], loss=5.8421
	step [83/190], loss=8.2872
	step [84/190], loss=9.5104
	step [85/190], loss=6.6434
	step [86/190], loss=6.3369
	step [87/190], loss=6.3017
	step [88/190], loss=7.8578
	step [89/190], loss=8.6591
	step [90/190], loss=7.6641
	step [91/190], loss=7.5440
	step [92/190], loss=6.4475
	step [93/190], loss=6.8583
	step [94/190], loss=8.1283
	step [95/190], loss=6.8408
	step [96/190], loss=5.5236
	step [97/190], loss=6.6246
	step [98/190], loss=7.8326
	step [99/190], loss=8.2661
	step [100/190], loss=8.9952
	step [101/190], loss=8.1103
	step [102/190], loss=6.4964
	step [103/190], loss=7.4144
	step [104/190], loss=6.8401
	step [105/190], loss=7.6619
	step [106/190], loss=7.2597
	step [107/190], loss=7.3794
	step [108/190], loss=6.9361
	step [109/190], loss=5.9524
	step [110/190], loss=8.5488
	step [111/190], loss=6.1138
	step [112/190], loss=8.0079
	step [113/190], loss=5.9148
	step [114/190], loss=7.7472
	step [115/190], loss=6.7443
	step [116/190], loss=9.3641
	step [117/190], loss=6.4300
	step [118/190], loss=7.2753
	step [119/190], loss=7.3504
	step [120/190], loss=7.7368
	step [121/190], loss=6.6058
	step [122/190], loss=6.7418
	step [123/190], loss=7.6447
	step [124/190], loss=6.6833
	step [125/190], loss=7.2045
	step [126/190], loss=6.3327
	step [127/190], loss=7.1108
	step [128/190], loss=6.5734
	step [129/190], loss=7.6136
	step [130/190], loss=7.1996
	step [131/190], loss=8.1400
	step [132/190], loss=6.4587
	step [133/190], loss=6.3805
	step [134/190], loss=6.2754
	step [135/190], loss=7.0203
	step [136/190], loss=7.6328
	step [137/190], loss=7.7431
	step [138/190], loss=6.7873
	step [139/190], loss=7.8218
	step [140/190], loss=9.8578
	step [141/190], loss=5.8104
	step [142/190], loss=7.8394
	step [143/190], loss=6.7067
	step [144/190], loss=8.6584
	step [145/190], loss=6.5439
	step [146/190], loss=7.1602
	step [147/190], loss=8.1233
	step [148/190], loss=8.4679
	step [149/190], loss=7.8170
	step [150/190], loss=7.4257
	step [151/190], loss=7.4039
	step [152/190], loss=6.9312
	step [153/190], loss=7.1102
	step [154/190], loss=8.4466
	step [155/190], loss=8.0241
	step [156/190], loss=8.3359
	step [157/190], loss=6.8832
	step [158/190], loss=8.2972
	step [159/190], loss=6.5195
	step [160/190], loss=5.5778
	step [161/190], loss=10.9544
	step [162/190], loss=6.9911
	step [163/190], loss=7.0403
	step [164/190], loss=6.8495
	step [165/190], loss=7.6940
	step [166/190], loss=6.8600
	step [167/190], loss=6.6174
	step [168/190], loss=6.4374
	step [169/190], loss=7.7294
	step [170/190], loss=6.6631
	step [171/190], loss=6.6817
	step [172/190], loss=7.6149
	step [173/190], loss=9.1329
	step [174/190], loss=6.4748
	step [175/190], loss=6.1423
	step [176/190], loss=7.3182
	step [177/190], loss=7.5499
	step [178/190], loss=7.0959
	step [179/190], loss=9.3897
	step [180/190], loss=5.4606
	step [181/190], loss=8.2632
	step [182/190], loss=6.6169
	step [183/190], loss=8.5585
	step [184/190], loss=7.3318
	step [185/190], loss=8.7314
	step [186/190], loss=7.9636
	step [187/190], loss=7.2994
	step [188/190], loss=6.4364
	step [189/190], loss=7.5963
	step [190/190], loss=5.3008
	Evaluating
	loss=0.0192, precision=0.2686, recall=0.9898, f1=0.4226
Training epoch 27
	step [1/190], loss=6.5455
	step [2/190], loss=6.0089
	step [3/190], loss=7.4650
	step [4/190], loss=7.4940
	step [5/190], loss=6.4920
	step [6/190], loss=8.4953
	step [7/190], loss=6.0032
	step [8/190], loss=8.3451
	step [9/190], loss=7.6376
	step [10/190], loss=8.0917
	step [11/190], loss=7.5669
	step [12/190], loss=6.7530
	step [13/190], loss=8.5773
	step [14/190], loss=6.8965
	step [15/190], loss=9.1006
	step [16/190], loss=7.8076
	step [17/190], loss=6.0888
	step [18/190], loss=8.3000
	step [19/190], loss=6.0987
	step [20/190], loss=9.0479
	step [21/190], loss=6.7057
	step [22/190], loss=8.1919
	step [23/190], loss=6.6628
	step [24/190], loss=7.1367
	step [25/190], loss=7.6885
	step [26/190], loss=6.3214
	step [27/190], loss=7.4276
	step [28/190], loss=7.3204
	step [29/190], loss=7.6342
	step [30/190], loss=7.6825
	step [31/190], loss=6.7533
	step [32/190], loss=7.2427
	step [33/190], loss=8.4256
	step [34/190], loss=5.8152
	step [35/190], loss=8.4506
	step [36/190], loss=7.2802
	step [37/190], loss=6.8369
	step [38/190], loss=7.6956
	step [39/190], loss=8.8306
	step [40/190], loss=8.1762
	step [41/190], loss=6.0794
	step [42/190], loss=7.0582
	step [43/190], loss=7.6350
	step [44/190], loss=7.7150
	step [45/190], loss=7.2460
	step [46/190], loss=8.7809
	step [47/190], loss=5.7501
	step [48/190], loss=7.0606
	step [49/190], loss=6.8670
	step [50/190], loss=6.2016
	step [51/190], loss=6.6067
	step [52/190], loss=7.8532
	step [53/190], loss=6.2760
	step [54/190], loss=6.3786
	step [55/190], loss=6.7616
	step [56/190], loss=7.5661
	step [57/190], loss=5.7726
	step [58/190], loss=6.9776
	step [59/190], loss=6.7930
	step [60/190], loss=6.8567
	step [61/190], loss=7.0939
	step [62/190], loss=7.8666
	step [63/190], loss=6.5448
	step [64/190], loss=7.1341
	step [65/190], loss=8.6907
	step [66/190], loss=6.4838
	step [67/190], loss=6.6538
	step [68/190], loss=6.4084
	step [69/190], loss=7.0244
	step [70/190], loss=6.9320
	step [71/190], loss=6.1818
	step [72/190], loss=6.9563
	step [73/190], loss=6.8685
	step [74/190], loss=6.1834
	step [75/190], loss=9.1524
	step [76/190], loss=6.1911
	step [77/190], loss=7.1233
	step [78/190], loss=7.7766
	step [79/190], loss=6.5201
	step [80/190], loss=7.8014
	step [81/190], loss=6.4262
	step [82/190], loss=6.2127
	step [83/190], loss=7.6416
	step [84/190], loss=7.3143
	step [85/190], loss=8.2579
	step [86/190], loss=6.9577
	step [87/190], loss=7.5139
	step [88/190], loss=6.9586
	step [89/190], loss=6.7321
	step [90/190], loss=6.6157
	step [91/190], loss=8.8726
	step [92/190], loss=7.9414
	step [93/190], loss=7.3981
	step [94/190], loss=7.1016
	step [95/190], loss=5.9591
	step [96/190], loss=7.6714
	step [97/190], loss=6.7307
	step [98/190], loss=9.9110
	step [99/190], loss=5.9812
	step [100/190], loss=7.9849
	step [101/190], loss=6.7430
	step [102/190], loss=7.6688
	step [103/190], loss=6.8471
	step [104/190], loss=7.1130
	step [105/190], loss=8.6718
	step [106/190], loss=6.1493
	step [107/190], loss=6.8535
	step [108/190], loss=7.4990
	step [109/190], loss=5.7940
	step [110/190], loss=8.3137
	step [111/190], loss=7.3420
	step [112/190], loss=7.2805
	step [113/190], loss=7.7596
	step [114/190], loss=7.2828
	step [115/190], loss=6.8155
	step [116/190], loss=7.9405
	step [117/190], loss=6.1198
	step [118/190], loss=7.3631
	step [119/190], loss=7.3628
	step [120/190], loss=6.8528
	step [121/190], loss=7.0916
	step [122/190], loss=7.6954
	step [123/190], loss=7.3591
	step [124/190], loss=8.6014
	step [125/190], loss=8.0145
	step [126/190], loss=6.3825
	step [127/190], loss=6.9995
	step [128/190], loss=7.3022
	step [129/190], loss=7.9191
	step [130/190], loss=7.5362
	step [131/190], loss=6.7661
	step [132/190], loss=6.4553
	step [133/190], loss=7.8724
	step [134/190], loss=7.4256
	step [135/190], loss=5.9468
	step [136/190], loss=7.2576
	step [137/190], loss=7.1751
	step [138/190], loss=6.8421
	step [139/190], loss=7.2338
	step [140/190], loss=5.1043
	step [141/190], loss=6.4953
	step [142/190], loss=7.1953
	step [143/190], loss=6.9261
	step [144/190], loss=6.0450
	step [145/190], loss=6.5940
	step [146/190], loss=7.8067
	step [147/190], loss=7.3436
	step [148/190], loss=10.0759
	step [149/190], loss=7.0693
	step [150/190], loss=7.9651
	step [151/190], loss=7.8208
	step [152/190], loss=7.0286
	step [153/190], loss=7.5837
	step [154/190], loss=8.2504
	step [155/190], loss=7.4107
	step [156/190], loss=7.3593
	step [157/190], loss=5.4738
	step [158/190], loss=6.9803
	step [159/190], loss=7.1397
	step [160/190], loss=7.1522
	step [161/190], loss=10.7409
	step [162/190], loss=6.4925
	step [163/190], loss=6.5833
	step [164/190], loss=6.8396
	step [165/190], loss=7.3971
	step [166/190], loss=8.5786
	step [167/190], loss=7.0777
	step [168/190], loss=6.6654
	step [169/190], loss=6.4712
	step [170/190], loss=8.2454
	step [171/190], loss=6.8550
	step [172/190], loss=7.1162
	step [173/190], loss=6.9180
	step [174/190], loss=6.5337
	step [175/190], loss=7.1114
	step [176/190], loss=6.0154
	step [177/190], loss=6.3578
	step [178/190], loss=8.0443
	step [179/190], loss=7.1022
	step [180/190], loss=6.9644
	step [181/190], loss=5.7382
	step [182/190], loss=8.3058
	step [183/190], loss=6.6575
	step [184/190], loss=8.5798
	step [185/190], loss=7.1369
	step [186/190], loss=6.5596
	step [187/190], loss=6.7710
	step [188/190], loss=6.8736
	step [189/190], loss=6.2574
	step [190/190], loss=6.8193
	Evaluating
	loss=0.0169, precision=0.2931, recall=0.9876, f1=0.4520
saving model as: 1_saved_model.pth
Training epoch 28
	step [1/190], loss=6.3901
	step [2/190], loss=6.7223
	step [3/190], loss=8.9348
	step [4/190], loss=8.1158
	step [5/190], loss=5.7919
	step [6/190], loss=7.0974
	step [7/190], loss=8.0816
	step [8/190], loss=8.5698
	step [9/190], loss=7.2686
	step [10/190], loss=6.9707
	step [11/190], loss=6.8134
	step [12/190], loss=8.1920
	step [13/190], loss=6.5547
	step [14/190], loss=7.0197
	step [15/190], loss=8.0854
	step [16/190], loss=6.5860
	step [17/190], loss=5.9656
	step [18/190], loss=7.3388
	step [19/190], loss=7.6130
	step [20/190], loss=7.3598
	step [21/190], loss=5.1180
	step [22/190], loss=6.6292
	step [23/190], loss=6.4900
	step [24/190], loss=6.6014
	step [25/190], loss=7.1568
	step [26/190], loss=6.9437
	step [27/190], loss=8.1034
	step [28/190], loss=6.1025
	step [29/190], loss=6.8720
	step [30/190], loss=7.0203
	step [31/190], loss=7.1995
	step [32/190], loss=6.7800
	step [33/190], loss=7.4262
	step [34/190], loss=6.0553
	step [35/190], loss=7.6441
	step [36/190], loss=6.3505
	step [37/190], loss=7.4670
	step [38/190], loss=7.6195
	step [39/190], loss=7.4751
	step [40/190], loss=6.5865
	step [41/190], loss=7.6000
	step [42/190], loss=6.0566
	step [43/190], loss=6.3608
	step [44/190], loss=6.8695
	step [45/190], loss=7.1499
	step [46/190], loss=6.1048
	step [47/190], loss=5.6950
	step [48/190], loss=7.8220
	step [49/190], loss=6.7347
	step [50/190], loss=7.1085
	step [51/190], loss=6.8335
	step [52/190], loss=6.9654
	step [53/190], loss=8.0465
	step [54/190], loss=8.4711
	step [55/190], loss=6.7727
	step [56/190], loss=8.1032
	step [57/190], loss=7.0261
	step [58/190], loss=6.4174
	step [59/190], loss=7.2207
	step [60/190], loss=6.5170
	step [61/190], loss=6.0356
	step [62/190], loss=5.9170
	step [63/190], loss=7.4009
	step [64/190], loss=6.3221
	step [65/190], loss=8.5688
	step [66/190], loss=5.9891
	step [67/190], loss=7.0293
	step [68/190], loss=6.5075
	step [69/190], loss=8.6029
	step [70/190], loss=6.7343
	step [71/190], loss=6.6433
	step [72/190], loss=7.9124
	step [73/190], loss=6.3167
	step [74/190], loss=7.8216
	step [75/190], loss=7.0974
	step [76/190], loss=6.3705
	step [77/190], loss=7.0560
	step [78/190], loss=7.0502
	step [79/190], loss=5.9361
	step [80/190], loss=6.5442
	step [81/190], loss=7.4549
	step [82/190], loss=7.7059
	step [83/190], loss=6.6513
	step [84/190], loss=6.7640
	step [85/190], loss=6.9063
	step [86/190], loss=7.2804
	step [87/190], loss=7.5197
	step [88/190], loss=6.5673
	step [89/190], loss=5.9700
	step [90/190], loss=6.8497
	step [91/190], loss=7.1101
	step [92/190], loss=7.5068
	step [93/190], loss=7.8191
	step [94/190], loss=7.6805
	step [95/190], loss=9.0753
	step [96/190], loss=6.3940
	step [97/190], loss=7.7637
	step [98/190], loss=7.0396
	step [99/190], loss=6.4350
	step [100/190], loss=5.8311
	step [101/190], loss=6.3497
	step [102/190], loss=6.1069
	step [103/190], loss=7.9900
	step [104/190], loss=6.4910
	step [105/190], loss=8.0495
	step [106/190], loss=6.8314
	step [107/190], loss=7.1216
	step [108/190], loss=6.5205
	step [109/190], loss=6.5562
	step [110/190], loss=7.3693
	step [111/190], loss=7.0626
	step [112/190], loss=6.2759
	step [113/190], loss=7.2850
	step [114/190], loss=6.9029
	step [115/190], loss=6.8870
	step [116/190], loss=7.5982
	step [117/190], loss=6.9653
	step [118/190], loss=5.9819
	step [119/190], loss=6.8166
	step [120/190], loss=6.2594
	step [121/190], loss=8.0472
	step [122/190], loss=6.2483
	step [123/190], loss=7.6288
	step [124/190], loss=6.6622
	step [125/190], loss=7.4874
	step [126/190], loss=6.4400
	step [127/190], loss=8.0925
	step [128/190], loss=7.7123
	step [129/190], loss=6.5055
	step [130/190], loss=6.3487
	step [131/190], loss=6.5387
	step [132/190], loss=7.2465
	step [133/190], loss=6.8127
	step [134/190], loss=8.2874
	step [135/190], loss=7.2172
	step [136/190], loss=7.0958
	step [137/190], loss=6.8424
	step [138/190], loss=6.4274
	step [139/190], loss=8.2806
	step [140/190], loss=6.5797
	step [141/190], loss=8.0998
	step [142/190], loss=6.3376
	step [143/190], loss=8.7576
	step [144/190], loss=6.3028
	step [145/190], loss=8.1646
	step [146/190], loss=7.0890
	step [147/190], loss=6.6382
	step [148/190], loss=7.4941
	step [149/190], loss=6.2635
	step [150/190], loss=6.8181
	step [151/190], loss=7.4455
	step [152/190], loss=7.0653
	step [153/190], loss=7.3119
	step [154/190], loss=6.5299
	step [155/190], loss=6.8451
	step [156/190], loss=8.9363
	step [157/190], loss=7.0935
	step [158/190], loss=6.8474
	step [159/190], loss=7.1172
	step [160/190], loss=6.2887
	step [161/190], loss=7.1574
	step [162/190], loss=6.7399
	step [163/190], loss=7.6556
	step [164/190], loss=6.0972
	step [165/190], loss=6.5517
	step [166/190], loss=6.5345
	step [167/190], loss=7.3647
	step [168/190], loss=7.6872
	step [169/190], loss=7.5833
	step [170/190], loss=6.7082
	step [171/190], loss=6.4734
	step [172/190], loss=6.0307
	step [173/190], loss=8.7458
	step [174/190], loss=7.9109
	step [175/190], loss=6.1385
	step [176/190], loss=8.1077
	step [177/190], loss=5.9633
	step [178/190], loss=6.9319
	step [179/190], loss=7.1585
	step [180/190], loss=6.5575
	step [181/190], loss=6.0430
	step [182/190], loss=7.4576
	step [183/190], loss=7.0480
	step [184/190], loss=6.2733
	step [185/190], loss=6.0509
	step [186/190], loss=5.8831
	step [187/190], loss=6.2608
	step [188/190], loss=6.7199
	step [189/190], loss=7.4618
	step [190/190], loss=6.3106
	Evaluating
	loss=0.0195, precision=0.2619, recall=0.9906, f1=0.4143
Training epoch 29
	step [1/190], loss=6.5719
	step [2/190], loss=6.5936
	step [3/190], loss=7.0680
	step [4/190], loss=7.7516
	step [5/190], loss=6.5756
	step [6/190], loss=6.3935
	step [7/190], loss=5.7356
	step [8/190], loss=8.1535
	step [9/190], loss=8.2569
	step [10/190], loss=7.4513
	step [11/190], loss=7.4527
	step [12/190], loss=6.1545
	step [13/190], loss=6.5265
	step [14/190], loss=7.5927
	step [15/190], loss=7.8619
	step [16/190], loss=5.5066
	step [17/190], loss=6.7736
	step [18/190], loss=6.6721
	step [19/190], loss=6.4838
	step [20/190], loss=6.3153
	step [21/190], loss=8.7287
	step [22/190], loss=8.0840
	step [23/190], loss=6.7698
	step [24/190], loss=6.7594
	step [25/190], loss=7.1318
	step [26/190], loss=6.8572
	step [27/190], loss=6.0499
	step [28/190], loss=6.8711
	step [29/190], loss=5.7983
	step [30/190], loss=6.8698
	step [31/190], loss=5.5303
	step [32/190], loss=5.5387
	step [33/190], loss=6.2203
	step [34/190], loss=7.2070
	step [35/190], loss=7.3768
	step [36/190], loss=5.4707
	step [37/190], loss=7.0142
	step [38/190], loss=5.4509
	step [39/190], loss=7.0315
	step [40/190], loss=8.5332
	step [41/190], loss=6.3709
	step [42/190], loss=7.4370
	step [43/190], loss=8.6287
	step [44/190], loss=6.7649
	step [45/190], loss=6.7449
	step [46/190], loss=8.1113
	step [47/190], loss=6.5697
	step [48/190], loss=6.2609
	step [49/190], loss=6.9647
	step [50/190], loss=6.8874
	step [51/190], loss=6.8613
	step [52/190], loss=6.1582
	step [53/190], loss=6.3353
	step [54/190], loss=6.7796
	step [55/190], loss=6.6127
	step [56/190], loss=7.4717
	step [57/190], loss=6.2000
	step [58/190], loss=6.5734
	step [59/190], loss=5.5741
	step [60/190], loss=5.9050
	step [61/190], loss=6.2248
	step [62/190], loss=5.6558
	step [63/190], loss=6.7866
	step [64/190], loss=6.5890
	step [65/190], loss=7.1481
	step [66/190], loss=7.4972
	step [67/190], loss=7.2788
	step [68/190], loss=8.2983
	step [69/190], loss=6.9480
	step [70/190], loss=7.3315
	step [71/190], loss=7.9523
	step [72/190], loss=6.9504
	step [73/190], loss=6.6503
	step [74/190], loss=8.3640
	step [75/190], loss=7.2634
	step [76/190], loss=6.2675
	step [77/190], loss=6.2535
	step [78/190], loss=7.3715
	step [79/190], loss=7.0606
	step [80/190], loss=5.8499
	step [81/190], loss=6.6809
	step [82/190], loss=5.7987
	step [83/190], loss=6.4085
	step [84/190], loss=7.5975
	step [85/190], loss=6.0274
	step [86/190], loss=5.6158
	step [87/190], loss=7.5575
	step [88/190], loss=7.0157
	step [89/190], loss=7.3175
	step [90/190], loss=6.2305
	step [91/190], loss=6.0801
	step [92/190], loss=6.9448
	step [93/190], loss=7.6408
	step [94/190], loss=7.5816
	step [95/190], loss=5.8973
	step [96/190], loss=6.5901
	step [97/190], loss=7.8744
	step [98/190], loss=7.5986
	step [99/190], loss=5.5363
	step [100/190], loss=6.4527
	step [101/190], loss=8.5431
	step [102/190], loss=6.6748
	step [103/190], loss=6.8787
	step [104/190], loss=7.1788
	step [105/190], loss=6.0813
	step [106/190], loss=6.9324
	step [107/190], loss=8.1784
	step [108/190], loss=5.9899
	step [109/190], loss=5.8515
	step [110/190], loss=6.9993
	step [111/190], loss=7.1879
	step [112/190], loss=7.4917
	step [113/190], loss=8.4894
	step [114/190], loss=6.9771
	step [115/190], loss=6.7805
	step [116/190], loss=6.6649
	step [117/190], loss=6.7138
	step [118/190], loss=6.6370
	step [119/190], loss=6.4840
	step [120/190], loss=6.2303
	step [121/190], loss=7.6220
	step [122/190], loss=6.9465
	step [123/190], loss=6.0359
	step [124/190], loss=8.0244
	step [125/190], loss=7.2613
	step [126/190], loss=6.8084
	step [127/190], loss=6.1706
	step [128/190], loss=7.9109
	step [129/190], loss=5.6183
	step [130/190], loss=6.8601
	step [131/190], loss=7.3924
	step [132/190], loss=7.3131
	step [133/190], loss=5.4249
	step [134/190], loss=6.1489
	step [135/190], loss=6.9483
	step [136/190], loss=7.9268
	step [137/190], loss=7.3194
	step [138/190], loss=7.9332
	step [139/190], loss=7.8997
	step [140/190], loss=7.5348
	step [141/190], loss=5.8050
	step [142/190], loss=7.4086
	step [143/190], loss=7.1772
	step [144/190], loss=7.4710
	step [145/190], loss=8.2795
	step [146/190], loss=7.6285
	step [147/190], loss=6.2305
	step [148/190], loss=7.5599
	step [149/190], loss=6.9735
	step [150/190], loss=7.1533
	step [151/190], loss=6.8691
	step [152/190], loss=8.0342
	step [153/190], loss=6.8727
	step [154/190], loss=5.6433
	step [155/190], loss=6.3834
	step [156/190], loss=5.8693
	step [157/190], loss=6.6392
	step [158/190], loss=6.2369
	step [159/190], loss=6.3042
	step [160/190], loss=7.0346
	step [161/190], loss=8.0821
	step [162/190], loss=5.6290
	step [163/190], loss=5.8935
	step [164/190], loss=6.6816
	step [165/190], loss=5.7174
	step [166/190], loss=5.7478
	step [167/190], loss=5.9058
	step [168/190], loss=7.0017
	step [169/190], loss=7.1769
	step [170/190], loss=7.6920
	step [171/190], loss=6.0922
	step [172/190], loss=5.3890
	step [173/190], loss=7.1969
	step [174/190], loss=5.8588
	step [175/190], loss=6.6997
	step [176/190], loss=6.7394
	step [177/190], loss=8.1354
	step [178/190], loss=6.5166
	step [179/190], loss=6.0284
	step [180/190], loss=5.5781
	step [181/190], loss=5.3063
	step [182/190], loss=5.7495
	step [183/190], loss=6.6989
	step [184/190], loss=7.0646
	step [185/190], loss=7.3992
	step [186/190], loss=6.1360
	step [187/190], loss=6.9546
	step [188/190], loss=6.5465
	step [189/190], loss=6.0273
	step [190/190], loss=5.9898
	Evaluating
	loss=0.0188, precision=0.2577, recall=0.9902, f1=0.4089
Training epoch 30
	step [1/190], loss=6.6983
	step [2/190], loss=6.0104
	step [3/190], loss=6.3655
	step [4/190], loss=6.3763
	step [5/190], loss=6.5559
	step [6/190], loss=6.1723
	step [7/190], loss=5.9516
	step [8/190], loss=6.6704
	step [9/190], loss=6.5026
	step [10/190], loss=8.1085
	step [11/190], loss=8.9236
	step [12/190], loss=5.3493
	step [13/190], loss=6.9717
	step [14/190], loss=5.8878
	step [15/190], loss=7.2459
	step [16/190], loss=7.0058
	step [17/190], loss=6.1959
	step [18/190], loss=6.3424
	step [19/190], loss=5.5316
	step [20/190], loss=6.4109
	step [21/190], loss=8.0098
	step [22/190], loss=6.2286
	step [23/190], loss=8.1318
	step [24/190], loss=6.0220
	step [25/190], loss=7.1678
	step [26/190], loss=6.4303
	step [27/190], loss=6.8326
	step [28/190], loss=6.3144
	step [29/190], loss=6.3472
	step [30/190], loss=7.8198
	step [31/190], loss=6.3333
	step [32/190], loss=5.4451
	step [33/190], loss=5.6711
	step [34/190], loss=5.9250
	step [35/190], loss=5.6992
	step [36/190], loss=6.0829
	step [37/190], loss=7.3130
	step [38/190], loss=6.3152
	step [39/190], loss=8.8924
	step [40/190], loss=6.7432
	step [41/190], loss=6.9935
	step [42/190], loss=8.1438
	step [43/190], loss=5.5333
	step [44/190], loss=7.2933
	step [45/190], loss=7.2647
	step [46/190], loss=6.2063
	step [47/190], loss=7.1928
	step [48/190], loss=6.2098
	step [49/190], loss=7.4559
	step [50/190], loss=5.8519
	step [51/190], loss=7.0963
	step [52/190], loss=6.6466
	step [53/190], loss=7.4521
	step [54/190], loss=6.8181
	step [55/190], loss=7.5233
	step [56/190], loss=7.2648
	step [57/190], loss=7.0917
	step [58/190], loss=7.2727
	step [59/190], loss=6.2696
	step [60/190], loss=7.2858
	step [61/190], loss=5.9812
	step [62/190], loss=7.0672
	step [63/190], loss=8.2056
	step [64/190], loss=6.6627
	step [65/190], loss=8.0438
	step [66/190], loss=6.4558
	step [67/190], loss=5.5278
	step [68/190], loss=7.4380
	step [69/190], loss=6.9041
	step [70/190], loss=7.5247
	step [71/190], loss=7.0514
	step [72/190], loss=6.8941
	step [73/190], loss=6.1379
	step [74/190], loss=7.2419
	step [75/190], loss=6.1142
	step [76/190], loss=7.2108
	step [77/190], loss=6.2051
	step [78/190], loss=8.0603
	step [79/190], loss=6.3170
	step [80/190], loss=6.0354
	step [81/190], loss=7.4979
	step [82/190], loss=6.2732
	step [83/190], loss=6.2451
	step [84/190], loss=6.9417
	step [85/190], loss=6.0999
	step [86/190], loss=6.7096
	step [87/190], loss=6.2888
	step [88/190], loss=7.3828
	step [89/190], loss=7.2849
	step [90/190], loss=5.2144
	step [91/190], loss=7.7699
	step [92/190], loss=6.5067
	step [93/190], loss=7.1949
	step [94/190], loss=5.5214
	step [95/190], loss=6.2087
	step [96/190], loss=6.7556
	step [97/190], loss=7.1814
	step [98/190], loss=5.9819
	step [99/190], loss=5.5816
	step [100/190], loss=7.5040
	step [101/190], loss=6.7814
	step [102/190], loss=5.5615
	step [103/190], loss=5.9650
	step [104/190], loss=6.6245
	step [105/190], loss=6.2122
	step [106/190], loss=6.8482
	step [107/190], loss=6.3222
	step [108/190], loss=6.6938
	step [109/190], loss=6.4734
	step [110/190], loss=6.4636
	step [111/190], loss=5.8273
	step [112/190], loss=6.5179
	step [113/190], loss=6.8428
	step [114/190], loss=7.2931
	step [115/190], loss=6.6971
	step [116/190], loss=7.1346
	step [117/190], loss=10.2401
	step [118/190], loss=7.1745
	step [119/190], loss=6.5132
	step [120/190], loss=6.6051
	step [121/190], loss=6.2295
	step [122/190], loss=6.8866
	step [123/190], loss=6.4966
	step [124/190], loss=7.0675
	step [125/190], loss=6.7912
	step [126/190], loss=6.0140
	step [127/190], loss=6.2275
	step [128/190], loss=6.8064
	step [129/190], loss=5.7679
	step [130/190], loss=6.5525
	step [131/190], loss=7.1815
	step [132/190], loss=6.2320
	step [133/190], loss=5.2677
	step [134/190], loss=8.6710
	step [135/190], loss=6.3578
	step [136/190], loss=7.0423
	step [137/190], loss=5.4881
	step [138/190], loss=5.1212
	step [139/190], loss=8.0172
	step [140/190], loss=8.0442
	step [141/190], loss=7.7100
	step [142/190], loss=6.3391
	step [143/190], loss=6.4112
	step [144/190], loss=6.5383
	step [145/190], loss=6.2511
	step [146/190], loss=6.8838
	step [147/190], loss=5.3121
	step [148/190], loss=6.9877
	step [149/190], loss=6.4053
	step [150/190], loss=6.3792
	step [151/190], loss=6.9859
	step [152/190], loss=6.6140
	step [153/190], loss=6.1868
	step [154/190], loss=8.0282
	step [155/190], loss=6.7773
	step [156/190], loss=8.1835
	step [157/190], loss=9.2222
	step [158/190], loss=5.3371
	step [159/190], loss=8.4582
	step [160/190], loss=6.1519
	step [161/190], loss=5.5973
	step [162/190], loss=6.6521
	step [163/190], loss=6.7637
	step [164/190], loss=7.5734
	step [165/190], loss=7.5488
	step [166/190], loss=6.3151
	step [167/190], loss=6.8809
	step [168/190], loss=6.0320
	step [169/190], loss=7.6999
	step [170/190], loss=5.6354
	step [171/190], loss=7.5579
	step [172/190], loss=7.4475
	step [173/190], loss=6.2157
	step [174/190], loss=6.1546
	step [175/190], loss=6.2852
	step [176/190], loss=7.1139
	step [177/190], loss=7.0736
	step [178/190], loss=7.4227
	step [179/190], loss=6.0755
	step [180/190], loss=8.0324
	step [181/190], loss=6.7183
	step [182/190], loss=6.3400
	step [183/190], loss=6.8073
	step [184/190], loss=6.4420
	step [185/190], loss=7.6744
	step [186/190], loss=7.0697
	step [187/190], loss=6.9006
	step [188/190], loss=6.4893
	step [189/190], loss=7.9122
	step [190/190], loss=6.4881
	Evaluating
	loss=0.0236, precision=0.2118, recall=0.9907, f1=0.3489
Training finished
best_f1: 0.4520436022062051
directing: X rim_enhanced: False test_id 2
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9348 # image files with weight 9316
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2534 # image files with weight 2516
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9316
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/146], loss=628.0723
	step [2/146], loss=352.1444
	step [3/146], loss=255.9201
	step [4/146], loss=200.8872
	step [5/146], loss=180.1655
	step [6/146], loss=177.4174
	step [7/146], loss=173.4353
	step [8/146], loss=172.2341
	step [9/146], loss=169.1858
	step [10/146], loss=167.4616
	step [11/146], loss=166.3637
	step [12/146], loss=165.3860
	step [13/146], loss=164.7300
	step [14/146], loss=162.3134
	step [15/146], loss=160.0609
	step [16/146], loss=160.7141
	step [17/146], loss=159.8413
	step [18/146], loss=158.1050
	step [19/146], loss=156.0311
	step [20/146], loss=154.3392
	step [21/146], loss=153.6140
	step [22/146], loss=153.4996
	step [23/146], loss=149.0388
	step [24/146], loss=152.1921
	step [25/146], loss=148.2449
	step [26/146], loss=147.8444
	step [27/146], loss=146.5265
	step [28/146], loss=143.8013
	step [29/146], loss=144.1559
	step [30/146], loss=143.9785
	step [31/146], loss=143.5800
	step [32/146], loss=140.8922
	step [33/146], loss=139.0605
	step [34/146], loss=139.9524
	step [35/146], loss=138.0498
	step [36/146], loss=136.7582
	step [37/146], loss=135.9777
	step [38/146], loss=135.7017
	step [39/146], loss=135.6355
	step [40/146], loss=132.9805
	step [41/146], loss=133.3170
	step [42/146], loss=132.4441
	step [43/146], loss=128.7809
	step [44/146], loss=129.3049
	step [45/146], loss=131.2603
	step [46/146], loss=129.0024
	step [47/146], loss=126.4012
	step [48/146], loss=128.0176
	step [49/146], loss=128.7570
	step [50/146], loss=127.2654
	step [51/146], loss=123.1518
	step [52/146], loss=124.3028
	step [53/146], loss=122.5081
	step [54/146], loss=123.5963
	step [55/146], loss=122.3244
	step [56/146], loss=119.9246
	step [57/146], loss=121.2678
	step [58/146], loss=121.2682
	step [59/146], loss=121.9799
	step [60/146], loss=121.0886
	step [61/146], loss=118.5908
	step [62/146], loss=117.7098
	step [63/146], loss=117.5917
	step [64/146], loss=118.4347
	step [65/146], loss=116.5471
	step [66/146], loss=114.3239
	step [67/146], loss=116.5542
	step [68/146], loss=113.9265
	step [69/146], loss=112.9563
	step [70/146], loss=113.3206
	step [71/146], loss=113.1639
	step [72/146], loss=111.6184
	step [73/146], loss=112.0772
	step [74/146], loss=111.5609
	step [75/146], loss=109.8587
	step [76/146], loss=111.0537
	step [77/146], loss=110.8674
	step [78/146], loss=108.1397
	step [79/146], loss=110.2231
	step [80/146], loss=106.4443
	step [81/146], loss=108.5067
	step [82/146], loss=107.5037
	step [83/146], loss=106.9515
	step [84/146], loss=107.0156
	step [85/146], loss=105.9348
	step [86/146], loss=105.2519
	step [87/146], loss=106.1178
	step [88/146], loss=105.3914
	step [89/146], loss=104.6823
	step [90/146], loss=105.9708
	step [91/146], loss=102.5316
	step [92/146], loss=103.3383
	step [93/146], loss=101.5840
	step [94/146], loss=101.9749
	step [95/146], loss=103.1628
	step [96/146], loss=105.0294
	step [97/146], loss=101.3461
	step [98/146], loss=100.0757
	step [99/146], loss=102.0220
	step [100/146], loss=100.4643
	step [101/146], loss=100.9220
	step [102/146], loss=99.0725
	step [103/146], loss=100.3584
	step [104/146], loss=98.2097
	step [105/146], loss=98.5758
	step [106/146], loss=98.7785
	step [107/146], loss=98.6541
	step [108/146], loss=98.4717
	step [109/146], loss=97.7267
	step [110/146], loss=97.5973
	step [111/146], loss=97.5327
	step [112/146], loss=97.3384
	step [113/146], loss=97.0199
	step [114/146], loss=96.4173
	step [115/146], loss=96.1852
	step [116/146], loss=96.6295
	step [117/146], loss=95.9985
	step [118/146], loss=95.4865
	step [119/146], loss=96.7270
	step [120/146], loss=94.5288
	step [121/146], loss=94.8549
	step [122/146], loss=94.9083
	step [123/146], loss=94.5779
	step [124/146], loss=95.1427
	step [125/146], loss=93.9278
	step [126/146], loss=92.8633
	step [127/146], loss=93.5809
	step [128/146], loss=94.1499
	step [129/146], loss=93.6121
	step [130/146], loss=91.9382
	step [131/146], loss=92.5724
	step [132/146], loss=90.2531
	step [133/146], loss=91.5884
	step [134/146], loss=92.6687
	step [135/146], loss=90.9208
	step [136/146], loss=93.6959
	step [137/146], loss=92.1305
	step [138/146], loss=90.5312
	step [139/146], loss=89.7669
	step [140/146], loss=91.2379
	step [141/146], loss=92.9892
	step [142/146], loss=91.3535
	step [143/146], loss=89.9281
	step [144/146], loss=90.0238
	step [145/146], loss=89.7962
	step [146/146], loss=51.0270
	Evaluating
	loss=0.3416, precision=0.1420, recall=0.9976, f1=0.2487
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/146], loss=90.2615
	step [2/146], loss=88.9943
	step [3/146], loss=87.3101
	step [4/146], loss=89.6808
	step [5/146], loss=88.8805
	step [6/146], loss=89.7308
	step [7/146], loss=89.3399
	step [8/146], loss=87.7473
	step [9/146], loss=87.4472
	step [10/146], loss=88.2604
	step [11/146], loss=88.6093
	step [12/146], loss=88.4548
	step [13/146], loss=86.4305
	step [14/146], loss=87.3118
	step [15/146], loss=86.2871
	step [16/146], loss=87.1482
	step [17/146], loss=88.8544
	step [18/146], loss=86.0649
	step [19/146], loss=88.3551
	step [20/146], loss=86.3529
	step [21/146], loss=87.0237
	step [22/146], loss=86.1241
	step [23/146], loss=86.1187
	step [24/146], loss=85.7695
	step [25/146], loss=84.7310
	step [26/146], loss=87.2200
	step [27/146], loss=84.7922
	step [28/146], loss=84.4900
	step [29/146], loss=86.7537
	step [30/146], loss=86.3762
	step [31/146], loss=84.3991
	step [32/146], loss=84.8016
	step [33/146], loss=85.1841
	step [34/146], loss=82.2900
	step [35/146], loss=85.6367
	step [36/146], loss=83.9801
	step [37/146], loss=84.3882
	step [38/146], loss=83.7756
	step [39/146], loss=83.3979
	step [40/146], loss=83.6479
	step [41/146], loss=82.1008
	step [42/146], loss=83.4843
	step [43/146], loss=83.0253
	step [44/146], loss=82.7946
	step [45/146], loss=84.1278
	step [46/146], loss=82.5637
	step [47/146], loss=81.9896
	step [48/146], loss=82.1032
	step [49/146], loss=82.9418
	step [50/146], loss=81.9510
	step [51/146], loss=82.8077
	step [52/146], loss=82.5309
	step [53/146], loss=81.9982
	step [54/146], loss=82.2537
	step [55/146], loss=82.5637
	step [56/146], loss=80.5633
	step [57/146], loss=80.8269
	step [58/146], loss=80.2561
	step [59/146], loss=79.2743
	step [60/146], loss=80.7691
	step [61/146], loss=79.9951
	step [62/146], loss=82.6926
	step [63/146], loss=79.2100
	step [64/146], loss=83.0777
	step [65/146], loss=79.4537
	step [66/146], loss=79.2664
	step [67/146], loss=79.2263
	step [68/146], loss=78.9137
	step [69/146], loss=84.0997
	step [70/146], loss=82.1643
	step [71/146], loss=80.9029
	step [72/146], loss=80.9529
	step [73/146], loss=79.8884
	step [74/146], loss=81.8593
	step [75/146], loss=79.5763
	step [76/146], loss=79.7228
	step [77/146], loss=78.8105
	step [78/146], loss=79.2116
	step [79/146], loss=79.2440
	step [80/146], loss=80.7548
	step [81/146], loss=81.3959
	step [82/146], loss=80.1892
	step [83/146], loss=79.5472
	step [84/146], loss=80.1929
	step [85/146], loss=79.4914
	step [86/146], loss=79.3991
	step [87/146], loss=77.5550
	step [88/146], loss=78.7978
	step [89/146], loss=79.7541
	step [90/146], loss=79.2245
	step [91/146], loss=76.5485
	step [92/146], loss=79.5364
	step [93/146], loss=77.1800
	step [94/146], loss=77.0050
	step [95/146], loss=75.5639
	step [96/146], loss=80.6601
	step [97/146], loss=76.5790
	step [98/146], loss=75.9023
	step [99/146], loss=77.8240
	step [100/146], loss=77.4478
	step [101/146], loss=78.0912
	step [102/146], loss=76.4728
	step [103/146], loss=75.8424
	step [104/146], loss=78.7068
	step [105/146], loss=76.9412
	step [106/146], loss=75.4176
	step [107/146], loss=77.2770
	step [108/146], loss=75.4648
	step [109/146], loss=76.5670
	step [110/146], loss=76.1899
	step [111/146], loss=75.5966
	step [112/146], loss=75.9042
	step [113/146], loss=74.5966
	step [114/146], loss=75.0501
	step [115/146], loss=74.6091
	step [116/146], loss=75.7250
	step [117/146], loss=76.1427
	step [118/146], loss=76.2801
	step [119/146], loss=75.0183
	step [120/146], loss=74.1914
	step [121/146], loss=73.5718
	step [122/146], loss=76.1268
	step [123/146], loss=73.4598
	step [124/146], loss=74.0063
	step [125/146], loss=73.6013
	step [126/146], loss=74.0545
	step [127/146], loss=74.2633
	step [128/146], loss=73.9454
	step [129/146], loss=75.4115
	step [130/146], loss=73.8418
	step [131/146], loss=74.5828
	step [132/146], loss=73.6983
	step [133/146], loss=73.6075
	step [134/146], loss=73.8876
	step [135/146], loss=73.8630
	step [136/146], loss=73.7772
	step [137/146], loss=73.0628
	step [138/146], loss=72.3429
	step [139/146], loss=73.2841
	step [140/146], loss=72.0114
	step [141/146], loss=70.9843
	step [142/146], loss=72.8081
	step [143/146], loss=73.5212
	step [144/146], loss=71.2878
	step [145/146], loss=71.9391
	step [146/146], loss=41.4447
	Evaluating
	loss=0.2772, precision=0.1618, recall=0.9975, f1=0.2784
saving model as: 2_saved_model.pth
Training epoch 3
	step [1/146], loss=72.9610
	step [2/146], loss=71.7713
	step [3/146], loss=73.1376
	step [4/146], loss=72.4341
	step [5/146], loss=71.3903
	step [6/146], loss=71.4790
	step [7/146], loss=70.7090
	step [8/146], loss=71.0977
	step [9/146], loss=74.9471
	step [10/146], loss=71.9434
	step [11/146], loss=71.4620
	step [12/146], loss=71.8776
	step [13/146], loss=71.9125
	step [14/146], loss=69.0943
	step [15/146], loss=70.7955
	step [16/146], loss=70.7367
	step [17/146], loss=71.8917
	step [18/146], loss=71.0923
	step [19/146], loss=70.1700
	step [20/146], loss=70.7460
	step [21/146], loss=70.3766
	step [22/146], loss=71.9889
	step [23/146], loss=71.3674
	step [24/146], loss=71.3617
	step [25/146], loss=70.8849
	step [26/146], loss=69.6609
	step [27/146], loss=70.4860
	step [28/146], loss=68.9551
	step [29/146], loss=70.6901
	step [30/146], loss=70.0731
	step [31/146], loss=69.4317
	step [32/146], loss=70.1939
	step [33/146], loss=69.5077
	step [34/146], loss=69.7509
	step [35/146], loss=67.8622
	step [36/146], loss=70.5521
	step [37/146], loss=69.6264
	step [38/146], loss=68.4071
	step [39/146], loss=70.3174
	step [40/146], loss=69.4586
	step [41/146], loss=69.4967
	step [42/146], loss=68.5827
	step [43/146], loss=69.6335
	step [44/146], loss=68.9554
	step [45/146], loss=68.5960
	step [46/146], loss=70.6959
	step [47/146], loss=68.3305
	step [48/146], loss=68.6949
	step [49/146], loss=67.6764
	step [50/146], loss=69.7930
	step [51/146], loss=69.0435
	step [52/146], loss=67.3892
	step [53/146], loss=68.2528
	step [54/146], loss=67.7545
	step [55/146], loss=68.1789
	step [56/146], loss=68.1820
	step [57/146], loss=68.5319
	step [58/146], loss=68.7597
	step [59/146], loss=68.1948
	step [60/146], loss=67.9718
	step [61/146], loss=66.8439
	step [62/146], loss=67.1563
	step [63/146], loss=68.8103
	step [64/146], loss=66.5984
	step [65/146], loss=66.8091
	step [66/146], loss=67.7640
	step [67/146], loss=68.7684
	step [68/146], loss=69.3390
	step [69/146], loss=67.2322
	step [70/146], loss=67.7525
	step [71/146], loss=66.7122
	step [72/146], loss=69.0279
	step [73/146], loss=66.9207
	step [74/146], loss=68.4147
	step [75/146], loss=67.0861
	step [76/146], loss=67.0996
	step [77/146], loss=64.8439
	step [78/146], loss=67.5506
	step [79/146], loss=66.3797
	step [80/146], loss=64.7295
	step [81/146], loss=66.3774
	step [82/146], loss=63.9469
	step [83/146], loss=65.6736
	step [84/146], loss=65.8974
	step [85/146], loss=65.8898
	step [86/146], loss=66.0737
	step [87/146], loss=66.0080
	step [88/146], loss=64.2421
	step [89/146], loss=66.6809
	step [90/146], loss=63.4803
	step [91/146], loss=64.3677
	step [92/146], loss=66.0472
	step [93/146], loss=65.0926
	step [94/146], loss=67.5190
	step [95/146], loss=64.5635
	step [96/146], loss=64.5082
	step [97/146], loss=64.0254
	step [98/146], loss=64.9935
	step [99/146], loss=63.7380
	step [100/146], loss=63.9036
	step [101/146], loss=66.4253
	step [102/146], loss=65.8378
	step [103/146], loss=63.2512
	step [104/146], loss=62.4291
	step [105/146], loss=64.5340
	step [106/146], loss=64.5650
	step [107/146], loss=62.9939
	step [108/146], loss=64.6659
	step [109/146], loss=66.3609
	step [110/146], loss=65.3200
	step [111/146], loss=64.3530
	step [112/146], loss=64.2166
	step [113/146], loss=64.1239
	step [114/146], loss=64.0585
	step [115/146], loss=62.7383
	step [116/146], loss=64.2772
	step [117/146], loss=62.8565
	step [118/146], loss=62.2705
	step [119/146], loss=62.2689
	step [120/146], loss=63.5302
	step [121/146], loss=64.1920
	step [122/146], loss=61.5895
	step [123/146], loss=61.6380
	step [124/146], loss=60.1766
	step [125/146], loss=62.1171
	step [126/146], loss=62.6951
	step [127/146], loss=61.3293
	step [128/146], loss=61.8619
	step [129/146], loss=62.7100
	step [130/146], loss=62.0239
	step [131/146], loss=62.6143
	step [132/146], loss=62.1600
	step [133/146], loss=61.2087
	step [134/146], loss=62.3214
	step [135/146], loss=63.1806
	step [136/146], loss=63.8138
	step [137/146], loss=60.8884
	step [138/146], loss=62.4724
	step [139/146], loss=63.0028
	step [140/146], loss=61.9326
	step [141/146], loss=61.1864
	step [142/146], loss=63.0704
	step [143/146], loss=61.3444
	step [144/146], loss=62.8926
	step [145/146], loss=62.6842
	step [146/146], loss=34.1102
	Evaluating
	loss=0.2250, precision=0.1813, recall=0.9971, f1=0.3068
saving model as: 2_saved_model.pth
Training epoch 4
	step [1/146], loss=60.8212
	step [2/146], loss=62.9229
	step [3/146], loss=58.8026
	step [4/146], loss=60.9629
	step [5/146], loss=60.7594
	step [6/146], loss=63.2064
	step [7/146], loss=60.5428
	step [8/146], loss=59.9493
	step [9/146], loss=60.0774
	step [10/146], loss=59.3607
	step [11/146], loss=61.2005
	step [12/146], loss=60.3364
	step [13/146], loss=59.1173
	step [14/146], loss=60.9949
	step [15/146], loss=60.0492
	step [16/146], loss=60.4046
	step [17/146], loss=59.7022
	step [18/146], loss=61.0860
	step [19/146], loss=61.2058
	step [20/146], loss=59.5842
	step [21/146], loss=60.1909
	step [22/146], loss=61.3778
	step [23/146], loss=60.0238
	step [24/146], loss=60.2649
	step [25/146], loss=59.7155
	step [26/146], loss=59.7653
	step [27/146], loss=59.1115
	step [28/146], loss=59.2277
	step [29/146], loss=58.1451
	step [30/146], loss=62.1424
	step [31/146], loss=58.3587
	step [32/146], loss=60.1342
	step [33/146], loss=60.1353
	step [34/146], loss=59.7151
	step [35/146], loss=59.3960
	step [36/146], loss=59.3166
	step [37/146], loss=59.1389
	step [38/146], loss=59.7529
	step [39/146], loss=58.9606
	step [40/146], loss=59.1382
	step [41/146], loss=58.5859
	step [42/146], loss=59.3127
	step [43/146], loss=59.5680
	step [44/146], loss=58.9060
	step [45/146], loss=60.3966
	step [46/146], loss=57.8429
	step [47/146], loss=58.8609
	step [48/146], loss=59.1813
	step [49/146], loss=58.3554
	step [50/146], loss=59.9475
	step [51/146], loss=57.9211
	step [52/146], loss=59.6950
	step [53/146], loss=58.4540
	step [54/146], loss=57.3193
	step [55/146], loss=58.4373
	step [56/146], loss=56.7432
	step [57/146], loss=58.4212
	step [58/146], loss=58.1855
	step [59/146], loss=58.0895
	step [60/146], loss=58.8605
	step [61/146], loss=57.5537
	step [62/146], loss=56.9149
	step [63/146], loss=58.2604
	step [64/146], loss=57.3809
	step [65/146], loss=57.5231
	step [66/146], loss=56.1133
	step [67/146], loss=56.7105
	step [68/146], loss=57.4251
	step [69/146], loss=56.1236
	step [70/146], loss=58.5413
	step [71/146], loss=53.9240
	step [72/146], loss=57.1579
	step [73/146], loss=54.9496
	step [74/146], loss=55.9419
	step [75/146], loss=56.1358
	step [76/146], loss=56.8994
	step [77/146], loss=56.7897
	step [78/146], loss=55.8314
	step [79/146], loss=54.2536
	step [80/146], loss=57.0040
	step [81/146], loss=55.7700
	step [82/146], loss=56.7429
	step [83/146], loss=57.3239
	step [84/146], loss=56.0608
	step [85/146], loss=55.5245
	step [86/146], loss=54.9073
	step [87/146], loss=55.1129
	step [88/146], loss=57.6592
	step [89/146], loss=54.8367
	step [90/146], loss=55.7139
	step [91/146], loss=54.5585
	step [92/146], loss=55.4962
	step [93/146], loss=55.1974
	step [94/146], loss=54.9109
	step [95/146], loss=54.8774
	step [96/146], loss=56.1934
	step [97/146], loss=55.6046
	step [98/146], loss=54.7626
	step [99/146], loss=56.3512
	step [100/146], loss=54.5088
	step [101/146], loss=54.6399
	step [102/146], loss=54.9957
	step [103/146], loss=55.6895
	step [104/146], loss=55.4143
	step [105/146], loss=54.8979
	step [106/146], loss=54.5230
	step [107/146], loss=54.6361
	step [108/146], loss=55.4402
	step [109/146], loss=55.1111
	step [110/146], loss=54.1059
	step [111/146], loss=54.3478
	step [112/146], loss=55.2370
	step [113/146], loss=54.4230
	step [114/146], loss=53.4112
	step [115/146], loss=53.1754
	step [116/146], loss=55.2543
	step [117/146], loss=55.2101
	step [118/146], loss=54.4932
	step [119/146], loss=53.8973
	step [120/146], loss=55.9841
	step [121/146], loss=54.7157
	step [122/146], loss=53.7857
	step [123/146], loss=53.3952
	step [124/146], loss=54.7260
	step [125/146], loss=53.4945
	step [126/146], loss=53.2945
	step [127/146], loss=53.0784
	step [128/146], loss=53.8286
	step [129/146], loss=54.2565
	step [130/146], loss=54.0535
	step [131/146], loss=52.6583
	step [132/146], loss=52.2677
	step [133/146], loss=52.6902
	step [134/146], loss=54.2794
	step [135/146], loss=54.0181
	step [136/146], loss=51.9073
	step [137/146], loss=52.0826
	step [138/146], loss=51.9408
	step [139/146], loss=52.9512
	step [140/146], loss=54.7393
	step [141/146], loss=53.9452
	step [142/146], loss=53.0384
	step [143/146], loss=53.6337
	step [144/146], loss=52.3608
	step [145/146], loss=52.6420
	step [146/146], loss=29.4756
	Evaluating
	loss=0.1965, precision=0.2048, recall=0.9967, f1=0.3398
saving model as: 2_saved_model.pth
Training epoch 5
	step [1/146], loss=52.9003
	step [2/146], loss=54.1816
	step [3/146], loss=51.8933
	step [4/146], loss=50.8847
	step [5/146], loss=51.9453
	step [6/146], loss=52.5129
	step [7/146], loss=51.7521
	step [8/146], loss=51.5301
	step [9/146], loss=51.8107
	step [10/146], loss=51.2213
	step [11/146], loss=53.0902
	step [12/146], loss=53.9803
	step [13/146], loss=51.4553
	step [14/146], loss=51.7481
	step [15/146], loss=53.8922
	step [16/146], loss=50.9193
	step [17/146], loss=54.1907
	step [18/146], loss=51.4818
	step [19/146], loss=51.1806
	step [20/146], loss=49.4092
	step [21/146], loss=50.7397
	step [22/146], loss=52.6159
	step [23/146], loss=52.8602
	step [24/146], loss=50.7153
	step [25/146], loss=50.0154
	step [26/146], loss=50.5520
	step [27/146], loss=52.6121
	step [28/146], loss=51.6169
	step [29/146], loss=50.5799
	step [30/146], loss=50.7834
	step [31/146], loss=52.1391
	step [32/146], loss=51.2094
	step [33/146], loss=52.1588
	step [34/146], loss=50.8545
	step [35/146], loss=51.0635
	step [36/146], loss=51.5569
	step [37/146], loss=50.5296
	step [38/146], loss=50.9991
	step [39/146], loss=51.3092
	step [40/146], loss=49.6685
	step [41/146], loss=51.0883
	step [42/146], loss=51.6133
	step [43/146], loss=50.7391
	step [44/146], loss=51.0928
	step [45/146], loss=49.1922
	step [46/146], loss=51.8052
	step [47/146], loss=49.5565
	step [48/146], loss=50.6433
	step [49/146], loss=51.4826
	step [50/146], loss=49.2841
	step [51/146], loss=49.8975
	step [52/146], loss=49.9742
	step [53/146], loss=49.8610
	step [54/146], loss=49.1514
	step [55/146], loss=50.4040
	step [56/146], loss=49.3730
	step [57/146], loss=49.8355
	step [58/146], loss=51.5423
	step [59/146], loss=48.8265
	step [60/146], loss=49.8947
	step [61/146], loss=51.3815
	step [62/146], loss=49.8682
	step [63/146], loss=50.6673
	step [64/146], loss=48.4649
	step [65/146], loss=49.6616
	step [66/146], loss=48.9382
	step [67/146], loss=49.6485
	step [68/146], loss=49.5511
	step [69/146], loss=47.7651
	step [70/146], loss=49.2793
	step [71/146], loss=49.2595
	step [72/146], loss=49.4756
	step [73/146], loss=49.7851
	step [74/146], loss=49.0211
	step [75/146], loss=50.6803
	step [76/146], loss=48.2399
	step [77/146], loss=48.8070
	step [78/146], loss=48.8811
	step [79/146], loss=50.2086
	step [80/146], loss=48.4569
	step [81/146], loss=47.7005
	step [82/146], loss=48.3144
	step [83/146], loss=47.5990
	step [84/146], loss=48.8235
	step [85/146], loss=47.3144
	step [86/146], loss=48.8339
	step [87/146], loss=47.9019
	step [88/146], loss=48.7258
	step [89/146], loss=46.8512
	step [90/146], loss=47.4634
	step [91/146], loss=48.7097
	step [92/146], loss=46.5052
	step [93/146], loss=48.6850
	step [94/146], loss=47.6175
	step [95/146], loss=47.9322
	step [96/146], loss=47.8697
	step [97/146], loss=47.6252
	step [98/146], loss=48.1562
	step [99/146], loss=49.9049
	step [100/146], loss=47.5566
	step [101/146], loss=46.3669
	step [102/146], loss=48.3804
	step [103/146], loss=47.5541
	step [104/146], loss=47.5254
	step [105/146], loss=46.4985
	step [106/146], loss=47.1207
	step [107/146], loss=48.3463
	step [108/146], loss=46.9886
	step [109/146], loss=46.0805
	step [110/146], loss=45.3378
	step [111/146], loss=46.1450
	step [112/146], loss=47.4577
	step [113/146], loss=46.0360
	step [114/146], loss=47.6437
	step [115/146], loss=46.9495
	step [116/146], loss=46.0485
	step [117/146], loss=47.7425
	step [118/146], loss=47.7181
	step [119/146], loss=47.3756
	step [120/146], loss=48.1413
	step [121/146], loss=46.6755
	step [122/146], loss=45.5322
	step [123/146], loss=46.1196
	step [124/146], loss=45.8727
	step [125/146], loss=46.4596
	step [126/146], loss=45.9507
	step [127/146], loss=47.3906
	step [128/146], loss=46.1221
	step [129/146], loss=46.6097
	step [130/146], loss=46.7759
	step [131/146], loss=46.9372
	step [132/146], loss=45.4959
	step [133/146], loss=45.8362
	step [134/146], loss=47.4417
	step [135/146], loss=47.8784
	step [136/146], loss=46.3839
	step [137/146], loss=48.2554
	step [138/146], loss=46.5165
	step [139/146], loss=45.1136
	step [140/146], loss=47.8026
	step [141/146], loss=45.2603
	step [142/146], loss=46.9006
	step [143/146], loss=47.1374
	step [144/146], loss=45.2622
	step [145/146], loss=47.1985
	step [146/146], loss=26.4860
	Evaluating
	loss=0.1637, precision=0.2033, recall=0.9968, f1=0.3377
Training epoch 6
	step [1/146], loss=44.5650
	step [2/146], loss=45.4695
	step [3/146], loss=45.5290
	step [4/146], loss=45.6615
	step [5/146], loss=44.1090
	step [6/146], loss=44.9457
	step [7/146], loss=47.0958
	step [8/146], loss=44.0141
	step [9/146], loss=47.3544
	step [10/146], loss=47.1640
	step [11/146], loss=44.4272
	step [12/146], loss=44.2115
	step [13/146], loss=45.5128
	step [14/146], loss=45.4260
	step [15/146], loss=44.6126
	step [16/146], loss=45.0504
	step [17/146], loss=46.4438
	step [18/146], loss=44.5746
	step [19/146], loss=46.5565
	step [20/146], loss=44.0687
	step [21/146], loss=45.1100
	step [22/146], loss=46.1591
	step [23/146], loss=47.6265
	step [24/146], loss=45.1080
	step [25/146], loss=44.8071
	step [26/146], loss=47.8940
	step [27/146], loss=45.5594
	step [28/146], loss=44.6157
	step [29/146], loss=45.2569
	step [30/146], loss=45.2961
	step [31/146], loss=44.6278
	step [32/146], loss=44.9377
	step [33/146], loss=45.2823
	step [34/146], loss=44.5289
	step [35/146], loss=45.7569
	step [36/146], loss=44.5341
	step [37/146], loss=43.7533
	step [38/146], loss=45.0465
	step [39/146], loss=45.6937
	step [40/146], loss=43.8815
	step [41/146], loss=43.4824
	step [42/146], loss=44.0973
	step [43/146], loss=43.8879
	step [44/146], loss=46.1473
	step [45/146], loss=44.1035
	step [46/146], loss=42.5957
	step [47/146], loss=44.3741
	step [48/146], loss=43.9849
	step [49/146], loss=42.7973
	step [50/146], loss=42.9985
	step [51/146], loss=43.5745
	step [52/146], loss=44.6030
	step [53/146], loss=43.3308
	step [54/146], loss=42.9785
	step [55/146], loss=43.9704
	step [56/146], loss=44.4894
	step [57/146], loss=42.7143
	step [58/146], loss=42.9281
	step [59/146], loss=43.6211
	step [60/146], loss=41.6439
	step [61/146], loss=42.1340
	step [62/146], loss=44.3218
	step [63/146], loss=43.0126
	step [64/146], loss=42.9827
	step [65/146], loss=44.7145
	step [66/146], loss=42.6446
	step [67/146], loss=43.7315
	step [68/146], loss=42.8298
	step [69/146], loss=41.3800
	step [70/146], loss=42.4836
	step [71/146], loss=41.6798
	step [72/146], loss=42.2010
	step [73/146], loss=42.3852
	step [74/146], loss=41.6499
	step [75/146], loss=44.4840
	step [76/146], loss=43.1343
	step [77/146], loss=42.5391
	step [78/146], loss=41.7837
	step [79/146], loss=41.1138
	step [80/146], loss=43.4340
	step [81/146], loss=42.5957
	step [82/146], loss=42.9667
	step [83/146], loss=41.1049
	step [84/146], loss=41.6857
	step [85/146], loss=44.1067
	step [86/146], loss=41.3271
	step [87/146], loss=41.8023
	step [88/146], loss=40.6700
	step [89/146], loss=42.8059
	step [90/146], loss=41.2386
	step [91/146], loss=44.9716
	step [92/146], loss=40.6133
	step [93/146], loss=43.3520
	step [94/146], loss=42.1797
	step [95/146], loss=41.1779
	step [96/146], loss=42.7786
	step [97/146], loss=41.8930
	step [98/146], loss=42.1507
	step [99/146], loss=41.1270
	step [100/146], loss=42.1847
	step [101/146], loss=41.1156
	step [102/146], loss=41.0589
	step [103/146], loss=40.2911
	step [104/146], loss=41.8866
	step [105/146], loss=40.5363
	step [106/146], loss=40.0290
	step [107/146], loss=41.3999
	step [108/146], loss=39.8909
	step [109/146], loss=40.8901
	step [110/146], loss=42.1958
	step [111/146], loss=40.8344
	step [112/146], loss=40.7375
	step [113/146], loss=40.8959
	step [114/146], loss=40.0039
	step [115/146], loss=38.8042
	step [116/146], loss=39.6529
	step [117/146], loss=40.1196
	step [118/146], loss=40.5818
	step [119/146], loss=42.0954
	step [120/146], loss=41.3046
	step [121/146], loss=39.7052
	step [122/146], loss=41.6680
	step [123/146], loss=39.1723
	step [124/146], loss=41.4422
	step [125/146], loss=40.7288
	step [126/146], loss=43.4468
	step [127/146], loss=39.9707
	step [128/146], loss=41.3880
	step [129/146], loss=41.1606
	step [130/146], loss=39.9236
	step [131/146], loss=39.5901
	step [132/146], loss=38.8031
	step [133/146], loss=39.4278
	step [134/146], loss=39.6172
	step [135/146], loss=41.7864
	step [136/146], loss=38.9025
	step [137/146], loss=40.9004
	step [138/146], loss=39.1169
	step [139/146], loss=41.6196
	step [140/146], loss=41.7890
	step [141/146], loss=40.0412
	step [142/146], loss=39.8444
	step [143/146], loss=39.5933
	step [144/146], loss=39.2554
	step [145/146], loss=39.3728
	step [146/146], loss=24.4619
	Evaluating
	loss=0.1422, precision=0.1966, recall=0.9967, f1=0.3284
Training epoch 7
	step [1/146], loss=41.1171
	step [2/146], loss=38.9835
	step [3/146], loss=39.8778
	step [4/146], loss=38.4270
	step [5/146], loss=38.6321
	step [6/146], loss=40.8196
	step [7/146], loss=39.3988
	step [8/146], loss=39.9281
	step [9/146], loss=37.7301
	step [10/146], loss=38.4734
	step [11/146], loss=38.3030
	step [12/146], loss=37.8329
	step [13/146], loss=37.8172
	step [14/146], loss=39.4821
	step [15/146], loss=39.3673
	step [16/146], loss=38.2996
	step [17/146], loss=41.5097
	step [18/146], loss=39.4554
	step [19/146], loss=37.7207
	step [20/146], loss=38.4197
	step [21/146], loss=38.2356
	step [22/146], loss=38.8876
	step [23/146], loss=39.4259
	step [24/146], loss=38.6503
	step [25/146], loss=37.0670
	step [26/146], loss=39.2227
	step [27/146], loss=38.3140
	step [28/146], loss=38.5862
	step [29/146], loss=38.0891
	step [30/146], loss=37.8243
	step [31/146], loss=38.9552
	step [32/146], loss=37.7799
	step [33/146], loss=38.5369
	step [34/146], loss=37.7493
	step [35/146], loss=36.4155
	step [36/146], loss=39.6782
	step [37/146], loss=37.9076
	step [38/146], loss=39.1778
	step [39/146], loss=38.1537
	step [40/146], loss=35.9793
	step [41/146], loss=37.9974
	step [42/146], loss=38.8191
	step [43/146], loss=37.2761
	step [44/146], loss=39.0153
	step [45/146], loss=38.5374
	step [46/146], loss=38.9949
	step [47/146], loss=38.3015
	step [48/146], loss=37.7892
	step [49/146], loss=38.4177
	step [50/146], loss=38.6023
	step [51/146], loss=37.8977
	step [52/146], loss=37.1027
	step [53/146], loss=38.4350
	step [54/146], loss=38.2621
	step [55/146], loss=36.1038
	step [56/146], loss=41.0377
	step [57/146], loss=36.9830
	step [58/146], loss=36.7485
	step [59/146], loss=38.0967
	step [60/146], loss=35.7661
	step [61/146], loss=37.5507
	step [62/146], loss=37.3448
	step [63/146], loss=36.3776
	step [64/146], loss=37.5601
	step [65/146], loss=37.0214
	step [66/146], loss=37.2698
	step [67/146], loss=36.9291
	step [68/146], loss=37.7703
	step [69/146], loss=35.5057
	step [70/146], loss=40.2130
	step [71/146], loss=38.3648
	step [72/146], loss=35.6029
	step [73/146], loss=36.8296
	step [74/146], loss=36.1903
	step [75/146], loss=37.5163
	step [76/146], loss=37.6385
	step [77/146], loss=36.2677
	step [78/146], loss=35.2523
	step [79/146], loss=36.7069
	step [80/146], loss=37.4457
	step [81/146], loss=37.0545
	step [82/146], loss=35.9894
	step [83/146], loss=35.5711
	step [84/146], loss=37.0375
	step [85/146], loss=37.5234
	step [86/146], loss=35.4559
	step [87/146], loss=37.1019
	step [88/146], loss=36.0637
	step [89/146], loss=36.2629
	step [90/146], loss=35.1467
	step [91/146], loss=36.7204
	step [92/146], loss=36.6199
	step [93/146], loss=35.6614
	step [94/146], loss=36.6200
	step [95/146], loss=37.9975
	step [96/146], loss=36.2405
	step [97/146], loss=37.0084
	step [98/146], loss=36.0584
	step [99/146], loss=37.8030
	step [100/146], loss=35.3926
	step [101/146], loss=36.7319
	step [102/146], loss=35.5450
	step [103/146], loss=35.9254
	step [104/146], loss=33.7779
	step [105/146], loss=37.0549
	step [106/146], loss=34.8072
	step [107/146], loss=37.8605
	step [108/146], loss=35.6459
	step [109/146], loss=35.3994
	step [110/146], loss=35.4008
	step [111/146], loss=35.3247
	step [112/146], loss=33.8263
	step [113/146], loss=37.0773
	step [114/146], loss=35.0739
	step [115/146], loss=34.4997
	step [116/146], loss=35.0235
	step [117/146], loss=34.9508
	step [118/146], loss=34.4203
	step [119/146], loss=36.4172
	step [120/146], loss=34.9304
	step [121/146], loss=34.4450
	step [122/146], loss=36.5430
	step [123/146], loss=35.1023
	step [124/146], loss=35.0192
	step [125/146], loss=35.2668
	step [126/146], loss=34.2938
	step [127/146], loss=35.0833
	step [128/146], loss=35.4872
	step [129/146], loss=34.0474
	step [130/146], loss=36.8661
	step [131/146], loss=33.9413
	step [132/146], loss=35.0606
	step [133/146], loss=35.2724
	step [134/146], loss=33.2391
	step [135/146], loss=34.4773
	step [136/146], loss=33.5371
	step [137/146], loss=35.9445
	step [138/146], loss=33.2416
	step [139/146], loss=36.5426
	step [140/146], loss=35.2589
	step [141/146], loss=34.2924
	step [142/146], loss=33.8025
	step [143/146], loss=33.8718
	step [144/146], loss=35.2924
	step [145/146], loss=34.7338
	step [146/146], loss=19.3820
	Evaluating
	loss=0.1185, precision=0.2328, recall=0.9959, f1=0.3773
saving model as: 2_saved_model.pth
Training epoch 8
	step [1/146], loss=34.9614
	step [2/146], loss=35.3495
	step [3/146], loss=34.0388
	step [4/146], loss=34.5467
	step [5/146], loss=36.0533
	step [6/146], loss=34.3738
	step [7/146], loss=34.9709
	step [8/146], loss=32.3055
	step [9/146], loss=34.0386
	step [10/146], loss=33.7816
	step [11/146], loss=32.6872
	step [12/146], loss=33.7028
	step [13/146], loss=33.7237
	step [14/146], loss=35.8830
	step [15/146], loss=35.5596
	step [16/146], loss=34.6402
	step [17/146], loss=35.5692
	step [18/146], loss=34.9266
	step [19/146], loss=32.6583
	step [20/146], loss=32.8907
	step [21/146], loss=34.6773
	step [22/146], loss=34.3947
	step [23/146], loss=32.7515
	step [24/146], loss=33.2923
	step [25/146], loss=32.2331
	step [26/146], loss=32.7951
	step [27/146], loss=33.7016
	step [28/146], loss=33.5892
	step [29/146], loss=32.2641
	step [30/146], loss=33.7226
	step [31/146], loss=32.9951
	step [32/146], loss=34.7987
	step [33/146], loss=32.6635
	step [34/146], loss=32.9861
	step [35/146], loss=35.0115
	step [36/146], loss=33.6859
	step [37/146], loss=31.8682
	step [38/146], loss=31.8895
	step [39/146], loss=33.4329
	step [40/146], loss=32.6596
	step [41/146], loss=33.3015
	step [42/146], loss=32.5296
	step [43/146], loss=33.7341
	step [44/146], loss=31.3407
	step [45/146], loss=37.0659
	step [46/146], loss=31.9478
	step [47/146], loss=32.4380
	step [48/146], loss=33.4039
	step [49/146], loss=34.3603
	step [50/146], loss=33.5563
	step [51/146], loss=32.7362
	step [52/146], loss=33.6537
	step [53/146], loss=31.9282
	step [54/146], loss=36.3183
	step [55/146], loss=32.5949
	step [56/146], loss=31.7603
	step [57/146], loss=34.2545
	step [58/146], loss=31.9612
	step [59/146], loss=32.5434
	step [60/146], loss=33.5503
	step [61/146], loss=32.8916
	step [62/146], loss=33.2818
	step [63/146], loss=30.5613
	step [64/146], loss=31.4241
	step [65/146], loss=32.1063
	step [66/146], loss=32.6196
	step [67/146], loss=32.5839
	step [68/146], loss=33.4987
	step [69/146], loss=31.8319
	step [70/146], loss=31.5549
	step [71/146], loss=30.8153
	step [72/146], loss=31.3186
	step [73/146], loss=34.2835
	step [74/146], loss=31.6277
	step [75/146], loss=30.9641
	step [76/146], loss=32.7077
	step [77/146], loss=33.1735
	step [78/146], loss=32.7279
	step [79/146], loss=30.2393
	step [80/146], loss=30.6862
	step [81/146], loss=31.7542
	step [82/146], loss=30.5416
	step [83/146], loss=36.1972
	step [84/146], loss=32.0055
	step [85/146], loss=33.6450
	step [86/146], loss=32.7980
	step [87/146], loss=30.1615
	step [88/146], loss=31.6278
	step [89/146], loss=32.0797
	step [90/146], loss=33.1933
	step [91/146], loss=31.7757
	step [92/146], loss=30.8567
	step [93/146], loss=32.6105
	step [94/146], loss=31.9142
	step [95/146], loss=30.0031
	step [96/146], loss=32.6837
	step [97/146], loss=30.6888
	step [98/146], loss=31.9482
	step [99/146], loss=31.2496
	step [100/146], loss=29.6834
	step [101/146], loss=31.1573
	step [102/146], loss=31.3741
	step [103/146], loss=32.9000
	step [104/146], loss=32.5159
	step [105/146], loss=31.5301
	step [106/146], loss=31.6339
	step [107/146], loss=30.7779
	step [108/146], loss=30.6503
	step [109/146], loss=31.9294
	step [110/146], loss=31.7084
	step [111/146], loss=30.5925
	step [112/146], loss=31.8653
	step [113/146], loss=31.3209
	step [114/146], loss=33.2101
	step [115/146], loss=29.5998
	step [116/146], loss=31.9435
	step [117/146], loss=30.5268
	step [118/146], loss=31.2601
	step [119/146], loss=30.8361
	step [120/146], loss=29.4197
	step [121/146], loss=30.1517
	step [122/146], loss=31.1898
	step [123/146], loss=32.5741
	step [124/146], loss=32.7080
	step [125/146], loss=30.8356
	step [126/146], loss=31.3172
	step [127/146], loss=30.2008
	step [128/146], loss=30.4040
	step [129/146], loss=30.7826
	step [130/146], loss=30.6925
	step [131/146], loss=31.3156
	step [132/146], loss=30.2315
	step [133/146], loss=30.7103
	step [134/146], loss=31.8144
	step [135/146], loss=29.7282
	step [136/146], loss=30.2776
	step [137/146], loss=31.2355
	step [138/146], loss=30.9997
	step [139/146], loss=30.6888
	step [140/146], loss=31.4841
	step [141/146], loss=30.8361
	step [142/146], loss=31.3371
	step [143/146], loss=29.1150
	step [144/146], loss=29.6409
	step [145/146], loss=29.0392
	step [146/146], loss=16.8193
	Evaluating
	loss=0.1047, precision=0.2039, recall=0.9968, f1=0.3385
Training epoch 9
	step [1/146], loss=29.2486
	step [2/146], loss=30.2046
	step [3/146], loss=29.3029
	step [4/146], loss=29.5937
	step [5/146], loss=29.1252
	step [6/146], loss=31.0429
	step [7/146], loss=31.3127
	step [8/146], loss=29.8530
	step [9/146], loss=28.9541
	step [10/146], loss=29.3418
	step [11/146], loss=31.6984
	step [12/146], loss=30.3954
	step [13/146], loss=29.0938
	step [14/146], loss=30.6875
	step [15/146], loss=31.2370
	step [16/146], loss=31.1607
	step [17/146], loss=28.3936
	step [18/146], loss=28.7532
	step [19/146], loss=28.1970
	step [20/146], loss=28.7137
	step [21/146], loss=29.6673
	step [22/146], loss=30.4539
	step [23/146], loss=29.2552
	step [24/146], loss=29.7513
	step [25/146], loss=30.9669
	step [26/146], loss=30.8887
	step [27/146], loss=28.1468
	step [28/146], loss=29.9468
	step [29/146], loss=28.8197
	step [30/146], loss=29.9651
	step [31/146], loss=30.3221
	step [32/146], loss=31.0016
	step [33/146], loss=29.1948
	step [34/146], loss=31.4414
	step [35/146], loss=28.7559
	step [36/146], loss=31.6923
	step [37/146], loss=29.5801
	step [38/146], loss=30.0854
	step [39/146], loss=28.6814
	step [40/146], loss=28.8058
	step [41/146], loss=29.3477
	step [42/146], loss=28.2339
	step [43/146], loss=28.1381
	step [44/146], loss=29.8378
	step [45/146], loss=29.1691
	step [46/146], loss=29.0243
	step [47/146], loss=28.3730
	step [48/146], loss=28.6610
	step [49/146], loss=28.5481
	step [50/146], loss=29.1376
	step [51/146], loss=29.0624
	step [52/146], loss=29.3769
	step [53/146], loss=31.2418
	step [54/146], loss=30.5064
	step [55/146], loss=28.4103
	step [56/146], loss=29.2251
	step [57/146], loss=27.7437
	step [58/146], loss=29.7012
	step [59/146], loss=28.7558
	step [60/146], loss=31.0732
	step [61/146], loss=27.7073
	step [62/146], loss=28.4937
	step [63/146], loss=28.4681
	step [64/146], loss=28.5248
	step [65/146], loss=30.2848
	step [66/146], loss=30.0540
	step [67/146], loss=29.8711
	step [68/146], loss=29.2760
	step [69/146], loss=27.0105
	step [70/146], loss=29.3054
	step [71/146], loss=28.1001
	step [72/146], loss=30.5074
	step [73/146], loss=28.6599
	step [74/146], loss=29.6926
	step [75/146], loss=30.3181
	step [76/146], loss=28.6146
	step [77/146], loss=27.6568
	step [78/146], loss=27.6344
	step [79/146], loss=27.8822
	step [80/146], loss=27.9649
	step [81/146], loss=28.3547
	step [82/146], loss=26.9504
	step [83/146], loss=28.5985
	step [84/146], loss=30.6553
	step [85/146], loss=28.3338
	step [86/146], loss=28.6883
	step [87/146], loss=27.6522
	step [88/146], loss=27.9457
	step [89/146], loss=27.4503
	step [90/146], loss=29.4049
	step [91/146], loss=28.4346
	step [92/146], loss=30.1493
	step [93/146], loss=28.9623
	step [94/146], loss=27.0088
	step [95/146], loss=27.6261
	step [96/146], loss=27.9715
	step [97/146], loss=28.1348
	step [98/146], loss=26.6559
	step [99/146], loss=27.9479
	step [100/146], loss=28.6437
	step [101/146], loss=28.8737
	step [102/146], loss=26.9996
	step [103/146], loss=26.2532
	step [104/146], loss=27.1444
	step [105/146], loss=29.0016
	step [106/146], loss=27.1320
	step [107/146], loss=28.4901
	step [108/146], loss=27.9993
	step [109/146], loss=29.2548
	step [110/146], loss=27.2926
	step [111/146], loss=28.5277
	step [112/146], loss=28.5409
	step [113/146], loss=28.2052
	step [114/146], loss=27.4825
	step [115/146], loss=28.4000
	step [116/146], loss=28.5353
	step [117/146], loss=28.2741
	step [118/146], loss=28.2291
	step [119/146], loss=29.2066
	step [120/146], loss=29.7711
	step [121/146], loss=27.8379
	step [122/146], loss=27.7776
	step [123/146], loss=29.5041
	step [124/146], loss=28.6640
	step [125/146], loss=29.4786
	step [126/146], loss=26.7575
	step [127/146], loss=28.6382
	step [128/146], loss=27.1747
	step [129/146], loss=26.3901
	step [130/146], loss=27.3995
	step [131/146], loss=26.8917
	step [132/146], loss=28.7579
	step [133/146], loss=27.5979
	step [134/146], loss=26.4411
	step [135/146], loss=26.3470
	step [136/146], loss=26.5676
	step [137/146], loss=30.5489
	step [138/146], loss=27.0294
	step [139/146], loss=26.1527
	step [140/146], loss=27.2218
	step [141/146], loss=25.0132
	step [142/146], loss=28.9301
	step [143/146], loss=26.6744
	step [144/146], loss=25.8087
	step [145/146], loss=27.5091
	step [146/146], loss=14.8806
	Evaluating
	loss=0.0879, precision=0.2333, recall=0.9958, f1=0.3780
saving model as: 2_saved_model.pth
Training epoch 10
	step [1/146], loss=26.9920
	step [2/146], loss=29.3982
	step [3/146], loss=27.1384
	step [4/146], loss=26.4357
	step [5/146], loss=26.3700
	step [6/146], loss=26.6217
	step [7/146], loss=26.4706
	step [8/146], loss=26.3967
	step [9/146], loss=27.6445
	step [10/146], loss=28.2780
	step [11/146], loss=26.1547
	step [12/146], loss=28.7606
	step [13/146], loss=28.8909
	step [14/146], loss=26.2872
	step [15/146], loss=26.6512
	step [16/146], loss=25.9817
	step [17/146], loss=29.3672
	step [18/146], loss=26.7680
	step [19/146], loss=26.8600
	step [20/146], loss=27.6273
	step [21/146], loss=25.6721
	step [22/146], loss=26.0140
	step [23/146], loss=28.0223
	step [24/146], loss=26.5375
	step [25/146], loss=25.7211
	step [26/146], loss=26.2905
	step [27/146], loss=27.3054
	step [28/146], loss=25.6282
	step [29/146], loss=26.0877
	step [30/146], loss=27.9831
	step [31/146], loss=25.7462
	step [32/146], loss=26.7943
	step [33/146], loss=26.6029
	step [34/146], loss=28.0561
	step [35/146], loss=25.7602
	step [36/146], loss=25.3911
	step [37/146], loss=26.3514
	step [38/146], loss=25.7967
	step [39/146], loss=26.2936
	step [40/146], loss=26.8749
	step [41/146], loss=26.5566
	step [42/146], loss=28.2315
	step [43/146], loss=26.4432
	step [44/146], loss=25.5460
	step [45/146], loss=26.8517
	step [46/146], loss=25.3283
	step [47/146], loss=25.9797
	step [48/146], loss=26.9252
	step [49/146], loss=27.1852
	step [50/146], loss=25.6587
	step [51/146], loss=24.8663
	step [52/146], loss=26.6202
	step [53/146], loss=25.1440
	step [54/146], loss=27.1614
	step [55/146], loss=24.8200
	step [56/146], loss=26.7399
	step [57/146], loss=27.7081
	step [58/146], loss=27.2039
	step [59/146], loss=26.8999
	step [60/146], loss=24.2540
	step [61/146], loss=25.5283
	step [62/146], loss=25.8912
	step [63/146], loss=25.9267
	step [64/146], loss=25.5690
	step [65/146], loss=25.0653
	step [66/146], loss=25.1755
	step [67/146], loss=25.0030
	step [68/146], loss=27.8306
	step [69/146], loss=26.0275
	step [70/146], loss=26.5387
	step [71/146], loss=26.0107
	step [72/146], loss=28.2843
	step [73/146], loss=27.7572
	step [74/146], loss=25.6605
	step [75/146], loss=27.9084
	step [76/146], loss=25.4012
	step [77/146], loss=24.9452
	step [78/146], loss=25.4622
	step [79/146], loss=26.5735
	step [80/146], loss=24.7865
	step [81/146], loss=27.1582
	step [82/146], loss=26.5012
	step [83/146], loss=24.5582
	step [84/146], loss=23.5066
	step [85/146], loss=25.2643
	step [86/146], loss=25.1910
	step [87/146], loss=25.8274
	step [88/146], loss=26.1023
	step [89/146], loss=26.2611
	step [90/146], loss=26.4133
	step [91/146], loss=24.9278
	step [92/146], loss=25.6707
	step [93/146], loss=25.3472
	step [94/146], loss=25.7292
	step [95/146], loss=24.1308
	step [96/146], loss=25.0225
	step [97/146], loss=24.5096
	step [98/146], loss=25.2327
	step [99/146], loss=26.1810
	step [100/146], loss=25.3708
	step [101/146], loss=26.2489
	step [102/146], loss=27.6811
	step [103/146], loss=24.0617
	step [104/146], loss=25.0557
	step [105/146], loss=25.5366
	step [106/146], loss=26.7492
	step [107/146], loss=26.4054
	step [108/146], loss=24.2459
	step [109/146], loss=24.5928
	step [110/146], loss=25.2953
	step [111/146], loss=27.6334
	step [112/146], loss=24.3565
	step [113/146], loss=28.0024
	step [114/146], loss=25.3949
	step [115/146], loss=24.4806
	step [116/146], loss=26.4930
	step [117/146], loss=25.7742
	step [118/146], loss=25.6198
	step [119/146], loss=25.4662
	step [120/146], loss=25.6276
	step [121/146], loss=25.8532
	step [122/146], loss=24.2605
	step [123/146], loss=27.3508
	step [124/146], loss=25.4164
	step [125/146], loss=25.3086
	step [126/146], loss=24.8014
	step [127/146], loss=24.5600
	step [128/146], loss=25.0777
	step [129/146], loss=23.3203
	step [130/146], loss=25.8367
	step [131/146], loss=23.6722
	step [132/146], loss=25.4645
	step [133/146], loss=25.9605
	step [134/146], loss=22.9210
	step [135/146], loss=24.1565
	step [136/146], loss=25.1907
	step [137/146], loss=23.7771
	step [138/146], loss=24.8645
	step [139/146], loss=22.5814
	step [140/146], loss=24.6592
	step [141/146], loss=23.8317
	step [142/146], loss=24.3510
	step [143/146], loss=24.4275
	step [144/146], loss=24.2421
	step [145/146], loss=24.0443
	step [146/146], loss=13.2221
	Evaluating
	loss=0.0820, precision=0.2258, recall=0.9963, f1=0.3682
Training epoch 11
	step [1/146], loss=23.3329
	step [2/146], loss=24.7412
	step [3/146], loss=24.6751
	step [4/146], loss=23.8439
	step [5/146], loss=24.0066
	step [6/146], loss=24.5486
	step [7/146], loss=25.3469
	step [8/146], loss=25.4555
	step [9/146], loss=23.0571
	step [10/146], loss=24.0108
	step [11/146], loss=24.4008
	step [12/146], loss=23.9900
	step [13/146], loss=25.0505
	step [14/146], loss=26.3574
	step [15/146], loss=25.6652
	step [16/146], loss=24.0525
	step [17/146], loss=25.8915
	step [18/146], loss=24.1248
	step [19/146], loss=24.4779
	step [20/146], loss=24.0044
	step [21/146], loss=23.6877
	step [22/146], loss=24.1780
	step [23/146], loss=23.7044
	step [24/146], loss=24.1173
	step [25/146], loss=23.4095
	step [26/146], loss=23.5103
	step [27/146], loss=23.0509
	step [28/146], loss=24.4298
	step [29/146], loss=23.5088
	step [30/146], loss=24.7610
	step [31/146], loss=22.6770
	step [32/146], loss=22.4408
	step [33/146], loss=23.2959
	step [34/146], loss=26.6662
	step [35/146], loss=25.3701
	step [36/146], loss=25.4076
	step [37/146], loss=24.4341
	step [38/146], loss=24.1565
	step [39/146], loss=23.6043
	step [40/146], loss=25.6184
	step [41/146], loss=24.7883
	step [42/146], loss=23.7371
	step [43/146], loss=23.9732
	step [44/146], loss=25.0513
	step [45/146], loss=25.1204
	step [46/146], loss=23.2613
	step [47/146], loss=24.4076
	step [48/146], loss=25.6082
	step [49/146], loss=23.5447
	step [50/146], loss=23.6548
	step [51/146], loss=24.4161
	step [52/146], loss=24.0779
	step [53/146], loss=24.4649
	step [54/146], loss=24.4024
	step [55/146], loss=22.9218
	step [56/146], loss=23.3611
	step [57/146], loss=25.2672
	step [58/146], loss=23.4777
	step [59/146], loss=25.1022
	step [60/146], loss=24.1500
	step [61/146], loss=23.5629
	step [62/146], loss=25.6430
	step [63/146], loss=24.1944
	step [64/146], loss=24.5749
	step [65/146], loss=22.3964
	step [66/146], loss=24.0821
	step [67/146], loss=24.2046
	step [68/146], loss=24.3431
	step [69/146], loss=23.1340
	step [70/146], loss=24.0221
	step [71/146], loss=23.7709
	step [72/146], loss=23.4605
	step [73/146], loss=22.9394
	step [74/146], loss=22.6529
	step [75/146], loss=24.1668
	step [76/146], loss=24.2891
	step [77/146], loss=24.0778
	step [78/146], loss=24.3159
	step [79/146], loss=23.8444
	step [80/146], loss=23.0132
	step [81/146], loss=21.9516
	step [82/146], loss=23.9908
	step [83/146], loss=24.8482
	step [84/146], loss=21.6031
	step [85/146], loss=23.5623
	step [86/146], loss=24.8708
	step [87/146], loss=23.8224
	step [88/146], loss=22.8480
	step [89/146], loss=22.8169
	step [90/146], loss=23.1326
	step [91/146], loss=21.1898
	step [92/146], loss=24.9858
	step [93/146], loss=21.2303
	step [94/146], loss=22.1430
	step [95/146], loss=23.4679
	step [96/146], loss=22.1030
	step [97/146], loss=23.4881
	step [98/146], loss=23.3175
	step [99/146], loss=20.7970
	step [100/146], loss=22.6252
	step [101/146], loss=23.4203
	step [102/146], loss=22.9667
	step [103/146], loss=23.7326
	step [104/146], loss=22.1435
	step [105/146], loss=23.1787
	step [106/146], loss=23.7546
	step [107/146], loss=21.9033
	step [108/146], loss=23.1405
	step [109/146], loss=24.0218
	step [110/146], loss=23.0431
	step [111/146], loss=24.7055
	step [112/146], loss=23.1293
	step [113/146], loss=23.4732
	step [114/146], loss=22.7111
	step [115/146], loss=23.6376
	step [116/146], loss=22.9173
	step [117/146], loss=22.6522
	step [118/146], loss=21.4625
	step [119/146], loss=22.8619
	step [120/146], loss=24.0892
	step [121/146], loss=23.4314
	step [122/146], loss=23.0773
	step [123/146], loss=23.1339
	step [124/146], loss=21.0502
	step [125/146], loss=22.6305
	step [126/146], loss=21.9073
	step [127/146], loss=22.8392
	step [128/146], loss=21.4376
	step [129/146], loss=22.6254
	step [130/146], loss=22.6839
	step [131/146], loss=21.6000
	step [132/146], loss=22.6346
	step [133/146], loss=21.5430
	step [134/146], loss=22.1683
	step [135/146], loss=21.4581
	step [136/146], loss=24.9401
	step [137/146], loss=22.4457
	step [138/146], loss=23.9890
	step [139/146], loss=23.3220
	step [140/146], loss=22.8100
	step [141/146], loss=20.9180
	step [142/146], loss=22.4604
	step [143/146], loss=21.9794
	step [144/146], loss=25.1283
	step [145/146], loss=22.8605
	step [146/146], loss=10.9212
	Evaluating
	loss=0.0778, precision=0.1801, recall=0.9972, f1=0.3051
Training epoch 12
	step [1/146], loss=20.9978
	step [2/146], loss=22.7461
	step [3/146], loss=21.3306
	step [4/146], loss=20.6848
	step [5/146], loss=21.2408
	step [6/146], loss=22.1674
	step [7/146], loss=20.4914
	step [8/146], loss=21.9815
	step [9/146], loss=23.7928
	step [10/146], loss=22.1504
	step [11/146], loss=23.3278
	step [12/146], loss=21.9732
	step [13/146], loss=21.6390
	step [14/146], loss=21.7555
	step [15/146], loss=22.7141
	step [16/146], loss=21.7544
	step [17/146], loss=21.6960
	step [18/146], loss=21.8919
	step [19/146], loss=22.9687
	step [20/146], loss=22.2029
	step [21/146], loss=22.5434
	step [22/146], loss=22.4235
	step [23/146], loss=22.1078
	step [24/146], loss=22.2590
	step [25/146], loss=23.0322
	step [26/146], loss=23.3775
	step [27/146], loss=19.5750
	step [28/146], loss=21.6123
	step [29/146], loss=22.1232
	step [30/146], loss=23.8705
	step [31/146], loss=21.8431
	step [32/146], loss=22.4266
	step [33/146], loss=21.2613
	step [34/146], loss=23.4018
	step [35/146], loss=21.4941
	step [36/146], loss=21.1304
	step [37/146], loss=20.7718
	step [38/146], loss=24.3352
	step [39/146], loss=21.2997
	step [40/146], loss=20.2107
	step [41/146], loss=20.8419
	step [42/146], loss=19.5153
	step [43/146], loss=22.2367
	step [44/146], loss=21.0949
	step [45/146], loss=23.5243
	step [46/146], loss=22.0884
	step [47/146], loss=21.2405
	step [48/146], loss=20.6639
	step [49/146], loss=23.3042
	step [50/146], loss=21.6692
	step [51/146], loss=20.9494
	step [52/146], loss=21.4745
	step [53/146], loss=22.4326
	step [54/146], loss=22.4416
	step [55/146], loss=22.6241
	step [56/146], loss=23.6546
	step [57/146], loss=20.4954
	step [58/146], loss=22.3335
	step [59/146], loss=20.9272
	step [60/146], loss=21.0730
	step [61/146], loss=22.6420
	step [62/146], loss=20.6775
	step [63/146], loss=20.8180
	step [64/146], loss=22.1948
	step [65/146], loss=20.9566
	step [66/146], loss=21.7859
	step [67/146], loss=22.4485
	step [68/146], loss=19.5065
	step [69/146], loss=21.8359
	step [70/146], loss=22.2471
	step [71/146], loss=21.0618
	step [72/146], loss=21.2874
	step [73/146], loss=19.9063
	step [74/146], loss=21.0350
	step [75/146], loss=22.5127
	step [76/146], loss=20.6011
	step [77/146], loss=21.5445
	step [78/146], loss=20.7930
	step [79/146], loss=21.0004
	step [80/146], loss=22.5884
	step [81/146], loss=22.0069
	step [82/146], loss=20.4315
	step [83/146], loss=21.1839
	step [84/146], loss=22.1032
	step [85/146], loss=20.1623
	step [86/146], loss=20.6032
	step [87/146], loss=24.3136
	step [88/146], loss=22.1915
	step [89/146], loss=21.0830
	step [90/146], loss=21.0718
	step [91/146], loss=21.8837
	step [92/146], loss=21.9796
	step [93/146], loss=20.9086
	step [94/146], loss=21.5321
	step [95/146], loss=21.7082
	step [96/146], loss=21.3280
	step [97/146], loss=20.6217
	step [98/146], loss=22.1649
	step [99/146], loss=19.1841
	step [100/146], loss=21.2334
	step [101/146], loss=19.7582
	step [102/146], loss=20.4938
	step [103/146], loss=20.9895
	step [104/146], loss=22.1074
	step [105/146], loss=20.3849
	step [106/146], loss=23.2527
	step [107/146], loss=20.2564
	step [108/146], loss=20.8571
	step [109/146], loss=18.6744
	step [110/146], loss=20.8238
	step [111/146], loss=20.8629
	step [112/146], loss=20.1029
	step [113/146], loss=21.7646
	step [114/146], loss=22.8915
	step [115/146], loss=21.0737
	step [116/146], loss=21.4983
	step [117/146], loss=23.7156
	step [118/146], loss=20.5992
	step [119/146], loss=20.8605
	step [120/146], loss=19.6975
	step [121/146], loss=21.4599
	step [122/146], loss=25.5843
	step [123/146], loss=21.1276
	step [124/146], loss=21.4480
	step [125/146], loss=22.0832
	step [126/146], loss=20.9005
	step [127/146], loss=20.9407
	step [128/146], loss=21.0045
	step [129/146], loss=23.9453
	step [130/146], loss=20.1581
	step [131/146], loss=19.7669
	step [132/146], loss=21.9764
	step [133/146], loss=21.6604
	step [134/146], loss=20.8612
	step [135/146], loss=22.9882
	step [136/146], loss=21.6793
	step [137/146], loss=20.1138
	step [138/146], loss=21.7086
	step [139/146], loss=20.8205
	step [140/146], loss=20.8594
	step [141/146], loss=20.2694
	step [142/146], loss=19.3145
	step [143/146], loss=21.5014
	step [144/146], loss=19.1782
	step [145/146], loss=22.5726
	step [146/146], loss=11.6420
	Evaluating
	loss=0.0724, precision=0.1872, recall=0.9971, f1=0.3152
Training epoch 13
	step [1/146], loss=21.3501
	step [2/146], loss=18.7132
	step [3/146], loss=19.4742
	step [4/146], loss=22.0277
	step [5/146], loss=20.8929
	step [6/146], loss=20.6005
	step [7/146], loss=22.2165
	step [8/146], loss=20.9871
	step [9/146], loss=18.9287
	step [10/146], loss=18.0673
	step [11/146], loss=21.9446
	step [12/146], loss=20.8781
	step [13/146], loss=21.0036
	step [14/146], loss=18.9096
	step [15/146], loss=18.6463
	step [16/146], loss=20.1960
	step [17/146], loss=20.7066
	step [18/146], loss=20.0443
	step [19/146], loss=19.6377
	step [20/146], loss=21.5297
	step [21/146], loss=19.0483
	step [22/146], loss=18.9686
	step [23/146], loss=19.7331
	step [24/146], loss=20.0757
	step [25/146], loss=19.9952
	step [26/146], loss=20.6757
	step [27/146], loss=20.6334
	step [28/146], loss=19.2525
	step [29/146], loss=20.4435
	step [30/146], loss=21.7417
	step [31/146], loss=19.7377
	step [32/146], loss=19.4734
	step [33/146], loss=20.7459
	step [34/146], loss=20.5514
	step [35/146], loss=21.7183
	step [36/146], loss=20.3523
	step [37/146], loss=19.5500
	step [38/146], loss=19.6055
	step [39/146], loss=20.6411
	step [40/146], loss=19.7919
	step [41/146], loss=21.6008
	step [42/146], loss=19.3933
	step [43/146], loss=20.0765
	step [44/146], loss=19.4628
	step [45/146], loss=20.5276
	step [46/146], loss=21.7909
	step [47/146], loss=18.2748
	step [48/146], loss=22.0628
	step [49/146], loss=19.2820
	step [50/146], loss=21.1732
	step [51/146], loss=20.6184
	step [52/146], loss=20.4643
	step [53/146], loss=19.9815
	step [54/146], loss=20.7798
	step [55/146], loss=20.2137
	step [56/146], loss=19.4207
	step [57/146], loss=20.1647
	step [58/146], loss=20.5272
	step [59/146], loss=20.8222
	step [60/146], loss=18.7441
	step [61/146], loss=19.1555
	step [62/146], loss=19.7707
	step [63/146], loss=21.5303
	step [64/146], loss=19.6653
	step [65/146], loss=18.7137
	step [66/146], loss=19.7631
	step [67/146], loss=19.4422
	step [68/146], loss=19.4706
	step [69/146], loss=20.0531
	step [70/146], loss=20.2065
	step [71/146], loss=20.5656
	step [72/146], loss=21.5699
	step [73/146], loss=20.3164
	step [74/146], loss=19.9821
	step [75/146], loss=17.3747
	step [76/146], loss=19.7367
	step [77/146], loss=18.6713
	step [78/146], loss=19.0489
	step [79/146], loss=19.9385
	step [80/146], loss=19.4268
	step [81/146], loss=20.8720
	step [82/146], loss=19.0858
	step [83/146], loss=20.7214
	step [84/146], loss=21.9021
	step [85/146], loss=20.3793
	step [86/146], loss=20.1755
	step [87/146], loss=19.6010
	step [88/146], loss=19.5638
	step [89/146], loss=20.1405
	step [90/146], loss=19.7091
	step [91/146], loss=20.4854
	step [92/146], loss=18.8347
	step [93/146], loss=18.3575
	step [94/146], loss=21.7635
	step [95/146], loss=20.2182
	step [96/146], loss=20.6569
	step [97/146], loss=21.7267
	step [98/146], loss=19.4391
	step [99/146], loss=21.4637
	step [100/146], loss=19.3913
	step [101/146], loss=20.2448
	step [102/146], loss=19.8127
	step [103/146], loss=19.0066
	step [104/146], loss=19.1138
	step [105/146], loss=19.6943
	step [106/146], loss=19.1021
	step [107/146], loss=19.6519
	step [108/146], loss=18.4081
	step [109/146], loss=19.6117
	step [110/146], loss=21.3365
	step [111/146], loss=19.3641
	step [112/146], loss=20.5483
	step [113/146], loss=18.4540
	step [114/146], loss=18.7086
	step [115/146], loss=19.2366
	step [116/146], loss=18.1715
	step [117/146], loss=21.6078
	step [118/146], loss=17.6113
	step [119/146], loss=17.9641
	step [120/146], loss=19.7068
	step [121/146], loss=20.3274
	step [122/146], loss=19.3078
	step [123/146], loss=20.5930
	step [124/146], loss=19.1048
	step [125/146], loss=21.8902
	step [126/146], loss=19.0215
	step [127/146], loss=19.5743
	step [128/146], loss=20.8460
	step [129/146], loss=18.9817
	step [130/146], loss=20.5167
	step [131/146], loss=18.4772
	step [132/146], loss=18.3705
	step [133/146], loss=18.7423
	step [134/146], loss=18.4828
	step [135/146], loss=19.4508
	step [136/146], loss=17.9601
	step [137/146], loss=17.7651
	step [138/146], loss=20.3673
	step [139/146], loss=17.1647
	step [140/146], loss=19.5387
	step [141/146], loss=19.8369
	step [142/146], loss=17.8520
	step [143/146], loss=19.7004
	step [144/146], loss=18.4787
	step [145/146], loss=21.5316
	step [146/146], loss=12.4050
	Evaluating
	loss=0.0726, precision=0.1634, recall=0.9976, f1=0.2808
Training epoch 14
	step [1/146], loss=19.5646
	step [2/146], loss=17.2099
	step [3/146], loss=18.2651
	step [4/146], loss=19.6566
	step [5/146], loss=19.9673
	step [6/146], loss=17.4406
	step [7/146], loss=19.8834
	step [8/146], loss=19.6134
	step [9/146], loss=18.6164
	step [10/146], loss=20.0575
	step [11/146], loss=21.3274
	step [12/146], loss=19.5063
	step [13/146], loss=17.4908
	step [14/146], loss=18.4878
	step [15/146], loss=18.6991
	step [16/146], loss=19.3816
	step [17/146], loss=18.8423
	step [18/146], loss=19.7946
	step [19/146], loss=20.6363
	step [20/146], loss=19.0222
	step [21/146], loss=17.7117
	step [22/146], loss=19.6749
	step [23/146], loss=19.0950
	step [24/146], loss=19.5968
	step [25/146], loss=18.9367
	step [26/146], loss=20.8774
	step [27/146], loss=18.9648
	step [28/146], loss=20.3786
	step [29/146], loss=17.9034
	step [30/146], loss=18.2194
	step [31/146], loss=18.8700
	step [32/146], loss=18.0620
	step [33/146], loss=20.4051
	step [34/146], loss=19.4847
	step [35/146], loss=18.9330
	step [36/146], loss=17.9665
	step [37/146], loss=18.2312
	step [38/146], loss=19.2585
	step [39/146], loss=17.3396
	step [40/146], loss=18.7050
	step [41/146], loss=18.5337
	step [42/146], loss=18.5837
	step [43/146], loss=18.8102
	step [44/146], loss=18.9490
	step [45/146], loss=17.1721
	step [46/146], loss=19.9978
	step [47/146], loss=19.8405
	step [48/146], loss=20.2754
	step [49/146], loss=17.6391
	step [50/146], loss=17.9659
	step [51/146], loss=17.6729
	step [52/146], loss=17.5397
	step [53/146], loss=17.7876
	step [54/146], loss=17.8104
	step [55/146], loss=17.7248
	step [56/146], loss=22.6364
	step [57/146], loss=17.2029
	step [58/146], loss=18.1472
	step [59/146], loss=18.7118
	step [60/146], loss=18.0172
	step [61/146], loss=18.1839
	step [62/146], loss=19.7933
	step [63/146], loss=17.1362
	step [64/146], loss=17.4140
	step [65/146], loss=17.2340
	step [66/146], loss=17.6396
	step [67/146], loss=20.0425
	step [68/146], loss=19.3883
	step [69/146], loss=18.6492
	step [70/146], loss=20.1589
	step [71/146], loss=17.3185
	step [72/146], loss=19.1522
	step [73/146], loss=18.3173
	step [74/146], loss=18.9427
	step [75/146], loss=18.7491
	step [76/146], loss=19.3276
	step [77/146], loss=18.2265
	step [78/146], loss=18.0413
	step [79/146], loss=18.5977
	step [80/146], loss=17.7684
	step [81/146], loss=17.8180
	step [82/146], loss=19.2986
	step [83/146], loss=19.2375
	step [84/146], loss=18.7868
	step [85/146], loss=17.3801
	step [86/146], loss=17.6683
	step [87/146], loss=20.7677
	step [88/146], loss=20.3832
	step [89/146], loss=17.0110
	step [90/146], loss=18.8112
	step [91/146], loss=18.0802
	step [92/146], loss=17.0312
	step [93/146], loss=17.9223
	step [94/146], loss=17.2307
	step [95/146], loss=19.2545
	step [96/146], loss=16.8747
	step [97/146], loss=17.6814
	step [98/146], loss=17.2129
	step [99/146], loss=19.4354
	step [100/146], loss=18.1906
	step [101/146], loss=18.5918
	step [102/146], loss=19.5218
	step [103/146], loss=18.8502
	step [104/146], loss=17.2183
	step [105/146], loss=18.0701
	step [106/146], loss=18.5589
	step [107/146], loss=17.6224
	step [108/146], loss=17.5782
	step [109/146], loss=21.7891
	step [110/146], loss=20.0519
	step [111/146], loss=17.5507
	step [112/146], loss=19.8170
	step [113/146], loss=20.8904
	step [114/146], loss=19.5829
	step [115/146], loss=23.2003
	step [116/146], loss=17.7896
	step [117/146], loss=19.8417
	step [118/146], loss=19.7899
	step [119/146], loss=20.7736
	step [120/146], loss=21.9489
	step [121/146], loss=17.8413
	step [122/146], loss=19.1096
	step [123/146], loss=18.4796
	step [124/146], loss=19.1795
	step [125/146], loss=19.0476
	step [126/146], loss=17.4403
	step [127/146], loss=20.0677
	step [128/146], loss=17.9520
	step [129/146], loss=19.2967
	step [130/146], loss=19.5541
	step [131/146], loss=17.0820
	step [132/146], loss=19.1950
	step [133/146], loss=19.2252
	step [134/146], loss=20.3630
	step [135/146], loss=20.7968
	step [136/146], loss=20.0613
	step [137/146], loss=18.6913
	step [138/146], loss=20.3916
	step [139/146], loss=18.2566
	step [140/146], loss=19.5720
	step [141/146], loss=19.0193
	step [142/146], loss=18.4150
	step [143/146], loss=18.0359
	step [144/146], loss=19.3528
	step [145/146], loss=18.6088
	step [146/146], loss=12.1319
	Evaluating
	loss=0.0956, precision=0.2261, recall=0.9958, f1=0.3685
Training epoch 15
	step [1/146], loss=16.6713
	step [2/146], loss=19.3931
	step [3/146], loss=20.9314
	step [4/146], loss=19.1705
	step [5/146], loss=21.3251
	step [6/146], loss=19.0974
	step [7/146], loss=20.0282
	step [8/146], loss=20.1601
	step [9/146], loss=18.5886
	step [10/146], loss=18.4052
	step [11/146], loss=18.7314
	step [12/146], loss=17.6801
	step [13/146], loss=21.8080
	step [14/146], loss=18.9013
	step [15/146], loss=19.8180
	step [16/146], loss=17.7973
	step [17/146], loss=17.4281
	step [18/146], loss=20.5526
	step [19/146], loss=20.5560
	step [20/146], loss=20.1959
	step [21/146], loss=19.4951
	step [22/146], loss=19.8525
	step [23/146], loss=19.1085
	step [24/146], loss=19.3533
	step [25/146], loss=19.9376
	step [26/146], loss=20.4235
	step [27/146], loss=17.9374
	step [28/146], loss=17.2077
	step [29/146], loss=19.0674
	step [30/146], loss=19.0419
	step [31/146], loss=17.6275
	step [32/146], loss=21.7644
	step [33/146], loss=17.8190
	step [34/146], loss=18.4603
	step [35/146], loss=17.7319
	step [36/146], loss=18.7471
	step [37/146], loss=20.3978
	step [38/146], loss=18.2267
	step [39/146], loss=19.2411
	step [40/146], loss=19.6672
	step [41/146], loss=19.3516
	step [42/146], loss=20.7963
	step [43/146], loss=19.4648
	step [44/146], loss=19.3120
	step [45/146], loss=16.0119
	step [46/146], loss=19.0557
	step [47/146], loss=19.6669
	step [48/146], loss=19.0014
	step [49/146], loss=19.3324
	step [50/146], loss=19.2447
	step [51/146], loss=19.2499
	step [52/146], loss=17.2708
	step [53/146], loss=16.2904
	step [54/146], loss=18.4810
	step [55/146], loss=17.1518
	step [56/146], loss=18.9010
	step [57/146], loss=17.5230
	step [58/146], loss=17.8039
	step [59/146], loss=19.0848
	step [60/146], loss=18.3243
	step [61/146], loss=16.9742
	step [62/146], loss=16.5487
	step [63/146], loss=17.4802
	step [64/146], loss=18.7296
	step [65/146], loss=18.8083
	step [66/146], loss=18.4569
	step [67/146], loss=19.3858
	step [68/146], loss=16.5946
	step [69/146], loss=17.9694
	step [70/146], loss=18.3246
	step [71/146], loss=19.1808
	step [72/146], loss=17.5192
	step [73/146], loss=17.4800
	step [74/146], loss=18.3113
	step [75/146], loss=17.8347
	step [76/146], loss=19.0319
	step [77/146], loss=17.1426
	step [78/146], loss=20.0117
	step [79/146], loss=18.8827
	step [80/146], loss=18.1551
	step [81/146], loss=16.4073
	step [82/146], loss=18.7049
	step [83/146], loss=16.9544
	step [84/146], loss=17.5014
	step [85/146], loss=16.4963
	step [86/146], loss=16.8486
	step [87/146], loss=19.8024
	step [88/146], loss=18.3825
	step [89/146], loss=16.7703
	step [90/146], loss=18.3594
	step [91/146], loss=20.1250
	step [92/146], loss=17.2270
	step [93/146], loss=16.0871
	step [94/146], loss=19.1528
	step [95/146], loss=17.1164
	step [96/146], loss=16.0235
	step [97/146], loss=18.5771
	step [98/146], loss=16.8808
	step [99/146], loss=17.5713
	step [100/146], loss=19.3682
	step [101/146], loss=16.1826
	step [102/146], loss=18.7043
	step [103/146], loss=16.4508
	step [104/146], loss=18.2247
	step [105/146], loss=19.0380
	step [106/146], loss=20.1174
	step [107/146], loss=16.5289
	step [108/146], loss=17.7118
	step [109/146], loss=17.7900
	step [110/146], loss=18.6501
	step [111/146], loss=19.9573
	step [112/146], loss=18.3983
	step [113/146], loss=17.2083
	step [114/146], loss=17.7248
	step [115/146], loss=17.3541
	step [116/146], loss=18.5408
	step [117/146], loss=19.1531
	step [118/146], loss=16.5629
	step [119/146], loss=15.6612
	step [120/146], loss=17.1844
	step [121/146], loss=16.5131
	step [122/146], loss=17.5875
	step [123/146], loss=17.2095
	step [124/146], loss=18.1554
	step [125/146], loss=19.0677
	step [126/146], loss=17.0073
	step [127/146], loss=16.6746
	step [128/146], loss=17.8267
	step [129/146], loss=17.7088
	step [130/146], loss=19.1197
	step [131/146], loss=15.7107
	step [132/146], loss=17.6679
	step [133/146], loss=17.2921
	step [134/146], loss=18.7997
	step [135/146], loss=17.8493
	step [136/146], loss=17.0569
	step [137/146], loss=17.6817
	step [138/146], loss=17.1438
	step [139/146], loss=15.7674
	step [140/146], loss=18.0242
	step [141/146], loss=16.0452
	step [142/146], loss=17.3526
	step [143/146], loss=16.8888
	step [144/146], loss=18.8465
	step [145/146], loss=16.0959
	step [146/146], loss=11.6590
	Evaluating
	loss=0.0723, precision=0.1370, recall=0.9981, f1=0.2409
Training epoch 16
	step [1/146], loss=16.8036
	step [2/146], loss=17.4094
	step [3/146], loss=17.6106
	step [4/146], loss=17.8454
	step [5/146], loss=15.9347
	step [6/146], loss=17.1130
	step [7/146], loss=16.9938
	step [8/146], loss=15.8941
	step [9/146], loss=15.5165
	step [10/146], loss=17.2394
	step [11/146], loss=18.8888
	step [12/146], loss=18.6883
	step [13/146], loss=18.2848
	step [14/146], loss=16.3704
	step [15/146], loss=17.6403
	step [16/146], loss=16.9980
	step [17/146], loss=15.0111
	step [18/146], loss=19.0530
	step [19/146], loss=16.1317
	step [20/146], loss=15.8595
	step [21/146], loss=17.7485
	step [22/146], loss=16.9583
	step [23/146], loss=16.2813
	step [24/146], loss=18.6936
	step [25/146], loss=15.0174
	step [26/146], loss=16.3838
	step [27/146], loss=16.3330
	step [28/146], loss=15.7658
	step [29/146], loss=16.6839
	step [30/146], loss=17.6191
	step [31/146], loss=17.5582
	step [32/146], loss=18.2801
	step [33/146], loss=19.0097
	step [34/146], loss=17.4665
	step [35/146], loss=16.7719
	step [36/146], loss=15.7730
	step [37/146], loss=18.1460
	step [38/146], loss=16.9605
	step [39/146], loss=18.3243
	step [40/146], loss=18.2097
	step [41/146], loss=16.9698
	step [42/146], loss=16.5021
	step [43/146], loss=17.2673
	step [44/146], loss=15.3324
	step [45/146], loss=16.6912
	step [46/146], loss=16.1098
	step [47/146], loss=16.8813
	step [48/146], loss=19.0111
	step [49/146], loss=17.1780
	step [50/146], loss=17.6737
	step [51/146], loss=16.5208
	step [52/146], loss=17.3342
	step [53/146], loss=18.1276
	step [54/146], loss=15.3055
	step [55/146], loss=17.9201
	step [56/146], loss=16.1420
	step [57/146], loss=17.1565
	step [58/146], loss=17.0967
	step [59/146], loss=18.3510
	step [60/146], loss=15.6274
	step [61/146], loss=15.7209
	step [62/146], loss=15.2656
	step [63/146], loss=16.0906
	step [64/146], loss=17.2710
	step [65/146], loss=17.7408
	step [66/146], loss=17.1780
	step [67/146], loss=17.1513
	step [68/146], loss=16.6245
	step [69/146], loss=16.6335
	step [70/146], loss=15.2905
	step [71/146], loss=17.2166
	step [72/146], loss=17.7993
	step [73/146], loss=17.0565
	step [74/146], loss=14.9417
	step [75/146], loss=16.0191
	step [76/146], loss=18.3576
	step [77/146], loss=15.2049
	step [78/146], loss=16.7777
	step [79/146], loss=16.7117
	step [80/146], loss=14.8933
	step [81/146], loss=16.2412
	step [82/146], loss=17.2370
	step [83/146], loss=15.5843
	step [84/146], loss=15.9385
	step [85/146], loss=15.5629
	step [86/146], loss=15.3725
	step [87/146], loss=15.5318
	step [88/146], loss=19.7065
	step [89/146], loss=17.2144
	step [90/146], loss=17.6220
	step [91/146], loss=18.2683
	step [92/146], loss=17.0614
	step [93/146], loss=18.1018
	step [94/146], loss=18.0364
	step [95/146], loss=17.0219
	step [96/146], loss=16.4880
	step [97/146], loss=15.8463
	step [98/146], loss=16.3365
	step [99/146], loss=17.2516
	step [100/146], loss=15.5970
	step [101/146], loss=17.4848
	step [102/146], loss=17.3456
	step [103/146], loss=16.4532
	step [104/146], loss=17.2669
	step [105/146], loss=17.3446
	step [106/146], loss=13.9425
	step [107/146], loss=15.9755
	step [108/146], loss=16.9351
	step [109/146], loss=15.7458
	step [110/146], loss=15.9075
	step [111/146], loss=16.2772
	step [112/146], loss=16.6116
	step [113/146], loss=15.9796
	step [114/146], loss=15.4904
	step [115/146], loss=15.1901
	step [116/146], loss=16.2031
	step [117/146], loss=16.4458
	step [118/146], loss=15.7753
	step [119/146], loss=16.1051
	step [120/146], loss=17.1617
	step [121/146], loss=15.3594
	step [122/146], loss=16.5759
	step [123/146], loss=15.3450
	step [124/146], loss=17.5396
	step [125/146], loss=15.6107
	step [126/146], loss=19.4030
	step [127/146], loss=14.8519
	step [128/146], loss=16.1991
	step [129/146], loss=15.6960
	step [130/146], loss=17.6476
	step [131/146], loss=15.2629
	step [132/146], loss=15.3865
	step [133/146], loss=15.4605
	step [134/146], loss=16.1316
	step [135/146], loss=15.9845
	step [136/146], loss=16.2410
	step [137/146], loss=17.1564
	step [138/146], loss=15.1604
	step [139/146], loss=14.8277
	step [140/146], loss=16.7857
	step [141/146], loss=16.0663
	step [142/146], loss=18.0120
	step [143/146], loss=16.9736
	step [144/146], loss=15.4582
	step [145/146], loss=14.6372
	step [146/146], loss=9.5708
	Evaluating
	loss=0.0528, precision=0.2101, recall=0.9968, f1=0.3471
Training epoch 17
	step [1/146], loss=17.7350
	step [2/146], loss=16.7733
	step [3/146], loss=14.2483
	step [4/146], loss=14.8787
	step [5/146], loss=14.9971
	step [6/146], loss=16.6518
	step [7/146], loss=15.1501
	step [8/146], loss=15.5756
	step [9/146], loss=15.3415
	step [10/146], loss=14.7163
	step [11/146], loss=15.7918
	step [12/146], loss=18.0006
	step [13/146], loss=15.5509
	step [14/146], loss=15.5738
	step [15/146], loss=15.6057
	step [16/146], loss=15.4706
	step [17/146], loss=14.2683
	step [18/146], loss=15.3429
	step [19/146], loss=18.0055
	step [20/146], loss=16.5763
	step [21/146], loss=16.2251
	step [22/146], loss=15.7903
	step [23/146], loss=16.8490
	step [24/146], loss=15.1690
	step [25/146], loss=14.9341
	step [26/146], loss=14.8524
	step [27/146], loss=14.9962
	step [28/146], loss=15.1844
	step [29/146], loss=13.9622
	step [30/146], loss=15.9176
	step [31/146], loss=14.5680
	step [32/146], loss=16.9268
	step [33/146], loss=14.3809
	step [34/146], loss=14.2767
	step [35/146], loss=17.6949
	step [36/146], loss=17.2292
	step [37/146], loss=15.9614
	step [38/146], loss=17.3697
	step [39/146], loss=15.8813
	step [40/146], loss=18.3201
	step [41/146], loss=17.8544
	step [42/146], loss=15.9314
	step [43/146], loss=15.5374
	step [44/146], loss=17.1117
	step [45/146], loss=15.3264
	step [46/146], loss=17.3834
	step [47/146], loss=15.3074
	step [48/146], loss=14.8069
	step [49/146], loss=16.4931
	step [50/146], loss=16.9098
	step [51/146], loss=16.2444
	step [52/146], loss=16.7358
	step [53/146], loss=15.8859
	step [54/146], loss=14.1861
	step [55/146], loss=14.4519
	step [56/146], loss=16.4239
	step [57/146], loss=16.8836
	step [58/146], loss=14.8380
	step [59/146], loss=17.2438
	step [60/146], loss=16.1921
	step [61/146], loss=14.7637
	step [62/146], loss=14.9726
	step [63/146], loss=16.4725
	step [64/146], loss=14.7273
	step [65/146], loss=15.1146
	step [66/146], loss=14.9002
	step [67/146], loss=16.7865
	step [68/146], loss=15.2089
	step [69/146], loss=16.1715
	step [70/146], loss=17.1338
	step [71/146], loss=14.1258
	step [72/146], loss=16.6258
	step [73/146], loss=16.4898
	step [74/146], loss=16.4303
	step [75/146], loss=18.1295
	step [76/146], loss=15.6733
	step [77/146], loss=15.9368
	step [78/146], loss=15.1195
	step [79/146], loss=17.1532
	step [80/146], loss=16.5080
	step [81/146], loss=15.5945
	step [82/146], loss=14.7424
	step [83/146], loss=15.4223
	step [84/146], loss=16.6771
	step [85/146], loss=15.6847
	step [86/146], loss=15.7518
	step [87/146], loss=15.7055
	step [88/146], loss=15.8568
	step [89/146], loss=14.8482
	step [90/146], loss=14.5419
	step [91/146], loss=17.9092
	step [92/146], loss=15.5529
	step [93/146], loss=15.1254
	step [94/146], loss=16.6913
	step [95/146], loss=15.1255
	step [96/146], loss=14.8035
	step [97/146], loss=14.9833
	step [98/146], loss=16.9348
	step [99/146], loss=15.5069
	step [100/146], loss=14.1016
	step [101/146], loss=14.5971
	step [102/146], loss=14.6439
	step [103/146], loss=16.6149
	step [104/146], loss=15.3616
	step [105/146], loss=14.1916
	step [106/146], loss=17.2657
	step [107/146], loss=15.7675
	step [108/146], loss=15.2700
	step [109/146], loss=15.4468
	step [110/146], loss=14.8981
	step [111/146], loss=15.0591
	step [112/146], loss=16.1388
	step [113/146], loss=15.1483
	step [114/146], loss=15.1778
	step [115/146], loss=16.0208
	step [116/146], loss=15.2073
	step [117/146], loss=14.6259
	step [118/146], loss=15.4179
	step [119/146], loss=17.5761
	step [120/146], loss=14.7929
	step [121/146], loss=15.3570
	step [122/146], loss=16.2664
	step [123/146], loss=16.5816
	step [124/146], loss=14.6302
	step [125/146], loss=14.7710
	step [126/146], loss=15.7131
	step [127/146], loss=16.2697
	step [128/146], loss=14.9675
	step [129/146], loss=13.8809
	step [130/146], loss=15.4673
	step [131/146], loss=14.8727
	step [132/146], loss=16.3112
	step [133/146], loss=14.8525
	step [134/146], loss=14.2224
	step [135/146], loss=15.2913
	step [136/146], loss=16.0721
	step [137/146], loss=14.9269
	step [138/146], loss=13.8645
	step [139/146], loss=14.3460
	step [140/146], loss=14.4843
	step [141/146], loss=16.5075
	step [142/146], loss=15.0455
	step [143/146], loss=15.8289
	step [144/146], loss=14.6472
	step [145/146], loss=13.5092
	step [146/146], loss=9.3186
	Evaluating
	loss=0.0439, precision=0.2578, recall=0.9954, f1=0.4096
saving model as: 2_saved_model.pth
Training epoch 18
	step [1/146], loss=16.2743
	step [2/146], loss=16.0728
	step [3/146], loss=14.8270
	step [4/146], loss=14.0289
	step [5/146], loss=14.9133
	step [6/146], loss=16.6381
	step [7/146], loss=14.5568
	step [8/146], loss=13.1703
	step [9/146], loss=13.7176
	step [10/146], loss=16.4637
	step [11/146], loss=13.3844
	step [12/146], loss=14.5766
	step [13/146], loss=15.4290
	step [14/146], loss=14.5604
	step [15/146], loss=15.5047
	step [16/146], loss=14.2719
	step [17/146], loss=15.3439
	step [18/146], loss=15.0289
	step [19/146], loss=13.0223
	step [20/146], loss=15.2115
	step [21/146], loss=14.5670
	step [22/146], loss=14.6423
	step [23/146], loss=14.9941
	step [24/146], loss=16.0688
	step [25/146], loss=13.8013
	step [26/146], loss=14.8582
	step [27/146], loss=15.1943
	step [28/146], loss=15.0446
	step [29/146], loss=16.6343
	step [30/146], loss=13.8529
	step [31/146], loss=16.2114
	step [32/146], loss=16.6744
	step [33/146], loss=14.6863
	step [34/146], loss=14.4706
	step [35/146], loss=16.2026
	step [36/146], loss=15.5163
	step [37/146], loss=13.7107
	step [38/146], loss=14.7268
	step [39/146], loss=13.6111
	step [40/146], loss=16.7430
	step [41/146], loss=17.2074
	step [42/146], loss=14.6797
	step [43/146], loss=14.6404
	step [44/146], loss=14.5618
	step [45/146], loss=15.4517
	step [46/146], loss=17.0016
	step [47/146], loss=15.0916
	step [48/146], loss=15.5113
	step [49/146], loss=15.0830
	step [50/146], loss=16.9015
	step [51/146], loss=17.3043
	step [52/146], loss=13.4158
	step [53/146], loss=15.5777
	step [54/146], loss=17.0790
	step [55/146], loss=13.8399
	step [56/146], loss=13.6489
	step [57/146], loss=15.7879
	step [58/146], loss=14.9667
	step [59/146], loss=14.1333
	step [60/146], loss=13.9670
	step [61/146], loss=14.7320
	step [62/146], loss=15.1698
	step [63/146], loss=15.2382
	step [64/146], loss=15.3625
	step [65/146], loss=15.6996
	step [66/146], loss=12.8542
	step [67/146], loss=14.4817
	step [68/146], loss=14.8544
	step [69/146], loss=14.6976
	step [70/146], loss=14.5418
	step [71/146], loss=15.0907
	step [72/146], loss=16.6547
	step [73/146], loss=14.3141
	step [74/146], loss=15.3730
	step [75/146], loss=14.5973
	step [76/146], loss=15.8670
	step [77/146], loss=13.6314
	step [78/146], loss=13.4252
	step [79/146], loss=14.9452
	step [80/146], loss=15.8554
	step [81/146], loss=13.8749
	step [82/146], loss=14.2128
	step [83/146], loss=14.3888
	step [84/146], loss=13.9851
	step [85/146], loss=14.7270
	step [86/146], loss=16.0750
	step [87/146], loss=15.5136
	step [88/146], loss=15.0941
	step [89/146], loss=14.0472
	step [90/146], loss=15.8448
	step [91/146], loss=15.6109
	step [92/146], loss=14.8027
	step [93/146], loss=15.9638
	step [94/146], loss=13.9312
	step [95/146], loss=15.9184
	step [96/146], loss=14.2295
	step [97/146], loss=15.1013
	step [98/146], loss=15.6036
	step [99/146], loss=13.5514
	step [100/146], loss=14.4018
	step [101/146], loss=16.2432
	step [102/146], loss=13.6612
	step [103/146], loss=15.2665
	step [104/146], loss=16.5510
	step [105/146], loss=13.9010
	step [106/146], loss=16.6890
	step [107/146], loss=12.8496
	step [108/146], loss=14.0925
	step [109/146], loss=14.5006
	step [110/146], loss=14.0832
	step [111/146], loss=13.4243
	step [112/146], loss=14.1547
	step [113/146], loss=16.4553
	step [114/146], loss=14.3314
	step [115/146], loss=13.0107
	step [116/146], loss=17.1380
	step [117/146], loss=14.0985
	step [118/146], loss=15.2062
	step [119/146], loss=13.1376
	step [120/146], loss=14.6627
	step [121/146], loss=14.7097
	step [122/146], loss=15.8601
	step [123/146], loss=14.1852
	step [124/146], loss=15.7416
	step [125/146], loss=12.4012
	step [126/146], loss=15.6741
	step [127/146], loss=13.9775
	step [128/146], loss=13.5304
	step [129/146], loss=14.0207
	step [130/146], loss=14.0313
	step [131/146], loss=13.9200
	step [132/146], loss=14.0573
	step [133/146], loss=14.0005
	step [134/146], loss=14.8455
	step [135/146], loss=16.1763
	step [136/146], loss=13.6854
	step [137/146], loss=14.9473
	step [138/146], loss=14.0801
	step [139/146], loss=14.8637
	step [140/146], loss=13.8890
	step [141/146], loss=15.1995
	step [142/146], loss=14.6066
	step [143/146], loss=14.8558
	step [144/146], loss=15.2837
	step [145/146], loss=14.9047
	step [146/146], loss=7.1373
	Evaluating
	loss=0.0456, precision=0.2241, recall=0.9963, f1=0.3659
Training epoch 19
	step [1/146], loss=12.3293
	step [2/146], loss=14.1106
	step [3/146], loss=13.7429
	step [4/146], loss=13.0662
	step [5/146], loss=12.5145
	step [6/146], loss=14.4928
	step [7/146], loss=14.9994
	step [8/146], loss=13.5402
	step [9/146], loss=12.5803
	step [10/146], loss=14.7058
	step [11/146], loss=13.8734
	step [12/146], loss=14.6428
	step [13/146], loss=13.5858
	step [14/146], loss=14.9634
	step [15/146], loss=13.4851
	step [16/146], loss=15.9822
	step [17/146], loss=13.5060
	step [18/146], loss=13.6301
	step [19/146], loss=14.3451
	step [20/146], loss=13.5871
	step [21/146], loss=14.4479
	step [22/146], loss=13.0056
	step [23/146], loss=16.1846
	step [24/146], loss=14.9836
	step [25/146], loss=15.2599
	step [26/146], loss=12.2655
	step [27/146], loss=13.8805
	step [28/146], loss=14.1710
	step [29/146], loss=13.8622
	step [30/146], loss=16.5310
	step [31/146], loss=14.6768
	step [32/146], loss=14.8303
	step [33/146], loss=14.5995
	step [34/146], loss=14.6185
	step [35/146], loss=13.5970
	step [36/146], loss=14.1979
	step [37/146], loss=15.3113
	step [38/146], loss=14.3259
	step [39/146], loss=12.6736
	step [40/146], loss=15.6786
	step [41/146], loss=14.5768
	step [42/146], loss=14.0041
	step [43/146], loss=13.1180
	step [44/146], loss=12.9501
	step [45/146], loss=15.2836
	step [46/146], loss=13.1014
	step [47/146], loss=13.1720
	step [48/146], loss=15.5254
	step [49/146], loss=14.0863
	step [50/146], loss=14.8540
	step [51/146], loss=14.9639
	step [52/146], loss=14.9273
	step [53/146], loss=13.9439
	step [54/146], loss=15.1459
	step [55/146], loss=12.9435
	step [56/146], loss=13.4315
	step [57/146], loss=12.5272
	step [58/146], loss=16.6371
	step [59/146], loss=15.0909
	step [60/146], loss=14.1756
	step [61/146], loss=14.3781
	step [62/146], loss=13.4293
	step [63/146], loss=14.5305
	step [64/146], loss=13.2157
	step [65/146], loss=15.5633
	step [66/146], loss=14.2815
	step [67/146], loss=14.3688
	step [68/146], loss=13.1118
	step [69/146], loss=13.2103
	step [70/146], loss=13.8548
	step [71/146], loss=13.6539
	step [72/146], loss=14.0559
	step [73/146], loss=12.8958
	step [74/146], loss=13.0622
	step [75/146], loss=14.6743
	step [76/146], loss=13.9574
	step [77/146], loss=13.9836
	step [78/146], loss=13.0372
	step [79/146], loss=14.5603
	step [80/146], loss=12.9795
	step [81/146], loss=16.1027
	step [82/146], loss=13.3420
	step [83/146], loss=15.0234
	step [84/146], loss=16.0770
	step [85/146], loss=14.3553
	step [86/146], loss=15.8851
	step [87/146], loss=13.3041
	step [88/146], loss=15.0571
	step [89/146], loss=12.3862
	step [90/146], loss=13.4512
	step [91/146], loss=14.4851
	step [92/146], loss=13.3289
	step [93/146], loss=14.2524
	step [94/146], loss=13.7509
	step [95/146], loss=15.0021
	step [96/146], loss=14.4585
	step [97/146], loss=14.3736
	step [98/146], loss=14.9225
	step [99/146], loss=13.0338
	step [100/146], loss=15.3959
	step [101/146], loss=14.0091
	step [102/146], loss=15.6074
	step [103/146], loss=12.8748
	step [104/146], loss=13.4662
	step [105/146], loss=13.7692
	step [106/146], loss=12.6575
	step [107/146], loss=15.1579
	step [108/146], loss=15.1513
	step [109/146], loss=16.6004
	step [110/146], loss=14.1457
	step [111/146], loss=12.9895
	step [112/146], loss=15.2467
	step [113/146], loss=13.6050
	step [114/146], loss=13.4172
	step [115/146], loss=16.5525
	step [116/146], loss=14.5648
	step [117/146], loss=13.8730
	step [118/146], loss=13.3464
	step [119/146], loss=13.8604
	step [120/146], loss=12.6890
	step [121/146], loss=13.8454
	step [122/146], loss=14.0731
	step [123/146], loss=13.9304
	step [124/146], loss=14.9680
	step [125/146], loss=14.4043
	step [126/146], loss=15.3714
	step [127/146], loss=13.2348
	step [128/146], loss=13.9160
	step [129/146], loss=12.4972
	step [130/146], loss=12.6361
	step [131/146], loss=14.3057
	step [132/146], loss=14.6061
	step [133/146], loss=13.7406
	step [134/146], loss=12.1041
	step [135/146], loss=13.8497
	step [136/146], loss=13.2624
	step [137/146], loss=13.5166
	step [138/146], loss=13.3355
	step [139/146], loss=14.0509
	step [140/146], loss=12.0233
	step [141/146], loss=13.7819
	step [142/146], loss=14.3642
	step [143/146], loss=12.5047
	step [144/146], loss=12.8772
	step [145/146], loss=13.2108
	step [146/146], loss=8.1838
	Evaluating
	loss=0.0458, precision=0.2099, recall=0.9965, f1=0.3467
Training epoch 20
	step [1/146], loss=14.1164
	step [2/146], loss=14.6036
	step [3/146], loss=13.9326
	step [4/146], loss=15.4203
	step [5/146], loss=14.5255
	step [6/146], loss=15.0642
	step [7/146], loss=12.7845
	step [8/146], loss=13.5683
	step [9/146], loss=13.7128
	step [10/146], loss=14.6229
	step [11/146], loss=12.3618
	step [12/146], loss=13.7075
	step [13/146], loss=13.5448
	step [14/146], loss=14.1260
	step [15/146], loss=14.8971
	step [16/146], loss=15.8319
	step [17/146], loss=13.5391
	step [18/146], loss=13.4964
	step [19/146], loss=14.2359
	step [20/146], loss=13.9350
	step [21/146], loss=12.7713
	step [22/146], loss=13.0387
	step [23/146], loss=13.6939
	step [24/146], loss=14.9579
	step [25/146], loss=13.8496
	step [26/146], loss=15.5621
	step [27/146], loss=13.7208
	step [28/146], loss=13.0644
	step [29/146], loss=13.0575
	step [30/146], loss=12.3524
	step [31/146], loss=14.9093
	step [32/146], loss=14.9608
	step [33/146], loss=15.9454
	step [34/146], loss=13.4656
	step [35/146], loss=12.3607
	step [36/146], loss=13.9935
	step [37/146], loss=14.0475
	step [38/146], loss=13.8052
	step [39/146], loss=13.0522
	step [40/146], loss=12.3311
	step [41/146], loss=12.8178
	step [42/146], loss=13.7224
	step [43/146], loss=12.9847
	step [44/146], loss=11.1820
	step [45/146], loss=13.2844
	step [46/146], loss=13.1924
	step [47/146], loss=14.6490
	step [48/146], loss=13.9439
	step [49/146], loss=13.4974
	step [50/146], loss=12.8000
	step [51/146], loss=13.5346
	step [52/146], loss=13.7145
	step [53/146], loss=13.9734
	step [54/146], loss=13.4417
	step [55/146], loss=13.1663
	step [56/146], loss=14.0728
	step [57/146], loss=14.0556
	step [58/146], loss=14.9155
	step [59/146], loss=13.1627
	step [60/146], loss=14.8535
	step [61/146], loss=13.1135
	step [62/146], loss=11.9260
	step [63/146], loss=14.2988
	step [64/146], loss=14.0722
	step [65/146], loss=14.5501
	step [66/146], loss=14.8374
	step [67/146], loss=14.5270
	step [68/146], loss=13.9135
	step [69/146], loss=12.0928
	step [70/146], loss=12.0774
	step [71/146], loss=13.4047
	step [72/146], loss=14.0433
	step [73/146], loss=11.9419
	step [74/146], loss=15.0120
	step [75/146], loss=12.6444
	step [76/146], loss=12.4507
	step [77/146], loss=13.0762
	step [78/146], loss=12.0248
	step [79/146], loss=13.6185
	step [80/146], loss=12.1450
	step [81/146], loss=15.7315
	step [82/146], loss=13.3407
	step [83/146], loss=11.6203
	step [84/146], loss=12.8579
	step [85/146], loss=12.9649
	step [86/146], loss=14.2336
	step [87/146], loss=12.5379
	step [88/146], loss=12.3006
	step [89/146], loss=11.0773
	step [90/146], loss=14.4133
	step [91/146], loss=12.8585
	step [92/146], loss=13.9012
	step [93/146], loss=13.6588
	step [94/146], loss=14.8419
	step [95/146], loss=13.2282
	step [96/146], loss=12.3303
	step [97/146], loss=11.8597
	step [98/146], loss=12.7546
	step [99/146], loss=14.0815
	step [100/146], loss=13.4944
	step [101/146], loss=12.4211
	step [102/146], loss=13.6009
	step [103/146], loss=13.2530
	step [104/146], loss=12.8569
	step [105/146], loss=12.7860
	step [106/146], loss=14.0253
	step [107/146], loss=13.7289
	step [108/146], loss=14.3612
	step [109/146], loss=11.5724
	step [110/146], loss=14.0140
	step [111/146], loss=13.2182
	step [112/146], loss=11.8596
	step [113/146], loss=11.9915
	step [114/146], loss=13.8877
	step [115/146], loss=14.2551
	step [116/146], loss=12.1329
	step [117/146], loss=13.2359
	step [118/146], loss=13.5358
	step [119/146], loss=13.5471
	step [120/146], loss=11.9861
	step [121/146], loss=12.8647
	step [122/146], loss=12.3058
	step [123/146], loss=12.8354
	step [124/146], loss=13.5995
	step [125/146], loss=13.6225
	step [126/146], loss=13.1422
	step [127/146], loss=14.4480
	step [128/146], loss=12.7782
	step [129/146], loss=13.0642
	step [130/146], loss=13.6746
	step [131/146], loss=13.2443
	step [132/146], loss=12.1156
	step [133/146], loss=13.4296
	step [134/146], loss=15.3547
	step [135/146], loss=12.0374
	step [136/146], loss=12.4528
	step [137/146], loss=12.2897
	step [138/146], loss=12.5826
	step [139/146], loss=13.2924
	step [140/146], loss=14.3187
	step [141/146], loss=13.4407
	step [142/146], loss=12.1412
	step [143/146], loss=11.9692
	step [144/146], loss=13.0630
	step [145/146], loss=14.1002
	step [146/146], loss=7.5594
	Evaluating
	loss=0.0421, precision=0.2235, recall=0.9963, f1=0.3651
Training epoch 21
	step [1/146], loss=13.6440
	step [2/146], loss=12.9063
	step [3/146], loss=11.7073
	step [4/146], loss=13.0884
	step [5/146], loss=13.3033
	step [6/146], loss=12.5664
	step [7/146], loss=13.8631
	step [8/146], loss=12.9728
	step [9/146], loss=12.2848
	step [10/146], loss=12.8369
	step [11/146], loss=13.1903
	step [12/146], loss=11.3538
	step [13/146], loss=12.2637
	step [14/146], loss=12.0752
	step [15/146], loss=12.3436
	step [16/146], loss=13.5347
	step [17/146], loss=13.0116
	step [18/146], loss=13.3019
	step [19/146], loss=14.5397
	step [20/146], loss=12.8958
	step [21/146], loss=11.4635
	step [22/146], loss=12.4445
	step [23/146], loss=14.2746
	step [24/146], loss=12.9659
	step [25/146], loss=13.4897
	step [26/146], loss=13.0563
	step [27/146], loss=13.1476
	step [28/146], loss=12.8017
	step [29/146], loss=12.6492
	step [30/146], loss=13.4553
	step [31/146], loss=11.9744
	step [32/146], loss=14.5473
	step [33/146], loss=12.2022
	step [34/146], loss=12.0529
	step [35/146], loss=12.8709
	step [36/146], loss=13.2843
	step [37/146], loss=12.3474
	step [38/146], loss=13.0961
	step [39/146], loss=14.1392
	step [40/146], loss=12.9156
	step [41/146], loss=14.3540
	step [42/146], loss=12.0528
	step [43/146], loss=13.1007
	step [44/146], loss=11.8858
	step [45/146], loss=13.4294
	step [46/146], loss=13.1130
	step [47/146], loss=11.9050
	step [48/146], loss=11.8501
	step [49/146], loss=13.5409
	step [50/146], loss=13.7646
	step [51/146], loss=12.9879
	step [52/146], loss=12.0279
	step [53/146], loss=14.9604
	step [54/146], loss=13.2196
	step [55/146], loss=12.6544
	step [56/146], loss=13.3062
	step [57/146], loss=13.9072
	step [58/146], loss=17.9046
	step [59/146], loss=12.8731
	step [60/146], loss=14.5584
	step [61/146], loss=14.1727
	step [62/146], loss=12.2550
	step [63/146], loss=13.5584
	step [64/146], loss=13.2005
	step [65/146], loss=14.2430
	step [66/146], loss=10.7390
	step [67/146], loss=13.8793
	step [68/146], loss=13.2544
	step [69/146], loss=11.9557
	step [70/146], loss=12.8927
	step [71/146], loss=12.5654
	step [72/146], loss=11.9376
	step [73/146], loss=12.7739
	step [74/146], loss=12.8324
	step [75/146], loss=14.1361
	step [76/146], loss=10.9221
	step [77/146], loss=12.9121
	step [78/146], loss=12.1586
	step [79/146], loss=12.0927
	step [80/146], loss=12.2251
	step [81/146], loss=12.9769
	step [82/146], loss=12.8857
	step [83/146], loss=12.9456
	step [84/146], loss=13.0165
	step [85/146], loss=11.7398
	step [86/146], loss=13.3097
	step [87/146], loss=15.6400
	step [88/146], loss=12.2550
	step [89/146], loss=13.0384
	step [90/146], loss=12.1202
	step [91/146], loss=13.5329
	step [92/146], loss=12.2934
	step [93/146], loss=11.4453
	step [94/146], loss=12.7421
	step [95/146], loss=12.9668
	step [96/146], loss=11.8031
	step [97/146], loss=12.7508
	step [98/146], loss=14.6137
	step [99/146], loss=13.2299
	step [100/146], loss=13.3815
	step [101/146], loss=13.5832
	step [102/146], loss=13.7231
	step [103/146], loss=12.4849
	step [104/146], loss=12.2402
	step [105/146], loss=11.4149
	step [106/146], loss=12.6312
	step [107/146], loss=13.8613
	step [108/146], loss=11.6651
	step [109/146], loss=13.0331
	step [110/146], loss=10.8634
	step [111/146], loss=13.5836
	step [112/146], loss=12.1728
	step [113/146], loss=12.8394
	step [114/146], loss=11.8887
	step [115/146], loss=13.0743
	step [116/146], loss=12.9626
	step [117/146], loss=13.1189
	step [118/146], loss=14.3319
	step [119/146], loss=12.2857
	step [120/146], loss=12.5954
	step [121/146], loss=11.3939
	step [122/146], loss=12.3353
	step [123/146], loss=11.7786
	step [124/146], loss=14.1996
	step [125/146], loss=12.9383
	step [126/146], loss=11.8696
	step [127/146], loss=13.7436
	step [128/146], loss=11.3874
	step [129/146], loss=11.9736
	step [130/146], loss=13.5805
	step [131/146], loss=12.4429
	step [132/146], loss=13.1724
	step [133/146], loss=12.2708
	step [134/146], loss=12.8642
	step [135/146], loss=13.5004
	step [136/146], loss=12.6660
	step [137/146], loss=12.1039
	step [138/146], loss=14.6125
	step [139/146], loss=13.0154
	step [140/146], loss=13.0056
	step [141/146], loss=11.4198
	step [142/146], loss=12.8823
	step [143/146], loss=12.0167
	step [144/146], loss=13.6538
	step [145/146], loss=13.2455
	step [146/146], loss=7.6733
	Evaluating
	loss=0.0365, precision=0.2545, recall=0.9954, f1=0.4053
Training epoch 22
	step [1/146], loss=13.0876
	step [2/146], loss=11.6534
	step [3/146], loss=11.3666
	step [4/146], loss=11.8209
	step [5/146], loss=13.4771
	step [6/146], loss=12.4934
	step [7/146], loss=12.4248
	step [8/146], loss=11.7454
	step [9/146], loss=13.3875
	step [10/146], loss=11.1457
	step [11/146], loss=11.5641
	step [12/146], loss=12.7300
	step [13/146], loss=11.9182
	step [14/146], loss=10.6210
	step [15/146], loss=12.8300
	step [16/146], loss=12.3629
	step [17/146], loss=10.7353
	step [18/146], loss=13.0218
	step [19/146], loss=12.0248
	step [20/146], loss=11.6859
	step [21/146], loss=14.5592
	step [22/146], loss=12.5781
	step [23/146], loss=13.2020
	step [24/146], loss=12.5042
	step [25/146], loss=12.8495
	step [26/146], loss=11.8085
	step [27/146], loss=12.3005
	step [28/146], loss=11.7379
	step [29/146], loss=11.5500
	step [30/146], loss=11.9041
	step [31/146], loss=12.0013
	step [32/146], loss=13.5453
	step [33/146], loss=13.5926
	step [34/146], loss=12.1993
	step [35/146], loss=11.8972
	step [36/146], loss=12.8180
	step [37/146], loss=12.0123
	step [38/146], loss=11.5589
	step [39/146], loss=13.1771
	step [40/146], loss=12.2508
	step [41/146], loss=14.0959
	step [42/146], loss=12.1367
	step [43/146], loss=12.7435
	step [44/146], loss=12.1141
	step [45/146], loss=12.3949
	step [46/146], loss=13.7566
	step [47/146], loss=12.0213
	step [48/146], loss=13.0282
	step [49/146], loss=11.6693
	step [50/146], loss=12.8046
	step [51/146], loss=12.4112
	step [52/146], loss=12.7057
	step [53/146], loss=12.6159
	step [54/146], loss=13.0626
	step [55/146], loss=11.1979
	step [56/146], loss=12.6867
	step [57/146], loss=9.9650
	step [58/146], loss=12.3586
	step [59/146], loss=11.8591
	step [60/146], loss=11.6265
	step [61/146], loss=11.4924
	step [62/146], loss=12.6902
	step [63/146], loss=11.0280
	step [64/146], loss=11.6924
	step [65/146], loss=13.6186
	step [66/146], loss=11.7427
	step [67/146], loss=13.2582
	step [68/146], loss=11.8819
	step [69/146], loss=11.7583
	step [70/146], loss=11.7334
	step [71/146], loss=13.1783
	step [72/146], loss=11.3485
	step [73/146], loss=12.8046
	step [74/146], loss=13.8042
	step [75/146], loss=12.0339
	step [76/146], loss=11.3023
	step [77/146], loss=14.0202
	step [78/146], loss=14.1381
	step [79/146], loss=11.9413
	step [80/146], loss=11.6690
	step [81/146], loss=11.7074
	step [82/146], loss=13.0387
	step [83/146], loss=11.7548
	step [84/146], loss=13.5651
	step [85/146], loss=12.7708
	step [86/146], loss=11.5304
	step [87/146], loss=12.8907
	step [88/146], loss=11.5780
	step [89/146], loss=11.8882
	step [90/146], loss=10.8071
	step [91/146], loss=12.5536
	step [92/146], loss=13.0954
	step [93/146], loss=11.5994
	step [94/146], loss=12.9754
	step [95/146], loss=13.3673
	step [96/146], loss=11.8682
	step [97/146], loss=11.0032
	step [98/146], loss=11.0092
	step [99/146], loss=12.0947
	step [100/146], loss=12.3954
	step [101/146], loss=12.6067
	step [102/146], loss=13.6474
	step [103/146], loss=11.9304
	step [104/146], loss=11.1557
	step [105/146], loss=11.1163
	step [106/146], loss=10.5938
	step [107/146], loss=11.5542
	step [108/146], loss=12.5875
	step [109/146], loss=11.2563
	step [110/146], loss=11.7276
	step [111/146], loss=12.8827
	step [112/146], loss=13.0172
	step [113/146], loss=12.1683
	step [114/146], loss=12.1849
	step [115/146], loss=13.4305
	step [116/146], loss=12.5438
	step [117/146], loss=12.0084
	step [118/146], loss=13.5842
	step [119/146], loss=13.0179
	step [120/146], loss=11.9155
	step [121/146], loss=11.7784
	step [122/146], loss=14.5125
	step [123/146], loss=12.0436
	step [124/146], loss=12.7823
	step [125/146], loss=12.2713
	step [126/146], loss=11.2592
	step [127/146], loss=12.6376
	step [128/146], loss=11.4493
	step [129/146], loss=13.4949
	step [130/146], loss=11.9137
	step [131/146], loss=14.1799
	step [132/146], loss=11.8228
	step [133/146], loss=10.2056
	step [134/146], loss=12.0452
	step [135/146], loss=12.2143
	step [136/146], loss=12.1024
	step [137/146], loss=11.4178
	step [138/146], loss=13.4142
	step [139/146], loss=11.3167
	step [140/146], loss=11.8415
	step [141/146], loss=12.3264
	step [142/146], loss=12.3681
	step [143/146], loss=12.7154
	step [144/146], loss=11.7829
	step [145/146], loss=11.5942
	step [146/146], loss=7.9453
	Evaluating
	loss=0.0391, precision=0.2172, recall=0.9962, f1=0.3567
Training epoch 23
	step [1/146], loss=10.9220
	step [2/146], loss=11.9209
	step [3/146], loss=10.8185
	step [4/146], loss=9.8970
	step [5/146], loss=11.9593
	step [6/146], loss=11.2054
	step [7/146], loss=12.3204
	step [8/146], loss=13.7834
	step [9/146], loss=12.3508
	step [10/146], loss=13.5334
	step [11/146], loss=11.8964
	step [12/146], loss=12.9052
	step [13/146], loss=12.9483
	step [14/146], loss=11.7284
	step [15/146], loss=12.9182
	step [16/146], loss=11.5915
	step [17/146], loss=12.4307
	step [18/146], loss=10.7799
	step [19/146], loss=12.5508
	step [20/146], loss=11.1488
	step [21/146], loss=11.1242
	step [22/146], loss=11.5817
	step [23/146], loss=12.7660
	step [24/146], loss=12.7790
	step [25/146], loss=12.9204
	step [26/146], loss=13.2434
	step [27/146], loss=12.1081
	step [28/146], loss=12.2929
	step [29/146], loss=11.3026
	step [30/146], loss=10.8315
	step [31/146], loss=12.0185
	step [32/146], loss=11.6914
	step [33/146], loss=10.8707
	step [34/146], loss=11.5977
	step [35/146], loss=11.8685
	step [36/146], loss=11.6651
	step [37/146], loss=11.5799
	step [38/146], loss=11.5590
	step [39/146], loss=12.8449
	step [40/146], loss=13.7423
	step [41/146], loss=13.5751
	step [42/146], loss=11.5299
	step [43/146], loss=12.3833
	step [44/146], loss=11.5236
	step [45/146], loss=12.6912
	step [46/146], loss=13.6257
	step [47/146], loss=12.9096
	step [48/146], loss=11.7958
	step [49/146], loss=11.6007
	step [50/146], loss=11.0533
	step [51/146], loss=12.6908
	step [52/146], loss=13.1162
	step [53/146], loss=12.2026
	step [54/146], loss=11.7892
	step [55/146], loss=11.4301
	step [56/146], loss=11.7294
	step [57/146], loss=11.2266
	step [58/146], loss=10.9501
	step [59/146], loss=12.3027
	step [60/146], loss=10.4106
	step [61/146], loss=12.2849
	step [62/146], loss=12.1712
	step [63/146], loss=11.4964
	step [64/146], loss=10.2340
	step [65/146], loss=12.3894
	step [66/146], loss=11.4011
	step [67/146], loss=10.8011
	step [68/146], loss=11.9340
	step [69/146], loss=11.8748
	step [70/146], loss=11.7108
	step [71/146], loss=11.7256
	step [72/146], loss=11.0835
	step [73/146], loss=10.1289
	step [74/146], loss=11.5099
	step [75/146], loss=12.2624
	step [76/146], loss=10.4593
	step [77/146], loss=11.3087
	step [78/146], loss=12.6948
	step [79/146], loss=11.4198
	step [80/146], loss=11.6523
	step [81/146], loss=11.2069
	step [82/146], loss=12.4520
	step [83/146], loss=11.8611
	step [84/146], loss=14.4337
	step [85/146], loss=11.0597
	step [86/146], loss=10.7598
	step [87/146], loss=11.2357
	step [88/146], loss=11.1592
	step [89/146], loss=12.3133
	step [90/146], loss=11.4317
	step [91/146], loss=12.5436
	step [92/146], loss=11.8654
	step [93/146], loss=11.4059
	step [94/146], loss=13.1899
	step [95/146], loss=11.8889
	step [96/146], loss=12.9732
	step [97/146], loss=13.0614
	step [98/146], loss=12.0110
	step [99/146], loss=10.5279
	step [100/146], loss=11.3681
	step [101/146], loss=12.0736
	step [102/146], loss=11.6175
	step [103/146], loss=13.0768
	step [104/146], loss=10.7407
	step [105/146], loss=10.4811
	step [106/146], loss=11.1041
	step [107/146], loss=11.4225
	step [108/146], loss=9.9250
	step [109/146], loss=12.7557
	step [110/146], loss=12.5269
	step [111/146], loss=12.7720
	step [112/146], loss=12.4576
	step [113/146], loss=11.8370
	step [114/146], loss=11.2096
	step [115/146], loss=13.0269
	step [116/146], loss=11.8772
	step [117/146], loss=13.6327
	step [118/146], loss=11.8597
	step [119/146], loss=12.9930
	step [120/146], loss=12.1418
	step [121/146], loss=12.0382
	step [122/146], loss=13.2377
	step [123/146], loss=11.5680
	step [124/146], loss=10.8089
	step [125/146], loss=12.6018
	step [126/146], loss=11.7355
	step [127/146], loss=12.1406
	step [128/146], loss=11.3364
	step [129/146], loss=11.7505
	step [130/146], loss=10.8722
	step [131/146], loss=10.9572
	step [132/146], loss=11.0826
	step [133/146], loss=10.3019
	step [134/146], loss=10.9251
	step [135/146], loss=11.5434
	step [136/146], loss=11.1878
	step [137/146], loss=11.3728
	step [138/146], loss=12.7392
	step [139/146], loss=13.3040
	step [140/146], loss=11.5908
	step [141/146], loss=11.1706
	step [142/146], loss=11.5474
	step [143/146], loss=11.4789
	step [144/146], loss=11.1728
	step [145/146], loss=10.3034
	step [146/146], loss=6.6443
	Evaluating
	loss=0.0349, precision=0.2456, recall=0.9957, f1=0.3940
Training epoch 24
	step [1/146], loss=12.4498
	step [2/146], loss=12.9737
	step [3/146], loss=11.1516
	step [4/146], loss=11.7826
	step [5/146], loss=11.0667
	step [6/146], loss=11.7935
	step [7/146], loss=11.8232
	step [8/146], loss=12.4016
	step [9/146], loss=11.2814
	step [10/146], loss=11.6323
	step [11/146], loss=11.8441
	step [12/146], loss=10.3279
	step [13/146], loss=10.2279
	step [14/146], loss=10.9853
	step [15/146], loss=10.1889
	step [16/146], loss=11.1881
	step [17/146], loss=11.5085
	step [18/146], loss=10.2580
	step [19/146], loss=10.4496
	step [20/146], loss=12.3604
	step [21/146], loss=12.6555
	step [22/146], loss=9.9021
	step [23/146], loss=10.8652
	step [24/146], loss=11.8480
	step [25/146], loss=11.9699
	step [26/146], loss=13.5286
	step [27/146], loss=12.0092
	step [28/146], loss=12.3837
	step [29/146], loss=11.9214
	step [30/146], loss=11.7793
	step [31/146], loss=10.7180
	step [32/146], loss=12.0164
	step [33/146], loss=11.9099
	step [34/146], loss=11.1558
	step [35/146], loss=11.5009
	step [36/146], loss=10.6966
	step [37/146], loss=11.8152
	step [38/146], loss=10.8958
	step [39/146], loss=12.6653
	step [40/146], loss=11.5291
	step [41/146], loss=11.2511
	step [42/146], loss=12.2758
	step [43/146], loss=9.9880
	step [44/146], loss=11.2355
	step [45/146], loss=11.0785
	step [46/146], loss=11.7961
	step [47/146], loss=10.8984
	step [48/146], loss=11.4632
	step [49/146], loss=9.8897
	step [50/146], loss=11.0245
	step [51/146], loss=12.0163
	step [52/146], loss=11.6956
	step [53/146], loss=11.9721
	step [54/146], loss=10.8104
	step [55/146], loss=10.8309
	step [56/146], loss=10.0769
	step [57/146], loss=11.0961
	step [58/146], loss=11.2050
	step [59/146], loss=10.3580
	step [60/146], loss=10.1410
	step [61/146], loss=11.3110
	step [62/146], loss=10.9955
	step [63/146], loss=10.2211
	step [64/146], loss=11.0683
	step [65/146], loss=11.9128
	step [66/146], loss=11.2804
	step [67/146], loss=11.2103
	step [68/146], loss=11.7721
	step [69/146], loss=12.5535
	step [70/146], loss=10.7734
	step [71/146], loss=11.2124
	step [72/146], loss=9.8474
	step [73/146], loss=12.0891
	step [74/146], loss=11.7364
	step [75/146], loss=12.1288
	step [76/146], loss=9.7238
	step [77/146], loss=13.0569
	step [78/146], loss=12.5875
	step [79/146], loss=11.2146
	step [80/146], loss=13.1802
	step [81/146], loss=10.3505
	step [82/146], loss=12.2797
	step [83/146], loss=10.6693
	step [84/146], loss=11.6463
	step [85/146], loss=10.0427
	step [86/146], loss=11.8030
	step [87/146], loss=10.6408
	step [88/146], loss=10.7338
	step [89/146], loss=11.6694
	step [90/146], loss=12.8267
	step [91/146], loss=9.6582
	step [92/146], loss=12.3817
	step [93/146], loss=11.7614
	step [94/146], loss=10.9577
	step [95/146], loss=10.6753
	step [96/146], loss=12.7121
	step [97/146], loss=10.8326
	step [98/146], loss=10.2711
	step [99/146], loss=10.8785
	step [100/146], loss=11.2089
	step [101/146], loss=12.5487
	step [102/146], loss=9.1858
	step [103/146], loss=10.4336
	step [104/146], loss=10.7242
	step [105/146], loss=12.6800
	step [106/146], loss=12.0198
	step [107/146], loss=10.9530
	step [108/146], loss=11.7507
	step [109/146], loss=11.0352
	step [110/146], loss=10.6252
	step [111/146], loss=11.5539
	step [112/146], loss=9.7717
	step [113/146], loss=11.3838
	step [114/146], loss=11.4170
	step [115/146], loss=11.3273
	step [116/146], loss=12.3182
	step [117/146], loss=11.7248
	step [118/146], loss=11.4147
	step [119/146], loss=12.2358
	step [120/146], loss=11.7484
	step [121/146], loss=11.7950
	step [122/146], loss=10.5612
	step [123/146], loss=11.2507
	step [124/146], loss=10.9468
	step [125/146], loss=11.2955
	step [126/146], loss=10.5621
	step [127/146], loss=11.3581
	step [128/146], loss=12.4327
	step [129/146], loss=11.7424
	step [130/146], loss=10.6825
	step [131/146], loss=12.5595
	step [132/146], loss=10.8622
	step [133/146], loss=10.4698
	step [134/146], loss=12.3531
	step [135/146], loss=11.4127
	step [136/146], loss=11.1690
	step [137/146], loss=11.2577
	step [138/146], loss=11.6768
	step [139/146], loss=10.9679
	step [140/146], loss=11.1894
	step [141/146], loss=12.6193
	step [142/146], loss=11.6801
	step [143/146], loss=10.9099
	step [144/146], loss=10.7080
	step [145/146], loss=11.3575
	step [146/146], loss=6.0840
	Evaluating
	loss=0.0355, precision=0.2169, recall=0.9962, f1=0.3562
Training epoch 25
	step [1/146], loss=10.2996
	step [2/146], loss=11.6447
	step [3/146], loss=11.1905
	step [4/146], loss=10.7874
	step [5/146], loss=10.2937
	step [6/146], loss=11.9350
	step [7/146], loss=10.3601
	step [8/146], loss=11.8792
	step [9/146], loss=9.8536
	step [10/146], loss=11.1690
	step [11/146], loss=12.4694
	step [12/146], loss=10.6953
	step [13/146], loss=11.1006
	step [14/146], loss=10.9838
	step [15/146], loss=11.3240
	step [16/146], loss=11.2745
	step [17/146], loss=10.3952
	step [18/146], loss=13.1676
	step [19/146], loss=9.8691
	step [20/146], loss=11.6886
	step [21/146], loss=11.1136
	step [22/146], loss=11.4098
	step [23/146], loss=11.5580
	step [24/146], loss=11.3269
	step [25/146], loss=12.1433
	step [26/146], loss=10.8929
	step [27/146], loss=10.8396
	step [28/146], loss=9.7799
	step [29/146], loss=11.4680
	step [30/146], loss=9.7184
	step [31/146], loss=10.4200
	step [32/146], loss=10.8238
	step [33/146], loss=11.5268
	step [34/146], loss=11.6784
	step [35/146], loss=11.2467
	step [36/146], loss=10.6971
	step [37/146], loss=10.7468
	step [38/146], loss=11.2033
	step [39/146], loss=11.2402
	step [40/146], loss=11.6084
	step [41/146], loss=9.6016
	step [42/146], loss=10.9286
	step [43/146], loss=11.7806
	step [44/146], loss=10.9147
	step [45/146], loss=10.4360
	step [46/146], loss=11.6764
	step [47/146], loss=10.3352
	step [48/146], loss=11.8373
	step [49/146], loss=10.8934
	step [50/146], loss=11.7648
	step [51/146], loss=9.6975
	step [52/146], loss=11.7479
	step [53/146], loss=10.3603
	step [54/146], loss=10.6316
	step [55/146], loss=10.9804
	step [56/146], loss=10.9194
	step [57/146], loss=10.8445
	step [58/146], loss=10.9971
	step [59/146], loss=9.8946
	step [60/146], loss=11.8099
	step [61/146], loss=10.4192
	step [62/146], loss=12.0307
	step [63/146], loss=11.0757
	step [64/146], loss=11.7598
	step [65/146], loss=12.3981
	step [66/146], loss=11.5883
	step [67/146], loss=10.8221
	step [68/146], loss=10.6343
	step [69/146], loss=10.1648
	step [70/146], loss=10.3223
	step [71/146], loss=11.4508
	step [72/146], loss=11.5889
	step [73/146], loss=11.6644
	step [74/146], loss=10.9736
	step [75/146], loss=11.2434
	step [76/146], loss=11.3381
	step [77/146], loss=11.6378
	step [78/146], loss=11.2322
	step [79/146], loss=10.2364
	step [80/146], loss=10.1010
	step [81/146], loss=10.8130
	step [82/146], loss=12.3663
	step [83/146], loss=9.9944
	step [84/146], loss=12.8462
	step [85/146], loss=10.4774
	step [86/146], loss=11.2563
	step [87/146], loss=10.7449
	step [88/146], loss=11.5268
	step [89/146], loss=10.6197
	step [90/146], loss=11.4730
	step [91/146], loss=9.5667
	step [92/146], loss=10.2534
	step [93/146], loss=11.0867
	step [94/146], loss=9.6104
	step [95/146], loss=10.7467
	step [96/146], loss=10.0792
	step [97/146], loss=11.7252
	step [98/146], loss=12.3169
	step [99/146], loss=11.0473
	step [100/146], loss=11.7481
	step [101/146], loss=10.1927
	step [102/146], loss=11.3097
	step [103/146], loss=11.8041
	step [104/146], loss=10.4166
	step [105/146], loss=11.7287
	step [106/146], loss=11.0603
	step [107/146], loss=12.9042
	step [108/146], loss=11.6928
	step [109/146], loss=10.5282
	step [110/146], loss=10.7334
	step [111/146], loss=10.9288
	step [112/146], loss=10.2728
	step [113/146], loss=9.7673
	step [114/146], loss=9.7173
	step [115/146], loss=10.8280
	step [116/146], loss=10.5929
	step [117/146], loss=12.3946
	step [118/146], loss=11.6115
	step [119/146], loss=10.7962
	step [120/146], loss=9.7019
	step [121/146], loss=11.7260
	step [122/146], loss=10.5920
	step [123/146], loss=10.2590
	step [124/146], loss=11.9769
	step [125/146], loss=10.0214
	step [126/146], loss=12.1770
	step [127/146], loss=10.2669
	step [128/146], loss=12.6907
	step [129/146], loss=10.2773
	step [130/146], loss=10.3621
	step [131/146], loss=9.6883
	step [132/146], loss=11.5861
	step [133/146], loss=10.7263
	step [134/146], loss=10.6146
	step [135/146], loss=10.6045
	step [136/146], loss=12.7974
	step [137/146], loss=11.2780
	step [138/146], loss=10.3512
	step [139/146], loss=10.5981
	step [140/146], loss=10.4405
	step [141/146], loss=10.2580
	step [142/146], loss=10.4469
	step [143/146], loss=11.3879
	step [144/146], loss=10.6743
	step [145/146], loss=14.7364
	step [146/146], loss=6.3191
	Evaluating
	loss=0.0312, precision=0.2603, recall=0.9955, f1=0.4127
saving model as: 2_saved_model.pth
Training epoch 26
	step [1/146], loss=11.4007
	step [2/146], loss=9.8921
	step [3/146], loss=10.1700
	step [4/146], loss=10.0812
	step [5/146], loss=10.4894
	step [6/146], loss=11.4432
	step [7/146], loss=9.6630
	step [8/146], loss=9.8866
	step [9/146], loss=11.7231
	step [10/146], loss=12.2354
	step [11/146], loss=11.9770
	step [12/146], loss=10.1868
	step [13/146], loss=10.6996
	step [14/146], loss=10.4790
	step [15/146], loss=11.4841
	step [16/146], loss=9.4250
	step [17/146], loss=10.3039
	step [18/146], loss=10.7899
	step [19/146], loss=10.9016
	step [20/146], loss=9.9439
	step [21/146], loss=11.9308
	step [22/146], loss=11.1281
	step [23/146], loss=12.2860
	step [24/146], loss=10.4672
	step [25/146], loss=11.0297
	step [26/146], loss=11.5021
	step [27/146], loss=10.2832
	step [28/146], loss=10.0316
	step [29/146], loss=10.7592
	step [30/146], loss=12.2377
	step [31/146], loss=9.8450
	step [32/146], loss=11.5779
	step [33/146], loss=10.3925
	step [34/146], loss=10.5547
	step [35/146], loss=9.8993
	step [36/146], loss=10.1049
	step [37/146], loss=10.7075
	step [38/146], loss=11.2248
	step [39/146], loss=10.1339
	step [40/146], loss=10.0823
	step [41/146], loss=9.1665
	step [42/146], loss=10.2798
	step [43/146], loss=10.7783
	step [44/146], loss=10.7158
	step [45/146], loss=11.9571
	step [46/146], loss=9.7629
	step [47/146], loss=10.4758
	step [48/146], loss=9.3774
	step [49/146], loss=11.2308
	step [50/146], loss=9.6559
	step [51/146], loss=9.6917
	step [52/146], loss=9.0225
	step [53/146], loss=10.5306
	step [54/146], loss=11.0583
	step [55/146], loss=10.8016
	step [56/146], loss=10.4703
	step [57/146], loss=10.2386
	step [58/146], loss=10.6044
	step [59/146], loss=11.9941
	step [60/146], loss=9.4553
	step [61/146], loss=10.8855
	step [62/146], loss=9.8531
	step [63/146], loss=10.5275
	step [64/146], loss=9.5787
	step [65/146], loss=9.8076
	step [66/146], loss=10.4289
	step [67/146], loss=12.5036
	step [68/146], loss=10.5497
	step [69/146], loss=9.8626
	step [70/146], loss=10.8718
	step [71/146], loss=10.9142
	step [72/146], loss=10.2420
	step [73/146], loss=9.9723
	step [74/146], loss=11.5089
	step [75/146], loss=9.6445
	step [76/146], loss=10.3623
	step [77/146], loss=11.6932
	step [78/146], loss=10.2242
	step [79/146], loss=11.0131
	step [80/146], loss=10.0922
	step [81/146], loss=10.4539
	step [82/146], loss=11.3859
	step [83/146], loss=10.5981
	step [84/146], loss=10.1651
	step [85/146], loss=11.3002
	step [86/146], loss=10.3793
	step [87/146], loss=10.6204
	step [88/146], loss=10.2459
	step [89/146], loss=12.2281
	step [90/146], loss=10.5423
	step [91/146], loss=12.5879
	step [92/146], loss=10.1827
	step [93/146], loss=10.3561
	step [94/146], loss=9.9942
	step [95/146], loss=9.6899
	step [96/146], loss=10.3950
	step [97/146], loss=10.1039
	step [98/146], loss=10.3496
	step [99/146], loss=10.0237
	step [100/146], loss=11.1619
	step [101/146], loss=9.7815
	step [102/146], loss=8.9968
	step [103/146], loss=10.8343
	step [104/146], loss=10.3254
	step [105/146], loss=10.7544
	step [106/146], loss=10.6950
	step [107/146], loss=12.5544
	step [108/146], loss=11.5056
	step [109/146], loss=10.3263
	step [110/146], loss=10.6687
	step [111/146], loss=9.6055
	step [112/146], loss=9.7399
	step [113/146], loss=10.5856
	step [114/146], loss=9.5066
	step [115/146], loss=11.0525
	step [116/146], loss=8.4250
	step [117/146], loss=10.7664
	step [118/146], loss=10.3693
	step [119/146], loss=11.2876
	step [120/146], loss=12.3339
	step [121/146], loss=12.0522
	step [122/146], loss=10.0803
	step [123/146], loss=12.0052
	step [124/146], loss=10.1035
	step [125/146], loss=9.0943
	step [126/146], loss=11.3113
	step [127/146], loss=11.0992
	step [128/146], loss=9.7740
	step [129/146], loss=10.9057
	step [130/146], loss=12.1974
	step [131/146], loss=10.2047
	step [132/146], loss=10.3387
	step [133/146], loss=10.5291
	step [134/146], loss=10.5254
	step [135/146], loss=10.8498
	step [136/146], loss=10.9524
	step [137/146], loss=11.2071
	step [138/146], loss=10.6625
	step [139/146], loss=10.9458
	step [140/146], loss=9.7722
	step [141/146], loss=9.4611
	step [142/146], loss=11.3153
	step [143/146], loss=10.5411
	step [144/146], loss=9.8521
	step [145/146], loss=10.7800
	step [146/146], loss=5.5959
	Evaluating
	loss=0.0325, precision=0.2379, recall=0.9960, f1=0.3841
Training epoch 27
	step [1/146], loss=12.0207
	step [2/146], loss=9.9036
	step [3/146], loss=10.2149
	step [4/146], loss=9.3340
	step [5/146], loss=9.5192
	step [6/146], loss=10.2938
	step [7/146], loss=9.5931
	step [8/146], loss=11.2440
	step [9/146], loss=10.4365
	step [10/146], loss=10.9159
	step [11/146], loss=10.0994
	step [12/146], loss=10.2306
	step [13/146], loss=10.4283
	step [14/146], loss=10.4856
	step [15/146], loss=10.8246
	step [16/146], loss=10.0973
	step [17/146], loss=10.9687
	step [18/146], loss=11.6925
	step [19/146], loss=10.7938
	step [20/146], loss=8.8622
	step [21/146], loss=9.7350
	step [22/146], loss=10.2013
	step [23/146], loss=10.3366
	step [24/146], loss=10.2189
	step [25/146], loss=12.4713
	step [26/146], loss=11.2685
	step [27/146], loss=9.3408
	step [28/146], loss=9.5130
	step [29/146], loss=10.8213
	step [30/146], loss=10.2092
	step [31/146], loss=11.3014
	step [32/146], loss=9.1111
	step [33/146], loss=9.4036
	step [34/146], loss=9.0438
	step [35/146], loss=9.3764
	step [36/146], loss=10.6892
	step [37/146], loss=11.8245
	step [38/146], loss=10.7286
	step [39/146], loss=9.6784
	step [40/146], loss=11.3297
	step [41/146], loss=8.9874
	step [42/146], loss=10.5874
	step [43/146], loss=11.5004
	step [44/146], loss=11.0635
	step [45/146], loss=9.0240
	step [46/146], loss=11.1876
	step [47/146], loss=11.3299
	step [48/146], loss=11.6316
	step [49/146], loss=9.7545
	step [50/146], loss=9.3907
	step [51/146], loss=13.8492
	step [52/146], loss=10.1343
	step [53/146], loss=10.0375
	step [54/146], loss=11.3423
	step [55/146], loss=10.6269
	step [56/146], loss=10.5670
	step [57/146], loss=11.1339
	step [58/146], loss=10.1341
	step [59/146], loss=8.2649
	step [60/146], loss=9.6544
	step [61/146], loss=10.0238
	step [62/146], loss=11.4265
	step [63/146], loss=9.9568
	step [64/146], loss=9.9729
	step [65/146], loss=9.8265
	step [66/146], loss=9.0333
	step [67/146], loss=10.2152
	step [68/146], loss=10.1263
	step [69/146], loss=11.0317
	step [70/146], loss=10.5257
	step [71/146], loss=10.0209
	step [72/146], loss=8.6020
	step [73/146], loss=11.6586
	step [74/146], loss=10.6840
	step [75/146], loss=10.8510
	step [76/146], loss=8.8496
	step [77/146], loss=10.1484
	step [78/146], loss=10.9829
	step [79/146], loss=10.4722
	step [80/146], loss=10.4449
	step [81/146], loss=10.5269
	step [82/146], loss=10.1709
	step [83/146], loss=11.3275
	step [84/146], loss=8.9176
	step [85/146], loss=8.8241
	step [86/146], loss=10.5460
	step [87/146], loss=10.1588
	step [88/146], loss=10.8217
	step [89/146], loss=8.8945
	step [90/146], loss=9.8094
	step [91/146], loss=9.6536
	step [92/146], loss=8.9938
	step [93/146], loss=10.6261
	step [94/146], loss=10.0684
	step [95/146], loss=10.4515
	step [96/146], loss=9.4841
	step [97/146], loss=9.7397
	step [98/146], loss=9.2712
	step [99/146], loss=10.0581
	step [100/146], loss=10.7807
	step [101/146], loss=9.7217
	step [102/146], loss=10.6101
	step [103/146], loss=11.2846
	step [104/146], loss=9.4092
	step [105/146], loss=10.6427
	step [106/146], loss=8.5709
	step [107/146], loss=10.4078
	step [108/146], loss=9.6007
	step [109/146], loss=10.3972
	step [110/146], loss=10.6809
	step [111/146], loss=10.3909
	step [112/146], loss=9.2651
	step [113/146], loss=9.2182
	step [114/146], loss=10.7002
	step [115/146], loss=9.6429
	step [116/146], loss=10.2779
	step [117/146], loss=8.5555
	step [118/146], loss=10.8548
	step [119/146], loss=10.3535
	step [120/146], loss=9.9823
	step [121/146], loss=10.7117
	step [122/146], loss=10.2562
	step [123/146], loss=9.7564
	step [124/146], loss=9.8268
	step [125/146], loss=9.9779
	step [126/146], loss=8.7537
	step [127/146], loss=10.1262
	step [128/146], loss=10.6067
	step [129/146], loss=11.3307
	step [130/146], loss=10.5809
	step [131/146], loss=11.0140
	step [132/146], loss=9.3012
	step [133/146], loss=9.5977
	step [134/146], loss=11.0105
	step [135/146], loss=9.5354
	step [136/146], loss=10.8177
	step [137/146], loss=9.4642
	step [138/146], loss=11.0311
	step [139/146], loss=10.4104
	step [140/146], loss=9.5752
	step [141/146], loss=9.8944
	step [142/146], loss=10.2933
	step [143/146], loss=9.6435
	step [144/146], loss=10.2617
	step [145/146], loss=8.3031
	step [146/146], loss=5.1979
	Evaluating
	loss=0.0276, precision=0.2737, recall=0.9949, f1=0.4292
saving model as: 2_saved_model.pth
Training epoch 28
	step [1/146], loss=9.4007
	step [2/146], loss=10.4510
	step [3/146], loss=8.6776
	step [4/146], loss=10.6280
	step [5/146], loss=10.1984
	step [6/146], loss=9.3975
	step [7/146], loss=11.1406
	step [8/146], loss=9.9758
	step [9/146], loss=8.6033
	step [10/146], loss=9.4806
	step [11/146], loss=11.0122
	step [12/146], loss=12.7435
	step [13/146], loss=9.8639
	step [14/146], loss=8.6414
	step [15/146], loss=8.2958
	step [16/146], loss=9.5594
	step [17/146], loss=8.6823
	step [18/146], loss=9.2014
	step [19/146], loss=10.5127
	step [20/146], loss=9.7228
	step [21/146], loss=9.6505
	step [22/146], loss=10.9349
	step [23/146], loss=9.5265
	step [24/146], loss=9.5243
	step [25/146], loss=10.7051
	step [26/146], loss=9.6079
	step [27/146], loss=10.0562
	step [28/146], loss=9.3969
	step [29/146], loss=10.1617
	step [30/146], loss=9.7065
	step [31/146], loss=10.3174
	step [32/146], loss=9.5824
	step [33/146], loss=11.0970
	step [34/146], loss=10.2996
	step [35/146], loss=10.5401
	step [36/146], loss=11.0291
	step [37/146], loss=11.0449
	step [38/146], loss=10.1497
	step [39/146], loss=9.1519
	step [40/146], loss=9.5849
	step [41/146], loss=9.3185
	step [42/146], loss=9.6951
	step [43/146], loss=9.6768
	step [44/146], loss=8.9929
	step [45/146], loss=10.4852
	step [46/146], loss=10.1731
	step [47/146], loss=10.4235
	step [48/146], loss=9.4336
	step [49/146], loss=12.9061
	step [50/146], loss=9.4277
	step [51/146], loss=10.4617
	step [52/146], loss=11.2473
	step [53/146], loss=9.9542
	step [54/146], loss=10.1214
	step [55/146], loss=8.6533
	step [56/146], loss=8.4419
	step [57/146], loss=9.9651
	step [58/146], loss=9.1817
	step [59/146], loss=9.9373
	step [60/146], loss=10.3104
	step [61/146], loss=10.7169
	step [62/146], loss=8.6699
	step [63/146], loss=8.5649
	step [64/146], loss=11.4822
	step [65/146], loss=9.8891
	step [66/146], loss=9.2372
	step [67/146], loss=8.8035
	step [68/146], loss=10.2144
	step [69/146], loss=8.7554
	step [70/146], loss=9.7427
	step [71/146], loss=10.2083
	step [72/146], loss=9.5566
	step [73/146], loss=10.4354
	step [74/146], loss=9.7114
	step [75/146], loss=10.1946
	step [76/146], loss=10.5161
	step [77/146], loss=9.1353
	step [78/146], loss=10.9139
	step [79/146], loss=9.1531
	step [80/146], loss=9.4437
	step [81/146], loss=11.5723
	step [82/146], loss=10.5451
	step [83/146], loss=8.9850
	step [84/146], loss=10.9856
	step [85/146], loss=10.3942
	step [86/146], loss=8.8209
	step [87/146], loss=9.2267
	step [88/146], loss=8.8856
	step [89/146], loss=10.1901
	step [90/146], loss=10.8426
	step [91/146], loss=9.5183
	step [92/146], loss=8.8161
	step [93/146], loss=9.0149
	step [94/146], loss=10.1130
	step [95/146], loss=9.8673
	step [96/146], loss=8.4326
	step [97/146], loss=9.0151
	step [98/146], loss=10.3143
	step [99/146], loss=9.8433
	step [100/146], loss=10.5282
	step [101/146], loss=9.8255
	step [102/146], loss=8.8706
	step [103/146], loss=9.9428
	step [104/146], loss=9.5802
	step [105/146], loss=9.6886
	step [106/146], loss=10.1491
	step [107/146], loss=10.6012
	step [108/146], loss=10.1081
	step [109/146], loss=9.9966
	step [110/146], loss=10.1228
	step [111/146], loss=10.6853
	step [112/146], loss=9.6525
	step [113/146], loss=8.8192
	step [114/146], loss=9.5364
	step [115/146], loss=8.8938
	step [116/146], loss=8.9952
	step [117/146], loss=9.6553
	step [118/146], loss=9.4422
	step [119/146], loss=9.7975
	step [120/146], loss=11.1043
	step [121/146], loss=10.2493
	step [122/146], loss=9.7309
	step [123/146], loss=9.7942
	step [124/146], loss=10.4212
	step [125/146], loss=10.5327
	step [126/146], loss=9.2745
	step [127/146], loss=9.3099
	step [128/146], loss=9.5481
	step [129/146], loss=9.3984
	step [130/146], loss=9.8753
	step [131/146], loss=8.3756
	step [132/146], loss=9.6370
	step [133/146], loss=10.4966
	step [134/146], loss=9.9952
	step [135/146], loss=9.7087
	step [136/146], loss=10.1856
	step [137/146], loss=10.8584
	step [138/146], loss=10.3385
	step [139/146], loss=10.6893
	step [140/146], loss=10.0382
	step [141/146], loss=9.6232
	step [142/146], loss=10.8764
	step [143/146], loss=9.3071
	step [144/146], loss=8.5728
	step [145/146], loss=8.3349
	step [146/146], loss=5.2715
	Evaluating
	loss=0.0261, precision=0.2828, recall=0.9945, f1=0.4403
saving model as: 2_saved_model.pth
Training epoch 29
	step [1/146], loss=9.4735
	step [2/146], loss=9.9950
	step [3/146], loss=9.7676
	step [4/146], loss=11.4197
	step [5/146], loss=9.2677
	step [6/146], loss=9.7926
	step [7/146], loss=8.7005
	step [8/146], loss=10.6933
	step [9/146], loss=9.3591
	step [10/146], loss=11.7348
	step [11/146], loss=9.3517
	step [12/146], loss=9.0453
	step [13/146], loss=9.6050
	step [14/146], loss=9.3851
	step [15/146], loss=8.8024
	step [16/146], loss=9.2323
	step [17/146], loss=8.8917
	step [18/146], loss=9.2491
	step [19/146], loss=8.7007
	step [20/146], loss=12.2011
	step [21/146], loss=9.6868
	step [22/146], loss=8.5031
	step [23/146], loss=8.1337
	step [24/146], loss=10.2589
	step [25/146], loss=10.3795
	step [26/146], loss=9.6413
	step [27/146], loss=8.1482
	step [28/146], loss=9.3520
	step [29/146], loss=11.2949
	step [30/146], loss=8.9515
	step [31/146], loss=8.1491
	step [32/146], loss=9.2920
	step [33/146], loss=9.5156
	step [34/146], loss=9.5299
	step [35/146], loss=9.0056
	step [36/146], loss=10.4881
	step [37/146], loss=9.4999
	step [38/146], loss=8.4450
	step [39/146], loss=9.2754
	step [40/146], loss=9.5161
	step [41/146], loss=10.2078
	step [42/146], loss=9.0147
	step [43/146], loss=8.7299
	step [44/146], loss=9.1897
	step [45/146], loss=8.9935
	step [46/146], loss=9.2545
	step [47/146], loss=9.1525
	step [48/146], loss=7.8945
	step [49/146], loss=9.5119
	step [50/146], loss=9.3841
	step [51/146], loss=9.5922
	step [52/146], loss=7.3065
	step [53/146], loss=11.2251
	step [54/146], loss=10.1837
	step [55/146], loss=10.0527
	step [56/146], loss=9.6967
	step [57/146], loss=9.3516
	step [58/146], loss=8.8885
	step [59/146], loss=11.3685
	step [60/146], loss=9.9439
	step [61/146], loss=10.2338
	step [62/146], loss=8.5468
	step [63/146], loss=10.1608
	step [64/146], loss=8.7662
	step [65/146], loss=9.3328
	step [66/146], loss=8.9985
	step [67/146], loss=9.5445
	step [68/146], loss=9.7620
	step [69/146], loss=10.0761
	step [70/146], loss=9.7781
	step [71/146], loss=9.1810
	step [72/146], loss=9.5043
	step [73/146], loss=8.6754
	step [74/146], loss=10.0520
	step [75/146], loss=10.9372
	step [76/146], loss=9.4993
	step [77/146], loss=8.2929
	step [78/146], loss=10.7589
	step [79/146], loss=9.4918
	step [80/146], loss=9.9506
	step [81/146], loss=10.2679
	step [82/146], loss=9.1132
	step [83/146], loss=9.5560
	step [84/146], loss=9.9342
	step [85/146], loss=9.1721
	step [86/146], loss=9.6234
	step [87/146], loss=8.1890
	step [88/146], loss=10.4210
	step [89/146], loss=8.5970
	step [90/146], loss=9.0033
	step [91/146], loss=8.7147
	step [92/146], loss=9.6717
	step [93/146], loss=7.8075
	step [94/146], loss=10.0192
	step [95/146], loss=9.6426
	step [96/146], loss=9.7651
	step [97/146], loss=10.1505
	step [98/146], loss=8.8577
	step [99/146], loss=8.8397
	step [100/146], loss=10.3392
	step [101/146], loss=10.3252
	step [102/146], loss=9.9733
	step [103/146], loss=9.2313
	step [104/146], loss=8.6600
	step [105/146], loss=9.0879
	step [106/146], loss=9.0766
	step [107/146], loss=10.0706
	step [108/146], loss=9.9811
	step [109/146], loss=10.3865
	step [110/146], loss=9.3847
	step [111/146], loss=10.3767
	step [112/146], loss=11.0936
	step [113/146], loss=8.6679
	step [114/146], loss=9.2931
	step [115/146], loss=9.2911
	step [116/146], loss=9.2466
	step [117/146], loss=8.6772
	step [118/146], loss=9.3254
	step [119/146], loss=10.0235
	step [120/146], loss=8.9040
	step [121/146], loss=8.8738
	step [122/146], loss=10.4121
	step [123/146], loss=9.6943
	step [124/146], loss=9.3608
	step [125/146], loss=8.3210
	step [126/146], loss=9.5930
	step [127/146], loss=9.6426
	step [128/146], loss=9.2841
	step [129/146], loss=9.4947
	step [130/146], loss=8.6339
	step [131/146], loss=10.2604
	step [132/146], loss=9.6803
	step [133/146], loss=9.3859
	step [134/146], loss=8.9158
	step [135/146], loss=10.0394
	step [136/146], loss=10.2687
	step [137/146], loss=10.2420
	step [138/146], loss=8.8067
	step [139/146], loss=8.7667
	step [140/146], loss=9.5710
	step [141/146], loss=9.5585
	step [142/146], loss=8.1205
	step [143/146], loss=8.7409
	step [144/146], loss=10.4961
	step [145/146], loss=10.2458
	step [146/146], loss=5.4040
	Evaluating
	loss=0.0317, precision=0.2457, recall=0.9957, f1=0.3942
Training epoch 30
	step [1/146], loss=10.9059
	step [2/146], loss=8.5509
	step [3/146], loss=9.1613
	step [4/146], loss=8.7557
	step [5/146], loss=9.1571
	step [6/146], loss=8.9753
	step [7/146], loss=9.2027
	step [8/146], loss=7.5149
	step [9/146], loss=8.6855
	step [10/146], loss=7.8884
	step [11/146], loss=8.7063
	step [12/146], loss=9.0285
	step [13/146], loss=10.1752
	step [14/146], loss=10.1604
	step [15/146], loss=10.6600
	step [16/146], loss=9.5178
	step [17/146], loss=10.5144
	step [18/146], loss=9.1449
	step [19/146], loss=9.2930
	step [20/146], loss=8.8173
	step [21/146], loss=9.7091
	step [22/146], loss=10.1072
	step [23/146], loss=9.6996
	step [24/146], loss=8.6561
	step [25/146], loss=7.5738
	step [26/146], loss=9.5171
	step [27/146], loss=9.3766
	step [28/146], loss=12.1625
	step [29/146], loss=9.3175
	step [30/146], loss=9.3622
	step [31/146], loss=9.4864
	step [32/146], loss=10.5547
	step [33/146], loss=9.4525
	step [34/146], loss=8.5747
	step [35/146], loss=8.5172
	step [36/146], loss=9.3242
	step [37/146], loss=9.6663
	step [38/146], loss=8.7208
	step [39/146], loss=10.3058
	step [40/146], loss=10.4447
	step [41/146], loss=9.0690
	step [42/146], loss=10.3999
	step [43/146], loss=7.6956
	step [44/146], loss=9.0719
	step [45/146], loss=8.5248
	step [46/146], loss=9.4085
	step [47/146], loss=9.6330
	step [48/146], loss=9.1385
	step [49/146], loss=9.7320
	step [50/146], loss=9.3016
	step [51/146], loss=10.7418
	step [52/146], loss=10.3841
	step [53/146], loss=8.1615
	step [54/146], loss=9.3575
	step [55/146], loss=8.5894
	step [56/146], loss=8.3816
	step [57/146], loss=9.4612
	step [58/146], loss=9.2800
	step [59/146], loss=9.2080
	step [60/146], loss=9.8363
	step [61/146], loss=9.0756
	step [62/146], loss=8.5571
	step [63/146], loss=9.2068
	step [64/146], loss=8.3380
	step [65/146], loss=8.3470
	step [66/146], loss=10.2352
	step [67/146], loss=8.6459
	step [68/146], loss=9.4131
	step [69/146], loss=9.3414
	step [70/146], loss=10.2944
	step [71/146], loss=11.0019
	step [72/146], loss=8.7701
	step [73/146], loss=9.5131
	step [74/146], loss=9.0254
	step [75/146], loss=9.9442
	step [76/146], loss=8.9664
	step [77/146], loss=9.0362
	step [78/146], loss=10.8124
	step [79/146], loss=9.6520
	step [80/146], loss=8.2687
	step [81/146], loss=8.1686
	step [82/146], loss=9.8249
	step [83/146], loss=8.7169
	step [84/146], loss=10.7936
	step [85/146], loss=9.0449
	step [86/146], loss=9.3305
	step [87/146], loss=8.8012
	step [88/146], loss=9.1735
	step [89/146], loss=10.2252
	step [90/146], loss=9.8602
	step [91/146], loss=9.4150
	step [92/146], loss=11.2678
	step [93/146], loss=8.8137
	step [94/146], loss=9.2048
	step [95/146], loss=10.4392
	step [96/146], loss=9.7626
	step [97/146], loss=9.7223
	step [98/146], loss=9.9449
	step [99/146], loss=9.2307
	step [100/146], loss=8.8418
	step [101/146], loss=9.5797
	step [102/146], loss=9.2081
	step [103/146], loss=9.5511
	step [104/146], loss=7.1569
	step [105/146], loss=8.5872
	step [106/146], loss=8.1734
	step [107/146], loss=10.6163
	step [108/146], loss=8.7432
	step [109/146], loss=11.4908
	step [110/146], loss=9.0119
	step [111/146], loss=10.2678
	step [112/146], loss=10.0590
	step [113/146], loss=9.2727
	step [114/146], loss=9.4586
	step [115/146], loss=9.3960
	step [116/146], loss=9.3931
	step [117/146], loss=9.0409
	step [118/146], loss=9.6355
	step [119/146], loss=8.9518
	step [120/146], loss=8.7856
	step [121/146], loss=9.6752
	step [122/146], loss=8.5727
	step [123/146], loss=8.4248
	step [124/146], loss=10.3658
	step [125/146], loss=8.7915
	step [126/146], loss=9.5528
	step [127/146], loss=9.9976
	step [128/146], loss=10.2487
	step [129/146], loss=8.5081
	step [130/146], loss=8.4751
	step [131/146], loss=10.3736
	step [132/146], loss=9.6063
	step [133/146], loss=9.0488
	step [134/146], loss=9.3947
	step [135/146], loss=9.7025
	step [136/146], loss=9.2902
	step [137/146], loss=9.1693
	step [138/146], loss=8.3297
	step [139/146], loss=8.2683
	step [140/146], loss=7.8649
	step [141/146], loss=9.5633
	step [142/146], loss=8.1540
	step [143/146], loss=9.6616
	step [144/146], loss=9.0856
	step [145/146], loss=9.5893
	step [146/146], loss=5.3773
	Evaluating
	loss=0.0275, precision=0.2755, recall=0.9944, f1=0.4315
Training finished
best_f1: 0.4403332605242253
directing: Y rim_enhanced: False test_id 2
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15599 # image files with weight 15554
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4484 # image files with weight 4476
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15554
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/244], loss=314.4925
	step [2/244], loss=269.0817
	step [3/244], loss=253.4018
	step [4/244], loss=246.0878
	step [5/244], loss=242.2525
	step [6/244], loss=238.6290
	step [7/244], loss=236.2031
	step [8/244], loss=233.8881
	step [9/244], loss=229.0988
	step [10/244], loss=224.7231
	step [11/244], loss=225.5535
	step [12/244], loss=220.5033
	step [13/244], loss=219.2715
	step [14/244], loss=213.8280
	step [15/244], loss=209.3112
	step [16/244], loss=208.9833
	step [17/244], loss=202.7030
	step [18/244], loss=201.8653
	step [19/244], loss=196.5997
	step [20/244], loss=194.6958
	step [21/244], loss=192.6674
	step [22/244], loss=189.5569
	step [23/244], loss=187.0799
	step [24/244], loss=183.9585
	step [25/244], loss=184.2331
	step [26/244], loss=181.0199
	step [27/244], loss=181.3814
	step [28/244], loss=178.4308
	step [29/244], loss=174.6818
	step [30/244], loss=173.7658
	step [31/244], loss=170.3037
	step [32/244], loss=170.8274
	step [33/244], loss=171.3124
	step [34/244], loss=168.0517
	step [35/244], loss=164.2039
	step [36/244], loss=163.6135
	step [37/244], loss=163.8278
	step [38/244], loss=160.7252
	step [39/244], loss=159.1791
	step [40/244], loss=160.9249
	step [41/244], loss=157.5180
	step [42/244], loss=154.2404
	step [43/244], loss=155.7428
	step [44/244], loss=153.2668
	step [45/244], loss=153.3411
	step [46/244], loss=153.4466
	step [47/244], loss=150.6997
	step [48/244], loss=149.7334
	step [49/244], loss=147.7772
	step [50/244], loss=149.5012
	step [51/244], loss=146.6094
	step [52/244], loss=144.1197
	step [53/244], loss=147.6693
	step [54/244], loss=146.5446
	step [55/244], loss=140.4729
	step [56/244], loss=141.6969
	step [57/244], loss=142.2044
	step [58/244], loss=138.4871
	step [59/244], loss=140.0887
	step [60/244], loss=141.7361
	step [61/244], loss=138.5513
	step [62/244], loss=140.2453
	step [63/244], loss=135.4568
	step [64/244], loss=133.3052
	step [65/244], loss=136.8433
	step [66/244], loss=134.9090
	step [67/244], loss=134.9023
	step [68/244], loss=133.8475
	step [69/244], loss=134.4963
	step [70/244], loss=132.9721
	step [71/244], loss=130.7148
	step [72/244], loss=133.7308
	step [73/244], loss=132.1879
	step [74/244], loss=131.4193
	step [75/244], loss=131.5471
	step [76/244], loss=129.8270
	step [77/244], loss=129.8641
	step [78/244], loss=127.7998
	step [79/244], loss=126.5653
	step [80/244], loss=126.6416
	step [81/244], loss=128.5628
	step [82/244], loss=125.7379
	step [83/244], loss=126.3532
	step [84/244], loss=124.6489
	step [85/244], loss=126.6037
	step [86/244], loss=125.2438
	step [87/244], loss=127.5289
	step [88/244], loss=124.7720
	step [89/244], loss=127.2627
	step [90/244], loss=122.2656
	step [91/244], loss=125.0660
	step [92/244], loss=123.0233
	step [93/244], loss=122.7122
	step [94/244], loss=125.3396
	step [95/244], loss=121.0803
	step [96/244], loss=121.7942
	step [97/244], loss=121.3974
	step [98/244], loss=121.1922
	step [99/244], loss=121.2976
	step [100/244], loss=121.0246
	step [101/244], loss=122.8838
	step [102/244], loss=119.3627
	step [103/244], loss=119.9929
	step [104/244], loss=119.4585
	step [105/244], loss=120.2279
	step [106/244], loss=121.1825
	step [107/244], loss=119.0521
	step [108/244], loss=116.8338
	step [109/244], loss=119.7376
	step [110/244], loss=119.2707
	step [111/244], loss=119.0763
	step [112/244], loss=119.1785
	step [113/244], loss=118.7880
	step [114/244], loss=118.0543
	step [115/244], loss=118.3256
	step [116/244], loss=118.4482
	step [117/244], loss=115.2366
	step [118/244], loss=115.8615
	step [119/244], loss=119.2461
	step [120/244], loss=116.1875
	step [121/244], loss=115.6986
	step [122/244], loss=117.0403
	step [123/244], loss=115.7434
	step [124/244], loss=115.8715
	step [125/244], loss=115.2325
	step [126/244], loss=113.9912
	step [127/244], loss=116.5868
	step [128/244], loss=115.1089
	step [129/244], loss=113.7480
	step [130/244], loss=113.6519
	step [131/244], loss=116.1323
	step [132/244], loss=114.3831
	step [133/244], loss=114.1020
	step [134/244], loss=114.1643
	step [135/244], loss=114.2655
	step [136/244], loss=111.7554
	step [137/244], loss=113.4543
	step [138/244], loss=113.8701
	step [139/244], loss=112.5865
	step [140/244], loss=112.6621
	step [141/244], loss=111.2725
	step [142/244], loss=114.7354
	step [143/244], loss=112.6297
	step [144/244], loss=114.0959
	step [145/244], loss=111.3858
	step [146/244], loss=110.4867
	step [147/244], loss=112.7245
	step [148/244], loss=110.7810
	step [149/244], loss=110.6037
	step [150/244], loss=109.2657
	step [151/244], loss=112.6029
	step [152/244], loss=112.0514
	step [153/244], loss=110.6970
	step [154/244], loss=111.3564
	step [155/244], loss=108.4919
	step [156/244], loss=107.8439
	step [157/244], loss=111.1355
	step [158/244], loss=109.2123
	step [159/244], loss=108.4589
	step [160/244], loss=110.4162
	step [161/244], loss=106.7462
	step [162/244], loss=108.2681
	step [163/244], loss=109.4964
	step [164/244], loss=108.2562
	step [165/244], loss=107.3715
	step [166/244], loss=108.1916
	step [167/244], loss=108.9716
	step [168/244], loss=108.4875
	step [169/244], loss=108.1498
	step [170/244], loss=108.3141
	step [171/244], loss=107.5437
	step [172/244], loss=107.3333
	step [173/244], loss=106.1112
	step [174/244], loss=105.4488
	step [175/244], loss=105.2686
	step [176/244], loss=107.4665
	step [177/244], loss=105.5383
	step [178/244], loss=105.4073
	step [179/244], loss=106.1988
	step [180/244], loss=105.4311
	step [181/244], loss=108.0763
	step [182/244], loss=107.7246
	step [183/244], loss=103.9285
	step [184/244], loss=104.0189
	step [185/244], loss=105.2377
	step [186/244], loss=108.1710
	step [187/244], loss=105.5514
	step [188/244], loss=106.3741
	step [189/244], loss=104.3399
	step [190/244], loss=104.6165
	step [191/244], loss=103.9282
	step [192/244], loss=105.5136
	step [193/244], loss=103.2194
	step [194/244], loss=106.2240
	step [195/244], loss=103.8090
	step [196/244], loss=102.5885
	step [197/244], loss=105.6837
	step [198/244], loss=102.5205
	step [199/244], loss=103.1034
	step [200/244], loss=105.0858
	step [201/244], loss=102.7234
	step [202/244], loss=103.3193
	step [203/244], loss=103.7986
	step [204/244], loss=103.1264
	step [205/244], loss=103.4330
	step [206/244], loss=101.5031
	step [207/244], loss=101.7159
	step [208/244], loss=102.0936
	step [209/244], loss=101.9451
	step [210/244], loss=102.4363
	step [211/244], loss=100.5840
	step [212/244], loss=100.7461
	step [213/244], loss=100.8717
	step [214/244], loss=99.9994
	step [215/244], loss=101.2552
	step [216/244], loss=100.4294
	step [217/244], loss=99.9269
	step [218/244], loss=99.3994
	step [219/244], loss=100.3692
	step [220/244], loss=101.1677
	step [221/244], loss=99.2629
	step [222/244], loss=100.3822
	step [223/244], loss=100.7118
	step [224/244], loss=101.2797
	step [225/244], loss=98.6768
	step [226/244], loss=97.4841
	step [227/244], loss=99.8328
	step [228/244], loss=99.3654
	step [229/244], loss=99.0461
	step [230/244], loss=97.8501
	step [231/244], loss=97.9245
	step [232/244], loss=97.8383
	step [233/244], loss=97.3343
	step [234/244], loss=98.7326
	step [235/244], loss=99.0261
	step [236/244], loss=98.4242
	step [237/244], loss=98.5863
	step [238/244], loss=98.4699
	step [239/244], loss=97.5144
	step [240/244], loss=97.2199
	step [241/244], loss=97.2180
	step [242/244], loss=97.1961
	step [243/244], loss=98.2893
	step [244/244], loss=3.3410
	Evaluating
	loss=0.3986, precision=0.1122, recall=0.9982, f1=0.2017
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/244], loss=98.0052
	step [2/244], loss=97.4240
	step [3/244], loss=97.5011
	step [4/244], loss=95.3726
	step [5/244], loss=95.9503
	step [6/244], loss=97.3545
	step [7/244], loss=95.9727
	step [8/244], loss=96.4001
	step [9/244], loss=98.1073
	step [10/244], loss=95.8512
	step [11/244], loss=95.7929
	step [12/244], loss=96.2124
	step [13/244], loss=95.9897
	step [14/244], loss=94.9650
	step [15/244], loss=95.5481
	step [16/244], loss=93.5257
	step [17/244], loss=95.7382
	step [18/244], loss=95.4236
	step [19/244], loss=93.5593
	step [20/244], loss=93.2513
	step [21/244], loss=94.7433
	step [22/244], loss=91.0152
	step [23/244], loss=93.3521
	step [24/244], loss=92.4110
	step [25/244], loss=93.0976
	step [26/244], loss=92.1697
	step [27/244], loss=93.0318
	step [28/244], loss=93.6191
	step [29/244], loss=94.7139
	step [30/244], loss=94.1547
	step [31/244], loss=92.8492
	step [32/244], loss=93.4972
	step [33/244], loss=92.7486
	step [34/244], loss=93.1677
	step [35/244], loss=93.6451
	step [36/244], loss=93.5668
	step [37/244], loss=91.9362
	step [38/244], loss=92.7583
	step [39/244], loss=93.1523
	step [40/244], loss=90.9129
	step [41/244], loss=93.5552
	step [42/244], loss=91.6055
	step [43/244], loss=90.0949
	step [44/244], loss=90.9179
	step [45/244], loss=91.3690
	step [46/244], loss=93.8257
	step [47/244], loss=90.8873
	step [48/244], loss=90.7488
	step [49/244], loss=91.8439
	step [50/244], loss=91.9173
	step [51/244], loss=89.5529
	step [52/244], loss=89.7149
	step [53/244], loss=90.3670
	step [54/244], loss=88.4183
	step [55/244], loss=88.6303
	step [56/244], loss=92.9724
	step [57/244], loss=89.5365
	step [58/244], loss=89.7525
	step [59/244], loss=88.4941
	step [60/244], loss=88.9999
	step [61/244], loss=88.0094
	step [62/244], loss=89.4629
	step [63/244], loss=88.5064
	step [64/244], loss=88.8679
	step [65/244], loss=88.2226
	step [66/244], loss=88.2818
	step [67/244], loss=89.8382
	step [68/244], loss=89.6461
	step [69/244], loss=86.6026
	step [70/244], loss=87.6382
	step [71/244], loss=86.3534
	step [72/244], loss=86.4527
	step [73/244], loss=86.1823
	step [74/244], loss=86.0837
	step [75/244], loss=85.1683
	step [76/244], loss=86.0017
	step [77/244], loss=86.7213
	step [78/244], loss=87.7442
	step [79/244], loss=85.1221
	step [80/244], loss=87.6186
	step [81/244], loss=86.7096
	step [82/244], loss=85.7992
	step [83/244], loss=86.0947
	step [84/244], loss=84.5285
	step [85/244], loss=85.9416
	step [86/244], loss=86.8432
	step [87/244], loss=85.0456
	step [88/244], loss=84.9058
	step [89/244], loss=83.4886
	step [90/244], loss=84.7568
	step [91/244], loss=83.1781
	step [92/244], loss=84.7383
	step [93/244], loss=82.8401
	step [94/244], loss=83.4307
	step [95/244], loss=84.0325
	step [96/244], loss=82.3495
	step [97/244], loss=84.4822
	step [98/244], loss=80.4207
	step [99/244], loss=83.5288
	step [100/244], loss=85.6994
	step [101/244], loss=81.2929
	step [102/244], loss=82.8129
	step [103/244], loss=81.7767
	step [104/244], loss=82.2477
	step [105/244], loss=83.2000
	step [106/244], loss=83.4180
	step [107/244], loss=80.2476
	step [108/244], loss=81.8599
	step [109/244], loss=79.6820
	step [110/244], loss=82.7679
	step [111/244], loss=78.9991
	step [112/244], loss=80.5606
	step [113/244], loss=83.2006
	step [114/244], loss=82.2200
	step [115/244], loss=81.3728
	step [116/244], loss=81.0147
	step [117/244], loss=80.4544
	step [118/244], loss=80.4642
	step [119/244], loss=79.8787
	step [120/244], loss=79.1358
	step [121/244], loss=80.6577
	step [122/244], loss=81.0586
	step [123/244], loss=79.5024
	step [124/244], loss=80.6289
	step [125/244], loss=79.1846
	step [126/244], loss=80.8259
	step [127/244], loss=79.9691
	step [128/244], loss=79.8880
	step [129/244], loss=78.3362
	step [130/244], loss=78.0213
	step [131/244], loss=78.9281
	step [132/244], loss=79.1241
	step [133/244], loss=77.4523
	step [134/244], loss=79.5533
	step [135/244], loss=78.0821
	step [136/244], loss=78.3836
	step [137/244], loss=77.8841
	step [138/244], loss=76.3858
	step [139/244], loss=77.5955
	step [140/244], loss=77.2584
	step [141/244], loss=78.2474
	step [142/244], loss=78.1864
	step [143/244], loss=76.8075
	step [144/244], loss=77.6334
	step [145/244], loss=78.1355
	step [146/244], loss=77.6318
	step [147/244], loss=77.6833
	step [148/244], loss=77.4107
	step [149/244], loss=75.9893
	step [150/244], loss=76.9402
	step [151/244], loss=76.5213
	step [152/244], loss=76.2472
	step [153/244], loss=75.5586
	step [154/244], loss=76.2788
	step [155/244], loss=76.3306
	step [156/244], loss=75.1082
	step [157/244], loss=74.9931
	step [158/244], loss=74.4119
	step [159/244], loss=75.3304
	step [160/244], loss=74.6447
	step [161/244], loss=74.5391
	step [162/244], loss=76.6896
	step [163/244], loss=78.1669
	step [164/244], loss=74.4041
	step [165/244], loss=76.2959
	step [166/244], loss=77.4943
	step [167/244], loss=76.1967
	step [168/244], loss=75.3429
	step [169/244], loss=77.6986
	step [170/244], loss=73.8285
	step [171/244], loss=74.0129
	step [172/244], loss=74.0697
	step [173/244], loss=72.9964
	step [174/244], loss=73.4323
	step [175/244], loss=75.6851
	step [176/244], loss=75.0792
	step [177/244], loss=73.4299
	step [178/244], loss=74.5861
	step [179/244], loss=73.5994
	step [180/244], loss=75.0572
	step [181/244], loss=73.2533
	step [182/244], loss=73.5659
	step [183/244], loss=74.0893
	step [184/244], loss=73.1128
	step [185/244], loss=72.5560
	step [186/244], loss=72.9537
	step [187/244], loss=72.9122
	step [188/244], loss=70.9016
	step [189/244], loss=72.5873
	step [190/244], loss=74.5060
	step [191/244], loss=72.2894
	step [192/244], loss=73.5469
	step [193/244], loss=72.5982
	step [194/244], loss=73.1796
	step [195/244], loss=72.2588
	step [196/244], loss=74.7846
	step [197/244], loss=72.7825
	step [198/244], loss=70.9552
	step [199/244], loss=72.1935
	step [200/244], loss=71.6098
	step [201/244], loss=71.5860
	step [202/244], loss=71.7716
	step [203/244], loss=74.0441
	step [204/244], loss=71.2806
	step [205/244], loss=71.8125
	step [206/244], loss=70.9113
	step [207/244], loss=70.1557
	step [208/244], loss=70.7170
	step [209/244], loss=70.3795
	step [210/244], loss=71.9957
	step [211/244], loss=70.0145
	step [212/244], loss=72.3145
	step [213/244], loss=70.5869
	step [214/244], loss=70.6877
	step [215/244], loss=69.9986
	step [216/244], loss=73.2778
	step [217/244], loss=70.0833
	step [218/244], loss=69.7940
	step [219/244], loss=72.3982
	step [220/244], loss=69.8703
	step [221/244], loss=70.4945
	step [222/244], loss=69.5522
	step [223/244], loss=69.0870
	step [224/244], loss=70.9976
	step [225/244], loss=69.4414
	step [226/244], loss=70.7813
	step [227/244], loss=69.0405
	step [228/244], loss=70.7995
	step [229/244], loss=72.4633
	step [230/244], loss=70.5020
	step [231/244], loss=68.1251
	step [232/244], loss=69.4410
	step [233/244], loss=68.5013
	step [234/244], loss=67.5492
	step [235/244], loss=68.4319
	step [236/244], loss=69.8381
	step [237/244], loss=68.3182
	step [238/244], loss=66.9769
	step [239/244], loss=70.0580
	step [240/244], loss=66.9551
	step [241/244], loss=67.7363
	step [242/244], loss=68.1389
	step [243/244], loss=69.2049
	step [244/244], loss=2.7739
	Evaluating
	loss=0.2716, precision=0.1386, recall=0.9980, f1=0.2433
saving model as: 2_saved_model.pth
Training epoch 3
	step [1/244], loss=68.1484
	step [2/244], loss=69.0959
	step [3/244], loss=67.9101
	step [4/244], loss=66.8618
	step [5/244], loss=67.8511
	step [6/244], loss=66.1349
	step [7/244], loss=67.0433
	step [8/244], loss=68.9305
	step [9/244], loss=66.9427
	step [10/244], loss=67.9222
	step [11/244], loss=66.3294
	step [12/244], loss=68.5892
	step [13/244], loss=68.6788
	step [14/244], loss=68.2387
	step [15/244], loss=69.0747
	step [16/244], loss=64.5322
	step [17/244], loss=65.3891
	step [18/244], loss=67.1414
	step [19/244], loss=66.6580
	step [20/244], loss=67.2791
	step [21/244], loss=65.0005
	step [22/244], loss=65.6793
	step [23/244], loss=65.7262
	step [24/244], loss=65.3634
	step [25/244], loss=65.7025
	step [26/244], loss=63.8733
	step [27/244], loss=68.8370
	step [28/244], loss=65.9187
	step [29/244], loss=66.4088
	step [30/244], loss=63.6838
	step [31/244], loss=64.5037
	step [32/244], loss=64.5390
	step [33/244], loss=65.1538
	step [34/244], loss=65.3396
	step [35/244], loss=64.2350
	step [36/244], loss=65.7825
	step [37/244], loss=62.8100
	step [38/244], loss=64.6522
	step [39/244], loss=63.2050
	step [40/244], loss=65.4462
	step [41/244], loss=65.3703
	step [42/244], loss=64.7184
	step [43/244], loss=66.0159
	step [44/244], loss=62.6612
	step [45/244], loss=62.3527
	step [46/244], loss=63.6075
	step [47/244], loss=63.4203
	step [48/244], loss=63.5340
	step [49/244], loss=63.7565
	step [50/244], loss=63.7587
	step [51/244], loss=62.7258
	step [52/244], loss=62.8963
	step [53/244], loss=63.8370
	step [54/244], loss=63.6270
	step [55/244], loss=63.0384
	step [56/244], loss=62.5272
	step [57/244], loss=65.0885
	step [58/244], loss=61.4506
	step [59/244], loss=63.0839
	step [60/244], loss=62.4544
	step [61/244], loss=62.2354
	step [62/244], loss=61.1963
	step [63/244], loss=63.8648
	step [64/244], loss=61.6035
	step [65/244], loss=62.0158
	step [66/244], loss=62.5883
	step [67/244], loss=59.9745
	step [68/244], loss=61.1311
	step [69/244], loss=60.8365
	step [70/244], loss=61.9107
	step [71/244], loss=61.3589
	step [72/244], loss=63.0293
	step [73/244], loss=61.4717
	step [74/244], loss=59.5779
	step [75/244], loss=61.7014
	step [76/244], loss=61.3039
	step [77/244], loss=62.2334
	step [78/244], loss=61.8469
	step [79/244], loss=59.8265
	step [80/244], loss=60.7013
	step [81/244], loss=61.0165
	step [82/244], loss=61.0309
	step [83/244], loss=60.2601
	step [84/244], loss=60.5368
	step [85/244], loss=60.7404
	step [86/244], loss=61.3141
	step [87/244], loss=59.5870
	step [88/244], loss=60.2727
	step [89/244], loss=59.5028
	step [90/244], loss=61.4628
	step [91/244], loss=58.0365
	step [92/244], loss=58.6508
	step [93/244], loss=59.1166
	step [94/244], loss=59.2943
	step [95/244], loss=58.8839
	step [96/244], loss=58.1037
	step [97/244], loss=59.9339
	step [98/244], loss=59.0695
	step [99/244], loss=59.4229
	step [100/244], loss=58.3497
	step [101/244], loss=58.7084
	step [102/244], loss=59.8946
	step [103/244], loss=59.5236
	step [104/244], loss=58.9914
	step [105/244], loss=57.9008
	step [106/244], loss=59.0493
	step [107/244], loss=59.1673
	step [108/244], loss=59.1153
	step [109/244], loss=58.1787
	step [110/244], loss=56.8429
	step [111/244], loss=61.3651
	step [112/244], loss=59.0352
	step [113/244], loss=57.5484
	step [114/244], loss=56.9441
	step [115/244], loss=58.1304
	step [116/244], loss=57.0047
	step [117/244], loss=59.0803
	step [118/244], loss=59.1212
	step [119/244], loss=56.8401
	step [120/244], loss=56.0601
	step [121/244], loss=56.2155
	step [122/244], loss=55.0755
	step [123/244], loss=57.7237
	step [124/244], loss=58.6152
	step [125/244], loss=58.9649
	step [126/244], loss=57.8409
	step [127/244], loss=57.1632
	step [128/244], loss=57.6278
	step [129/244], loss=56.6112
	step [130/244], loss=56.8392
	step [131/244], loss=57.2977
	step [132/244], loss=56.4528
	step [133/244], loss=57.5585
	step [134/244], loss=57.8295
	step [135/244], loss=56.8445
	step [136/244], loss=57.1342
	step [137/244], loss=55.2519
	step [138/244], loss=54.7159
	step [139/244], loss=57.5961
	step [140/244], loss=54.7620
	step [141/244], loss=56.2603
	step [142/244], loss=57.8848
	step [143/244], loss=55.4377
	step [144/244], loss=54.7315
	step [145/244], loss=55.5300
	step [146/244], loss=54.3784
	step [147/244], loss=57.1538
	step [148/244], loss=54.5461
	step [149/244], loss=55.4453
	step [150/244], loss=55.0809
	step [151/244], loss=55.3576
	step [152/244], loss=54.4035
	step [153/244], loss=55.3677
	step [154/244], loss=55.5937
	step [155/244], loss=55.2472
	step [156/244], loss=55.0672
	step [157/244], loss=54.9214
	step [158/244], loss=54.7455
	step [159/244], loss=55.9453
	step [160/244], loss=55.1599
	step [161/244], loss=54.7497
	step [162/244], loss=55.0792
	step [163/244], loss=56.6153
	step [164/244], loss=56.1143
	step [165/244], loss=54.6139
	step [166/244], loss=54.2766
	step [167/244], loss=54.2665
	step [168/244], loss=53.5353
	step [169/244], loss=53.4400
	step [170/244], loss=54.4607
	step [171/244], loss=52.1946
	step [172/244], loss=52.7918
	step [173/244], loss=53.0815
	step [174/244], loss=53.1991
	step [175/244], loss=53.2597
	step [176/244], loss=54.8340
	step [177/244], loss=53.1980
	step [178/244], loss=54.5921
	step [179/244], loss=53.8311
	step [180/244], loss=53.4468
	step [181/244], loss=52.7699
	step [182/244], loss=55.5825
	step [183/244], loss=56.8934
	step [184/244], loss=51.8515
	step [185/244], loss=51.9984
	step [186/244], loss=52.3783
	step [187/244], loss=54.5599
	step [188/244], loss=52.5622
	step [189/244], loss=54.8808
	step [190/244], loss=52.6335
	step [191/244], loss=52.2316
	step [192/244], loss=50.6544
	step [193/244], loss=52.7761
	step [194/244], loss=52.8550
	step [195/244], loss=51.4501
	step [196/244], loss=53.2446
	step [197/244], loss=55.2778
	step [198/244], loss=55.3984
	step [199/244], loss=50.3193
	step [200/244], loss=52.0014
	step [201/244], loss=53.6435
	step [202/244], loss=52.3890
	step [203/244], loss=51.3980
	step [204/244], loss=51.6646
	step [205/244], loss=51.5161
	step [206/244], loss=50.7604
	step [207/244], loss=52.7007
	step [208/244], loss=51.0381
	step [209/244], loss=52.1100
	step [210/244], loss=51.2840
	step [211/244], loss=52.9175
	step [212/244], loss=51.0113
	step [213/244], loss=54.3133
	step [214/244], loss=52.6710
	step [215/244], loss=51.8915
	step [216/244], loss=51.1479
	step [217/244], loss=52.5513
	step [218/244], loss=51.9354
	step [219/244], loss=52.0462
	step [220/244], loss=50.7599
	step [221/244], loss=50.9609
	step [222/244], loss=50.5176
	step [223/244], loss=51.2430
	step [224/244], loss=50.3640
	step [225/244], loss=51.9041
	step [226/244], loss=49.6312
	step [227/244], loss=51.9671
	step [228/244], loss=50.5251
	step [229/244], loss=52.8046
	step [230/244], loss=53.0935
	step [231/244], loss=50.2414
	step [232/244], loss=49.4499
	step [233/244], loss=49.4327
	step [234/244], loss=48.9446
	step [235/244], loss=48.8186
	step [236/244], loss=50.9104
	step [237/244], loss=49.1659
	step [238/244], loss=48.2875
	step [239/244], loss=48.1233
	step [240/244], loss=50.2417
	step [241/244], loss=51.9924
	step [242/244], loss=48.4132
	step [243/244], loss=48.3407
	step [244/244], loss=3.4881
	Evaluating
	loss=0.1834, precision=0.1460, recall=0.9982, f1=0.2547
saving model as: 2_saved_model.pth
Training epoch 4
	step [1/244], loss=48.8885
	step [2/244], loss=51.4956
	step [3/244], loss=48.2110
	step [4/244], loss=47.6150
	step [5/244], loss=48.2329
	step [6/244], loss=47.5136
	step [7/244], loss=48.7011
	step [8/244], loss=49.9208
	step [9/244], loss=48.3916
	step [10/244], loss=48.3413
	step [11/244], loss=48.3840
	step [12/244], loss=47.3265
	step [13/244], loss=49.6945
	step [14/244], loss=48.4468
	step [15/244], loss=47.5219
	step [16/244], loss=48.4595
	step [17/244], loss=48.3949
	step [18/244], loss=50.9003
	step [19/244], loss=50.5169
	step [20/244], loss=48.0114
	step [21/244], loss=50.6201
	step [22/244], loss=48.0833
	step [23/244], loss=47.6274
	step [24/244], loss=47.1023
	step [25/244], loss=47.3613
	step [26/244], loss=47.0874
	step [27/244], loss=48.6467
	step [28/244], loss=48.2643
	step [29/244], loss=47.1466
	step [30/244], loss=47.1974
	step [31/244], loss=47.7714
	step [32/244], loss=47.8204
	step [33/244], loss=46.5193
	step [34/244], loss=46.3184
	step [35/244], loss=47.6626
	step [36/244], loss=47.0096
	step [37/244], loss=45.9674
	step [38/244], loss=45.9490
	step [39/244], loss=49.3932
	step [40/244], loss=45.5616
	step [41/244], loss=48.2902
	step [42/244], loss=47.7663
	step [43/244], loss=46.2510
	step [44/244], loss=47.3086
	step [45/244], loss=46.1606
	step [46/244], loss=46.9644
	step [47/244], loss=45.3868
	step [48/244], loss=47.6017
	step [49/244], loss=43.8847
	step [50/244], loss=46.2900
	step [51/244], loss=46.4102
	step [52/244], loss=45.3389
	step [53/244], loss=46.1665
	step [54/244], loss=45.1419
	step [55/244], loss=44.4454
	step [56/244], loss=44.1547
	step [57/244], loss=46.1411
	step [58/244], loss=44.3711
	step [59/244], loss=46.0722
	step [60/244], loss=45.9097
	step [61/244], loss=44.6994
	step [62/244], loss=45.2441
	step [63/244], loss=43.3360
	step [64/244], loss=43.3360
	step [65/244], loss=47.3272
	step [66/244], loss=46.3868
	step [67/244], loss=43.9128
	step [68/244], loss=44.4226
	step [69/244], loss=46.6276
	step [70/244], loss=43.6371
	step [71/244], loss=46.6626
	step [72/244], loss=43.3932
	step [73/244], loss=47.8158
	step [74/244], loss=45.4868
	step [75/244], loss=46.8009
	step [76/244], loss=44.2976
	step [77/244], loss=44.3348
	step [78/244], loss=43.9038
	step [79/244], loss=44.4406
	step [80/244], loss=45.6233
	step [81/244], loss=44.2532
	step [82/244], loss=42.5278
	step [83/244], loss=44.5273
	step [84/244], loss=46.1416
	step [85/244], loss=43.0015
	step [86/244], loss=46.8148
	step [87/244], loss=44.9845
	step [88/244], loss=44.5066
	step [89/244], loss=45.0590
	step [90/244], loss=44.3963
	step [91/244], loss=42.7302
	step [92/244], loss=42.4866
	step [93/244], loss=44.1976
	step [94/244], loss=41.5391
	step [95/244], loss=46.1211
	step [96/244], loss=41.7461
	step [97/244], loss=44.8887
	step [98/244], loss=45.0368
	step [99/244], loss=43.2859
	step [100/244], loss=43.7515
	step [101/244], loss=43.8003
	step [102/244], loss=42.3203
	step [103/244], loss=42.9066
	step [104/244], loss=43.3438
	step [105/244], loss=43.0522
	step [106/244], loss=43.6414
	step [107/244], loss=43.1438
	step [108/244], loss=41.8305
	step [109/244], loss=42.5603
	step [110/244], loss=41.1881
	step [111/244], loss=42.7200
	step [112/244], loss=41.8539
	step [113/244], loss=40.1394
	step [114/244], loss=42.6470
	step [115/244], loss=42.6210
	step [116/244], loss=42.7389
	step [117/244], loss=42.6618
	step [118/244], loss=41.5862
	step [119/244], loss=42.9254
	step [120/244], loss=43.1091
	step [121/244], loss=42.0747
	step [122/244], loss=40.3555
	step [123/244], loss=41.8276
	step [124/244], loss=41.7040
	step [125/244], loss=42.3225
	step [126/244], loss=42.0099
	step [127/244], loss=41.5852
	step [128/244], loss=42.2463
	step [129/244], loss=41.0753
	step [130/244], loss=42.5299
	step [131/244], loss=41.2282
	step [132/244], loss=43.3909
	step [133/244], loss=43.0681
	step [134/244], loss=42.9139
	step [135/244], loss=41.4889
	step [136/244], loss=39.2572
	step [137/244], loss=41.3381
	step [138/244], loss=40.9260
	step [139/244], loss=40.2820
	step [140/244], loss=41.2501
	step [141/244], loss=40.4051
	step [142/244], loss=39.6086
	step [143/244], loss=38.4371
	step [144/244], loss=39.6414
	step [145/244], loss=41.3179
	step [146/244], loss=41.2649
	step [147/244], loss=39.9333
	step [148/244], loss=39.2144
	step [149/244], loss=41.9662
	step [150/244], loss=39.1405
	step [151/244], loss=41.7018
	step [152/244], loss=40.3116
	step [153/244], loss=39.0279
	step [154/244], loss=39.2777
	step [155/244], loss=41.0911
	step [156/244], loss=39.4381
	step [157/244], loss=40.2632
	step [158/244], loss=39.4564
	step [159/244], loss=40.7141
	step [160/244], loss=40.8240
	step [161/244], loss=39.5420
	step [162/244], loss=40.3939
	step [163/244], loss=39.3338
	step [164/244], loss=39.1711
	step [165/244], loss=39.9490
	step [166/244], loss=38.3160
	step [167/244], loss=40.1233
	step [168/244], loss=40.2753
	step [169/244], loss=39.6423
	step [170/244], loss=38.5451
	step [171/244], loss=38.6253
	step [172/244], loss=39.0223
	step [173/244], loss=39.1383
	step [174/244], loss=39.4609
	step [175/244], loss=38.3263
	step [176/244], loss=40.0709
	step [177/244], loss=41.4990
	step [178/244], loss=38.4910
	step [179/244], loss=37.5697
	step [180/244], loss=38.6737
	step [181/244], loss=38.9637
	step [182/244], loss=38.7995
	step [183/244], loss=39.5147
	step [184/244], loss=39.7234
	step [185/244], loss=38.0611
	step [186/244], loss=39.6392
	step [187/244], loss=39.2367
	step [188/244], loss=37.1527
	step [189/244], loss=40.4259
	step [190/244], loss=38.2221
	step [191/244], loss=40.8769
	step [192/244], loss=38.8052
	step [193/244], loss=38.1816
	step [194/244], loss=37.4729
	step [195/244], loss=39.3272
	step [196/244], loss=39.1316
	step [197/244], loss=38.9138
	step [198/244], loss=39.5138
	step [199/244], loss=37.3629
	step [200/244], loss=39.1896
	step [201/244], loss=41.0720
	step [202/244], loss=38.1043
	step [203/244], loss=38.4391
	step [204/244], loss=41.2588
	step [205/244], loss=37.9005
	step [206/244], loss=40.2895
	step [207/244], loss=37.8273
	step [208/244], loss=38.8074
	step [209/244], loss=39.4661
	step [210/244], loss=39.0565
	step [211/244], loss=37.4798
	step [212/244], loss=39.0963
	step [213/244], loss=36.5027
	step [214/244], loss=38.9478
	step [215/244], loss=37.8330
	step [216/244], loss=37.9066
	step [217/244], loss=38.6589
	step [218/244], loss=36.8957
	step [219/244], loss=37.1461
	step [220/244], loss=38.8850
	step [221/244], loss=37.5444
	step [222/244], loss=37.4618
	step [223/244], loss=39.7371
	step [224/244], loss=36.9544
	step [225/244], loss=41.5897
	step [226/244], loss=37.0789
	step [227/244], loss=36.4500
	step [228/244], loss=37.5845
	step [229/244], loss=36.6134
	step [230/244], loss=38.1997
	step [231/244], loss=36.3872
	step [232/244], loss=38.7604
	step [233/244], loss=38.1906
	step [234/244], loss=36.3314
	step [235/244], loss=35.9703
	step [236/244], loss=35.8643
	step [237/244], loss=36.5618
	step [238/244], loss=37.7176
	step [239/244], loss=37.0080
	step [240/244], loss=35.6817
	step [241/244], loss=36.9976
	step [242/244], loss=36.9225
	step [243/244], loss=36.1206
	step [244/244], loss=1.4772
	Evaluating
	loss=0.1356, precision=0.1521, recall=0.9981, f1=0.2640
saving model as: 2_saved_model.pth
Training epoch 5
	step [1/244], loss=36.1662
	step [2/244], loss=36.4722
	step [3/244], loss=35.8536
	step [4/244], loss=36.2471
	step [5/244], loss=35.0322
	step [6/244], loss=35.1839
	step [7/244], loss=36.7790
	step [8/244], loss=38.6769
	step [9/244], loss=36.6282
	step [10/244], loss=35.0708
	step [11/244], loss=34.9623
	step [12/244], loss=36.4651
	step [13/244], loss=39.2559
	step [14/244], loss=36.7724
	step [15/244], loss=33.4202
	step [16/244], loss=34.0516
	step [17/244], loss=36.1000
	step [18/244], loss=35.4205
	step [19/244], loss=36.2178
	step [20/244], loss=36.9465
	step [21/244], loss=34.8772
	step [22/244], loss=35.5022
	step [23/244], loss=34.1038
	step [24/244], loss=35.9830
	step [25/244], loss=36.7347
	step [26/244], loss=35.3700
	step [27/244], loss=34.1954
	step [28/244], loss=36.8678
	step [29/244], loss=33.3374
	step [30/244], loss=32.8526
	step [31/244], loss=34.6375
	step [32/244], loss=35.8979
	step [33/244], loss=33.7456
	step [34/244], loss=36.4975
	step [35/244], loss=35.3959
	step [36/244], loss=33.3698
	step [37/244], loss=36.6502
	step [38/244], loss=35.0272
	step [39/244], loss=35.3325
	step [40/244], loss=34.4545
	step [41/244], loss=33.9426
	step [42/244], loss=34.2974
	step [43/244], loss=33.8916
	step [44/244], loss=34.4161
	step [45/244], loss=34.1769
	step [46/244], loss=34.8306
	step [47/244], loss=34.1400
	step [48/244], loss=35.2767
	step [49/244], loss=34.5090
	step [50/244], loss=33.5382
	step [51/244], loss=35.1876
	step [52/244], loss=31.8176
	step [53/244], loss=34.8842
	step [54/244], loss=34.9023
	step [55/244], loss=32.5614
	step [56/244], loss=33.9126
	step [57/244], loss=35.4705
	step [58/244], loss=35.5683
	step [59/244], loss=35.5539
	step [60/244], loss=33.6151
	step [61/244], loss=38.4381
	step [62/244], loss=36.6262
	step [63/244], loss=32.6983
	step [64/244], loss=34.2091
	step [65/244], loss=35.1551
	step [66/244], loss=32.7630
	step [67/244], loss=36.2314
	step [68/244], loss=33.2754
	step [69/244], loss=32.7632
	step [70/244], loss=31.9940
	step [71/244], loss=34.1875
	step [72/244], loss=33.9245
	step [73/244], loss=35.4172
	step [74/244], loss=33.6024
	step [75/244], loss=34.0104
	step [76/244], loss=31.4397
	step [77/244], loss=33.5943
	step [78/244], loss=32.3956
	step [79/244], loss=34.6422
	step [80/244], loss=32.6737
	step [81/244], loss=33.2664
	step [82/244], loss=33.0232
	step [83/244], loss=31.4797
	step [84/244], loss=32.4981
	step [85/244], loss=34.5666
	step [86/244], loss=33.0016
	step [87/244], loss=34.0027
	step [88/244], loss=32.2547
	step [89/244], loss=34.4991
	step [90/244], loss=32.3416
	step [91/244], loss=33.3456
	step [92/244], loss=33.9926
	step [93/244], loss=32.1854
	step [94/244], loss=33.1495
	step [95/244], loss=32.5903
	step [96/244], loss=33.2183
	step [97/244], loss=31.9958
	step [98/244], loss=33.6274
	step [99/244], loss=33.9372
	step [100/244], loss=32.8893
	step [101/244], loss=31.2016
	step [102/244], loss=33.9110
	step [103/244], loss=31.4369
	step [104/244], loss=33.0360
	step [105/244], loss=32.6102
	step [106/244], loss=32.3436
	step [107/244], loss=32.7372
	step [108/244], loss=32.7526
	step [109/244], loss=31.6741
	step [110/244], loss=32.9976
	step [111/244], loss=31.8590
	step [112/244], loss=33.7956
	step [113/244], loss=30.6706
	step [114/244], loss=30.2997
	step [115/244], loss=31.6195
	step [116/244], loss=33.8517
	step [117/244], loss=32.6097
	step [118/244], loss=32.9483
	step [119/244], loss=32.4475
	step [120/244], loss=32.0475
	step [121/244], loss=31.9794
	step [122/244], loss=30.8737
	step [123/244], loss=32.2239
	step [124/244], loss=30.7871
	step [125/244], loss=31.4027
	step [126/244], loss=31.3786
	step [127/244], loss=31.3151
	step [128/244], loss=31.8013
	step [129/244], loss=30.4697
	step [130/244], loss=34.1504
	step [131/244], loss=29.7310
	step [132/244], loss=32.5818
	step [133/244], loss=31.1092
	step [134/244], loss=33.5193
	step [135/244], loss=33.0024
	step [136/244], loss=31.6661
	step [137/244], loss=30.6521
	step [138/244], loss=30.4545
	step [139/244], loss=30.9251
	step [140/244], loss=31.0236
	step [141/244], loss=32.4893
	step [142/244], loss=30.2456
	step [143/244], loss=33.7535
	step [144/244], loss=30.8852
	step [145/244], loss=31.3228
	step [146/244], loss=32.3195
	step [147/244], loss=31.8003
	step [148/244], loss=31.6732
	step [149/244], loss=32.2464
	step [150/244], loss=31.0279
	step [151/244], loss=31.6411
	step [152/244], loss=29.2425
	step [153/244], loss=32.2365
	step [154/244], loss=31.8345
	step [155/244], loss=31.0254
	step [156/244], loss=29.0555
	step [157/244], loss=31.6345
	step [158/244], loss=31.6911
	step [159/244], loss=30.1885
	step [160/244], loss=31.9657
	step [161/244], loss=31.0189
	step [162/244], loss=30.3686
	step [163/244], loss=32.6794
	step [164/244], loss=32.0471
	step [165/244], loss=31.1986
	step [166/244], loss=30.8019
	step [167/244], loss=30.2462
	step [168/244], loss=29.5521
	step [169/244], loss=30.7050
	step [170/244], loss=29.5329
	step [171/244], loss=30.7472
	step [172/244], loss=30.9850
	step [173/244], loss=29.8730
	step [174/244], loss=30.0940
	step [175/244], loss=28.9781
	step [176/244], loss=30.0532
	step [177/244], loss=29.8415
	step [178/244], loss=28.1170
	step [179/244], loss=31.3404
	step [180/244], loss=32.6495
	step [181/244], loss=30.5772
	step [182/244], loss=29.2658
	step [183/244], loss=28.8546
	step [184/244], loss=29.2458
	step [185/244], loss=29.4348
	step [186/244], loss=30.7610
	step [187/244], loss=30.8728
	step [188/244], loss=29.3610
	step [189/244], loss=29.9069
	step [190/244], loss=29.1739
	step [191/244], loss=30.6907
	step [192/244], loss=30.4389
	step [193/244], loss=29.2728
	step [194/244], loss=30.0869
	step [195/244], loss=29.8406
	step [196/244], loss=29.2888
	step [197/244], loss=29.6321
	step [198/244], loss=29.5740
	step [199/244], loss=29.4567
	step [200/244], loss=27.5969
	step [201/244], loss=28.7227
	step [202/244], loss=29.1546
	step [203/244], loss=28.8958
	step [204/244], loss=28.9489
	step [205/244], loss=29.4322
	step [206/244], loss=29.5367
	step [207/244], loss=29.4675
	step [208/244], loss=30.4820
	step [209/244], loss=27.8112
	step [210/244], loss=28.6947
	step [211/244], loss=30.2162
	step [212/244], loss=29.1137
	step [213/244], loss=30.4208
	step [214/244], loss=28.7019
	step [215/244], loss=31.6048
	step [216/244], loss=27.3203
	step [217/244], loss=28.5548
	step [218/244], loss=30.5665
	step [219/244], loss=28.7174
	step [220/244], loss=27.3288
	step [221/244], loss=26.3814
	step [222/244], loss=29.2009
	step [223/244], loss=29.6203
	step [224/244], loss=30.8079
	step [225/244], loss=27.1798
	step [226/244], loss=28.4685
	step [227/244], loss=29.0320
	step [228/244], loss=28.8857
	step [229/244], loss=30.5153
	step [230/244], loss=29.0552
	step [231/244], loss=28.2157
	step [232/244], loss=30.0926
	step [233/244], loss=29.8906
	step [234/244], loss=28.1326
	step [235/244], loss=27.4133
	step [236/244], loss=27.7732
	step [237/244], loss=30.1093
	step [238/244], loss=30.2176
	step [239/244], loss=29.8714
	step [240/244], loss=27.5000
	step [241/244], loss=29.0385
	step [242/244], loss=28.6842
	step [243/244], loss=28.1289
	step [244/244], loss=0.9741
	Evaluating
	loss=0.1032, precision=0.1617, recall=0.9979, f1=0.2782
saving model as: 2_saved_model.pth
Training epoch 6
	step [1/244], loss=28.3585
	step [2/244], loss=29.0336
	step [3/244], loss=28.6023
	step [4/244], loss=28.8259
	step [5/244], loss=27.8460
	step [6/244], loss=28.9739
	step [7/244], loss=29.2297
	step [8/244], loss=27.6530
	step [9/244], loss=29.7020
	step [10/244], loss=28.3844
	step [11/244], loss=28.4273
	step [12/244], loss=28.0290
	step [13/244], loss=28.9856
	step [14/244], loss=29.7368
	step [15/244], loss=30.8072
	step [16/244], loss=28.4457
	step [17/244], loss=28.5157
	step [18/244], loss=26.6231
	step [19/244], loss=28.1100
	step [20/244], loss=28.6881
	step [21/244], loss=27.2598
	step [22/244], loss=28.0839
	step [23/244], loss=28.4595
	step [24/244], loss=25.8213
	step [25/244], loss=26.8010
	step [26/244], loss=28.5357
	step [27/244], loss=27.4480
	step [28/244], loss=31.9824
	step [29/244], loss=26.6336
	step [30/244], loss=28.1894
	step [31/244], loss=28.3164
	step [32/244], loss=27.2956
	step [33/244], loss=26.2715
	step [34/244], loss=30.4572
	step [35/244], loss=25.3955
	step [36/244], loss=28.0835
	step [37/244], loss=27.4514
	step [38/244], loss=28.8536
	step [39/244], loss=27.7892
	step [40/244], loss=26.2443
	step [41/244], loss=25.0639
	step [42/244], loss=25.8193
	step [43/244], loss=28.8065
	step [44/244], loss=26.6492
	step [45/244], loss=26.6075
	step [46/244], loss=26.0740
	step [47/244], loss=28.5675
	step [48/244], loss=27.2842
	step [49/244], loss=27.2907
	step [50/244], loss=27.0316
	step [51/244], loss=26.3199
	step [52/244], loss=26.7382
	step [53/244], loss=26.4254
	step [54/244], loss=25.9594
	step [55/244], loss=28.3884
	step [56/244], loss=26.8121
	step [57/244], loss=27.8140
	step [58/244], loss=26.5282
	step [59/244], loss=27.1781
	step [60/244], loss=26.6903
	step [61/244], loss=29.0843
	step [62/244], loss=28.4040
	step [63/244], loss=25.8911
	step [64/244], loss=27.5474
	step [65/244], loss=26.4228
	step [66/244], loss=26.1727
	step [67/244], loss=26.2960
	step [68/244], loss=26.0548
	step [69/244], loss=26.1393
	step [70/244], loss=27.3199
	step [71/244], loss=24.0336
	step [72/244], loss=25.7859
	step [73/244], loss=27.5517
	step [74/244], loss=26.6398
	step [75/244], loss=26.5700
	step [76/244], loss=26.4522
	step [77/244], loss=26.3767
	step [78/244], loss=25.0801
	step [79/244], loss=26.0233
	step [80/244], loss=27.4632
	step [81/244], loss=26.9722
	step [82/244], loss=26.6663
	step [83/244], loss=27.4925
	step [84/244], loss=25.2596
	step [85/244], loss=25.8905
	step [86/244], loss=27.1769
	step [87/244], loss=27.1690
	step [88/244], loss=25.8696
	step [89/244], loss=27.9177
	step [90/244], loss=27.3468
	step [91/244], loss=26.6818
	step [92/244], loss=26.4750
	step [93/244], loss=25.2500
	step [94/244], loss=24.3754
	step [95/244], loss=25.1902
	step [96/244], loss=25.1806
	step [97/244], loss=27.0731
	step [98/244], loss=26.1516
	step [99/244], loss=24.3924
	step [100/244], loss=24.3011
	step [101/244], loss=26.0338
	step [102/244], loss=24.8570
	step [103/244], loss=23.8208
	step [104/244], loss=26.8519
	step [105/244], loss=26.2888
	step [106/244], loss=24.8893
	step [107/244], loss=23.7393
	step [108/244], loss=24.6460
	step [109/244], loss=26.0696
	step [110/244], loss=25.9746
	step [111/244], loss=25.5732
	step [112/244], loss=24.6507
	step [113/244], loss=25.6649
	step [114/244], loss=24.5184
	step [115/244], loss=25.0101
	step [116/244], loss=25.2733
	step [117/244], loss=24.7252
	step [118/244], loss=24.3169
	step [119/244], loss=28.0660
	step [120/244], loss=24.6373
	step [121/244], loss=27.3722
	step [122/244], loss=24.1508
	step [123/244], loss=24.7215
	step [124/244], loss=26.1734
	step [125/244], loss=24.6351
	step [126/244], loss=26.3366
	step [127/244], loss=25.3003
	step [128/244], loss=25.3661
	step [129/244], loss=27.7831
	step [130/244], loss=23.0233
	step [131/244], loss=24.4918
	step [132/244], loss=24.0502
	step [133/244], loss=23.9957
	step [134/244], loss=24.9660
	step [135/244], loss=24.7369
	step [136/244], loss=25.2043
	step [137/244], loss=25.5847
	step [138/244], loss=25.6353
	step [139/244], loss=25.5056
	step [140/244], loss=26.6669
	step [141/244], loss=25.4016
	step [142/244], loss=23.6459
	step [143/244], loss=22.5444
	step [144/244], loss=26.0058
	step [145/244], loss=27.1527
	step [146/244], loss=25.5910
	step [147/244], loss=26.6376
	step [148/244], loss=22.7691
	step [149/244], loss=25.5192
	step [150/244], loss=25.5570
	step [151/244], loss=24.4340
	step [152/244], loss=24.7134
	step [153/244], loss=27.0941
	step [154/244], loss=25.1390
	step [155/244], loss=25.9859
	step [156/244], loss=23.2571
	step [157/244], loss=24.5017
	step [158/244], loss=24.1260
	step [159/244], loss=26.4770
	step [160/244], loss=24.8030
	step [161/244], loss=24.9243
	step [162/244], loss=23.7590
	step [163/244], loss=24.5028
	step [164/244], loss=23.8939
	step [165/244], loss=25.9010
	step [166/244], loss=22.5907
	step [167/244], loss=24.3687
	step [168/244], loss=23.4682
	step [169/244], loss=23.8915
	step [170/244], loss=26.5982
	step [171/244], loss=25.1819
	step [172/244], loss=25.5697
	step [173/244], loss=23.9063
	step [174/244], loss=25.0013
	step [175/244], loss=23.2430
	step [176/244], loss=23.3884
	step [177/244], loss=25.0646
	step [178/244], loss=23.1883
	step [179/244], loss=24.0608
	step [180/244], loss=23.9011
	step [181/244], loss=25.0262
	step [182/244], loss=23.5272
	step [183/244], loss=22.8038
	step [184/244], loss=25.6244
	step [185/244], loss=23.1269
	step [186/244], loss=24.4892
	step [187/244], loss=23.2425
	step [188/244], loss=22.7762
	step [189/244], loss=24.3768
	step [190/244], loss=23.0409
	step [191/244], loss=25.2256
	step [192/244], loss=26.3171
	step [193/244], loss=23.6546
	step [194/244], loss=23.0971
	step [195/244], loss=25.3038
	step [196/244], loss=23.1586
	step [197/244], loss=24.5622
	step [198/244], loss=23.6433
	step [199/244], loss=25.3467
	step [200/244], loss=23.1699
	step [201/244], loss=22.8916
	step [202/244], loss=26.2842
	step [203/244], loss=23.2231
	step [204/244], loss=23.2741
	step [205/244], loss=26.1902
	step [206/244], loss=26.1340
	step [207/244], loss=23.3012
	step [208/244], loss=26.3912
	step [209/244], loss=24.8659
	step [210/244], loss=24.1337
	step [211/244], loss=23.6585
	step [212/244], loss=24.1657
	step [213/244], loss=22.2325
	step [214/244], loss=24.6507
	step [215/244], loss=24.7703
	step [216/244], loss=23.1876
	step [217/244], loss=23.5587
	step [218/244], loss=23.7568
	step [219/244], loss=24.5984
	step [220/244], loss=22.2100
	step [221/244], loss=26.6969
	step [222/244], loss=23.3411
	step [223/244], loss=24.4079
	step [224/244], loss=24.0796
	step [225/244], loss=24.9495
	step [226/244], loss=24.6873
	step [227/244], loss=24.2668
	step [228/244], loss=25.1389
	step [229/244], loss=23.6136
	step [230/244], loss=25.2570
	step [231/244], loss=26.5252
	step [232/244], loss=25.1133
	step [233/244], loss=22.9759
	step [234/244], loss=24.0318
	step [235/244], loss=24.5119
	step [236/244], loss=23.6673
	step [237/244], loss=23.9980
	step [238/244], loss=21.0006
	step [239/244], loss=23.0075
	step [240/244], loss=23.5872
	step [241/244], loss=23.1660
	step [242/244], loss=23.6781
	step [243/244], loss=24.5151
	step [244/244], loss=0.8990
	Evaluating
	loss=0.0834, precision=0.1509, recall=0.9983, f1=0.2622
Training epoch 7
	step [1/244], loss=24.0473
	step [2/244], loss=22.0109
	step [3/244], loss=25.8176
	step [4/244], loss=21.7634
	step [5/244], loss=23.3348
	step [6/244], loss=23.3196
	step [7/244], loss=22.5419
	step [8/244], loss=21.9765
	step [9/244], loss=22.7653
	step [10/244], loss=24.2623
	step [11/244], loss=23.7995
	step [12/244], loss=23.7992
	step [13/244], loss=22.3785
	step [14/244], loss=21.8377
	step [15/244], loss=22.2736
	step [16/244], loss=21.6180
	step [17/244], loss=21.9830
	step [18/244], loss=21.9576
	step [19/244], loss=21.9522
	step [20/244], loss=21.8114
	step [21/244], loss=23.1219
	step [22/244], loss=20.7697
	step [23/244], loss=24.2910
	step [24/244], loss=22.3460
	step [25/244], loss=21.4899
	step [26/244], loss=24.6557
	step [27/244], loss=22.4728
	step [28/244], loss=21.2808
	step [29/244], loss=23.0408
	step [30/244], loss=22.5209
	step [31/244], loss=24.7069
	step [32/244], loss=22.1261
	step [33/244], loss=22.8474
	step [34/244], loss=22.2172
	step [35/244], loss=22.7229
	step [36/244], loss=21.4034
	step [37/244], loss=21.5854
	step [38/244], loss=22.0089
	step [39/244], loss=21.5891
	step [40/244], loss=21.9316
	step [41/244], loss=24.0690
	step [42/244], loss=22.5924
	step [43/244], loss=20.3316
	step [44/244], loss=23.1449
	step [45/244], loss=22.8103
	step [46/244], loss=24.0300
	step [47/244], loss=24.4029
	step [48/244], loss=21.8218
	step [49/244], loss=23.2294
	step [50/244], loss=22.7190
	step [51/244], loss=20.3617
	step [52/244], loss=21.5795
	step [53/244], loss=21.9779
	step [54/244], loss=22.1827
	step [55/244], loss=22.0025
	step [56/244], loss=22.5500
	step [57/244], loss=22.8141
	step [58/244], loss=22.8554
	step [59/244], loss=23.7366
	step [60/244], loss=21.8837
	step [61/244], loss=21.1286
	step [62/244], loss=21.8952
	step [63/244], loss=21.9792
	step [64/244], loss=22.0160
	step [65/244], loss=22.0343
	step [66/244], loss=22.7416
	step [67/244], loss=21.8793
	step [68/244], loss=21.2676
	step [69/244], loss=22.5623
	step [70/244], loss=20.6118
	step [71/244], loss=23.7553
	step [72/244], loss=23.3695
	step [73/244], loss=22.5574
	step [74/244], loss=23.0970
	step [75/244], loss=24.5804
	step [76/244], loss=23.8720
	step [77/244], loss=21.9593
	step [78/244], loss=22.3006
	step [79/244], loss=21.5548
	step [80/244], loss=23.8118
	step [81/244], loss=23.7140
	step [82/244], loss=21.4947
	step [83/244], loss=22.2464
	step [84/244], loss=21.4932
	step [85/244], loss=22.2531
	step [86/244], loss=23.5991
	step [87/244], loss=21.1769
	step [88/244], loss=21.9165
	step [89/244], loss=20.0748
	step [90/244], loss=21.8073
	step [91/244], loss=20.6717
	step [92/244], loss=21.7848
	step [93/244], loss=21.2851
	step [94/244], loss=21.0906
	step [95/244], loss=19.5105
	step [96/244], loss=23.4751
	step [97/244], loss=20.7576
	step [98/244], loss=20.1668
	step [99/244], loss=20.6565
	step [100/244], loss=21.0412
	step [101/244], loss=20.3314
	step [102/244], loss=20.2067
	step [103/244], loss=21.9640
	step [104/244], loss=21.9141
	step [105/244], loss=21.6166
	step [106/244], loss=22.0519
	step [107/244], loss=24.0194
	step [108/244], loss=21.8943
	step [109/244], loss=21.3718
	step [110/244], loss=23.5125
	step [111/244], loss=22.1367
	step [112/244], loss=21.0089
	step [113/244], loss=23.3990
	step [114/244], loss=19.9419
	step [115/244], loss=21.9119
	step [116/244], loss=19.8616
	step [117/244], loss=20.6853
	step [118/244], loss=22.4570
	step [119/244], loss=22.3581
	step [120/244], loss=22.1302
	step [121/244], loss=21.1756
	step [122/244], loss=21.1174
	step [123/244], loss=23.9894
	step [124/244], loss=22.7887
	step [125/244], loss=21.2406
	step [126/244], loss=20.8615
	step [127/244], loss=19.5186
	step [128/244], loss=20.6105
	step [129/244], loss=18.6964
	step [130/244], loss=20.1716
	step [131/244], loss=19.9041
	step [132/244], loss=19.5893
	step [133/244], loss=22.2411
	step [134/244], loss=20.7732
	step [135/244], loss=20.9747
	step [136/244], loss=23.2726
	step [137/244], loss=21.8081
	step [138/244], loss=20.0635
	step [139/244], loss=22.3526
	step [140/244], loss=19.6079
	step [141/244], loss=20.6940
	step [142/244], loss=20.6501
	step [143/244], loss=21.7607
	step [144/244], loss=22.9048
	step [145/244], loss=20.3404
	step [146/244], loss=19.5046
	step [147/244], loss=20.5752
	step [148/244], loss=21.6432
	step [149/244], loss=21.5017
	step [150/244], loss=19.3254
	step [151/244], loss=21.1210
	step [152/244], loss=20.5902
	step [153/244], loss=21.3634
	step [154/244], loss=23.1017
	step [155/244], loss=18.6512
	step [156/244], loss=20.6080
	step [157/244], loss=21.3280
	step [158/244], loss=20.3549
	step [159/244], loss=19.2246
	step [160/244], loss=20.6740
	step [161/244], loss=21.7097
	step [162/244], loss=21.0181
	step [163/244], loss=20.4429
	step [164/244], loss=21.0623
	step [165/244], loss=20.0572
	step [166/244], loss=20.5867
	step [167/244], loss=22.0255
	step [168/244], loss=21.3926
	step [169/244], loss=20.6632
	step [170/244], loss=19.6820
	step [171/244], loss=21.1342
	step [172/244], loss=20.0602
	step [173/244], loss=19.2863
	step [174/244], loss=19.7730
	step [175/244], loss=19.2564
	step [176/244], loss=22.2802
	step [177/244], loss=23.6307
	step [178/244], loss=21.7677
	step [179/244], loss=21.8146
	step [180/244], loss=19.4068
	step [181/244], loss=20.0304
	step [182/244], loss=20.7710
	step [183/244], loss=20.7786
	step [184/244], loss=18.8875
	step [185/244], loss=22.1386
	step [186/244], loss=18.3144
	step [187/244], loss=19.2210
	step [188/244], loss=22.0530
	step [189/244], loss=22.5771
	step [190/244], loss=19.6815
	step [191/244], loss=19.3301
	step [192/244], loss=19.7349
	step [193/244], loss=20.3845
	step [194/244], loss=19.4272
	step [195/244], loss=20.0513
	step [196/244], loss=19.1754
	step [197/244], loss=19.8371
	step [198/244], loss=20.6760
	step [199/244], loss=21.8067
	step [200/244], loss=21.4172
	step [201/244], loss=22.2240
	step [202/244], loss=20.7334
	step [203/244], loss=19.7347
	step [204/244], loss=19.8942
	step [205/244], loss=19.4024
	step [206/244], loss=19.1583
	step [207/244], loss=18.0238
	step [208/244], loss=22.2407
	step [209/244], loss=20.4991
	step [210/244], loss=19.4284
	step [211/244], loss=20.0628
	step [212/244], loss=19.7554
	step [213/244], loss=21.1202
	step [214/244], loss=22.5367
	step [215/244], loss=19.1311
	step [216/244], loss=18.1245
	step [217/244], loss=20.7995
	step [218/244], loss=21.3339
	step [219/244], loss=21.3549
	step [220/244], loss=19.2113
	step [221/244], loss=18.8135
	step [222/244], loss=21.6595
	step [223/244], loss=20.1965
	step [224/244], loss=21.2789
	step [225/244], loss=20.4173
	step [226/244], loss=20.2682
	step [227/244], loss=20.7058
	step [228/244], loss=19.2748
	step [229/244], loss=19.9847
	step [230/244], loss=19.8252
	step [231/244], loss=21.1264
	step [232/244], loss=19.1537
	step [233/244], loss=18.0814
	step [234/244], loss=20.0198
	step [235/244], loss=19.4081
	step [236/244], loss=20.3045
	step [237/244], loss=18.8967
	step [238/244], loss=20.1596
	step [239/244], loss=21.1625
	step [240/244], loss=19.0718
	step [241/244], loss=19.3927
	step [242/244], loss=17.9974
	step [243/244], loss=20.2630
	step [244/244], loss=1.3443
	Evaluating
	loss=0.0727, precision=0.1320, recall=0.9983, f1=0.2332
Training epoch 8
	step [1/244], loss=18.1709
	step [2/244], loss=19.4963
	step [3/244], loss=20.2835
	step [4/244], loss=19.6405
	step [5/244], loss=18.6237
	step [6/244], loss=20.5247
	step [7/244], loss=18.6113
	step [8/244], loss=19.0278
	step [9/244], loss=20.4474
	step [10/244], loss=19.0100
	step [11/244], loss=19.0139
	step [12/244], loss=17.9724
	step [13/244], loss=21.3183
	step [14/244], loss=19.4609
	step [15/244], loss=18.7941
	step [16/244], loss=17.5343
	step [17/244], loss=18.9590
	step [18/244], loss=18.7342
	step [19/244], loss=20.4071
	step [20/244], loss=19.5892
	step [21/244], loss=20.8619
	step [22/244], loss=18.2346
	step [23/244], loss=18.2051
	step [24/244], loss=19.1448
	step [25/244], loss=19.2063
	step [26/244], loss=19.0418
	step [27/244], loss=19.8004
	step [28/244], loss=16.4566
	step [29/244], loss=16.3387
	step [30/244], loss=19.1938
	step [31/244], loss=17.3390
	step [32/244], loss=17.6148
	step [33/244], loss=17.7758
	step [34/244], loss=17.8074
	step [35/244], loss=20.7112
	step [36/244], loss=18.8207
	step [37/244], loss=21.3776
	step [38/244], loss=19.3414
	step [39/244], loss=17.5124
	step [40/244], loss=17.5590
	step [41/244], loss=19.2265
	step [42/244], loss=19.1507
	step [43/244], loss=19.0205
	step [44/244], loss=19.5054
	step [45/244], loss=19.6376
	step [46/244], loss=17.4059
	step [47/244], loss=18.7953
	step [48/244], loss=18.7624
	step [49/244], loss=18.5989
	step [50/244], loss=19.8819
	step [51/244], loss=18.7822
	step [52/244], loss=18.7400
	step [53/244], loss=20.6287
	step [54/244], loss=21.2977
	step [55/244], loss=20.3317
	step [56/244], loss=18.2132
	step [57/244], loss=17.7084
	step [58/244], loss=19.7090
	step [59/244], loss=19.0392
	step [60/244], loss=19.8740
	step [61/244], loss=18.6948
	step [62/244], loss=18.4693
	step [63/244], loss=18.8882
	step [64/244], loss=18.6819
	step [65/244], loss=21.9115
	step [66/244], loss=19.6433
	step [67/244], loss=18.3545
	step [68/244], loss=17.8646
	step [69/244], loss=18.5709
	step [70/244], loss=17.9437
	step [71/244], loss=18.5590
	step [72/244], loss=17.6931
	step [73/244], loss=20.8466
	step [74/244], loss=17.9254
	step [75/244], loss=18.2894
	step [76/244], loss=19.9585
	step [77/244], loss=19.6973
	step [78/244], loss=19.9414
	step [79/244], loss=18.3520
	step [80/244], loss=18.4520
	step [81/244], loss=19.0061
	step [82/244], loss=19.7634
	step [83/244], loss=17.6343
	step [84/244], loss=19.9679
	step [85/244], loss=16.0408
	step [86/244], loss=18.9727
	step [87/244], loss=21.6649
	step [88/244], loss=18.0155
	step [89/244], loss=18.3513
	step [90/244], loss=18.1566
	step [91/244], loss=18.5491
	step [92/244], loss=18.2397
	step [93/244], loss=16.9334
	step [94/244], loss=19.3470
	step [95/244], loss=16.2850
	step [96/244], loss=18.6305
	step [97/244], loss=20.2304
	step [98/244], loss=20.4513
	step [99/244], loss=18.9551
	step [100/244], loss=22.5158
	step [101/244], loss=17.6408
	step [102/244], loss=17.3326
	step [103/244], loss=16.6693
	step [104/244], loss=20.2811
	step [105/244], loss=18.9774
	step [106/244], loss=17.3777
	step [107/244], loss=17.3852
	step [108/244], loss=18.5707
	step [109/244], loss=18.0862
	step [110/244], loss=16.7612
	step [111/244], loss=19.2029
	step [112/244], loss=19.2707
	step [113/244], loss=16.6505
	step [114/244], loss=18.6372
	step [115/244], loss=17.4855
	step [116/244], loss=18.4613
	step [117/244], loss=18.8574
	step [118/244], loss=18.9469
	step [119/244], loss=16.6126
	step [120/244], loss=16.5798
	step [121/244], loss=17.5832
	step [122/244], loss=15.9465
	step [123/244], loss=18.4357
	step [124/244], loss=17.7066
	step [125/244], loss=19.1150
	step [126/244], loss=19.6769
	step [127/244], loss=17.9911
	step [128/244], loss=17.5502
	step [129/244], loss=18.5410
	step [130/244], loss=18.3028
	step [131/244], loss=18.1282
	step [132/244], loss=18.8939
	step [133/244], loss=16.3664
	step [134/244], loss=17.0447
	step [135/244], loss=18.0079
	step [136/244], loss=21.2128
	step [137/244], loss=17.1580
	step [138/244], loss=16.6490
	step [139/244], loss=17.6088
	step [140/244], loss=17.1488
	step [141/244], loss=18.2475
	step [142/244], loss=18.0999
	step [143/244], loss=17.4784
	step [144/244], loss=18.4311
	step [145/244], loss=19.5950
	step [146/244], loss=18.0777
	step [147/244], loss=17.6187
	step [148/244], loss=17.9121
	step [149/244], loss=16.2210
	step [150/244], loss=16.9888
	step [151/244], loss=18.5464
	step [152/244], loss=16.6273
	step [153/244], loss=17.6349
	step [154/244], loss=18.1925
	step [155/244], loss=15.6038
	step [156/244], loss=15.6998
	step [157/244], loss=16.6851
	step [158/244], loss=17.4926
	step [159/244], loss=19.6268
	step [160/244], loss=19.3109
	step [161/244], loss=17.7461
	step [162/244], loss=17.0803
	step [163/244], loss=18.5657
	step [164/244], loss=17.3305
	step [165/244], loss=17.0515
	step [166/244], loss=18.6266
	step [167/244], loss=18.9119
	step [168/244], loss=17.2946
	step [169/244], loss=19.0844
	step [170/244], loss=18.6145
	step [171/244], loss=16.7635
	step [172/244], loss=17.2979
	step [173/244], loss=18.2838
	step [174/244], loss=16.5317
	step [175/244], loss=18.4918
	step [176/244], loss=16.1451
	step [177/244], loss=16.5940
	step [178/244], loss=17.9405
	step [179/244], loss=17.8010
	step [180/244], loss=17.3640
	step [181/244], loss=15.5816
	step [182/244], loss=15.9523
	step [183/244], loss=20.6197
	step [184/244], loss=16.1071
	step [185/244], loss=17.2902
	step [186/244], loss=19.6819
	step [187/244], loss=17.4151
	step [188/244], loss=16.8094
	step [189/244], loss=17.8933
	step [190/244], loss=16.5032
	step [191/244], loss=17.8228
	step [192/244], loss=18.2534
	step [193/244], loss=18.8519
	step [194/244], loss=17.1027
	step [195/244], loss=17.5666
	step [196/244], loss=17.5346
	step [197/244], loss=17.6240
	step [198/244], loss=16.2500
	step [199/244], loss=18.8186
	step [200/244], loss=18.7624
	step [201/244], loss=16.1210
	step [202/244], loss=15.5185
	step [203/244], loss=17.0305
	step [204/244], loss=15.6349
	step [205/244], loss=17.4801
	step [206/244], loss=16.5793
	step [207/244], loss=18.6419
	step [208/244], loss=17.5891
	step [209/244], loss=18.2105
	step [210/244], loss=17.8444
	step [211/244], loss=16.9171
	step [212/244], loss=17.6369
	step [213/244], loss=18.2976
	step [214/244], loss=15.9701
	step [215/244], loss=16.2297
	step [216/244], loss=17.7807
	step [217/244], loss=18.3461
	step [218/244], loss=19.5065
	step [219/244], loss=17.4849
	step [220/244], loss=16.5324
	step [221/244], loss=18.8615
	step [222/244], loss=17.6357
	step [223/244], loss=16.2543
	step [224/244], loss=16.0974
	step [225/244], loss=18.1619
	step [226/244], loss=16.1654
	step [227/244], loss=17.4340
	step [228/244], loss=16.7193
	step [229/244], loss=17.5764
	step [230/244], loss=18.0100
	step [231/244], loss=16.1103
	step [232/244], loss=15.4975
	step [233/244], loss=19.9476
	step [234/244], loss=16.4038
	step [235/244], loss=16.7211
	step [236/244], loss=15.7544
	step [237/244], loss=17.6990
	step [238/244], loss=15.7749
	step [239/244], loss=16.5259
	step [240/244], loss=17.2529
	step [241/244], loss=16.5995
	step [242/244], loss=17.0179
	step [243/244], loss=19.1541
	step [244/244], loss=0.8158
	Evaluating
	loss=0.0591, precision=0.1453, recall=0.9984, f1=0.2536
Training epoch 9
	step [1/244], loss=16.6930
	step [2/244], loss=19.2361
	step [3/244], loss=17.5530
	step [4/244], loss=16.9828
	step [5/244], loss=15.4076
	step [6/244], loss=16.3061
	step [7/244], loss=17.3861
	step [8/244], loss=16.9003
	step [9/244], loss=18.6147
	step [10/244], loss=19.5587
	step [11/244], loss=17.2012
	step [12/244], loss=17.6357
	step [13/244], loss=17.1567
	step [14/244], loss=17.3962
	step [15/244], loss=15.7959
	step [16/244], loss=15.6669
	step [17/244], loss=17.2929
	step [18/244], loss=17.1477
	step [19/244], loss=16.0877
	step [20/244], loss=16.3724
	step [21/244], loss=21.1310
	step [22/244], loss=14.7670
	step [23/244], loss=16.3795
	step [24/244], loss=15.7387
	step [25/244], loss=15.5303
	step [26/244], loss=15.8032
	step [27/244], loss=16.2052
	step [28/244], loss=16.1616
	step [29/244], loss=15.3181
	step [30/244], loss=16.2762
	step [31/244], loss=17.4232
	step [32/244], loss=17.2848
	step [33/244], loss=16.2010
	step [34/244], loss=17.5784
	step [35/244], loss=17.4174
	step [36/244], loss=16.2478
	step [37/244], loss=15.7168
	step [38/244], loss=15.4010
	step [39/244], loss=17.7084
	step [40/244], loss=14.9291
	step [41/244], loss=17.5398
	step [42/244], loss=16.6274
	step [43/244], loss=15.4588
	step [44/244], loss=14.5235
	step [45/244], loss=17.0097
	step [46/244], loss=16.4974
	step [47/244], loss=15.7343
	step [48/244], loss=15.4753
	step [49/244], loss=15.3210
	step [50/244], loss=16.3362
	step [51/244], loss=14.9685
	step [52/244], loss=13.7789
	step [53/244], loss=15.9486
	step [54/244], loss=16.8776
	step [55/244], loss=17.6544
	step [56/244], loss=18.1235
	step [57/244], loss=16.6313
	step [58/244], loss=16.9375
	step [59/244], loss=16.2922
	step [60/244], loss=15.9894
	step [61/244], loss=15.5421
	step [62/244], loss=15.8624
	step [63/244], loss=15.0291
	step [64/244], loss=16.0782
	step [65/244], loss=16.5643
	step [66/244], loss=18.2833
	step [67/244], loss=18.4009
	step [68/244], loss=16.3542
	step [69/244], loss=16.8186
	step [70/244], loss=17.8922
	step [71/244], loss=17.5625
	step [72/244], loss=16.3437
	step [73/244], loss=15.3087
	step [74/244], loss=15.8096
	step [75/244], loss=15.8087
	step [76/244], loss=16.6721
	step [77/244], loss=17.0628
	step [78/244], loss=15.2492
	step [79/244], loss=16.2849
	step [80/244], loss=15.9072
	step [81/244], loss=17.4877
	step [82/244], loss=16.1936
	step [83/244], loss=16.8423
	step [84/244], loss=16.6707
	step [85/244], loss=17.2714
	step [86/244], loss=16.1846
	step [87/244], loss=16.6498
	step [88/244], loss=16.7890
	step [89/244], loss=15.1006
	step [90/244], loss=16.6475
	step [91/244], loss=14.4604
	step [92/244], loss=15.4781
	step [93/244], loss=16.1678
	step [94/244], loss=14.9182
	step [95/244], loss=17.1548
	step [96/244], loss=14.6075
	step [97/244], loss=15.9908
	step [98/244], loss=15.6700
	step [99/244], loss=14.8255
	step [100/244], loss=14.5083
	step [101/244], loss=15.8617
	step [102/244], loss=15.9003
	step [103/244], loss=15.5954
	step [104/244], loss=15.9310
	step [105/244], loss=15.0290
	step [106/244], loss=18.7905
	step [107/244], loss=14.5273
	step [108/244], loss=16.2419
	step [109/244], loss=16.2329
	step [110/244], loss=15.1492
	step [111/244], loss=16.1774
	step [112/244], loss=17.5010
	step [113/244], loss=14.5462
	step [114/244], loss=13.6238
	step [115/244], loss=17.3325
	step [116/244], loss=16.2129
	step [117/244], loss=14.9762
	step [118/244], loss=18.0912
	step [119/244], loss=14.8066
	step [120/244], loss=17.7079
	step [121/244], loss=15.7258
	step [122/244], loss=14.8108
	step [123/244], loss=15.4930
	step [124/244], loss=14.3758
	step [125/244], loss=17.4770
	step [126/244], loss=17.6054
	step [127/244], loss=15.7343
	step [128/244], loss=15.0305
	step [129/244], loss=17.4793
	step [130/244], loss=17.0067
	step [131/244], loss=17.7894
	step [132/244], loss=15.6823
	step [133/244], loss=17.1163
	step [134/244], loss=15.4038
	step [135/244], loss=18.5527
	step [136/244], loss=14.5973
	step [137/244], loss=16.3229
	step [138/244], loss=15.7793
	step [139/244], loss=17.0354
	step [140/244], loss=16.6574
	step [141/244], loss=13.2243
	step [142/244], loss=16.7760
	step [143/244], loss=16.1721
	step [144/244], loss=15.4095
	step [145/244], loss=20.3846
	step [146/244], loss=15.5602
	step [147/244], loss=14.7879
	step [148/244], loss=16.4070
	step [149/244], loss=15.5716
	step [150/244], loss=16.6618
	step [151/244], loss=15.7866
	step [152/244], loss=16.0608
	step [153/244], loss=13.1630
	step [154/244], loss=16.0244
	step [155/244], loss=16.0411
	step [156/244], loss=16.3516
	step [157/244], loss=16.8824
	step [158/244], loss=14.8510
	step [159/244], loss=15.4060
	step [160/244], loss=14.2522
	step [161/244], loss=15.3563
	step [162/244], loss=16.5181
	step [163/244], loss=17.1714
	step [164/244], loss=14.9831
	step [165/244], loss=16.4864
	step [166/244], loss=14.6955
	step [167/244], loss=14.9603
	step [168/244], loss=16.1613
	step [169/244], loss=15.8005
	step [170/244], loss=16.1074
	step [171/244], loss=15.1899
	step [172/244], loss=16.2192
	step [173/244], loss=14.4891
	step [174/244], loss=14.4281
	step [175/244], loss=15.5537
	step [176/244], loss=15.7532
	step [177/244], loss=13.8647
	step [178/244], loss=16.5190
	step [179/244], loss=15.7342
	step [180/244], loss=14.1442
	step [181/244], loss=14.9863
	step [182/244], loss=14.0877
	step [183/244], loss=14.8276
	step [184/244], loss=14.7251
	step [185/244], loss=14.5396
	step [186/244], loss=18.5164
	step [187/244], loss=14.3882
	step [188/244], loss=16.0743
	step [189/244], loss=15.5831
	step [190/244], loss=14.7666
	step [191/244], loss=15.3709
	step [192/244], loss=14.1826
	step [193/244], loss=14.7614
	step [194/244], loss=14.3931
	step [195/244], loss=17.4596
	step [196/244], loss=15.1958
	step [197/244], loss=14.9648
	step [198/244], loss=15.4953
	step [199/244], loss=16.4885
	step [200/244], loss=15.0847
	step [201/244], loss=13.9012
	step [202/244], loss=16.1953
	step [203/244], loss=15.4746
	step [204/244], loss=15.4040
	step [205/244], loss=17.6310
	step [206/244], loss=15.2717
	step [207/244], loss=13.7779
	step [208/244], loss=15.2945
	step [209/244], loss=13.2251
	step [210/244], loss=13.6471
	step [211/244], loss=14.3562
	step [212/244], loss=16.5860
	step [213/244], loss=14.3891
	step [214/244], loss=14.8125
	step [215/244], loss=14.0250
	step [216/244], loss=14.4228
	step [217/244], loss=15.5946
	step [218/244], loss=13.4800
	step [219/244], loss=14.8595
	step [220/244], loss=15.1110
	step [221/244], loss=16.0811
	step [222/244], loss=14.2512
	step [223/244], loss=17.3910
	step [224/244], loss=18.0408
	step [225/244], loss=12.4250
	step [226/244], loss=16.4368
	step [227/244], loss=14.6194
	step [228/244], loss=14.8284
	step [229/244], loss=15.7240
	step [230/244], loss=15.6597
	step [231/244], loss=15.9920
	step [232/244], loss=16.0182
	step [233/244], loss=17.9840
	step [234/244], loss=15.1431
	step [235/244], loss=15.8509
	step [236/244], loss=14.6316
	step [237/244], loss=16.9592
	step [238/244], loss=17.1558
	step [239/244], loss=14.1911
	step [240/244], loss=15.5594
	step [241/244], loss=14.0354
	step [242/244], loss=17.9762
	step [243/244], loss=13.5888
	step [244/244], loss=1.2314
	Evaluating
	loss=0.0531, precision=0.1552, recall=0.9982, f1=0.2687
Training epoch 10
	step [1/244], loss=15.3714
	step [2/244], loss=18.1398
	step [3/244], loss=14.7825
	step [4/244], loss=13.2603
	step [5/244], loss=13.8860
	step [6/244], loss=16.7929
	step [7/244], loss=15.1368
	step [8/244], loss=15.9376
	step [9/244], loss=13.8865
	step [10/244], loss=15.2905
	step [11/244], loss=13.9933
	step [12/244], loss=15.6958
	step [13/244], loss=14.9863
	step [14/244], loss=15.1212
	step [15/244], loss=15.8418
	step [16/244], loss=13.8316
	step [17/244], loss=14.0273
	step [18/244], loss=12.3872
	step [19/244], loss=13.6290
	step [20/244], loss=15.3795
	step [21/244], loss=15.8565
	step [22/244], loss=14.6858
	step [23/244], loss=14.0771
	step [24/244], loss=14.8096
	step [25/244], loss=14.6919
	step [26/244], loss=13.7789
	step [27/244], loss=14.1478
	step [28/244], loss=13.7835
	step [29/244], loss=14.1636
	step [30/244], loss=14.1566
	step [31/244], loss=13.0612
	step [32/244], loss=16.5309
	step [33/244], loss=13.6879
	step [34/244], loss=15.1471
	step [35/244], loss=13.8277
	step [36/244], loss=14.0498
	step [37/244], loss=14.3555
	step [38/244], loss=14.6818
	step [39/244], loss=15.8378
	step [40/244], loss=13.0246
	step [41/244], loss=15.0973
	step [42/244], loss=14.2201
	step [43/244], loss=13.6807
	step [44/244], loss=13.4373
	step [45/244], loss=12.7768
	step [46/244], loss=15.9290
	step [47/244], loss=14.4582
	step [48/244], loss=16.7109
	step [49/244], loss=15.5601
	step [50/244], loss=14.6582
	step [51/244], loss=14.3152
	step [52/244], loss=14.2421
	step [53/244], loss=15.6152
	step [54/244], loss=14.1736
	step [55/244], loss=14.7542
	step [56/244], loss=14.0240
	step [57/244], loss=14.2674
	step [58/244], loss=15.2193
	step [59/244], loss=13.6115
	step [60/244], loss=16.0482
	step [61/244], loss=15.1598
	step [62/244], loss=15.2897
	step [63/244], loss=13.3512
	step [64/244], loss=14.4825
	step [65/244], loss=14.3875
	step [66/244], loss=13.1941
	step [67/244], loss=14.4560
	step [68/244], loss=12.6445
	step [69/244], loss=13.4083
	step [70/244], loss=15.0118
	step [71/244], loss=15.4075
	step [72/244], loss=13.4951
	step [73/244], loss=14.5267
	step [74/244], loss=14.1502
	step [75/244], loss=15.6091
	step [76/244], loss=15.8591
	step [77/244], loss=15.8512
	step [78/244], loss=13.4298
	step [79/244], loss=16.0475
	step [80/244], loss=13.0555
	step [81/244], loss=14.2542
	step [82/244], loss=15.6837
	step [83/244], loss=14.8498
	step [84/244], loss=14.5797
	step [85/244], loss=14.0124
	step [86/244], loss=13.2557
	step [87/244], loss=13.6431
	step [88/244], loss=14.4373
	step [89/244], loss=13.5846
	step [90/244], loss=12.8915
	step [91/244], loss=14.2139
	step [92/244], loss=12.9522
	step [93/244], loss=14.1124
	step [94/244], loss=14.3881
	step [95/244], loss=15.0205
	step [96/244], loss=16.8115
	step [97/244], loss=14.7413
	step [98/244], loss=14.5059
	step [99/244], loss=13.2219
	step [100/244], loss=15.1536
	step [101/244], loss=13.0285
	step [102/244], loss=14.4533
	step [103/244], loss=15.2898
	step [104/244], loss=13.1874
	step [105/244], loss=14.9952
	step [106/244], loss=13.8809
	step [107/244], loss=13.6246
	step [108/244], loss=12.2776
	step [109/244], loss=14.0791
	step [110/244], loss=14.6201
	step [111/244], loss=15.8804
	step [112/244], loss=16.7448
	step [113/244], loss=13.6171
	step [114/244], loss=16.4481
	step [115/244], loss=14.6878
	step [116/244], loss=13.2909
	step [117/244], loss=15.4431
	step [118/244], loss=16.5272
	step [119/244], loss=14.1756
	step [120/244], loss=12.7742
	step [121/244], loss=12.9930
	step [122/244], loss=16.3989
	step [123/244], loss=16.6934
	step [124/244], loss=14.2390
	step [125/244], loss=16.4083
	step [126/244], loss=14.1465
	step [127/244], loss=15.0055
	step [128/244], loss=14.3974
	step [129/244], loss=14.3932
	step [130/244], loss=13.8176
	step [131/244], loss=14.1003
	step [132/244], loss=13.4172
	step [133/244], loss=13.9892
	step [134/244], loss=13.5985
	step [135/244], loss=14.0029
	step [136/244], loss=15.4447
	step [137/244], loss=13.7400
	step [138/244], loss=14.3474
	step [139/244], loss=14.2446
	step [140/244], loss=13.2495
	step [141/244], loss=16.6434
	step [142/244], loss=13.4754
	step [143/244], loss=15.2228
	step [144/244], loss=12.9709
	step [145/244], loss=13.6852
	step [146/244], loss=12.6654
	step [147/244], loss=14.3014
	step [148/244], loss=13.2486
	step [149/244], loss=15.9830
	step [150/244], loss=13.4887
	step [151/244], loss=13.4929
	step [152/244], loss=14.1985
	step [153/244], loss=14.1768
	step [154/244], loss=15.9256
	step [155/244], loss=13.8021
	step [156/244], loss=14.8839
	step [157/244], loss=13.7496
	step [158/244], loss=12.0950
	step [159/244], loss=12.7956
	step [160/244], loss=16.6713
	step [161/244], loss=15.4138
	step [162/244], loss=17.5694
	step [163/244], loss=13.2688
	step [164/244], loss=13.8692
	step [165/244], loss=13.7128
	step [166/244], loss=14.6754
	step [167/244], loss=13.3651
	step [168/244], loss=14.8016
	step [169/244], loss=15.6436
	step [170/244], loss=14.3379
	step [171/244], loss=14.5131
	step [172/244], loss=11.4649
	step [173/244], loss=14.3898
	step [174/244], loss=13.7807
	step [175/244], loss=13.3563
	step [176/244], loss=12.2698
	step [177/244], loss=13.5798
	step [178/244], loss=13.0267
	step [179/244], loss=12.1071
	step [180/244], loss=14.0457
	step [181/244], loss=16.9267
	step [182/244], loss=13.8440
	step [183/244], loss=14.4998
	step [184/244], loss=13.3848
	step [185/244], loss=11.7307
	step [186/244], loss=14.5755
	step [187/244], loss=11.3792
	step [188/244], loss=15.4784
	step [189/244], loss=15.4545
	step [190/244], loss=13.5402
	step [191/244], loss=12.9158
	step [192/244], loss=16.5005
	step [193/244], loss=12.6122
	step [194/244], loss=14.0853
	step [195/244], loss=13.7133
	step [196/244], loss=15.3010
	step [197/244], loss=13.9610
	step [198/244], loss=15.4057
	step [199/244], loss=14.3535
	step [200/244], loss=12.3213
	step [201/244], loss=12.6672
	step [202/244], loss=13.6820
	step [203/244], loss=14.1427
	step [204/244], loss=13.5313
	step [205/244], loss=15.1655
	step [206/244], loss=13.1782
	step [207/244], loss=14.1756
	step [208/244], loss=13.1746
	step [209/244], loss=12.7767
	step [210/244], loss=14.1414
	step [211/244], loss=14.1880
	step [212/244], loss=13.5994
	step [213/244], loss=14.1683
	step [214/244], loss=13.1229
	step [215/244], loss=16.1628
	step [216/244], loss=13.8574
	step [217/244], loss=13.6021
	step [218/244], loss=15.2087
	step [219/244], loss=14.2714
	step [220/244], loss=14.1431
	step [221/244], loss=13.6462
	step [222/244], loss=15.1271
	step [223/244], loss=13.1677
	step [224/244], loss=12.1353
	step [225/244], loss=17.1222
	step [226/244], loss=11.6977
	step [227/244], loss=13.7806
	step [228/244], loss=15.8605
	step [229/244], loss=15.6813
	step [230/244], loss=15.1260
	step [231/244], loss=14.8583
	step [232/244], loss=14.4031
	step [233/244], loss=12.9959
	step [234/244], loss=14.6264
	step [235/244], loss=14.3889
	step [236/244], loss=11.3107
	step [237/244], loss=12.8227
	step [238/244], loss=12.6830
	step [239/244], loss=12.6651
	step [240/244], loss=13.6663
	step [241/244], loss=14.8763
	step [242/244], loss=13.1474
	step [243/244], loss=15.1483
	step [244/244], loss=1.4525
	Evaluating
	loss=0.0473, precision=0.1524, recall=0.9982, f1=0.2644
Training epoch 11
	step [1/244], loss=13.8626
	step [2/244], loss=12.3902
	step [3/244], loss=13.8780
	step [4/244], loss=14.6312
	step [5/244], loss=14.2629
	step [6/244], loss=13.7139
	step [7/244], loss=11.8163
	step [8/244], loss=12.9542
	step [9/244], loss=13.3465
	step [10/244], loss=12.8945
	step [11/244], loss=14.3280
	step [12/244], loss=13.5265
	step [13/244], loss=13.0795
	step [14/244], loss=15.7867
	step [15/244], loss=15.0566
	step [16/244], loss=14.2393
	step [17/244], loss=12.6605
	step [18/244], loss=12.3136
	step [19/244], loss=14.5801
	step [20/244], loss=12.6246
	step [21/244], loss=12.4706
	step [22/244], loss=13.1930
	step [23/244], loss=13.0774
	step [24/244], loss=13.4299
	step [25/244], loss=12.8215
	step [26/244], loss=14.3355
	step [27/244], loss=12.8262
	step [28/244], loss=13.1575
	step [29/244], loss=13.3159
	step [30/244], loss=11.7763
	step [31/244], loss=12.9420
	step [32/244], loss=14.4104
	step [33/244], loss=13.2778
	step [34/244], loss=13.0170
	step [35/244], loss=14.8808
	step [36/244], loss=13.6300
	step [37/244], loss=14.1062
	step [38/244], loss=13.2481
	step [39/244], loss=12.6624
	step [40/244], loss=14.5574
	step [41/244], loss=12.8002
	step [42/244], loss=12.2679
	step [43/244], loss=12.8617
	step [44/244], loss=15.1773
	step [45/244], loss=14.1758
	step [46/244], loss=13.7173
	step [47/244], loss=12.3536
	step [48/244], loss=11.4783
	step [49/244], loss=14.9337
	step [50/244], loss=12.4468
	step [51/244], loss=14.2693
	step [52/244], loss=14.1768
	step [53/244], loss=14.5158
	step [54/244], loss=11.6657
	step [55/244], loss=11.5926
	step [56/244], loss=12.5421
	step [57/244], loss=11.2726
	step [58/244], loss=14.5431
	step [59/244], loss=13.9403
	step [60/244], loss=13.6605
	step [61/244], loss=12.6849
	step [62/244], loss=15.2636
	step [63/244], loss=13.9377
	step [64/244], loss=14.4705
	step [65/244], loss=13.9583
	step [66/244], loss=15.5853
	step [67/244], loss=13.7143
	step [68/244], loss=11.6816
	step [69/244], loss=14.2401
	step [70/244], loss=12.7663
	step [71/244], loss=13.6844
	step [72/244], loss=13.1026
	step [73/244], loss=12.7157
	step [74/244], loss=13.8904
	step [75/244], loss=14.1465
	step [76/244], loss=15.5362
	step [77/244], loss=14.7259
	step [78/244], loss=13.8162
	step [79/244], loss=13.5701
	step [80/244], loss=15.3792
	step [81/244], loss=13.8005
	step [82/244], loss=12.2023
	step [83/244], loss=13.7735
	step [84/244], loss=14.9226
	step [85/244], loss=11.9797
	step [86/244], loss=11.5233
	step [87/244], loss=14.6233
	step [88/244], loss=12.4112
	step [89/244], loss=14.0189
	step [90/244], loss=13.2001
	step [91/244], loss=16.7966
	step [92/244], loss=13.5343
	step [93/244], loss=13.7516
	step [94/244], loss=14.5456
	step [95/244], loss=13.0735
	step [96/244], loss=14.3298
	step [97/244], loss=13.3341
	step [98/244], loss=12.1707
	step [99/244], loss=11.6328
	step [100/244], loss=13.8243
	step [101/244], loss=14.8536
	step [102/244], loss=14.2033
	step [103/244], loss=14.4371
	step [104/244], loss=12.4844
	step [105/244], loss=11.7421
	step [106/244], loss=11.3987
	step [107/244], loss=12.2312
	step [108/244], loss=12.8718
	step [109/244], loss=12.4742
	step [110/244], loss=12.4138
	step [111/244], loss=12.2531
	step [112/244], loss=11.4306
	step [113/244], loss=13.7566
	step [114/244], loss=14.0356
	step [115/244], loss=13.9212
	step [116/244], loss=13.0510
	step [117/244], loss=11.5953
	step [118/244], loss=12.2361
	step [119/244], loss=12.3640
	step [120/244], loss=13.2050
	step [121/244], loss=11.7568
	step [122/244], loss=11.4590
	step [123/244], loss=13.4144
	step [124/244], loss=11.3655
	step [125/244], loss=12.7441
	step [126/244], loss=14.3215
	step [127/244], loss=14.3335
	step [128/244], loss=12.2582
	step [129/244], loss=12.6166
	step [130/244], loss=11.9110
	step [131/244], loss=10.7365
	step [132/244], loss=14.1586
	step [133/244], loss=12.8832
	step [134/244], loss=12.0112
	step [135/244], loss=13.7022
	step [136/244], loss=11.7009
	step [137/244], loss=11.7167
	step [138/244], loss=13.9839
	step [139/244], loss=12.0484
	step [140/244], loss=13.0638
	step [141/244], loss=13.9018
	step [142/244], loss=13.8779
	step [143/244], loss=12.9709
	step [144/244], loss=10.6498
	step [145/244], loss=12.7252
	step [146/244], loss=12.7513
	step [147/244], loss=11.6624
	step [148/244], loss=13.2908
	step [149/244], loss=14.2653
	step [150/244], loss=13.0552
	step [151/244], loss=12.9226
	step [152/244], loss=13.0429
	step [153/244], loss=12.4111
	step [154/244], loss=12.7984
	step [155/244], loss=12.9249
	step [156/244], loss=13.1716
	step [157/244], loss=11.8542
	step [158/244], loss=15.8482
	step [159/244], loss=11.9653
	step [160/244], loss=12.7066
	step [161/244], loss=12.4092
	step [162/244], loss=13.6704
	step [163/244], loss=12.4655
	step [164/244], loss=11.5724
	step [165/244], loss=11.6118
	step [166/244], loss=11.0687
	step [167/244], loss=12.4588
	step [168/244], loss=11.0691
	step [169/244], loss=12.9070
	step [170/244], loss=12.5265
	step [171/244], loss=12.7919
	step [172/244], loss=13.2546
	step [173/244], loss=13.3635
	step [174/244], loss=12.8668
	step [175/244], loss=11.0442
	step [176/244], loss=13.3224
	step [177/244], loss=12.4911
	step [178/244], loss=11.8410
	step [179/244], loss=11.1711
	step [180/244], loss=10.7617
	step [181/244], loss=12.6061
	step [182/244], loss=11.5470
	step [183/244], loss=13.4151
	step [184/244], loss=12.9745
	step [185/244], loss=14.7035
	step [186/244], loss=11.7603
	step [187/244], loss=14.3380
	step [188/244], loss=12.4420
	step [189/244], loss=12.8289
	step [190/244], loss=11.9767
	step [191/244], loss=12.3515
	step [192/244], loss=12.7480
	step [193/244], loss=10.9210
	step [194/244], loss=12.3489
	step [195/244], loss=12.3486
	step [196/244], loss=12.8061
	step [197/244], loss=13.4065
	step [198/244], loss=14.3462
	step [199/244], loss=14.2700
	step [200/244], loss=11.3951
	step [201/244], loss=12.7995
	step [202/244], loss=13.3659
	step [203/244], loss=13.0987
	step [204/244], loss=13.1691
	step [205/244], loss=12.6101
	step [206/244], loss=11.8278
	step [207/244], loss=12.6513
	step [208/244], loss=11.9733
	step [209/244], loss=13.2112
	step [210/244], loss=12.4585
	step [211/244], loss=12.5433
	step [212/244], loss=11.7870
	step [213/244], loss=13.0187
	step [214/244], loss=11.2236
	step [215/244], loss=13.8330
	step [216/244], loss=12.5373
	step [217/244], loss=12.1753
	step [218/244], loss=12.3066
	step [219/244], loss=12.0435
	step [220/244], loss=13.7825
	step [221/244], loss=10.5740
	step [222/244], loss=11.5162
	step [223/244], loss=13.7123
	step [224/244], loss=15.2889
	step [225/244], loss=12.2378
	step [226/244], loss=14.4370
	step [227/244], loss=12.1283
	step [228/244], loss=10.7935
	step [229/244], loss=11.2520
	step [230/244], loss=11.9166
	step [231/244], loss=13.9634
	step [232/244], loss=11.5941
	step [233/244], loss=14.8936
	step [234/244], loss=13.3912
	step [235/244], loss=12.3185
	step [236/244], loss=12.3842
	step [237/244], loss=10.8954
	step [238/244], loss=12.1313
	step [239/244], loss=12.0266
	step [240/244], loss=14.6724
	step [241/244], loss=12.2221
	step [242/244], loss=12.9453
	step [243/244], loss=13.0861
	step [244/244], loss=0.9628
	Evaluating
	loss=0.0393, precision=0.1689, recall=0.9979, f1=0.2889
saving model as: 2_saved_model.pth
Training epoch 12
	step [1/244], loss=13.6222
	step [2/244], loss=11.0932
	step [3/244], loss=11.7196
	step [4/244], loss=14.2252
	step [5/244], loss=11.7653
	step [6/244], loss=11.7825
	step [7/244], loss=13.7630
	step [8/244], loss=12.7701
	step [9/244], loss=13.6650
	step [10/244], loss=10.2994
	step [11/244], loss=11.8578
	step [12/244], loss=11.5032
	step [13/244], loss=10.9315
	step [14/244], loss=12.3629
	step [15/244], loss=13.0406
	step [16/244], loss=12.8654
	step [17/244], loss=13.4144
	step [18/244], loss=12.5139
	step [19/244], loss=12.4155
	step [20/244], loss=11.3525
	step [21/244], loss=12.6703
	step [22/244], loss=10.4276
	step [23/244], loss=11.0799
	step [24/244], loss=13.2344
	step [25/244], loss=11.6810
	step [26/244], loss=10.9458
	step [27/244], loss=13.8893
	step [28/244], loss=11.7410
	step [29/244], loss=15.2061
	step [30/244], loss=11.9499
	step [31/244], loss=13.4538
	step [32/244], loss=11.0504
	step [33/244], loss=12.3178
	step [34/244], loss=10.2642
	step [35/244], loss=12.3546
	step [36/244], loss=12.6157
	step [37/244], loss=11.7903
	step [38/244], loss=14.0082
	step [39/244], loss=13.9644
	step [40/244], loss=11.3125
	step [41/244], loss=10.8854
	step [42/244], loss=12.2469
	step [43/244], loss=14.1116
	step [44/244], loss=11.6254
	step [45/244], loss=14.0163
	step [46/244], loss=12.5761
	step [47/244], loss=12.1851
	step [48/244], loss=12.0719
	step [49/244], loss=11.6778
	step [50/244], loss=12.4614
	step [51/244], loss=13.9949
	step [52/244], loss=14.3233
	step [53/244], loss=11.5824
	step [54/244], loss=11.2759
	step [55/244], loss=13.4183
	step [56/244], loss=11.3974
	step [57/244], loss=12.7505
	step [58/244], loss=11.2330
	step [59/244], loss=12.3695
	step [60/244], loss=10.9518
	step [61/244], loss=11.5230
	step [62/244], loss=10.8304
	step [63/244], loss=10.1163
	step [64/244], loss=12.9896
	step [65/244], loss=11.0078
	step [66/244], loss=12.8954
	step [67/244], loss=11.5679
	step [68/244], loss=10.9273
	step [69/244], loss=11.3975
	step [70/244], loss=11.7154
	step [71/244], loss=12.1962
	step [72/244], loss=11.5827
	step [73/244], loss=12.1060
	step [74/244], loss=11.9302
	step [75/244], loss=12.9295
	step [76/244], loss=12.7936
	step [77/244], loss=11.7243
	step [78/244], loss=10.9988
	step [79/244], loss=12.6046
	step [80/244], loss=13.3001
	step [81/244], loss=10.6835
	step [82/244], loss=10.7421
	step [83/244], loss=12.5089
	step [84/244], loss=12.1995
	step [85/244], loss=12.0336
	step [86/244], loss=12.5500
	step [87/244], loss=11.6931
	step [88/244], loss=11.3277
	step [89/244], loss=11.2293
	step [90/244], loss=11.9632
	step [91/244], loss=14.6319
	step [92/244], loss=13.8258
	step [93/244], loss=11.9357
	step [94/244], loss=12.2680
	step [95/244], loss=13.4118
	step [96/244], loss=12.0321
	step [97/244], loss=11.4432
	step [98/244], loss=10.4601
	step [99/244], loss=11.7333
	step [100/244], loss=10.8267
	step [101/244], loss=11.0971
	step [102/244], loss=12.9632
	step [103/244], loss=11.4056
	step [104/244], loss=11.2021
	step [105/244], loss=10.5196
	step [106/244], loss=10.1633
	step [107/244], loss=13.7435
	step [108/244], loss=12.3425
	step [109/244], loss=12.1589
	step [110/244], loss=10.9362
	step [111/244], loss=11.9874
	step [112/244], loss=13.3841
	step [113/244], loss=11.1604
	step [114/244], loss=10.9183
	step [115/244], loss=11.0227
	step [116/244], loss=14.5905
	step [117/244], loss=14.3376
	step [118/244], loss=11.3567
	step [119/244], loss=11.5564
	step [120/244], loss=12.7835
	step [121/244], loss=12.3174
	step [122/244], loss=12.1852
	step [123/244], loss=12.6005
	step [124/244], loss=13.5374
	step [125/244], loss=10.5174
	step [126/244], loss=11.2557
	step [127/244], loss=11.9851
	step [128/244], loss=11.7624
	step [129/244], loss=12.0201
	step [130/244], loss=12.0840
	step [131/244], loss=11.6622
	step [132/244], loss=11.6246
	step [133/244], loss=12.6964
	step [134/244], loss=12.1169
	step [135/244], loss=10.9126
	step [136/244], loss=11.2903
	step [137/244], loss=14.7285
	step [138/244], loss=11.4803
	step [139/244], loss=11.8099
	step [140/244], loss=11.4937
	step [141/244], loss=11.9010
	step [142/244], loss=10.9001
	step [143/244], loss=11.6134
	step [144/244], loss=11.2530
	step [145/244], loss=12.6632
	step [146/244], loss=12.4343
	step [147/244], loss=10.9486
	step [148/244], loss=11.4455
	step [149/244], loss=12.2388
	step [150/244], loss=9.9651
	step [151/244], loss=9.9442
	step [152/244], loss=12.5096
	step [153/244], loss=12.4810
	step [154/244], loss=10.4372
	step [155/244], loss=11.9017
	step [156/244], loss=14.2926
	step [157/244], loss=12.9365
	step [158/244], loss=11.8411
	step [159/244], loss=12.8072
	step [160/244], loss=12.4364
	step [161/244], loss=11.4381
	step [162/244], loss=13.7613
	step [163/244], loss=14.1231
	step [164/244], loss=11.2232
	step [165/244], loss=11.4229
	step [166/244], loss=10.8806
	step [167/244], loss=9.9413
	step [168/244], loss=12.1004
	step [169/244], loss=12.2039
	step [170/244], loss=13.2196
	step [171/244], loss=9.8718
	step [172/244], loss=11.2759
	step [173/244], loss=10.0220
	step [174/244], loss=12.5101
	step [175/244], loss=12.3021
	step [176/244], loss=11.4908
	step [177/244], loss=11.5909
	step [178/244], loss=12.3948
	step [179/244], loss=11.6874
	step [180/244], loss=13.5814
	step [181/244], loss=10.7398
	step [182/244], loss=12.2173
	step [183/244], loss=11.5661
	step [184/244], loss=11.3003
	step [185/244], loss=10.3068
	step [186/244], loss=12.0032
	step [187/244], loss=11.5080
	step [188/244], loss=12.7543
	step [189/244], loss=12.5064
	step [190/244], loss=12.2648
	step [191/244], loss=11.4893
	step [192/244], loss=11.7589
	step [193/244], loss=12.1986
	step [194/244], loss=12.0161
	step [195/244], loss=12.5537
	step [196/244], loss=11.9104
	step [197/244], loss=9.9869
	step [198/244], loss=13.3083
	step [199/244], loss=13.0235
	step [200/244], loss=12.8473
	step [201/244], loss=12.0557
	step [202/244], loss=13.7978
	step [203/244], loss=10.6801
	step [204/244], loss=13.8095
	step [205/244], loss=13.1523
	step [206/244], loss=11.5531
	step [207/244], loss=11.7760
	step [208/244], loss=13.6182
	step [209/244], loss=13.4136
	step [210/244], loss=10.5172
	step [211/244], loss=11.8363
	step [212/244], loss=11.8714
	step [213/244], loss=10.5088
	step [214/244], loss=12.3903
	step [215/244], loss=13.7001
	step [216/244], loss=10.6097
	step [217/244], loss=9.8543
	step [218/244], loss=10.4868
	step [219/244], loss=11.1118
	step [220/244], loss=12.5385
	step [221/244], loss=11.1046
	step [222/244], loss=14.9254
	step [223/244], loss=11.3530
	step [224/244], loss=9.6431
	step [225/244], loss=13.0270
	step [226/244], loss=11.3515
	step [227/244], loss=10.8694
	step [228/244], loss=12.2243
	step [229/244], loss=14.0164
	step [230/244], loss=13.0777
	step [231/244], loss=10.6662
	step [232/244], loss=11.1657
	step [233/244], loss=12.5283
	step [234/244], loss=12.3638
	step [235/244], loss=9.6503
	step [236/244], loss=10.6980
	step [237/244], loss=12.3571
	step [238/244], loss=14.2294
	step [239/244], loss=11.0253
	step [240/244], loss=10.5951
	step [241/244], loss=10.7938
	step [242/244], loss=11.2671
	step [243/244], loss=11.4620
	step [244/244], loss=0.5279
	Evaluating
	loss=0.0382, precision=0.1731, recall=0.9977, f1=0.2950
saving model as: 2_saved_model.pth
Training epoch 13
	step [1/244], loss=11.1518
	step [2/244], loss=12.2811
	step [3/244], loss=10.3589
	step [4/244], loss=11.1476
	step [5/244], loss=12.9996
	step [6/244], loss=10.6805
	step [7/244], loss=10.2272
	step [8/244], loss=13.5168
	step [9/244], loss=11.6264
	step [10/244], loss=10.6071
	step [11/244], loss=10.4514
	step [12/244], loss=11.3894
	step [13/244], loss=12.9294
	step [14/244], loss=10.4257
	step [15/244], loss=12.5776
	step [16/244], loss=11.5231
	step [17/244], loss=10.8302
	step [18/244], loss=11.4553
	step [19/244], loss=10.9605
	step [20/244], loss=12.6793
	step [21/244], loss=10.6871
	step [22/244], loss=10.6103
	step [23/244], loss=12.3900
	step [24/244], loss=12.4542
	step [25/244], loss=10.1412
	step [26/244], loss=10.4283
	step [27/244], loss=10.6177
	step [28/244], loss=11.1878
	step [29/244], loss=9.9692
	step [30/244], loss=13.9185
	step [31/244], loss=11.5303
	step [32/244], loss=11.0938
	step [33/244], loss=12.0961
	step [34/244], loss=11.5254
	step [35/244], loss=11.4825
	step [36/244], loss=11.1030
	step [37/244], loss=9.7289
	step [38/244], loss=10.3009
	step [39/244], loss=10.7715
	step [40/244], loss=14.1585
	step [41/244], loss=10.6682
	step [42/244], loss=10.0413
	step [43/244], loss=12.1409
	step [44/244], loss=14.8062
	step [45/244], loss=12.6472
	step [46/244], loss=11.7072
	step [47/244], loss=9.9847
	step [48/244], loss=12.3947
	step [49/244], loss=10.3282
	step [50/244], loss=12.7869
	step [51/244], loss=10.3644
	step [52/244], loss=12.4323
	step [53/244], loss=11.8599
	step [54/244], loss=11.1183
	step [55/244], loss=10.5337
	step [56/244], loss=12.4954
	step [57/244], loss=12.8834
	step [58/244], loss=13.3809
	step [59/244], loss=11.1493
	step [60/244], loss=10.3097
	step [61/244], loss=11.2281
	step [62/244], loss=11.8623
	step [63/244], loss=13.4016
	step [64/244], loss=10.3702
	step [65/244], loss=14.3790
	step [66/244], loss=10.1912
	step [67/244], loss=9.2314
	step [68/244], loss=12.5426
	step [69/244], loss=11.8007
	step [70/244], loss=10.6817
	step [71/244], loss=11.1668
	step [72/244], loss=11.0980
	step [73/244], loss=10.7280
	step [74/244], loss=10.9177
	step [75/244], loss=9.8196
	step [76/244], loss=15.8352
	step [77/244], loss=11.0205
	step [78/244], loss=10.8003
	step [79/244], loss=10.4003
	step [80/244], loss=11.9512
	step [81/244], loss=10.6313
	step [82/244], loss=13.2843
	step [83/244], loss=10.3090
	step [84/244], loss=10.2480
	step [85/244], loss=12.1165
	step [86/244], loss=10.4682
	step [87/244], loss=11.6966
	step [88/244], loss=13.3204
	step [89/244], loss=10.1276
	step [90/244], loss=11.5140
	step [91/244], loss=12.4882
	step [92/244], loss=11.3561
	step [93/244], loss=9.9492
	step [94/244], loss=12.6996
	step [95/244], loss=12.6340
	step [96/244], loss=12.1243
	step [97/244], loss=9.6382
	step [98/244], loss=12.1781
	step [99/244], loss=10.5635
	step [100/244], loss=11.9140
	step [101/244], loss=10.1726
	step [102/244], loss=10.3811
	step [103/244], loss=11.0603
	step [104/244], loss=9.4537
	step [105/244], loss=10.6371
	step [106/244], loss=10.9473
	step [107/244], loss=11.5943
	step [108/244], loss=12.1519
	step [109/244], loss=10.6215
	step [110/244], loss=12.1758
	step [111/244], loss=11.5808
	step [112/244], loss=10.5899
	step [113/244], loss=9.4680
	step [114/244], loss=11.3031
	step [115/244], loss=11.1001
	step [116/244], loss=11.3029
	step [117/244], loss=10.2705
	step [118/244], loss=11.0785
	step [119/244], loss=11.4161
	step [120/244], loss=9.9233
	step [121/244], loss=11.0482
	step [122/244], loss=10.8583
	step [123/244], loss=11.1852
	step [124/244], loss=10.1774
	step [125/244], loss=13.0541
	step [126/244], loss=10.3398
	step [127/244], loss=11.1166
	step [128/244], loss=9.5444
	step [129/244], loss=12.5266
	step [130/244], loss=10.2105
	step [131/244], loss=9.9285
	step [132/244], loss=10.2039
	step [133/244], loss=11.8748
	step [134/244], loss=11.7539
	step [135/244], loss=11.4762
	step [136/244], loss=10.8349
	step [137/244], loss=11.5814
	step [138/244], loss=11.6451
	step [139/244], loss=11.7746
	step [140/244], loss=11.5120
	step [141/244], loss=11.3906
	step [142/244], loss=10.3246
	step [143/244], loss=9.5058
	step [144/244], loss=11.4461
	step [145/244], loss=10.7158
	step [146/244], loss=11.0473
	step [147/244], loss=12.4164
	step [148/244], loss=12.6163
	step [149/244], loss=10.9993
	step [150/244], loss=9.8554
	step [151/244], loss=10.7918
	step [152/244], loss=11.6059
	step [153/244], loss=10.7874
	step [154/244], loss=13.6819
	step [155/244], loss=10.1464
	step [156/244], loss=10.2971
	step [157/244], loss=10.3646
	step [158/244], loss=10.9963
	step [159/244], loss=12.3184
	step [160/244], loss=10.0971
	step [161/244], loss=11.5405
	step [162/244], loss=13.5108
	step [163/244], loss=12.2551
	step [164/244], loss=14.2891
	step [165/244], loss=11.3923
	step [166/244], loss=10.8571
	step [167/244], loss=10.1298
	step [168/244], loss=10.1645
	step [169/244], loss=10.3190
	step [170/244], loss=11.1256
	step [171/244], loss=12.9459
	step [172/244], loss=10.5802
	step [173/244], loss=11.0853
	step [174/244], loss=13.2512
	step [175/244], loss=10.7342
	step [176/244], loss=11.5003
	step [177/244], loss=10.7613
	step [178/244], loss=13.3820
	step [179/244], loss=14.2544
	step [180/244], loss=9.9453
	step [181/244], loss=9.6312
	step [182/244], loss=11.2991
	step [183/244], loss=11.2814
	step [184/244], loss=10.3091
	step [185/244], loss=10.7345
	step [186/244], loss=11.3439
	step [187/244], loss=11.0878
	step [188/244], loss=10.6892
	step [189/244], loss=13.8734
	step [190/244], loss=11.9491
	step [191/244], loss=11.8921
	step [192/244], loss=11.4392
	step [193/244], loss=12.4095
	step [194/244], loss=13.5344
	step [195/244], loss=10.4594
	step [196/244], loss=11.8918
	step [197/244], loss=10.1786
	step [198/244], loss=11.8554
	step [199/244], loss=8.7808
	step [200/244], loss=10.3749
	step [201/244], loss=12.4501
	step [202/244], loss=11.2929
	step [203/244], loss=10.6294
	step [204/244], loss=12.0699
	step [205/244], loss=12.4253
	step [206/244], loss=14.4744
	step [207/244], loss=9.8118
	step [208/244], loss=11.9709
	step [209/244], loss=10.0816
	step [210/244], loss=9.9984
	step [211/244], loss=11.5486
	step [212/244], loss=10.2831
	step [213/244], loss=10.6911
	step [214/244], loss=10.2872
	step [215/244], loss=11.2484
	step [216/244], loss=11.4317
	step [217/244], loss=10.4387
	step [218/244], loss=10.1566
	step [219/244], loss=12.1027
	step [220/244], loss=9.6178
	step [221/244], loss=10.9284
	step [222/244], loss=10.7012
	step [223/244], loss=11.1730
	step [224/244], loss=10.7712
	step [225/244], loss=11.3623
	step [226/244], loss=10.1850
	step [227/244], loss=9.3206
	step [228/244], loss=10.8915
	step [229/244], loss=9.2629
	step [230/244], loss=12.2099
	step [231/244], loss=13.1110
	step [232/244], loss=10.1915
	step [233/244], loss=9.8831
	step [234/244], loss=10.5939
	step [235/244], loss=9.7568
	step [236/244], loss=11.0864
	step [237/244], loss=11.3947
	step [238/244], loss=9.8350
	step [239/244], loss=11.1273
	step [240/244], loss=9.6621
	step [241/244], loss=9.6063
	step [242/244], loss=11.3579
	step [243/244], loss=9.3402
	step [244/244], loss=1.1280
	Evaluating
	loss=0.0324, precision=0.1905, recall=0.9974, f1=0.3199
saving model as: 2_saved_model.pth
Training epoch 14
	step [1/244], loss=9.9820
	step [2/244], loss=11.7607
	step [3/244], loss=9.5380
	step [4/244], loss=11.5617
	step [5/244], loss=11.9572
	step [6/244], loss=9.1514
	step [7/244], loss=12.2041
	step [8/244], loss=10.0342
	step [9/244], loss=12.8820
	step [10/244], loss=10.5137
	step [11/244], loss=11.0558
	step [12/244], loss=11.0417
	step [13/244], loss=11.0924
	step [14/244], loss=10.6444
	step [15/244], loss=10.4890
	step [16/244], loss=11.0304
	step [17/244], loss=10.0830
	step [18/244], loss=12.0882
	step [19/244], loss=9.8363
	step [20/244], loss=11.5144
	step [21/244], loss=10.2779
	step [22/244], loss=11.7311
	step [23/244], loss=10.4876
	step [24/244], loss=10.9066
	step [25/244], loss=10.4405
	step [26/244], loss=10.9810
	step [27/244], loss=10.7091
	step [28/244], loss=10.6401
	step [29/244], loss=12.3974
	step [30/244], loss=10.1295
	step [31/244], loss=9.9928
	step [32/244], loss=10.3737
	step [33/244], loss=11.4472
	step [34/244], loss=11.1990
	step [35/244], loss=10.7994
	step [36/244], loss=12.6764
	step [37/244], loss=11.8599
	step [38/244], loss=12.3666
	step [39/244], loss=9.8701
	step [40/244], loss=11.6601
	step [41/244], loss=10.3117
	step [42/244], loss=11.2514
	step [43/244], loss=12.6735
	step [44/244], loss=9.8507
	step [45/244], loss=11.1064
	step [46/244], loss=10.0438
	step [47/244], loss=12.5868
	step [48/244], loss=9.9318
	step [49/244], loss=11.3077
	step [50/244], loss=11.9001
	step [51/244], loss=8.8202
	step [52/244], loss=11.0511
	step [53/244], loss=10.7265
	step [54/244], loss=10.2822
	step [55/244], loss=11.3598
	step [56/244], loss=9.4151
	step [57/244], loss=11.8155
	step [58/244], loss=10.9592
	step [59/244], loss=9.1713
	step [60/244], loss=10.6549
	step [61/244], loss=10.3412
	step [62/244], loss=11.0517
	step [63/244], loss=12.1095
	step [64/244], loss=10.5524
	step [65/244], loss=10.7967
	step [66/244], loss=10.0753
	step [67/244], loss=10.4468
	step [68/244], loss=10.3513
	step [69/244], loss=9.6393
	step [70/244], loss=9.4016
	step [71/244], loss=9.5245
	step [72/244], loss=9.6824
	step [73/244], loss=10.5408
	step [74/244], loss=9.9170
	step [75/244], loss=11.3449
	step [76/244], loss=11.4417
	step [77/244], loss=10.6139
	step [78/244], loss=11.5151
	step [79/244], loss=9.6317
	step [80/244], loss=9.2086
	step [81/244], loss=10.2431
	step [82/244], loss=10.1426
	step [83/244], loss=10.0513
	step [84/244], loss=10.8345
	step [85/244], loss=8.6926
	step [86/244], loss=11.6033
	step [87/244], loss=10.1866
	step [88/244], loss=9.2986
	step [89/244], loss=10.4962
	step [90/244], loss=9.9041
	step [91/244], loss=9.6108
	step [92/244], loss=10.5103
	step [93/244], loss=11.3325
	step [94/244], loss=10.4018
	step [95/244], loss=9.3407
	step [96/244], loss=10.4620
	step [97/244], loss=10.4393
	step [98/244], loss=11.1503
	step [99/244], loss=12.1579
	step [100/244], loss=9.8880
	step [101/244], loss=9.7545
	step [102/244], loss=11.7859
	step [103/244], loss=9.3417
	step [104/244], loss=8.8568
	step [105/244], loss=11.2468
	step [106/244], loss=9.5878
	step [107/244], loss=8.6985
	step [108/244], loss=8.4729
	step [109/244], loss=9.2986
	step [110/244], loss=11.0367
	step [111/244], loss=10.8699
	step [112/244], loss=10.2006
	step [113/244], loss=12.4618
	step [114/244], loss=9.9174
	step [115/244], loss=11.0233
	step [116/244], loss=12.0727
	step [117/244], loss=11.3827
	step [118/244], loss=10.2924
	step [119/244], loss=11.8814
	step [120/244], loss=13.1419
	step [121/244], loss=10.8934
	step [122/244], loss=11.1017
	step [123/244], loss=13.5684
	step [124/244], loss=12.0447
	step [125/244], loss=10.5922
	step [126/244], loss=10.2697
	step [127/244], loss=11.5318
	step [128/244], loss=10.7624
	step [129/244], loss=10.4570
	step [130/244], loss=9.8859
	step [131/244], loss=10.5742
	step [132/244], loss=11.8701
	step [133/244], loss=12.5990
	step [134/244], loss=10.8678
	step [135/244], loss=12.0155
	step [136/244], loss=10.4641
	step [137/244], loss=8.7846
	step [138/244], loss=10.8231
	step [139/244], loss=11.8381
	step [140/244], loss=10.8072
	step [141/244], loss=9.7169
	step [142/244], loss=9.9172
	step [143/244], loss=11.7243
	step [144/244], loss=9.4966
	step [145/244], loss=9.0111
	step [146/244], loss=10.1936
	step [147/244], loss=10.0566
	step [148/244], loss=11.0006
	step [149/244], loss=9.2278
	step [150/244], loss=9.9505
	step [151/244], loss=13.2392
	step [152/244], loss=11.0209
	step [153/244], loss=10.1377
	step [154/244], loss=9.7771
	step [155/244], loss=9.2233
	step [156/244], loss=10.7378
	step [157/244], loss=10.2272
	step [158/244], loss=10.4501
	step [159/244], loss=11.1672
	step [160/244], loss=9.2521
	step [161/244], loss=10.7667
	step [162/244], loss=9.7103
	step [163/244], loss=11.2037
	step [164/244], loss=9.6235
	step [165/244], loss=11.5173
	step [166/244], loss=10.7963
	step [167/244], loss=10.4985
	step [168/244], loss=9.7448
	step [169/244], loss=10.2176
	step [170/244], loss=10.4933
	step [171/244], loss=10.3923
	step [172/244], loss=10.7248
	step [173/244], loss=9.9110
	step [174/244], loss=10.4534
	step [175/244], loss=12.3943
	step [176/244], loss=9.8182
	step [177/244], loss=10.4744
	step [178/244], loss=11.4645
	step [179/244], loss=10.9938
	step [180/244], loss=10.3139
	step [181/244], loss=10.5503
	step [182/244], loss=10.7903
	step [183/244], loss=9.3986
	step [184/244], loss=11.2920
	step [185/244], loss=11.7951
	step [186/244], loss=9.7900
	step [187/244], loss=8.3857
	step [188/244], loss=11.4709
	step [189/244], loss=10.5347
	step [190/244], loss=10.9520
	step [191/244], loss=13.7020
	step [192/244], loss=10.9207
	step [193/244], loss=10.1009
	step [194/244], loss=9.2979
	step [195/244], loss=11.3107
	step [196/244], loss=8.6825
	step [197/244], loss=9.3771
	step [198/244], loss=9.6637
	step [199/244], loss=9.6265
	step [200/244], loss=12.0189
	step [201/244], loss=7.7565
	step [202/244], loss=11.1413
	step [203/244], loss=9.7623
	step [204/244], loss=13.3002
	step [205/244], loss=10.5936
	step [206/244], loss=9.2373
	step [207/244], loss=10.3105
	step [208/244], loss=10.1909
	step [209/244], loss=10.2071
	step [210/244], loss=12.1543
	step [211/244], loss=9.9651
	step [212/244], loss=10.6051
	step [213/244], loss=8.2935
	step [214/244], loss=8.8603
	step [215/244], loss=10.0069
	step [216/244], loss=9.2997
	step [217/244], loss=10.2554
	step [218/244], loss=9.1763
	step [219/244], loss=13.0058
	step [220/244], loss=10.5180
	step [221/244], loss=10.3797
	step [222/244], loss=9.1852
	step [223/244], loss=10.4355
	step [224/244], loss=11.2605
	step [225/244], loss=7.8822
	step [226/244], loss=10.9001
	step [227/244], loss=9.6940
	step [228/244], loss=9.6884
	step [229/244], loss=9.6497
	step [230/244], loss=9.8916
	step [231/244], loss=9.8053
	step [232/244], loss=9.7164
	step [233/244], loss=13.5146
	step [234/244], loss=9.4832
	step [235/244], loss=9.6679
	step [236/244], loss=10.0978
	step [237/244], loss=9.6608
	step [238/244], loss=10.2080
	step [239/244], loss=10.6871
	step [240/244], loss=9.2712
	step [241/244], loss=9.7204
	step [242/244], loss=9.3697
	step [243/244], loss=8.6707
	step [244/244], loss=0.6695
	Evaluating
	loss=0.0274, precision=0.2051, recall=0.9972, f1=0.3402
saving model as: 2_saved_model.pth
Training epoch 15
	step [1/244], loss=9.9716
	step [2/244], loss=12.0567
	step [3/244], loss=12.8131
	step [4/244], loss=8.1299
	step [5/244], loss=8.9406
	step [6/244], loss=11.7471
	step [7/244], loss=10.2275
	step [8/244], loss=9.2326
	step [9/244], loss=10.2980
	step [10/244], loss=12.2961
	step [11/244], loss=9.6823
	step [12/244], loss=8.4980
	step [13/244], loss=8.6219
	step [14/244], loss=8.4150
	step [15/244], loss=8.3631
	step [16/244], loss=8.5317
	step [17/244], loss=11.5285
	step [18/244], loss=13.4779
	step [19/244], loss=9.1761
	step [20/244], loss=10.0457
	step [21/244], loss=10.2444
	step [22/244], loss=10.1402
	step [23/244], loss=10.8829
	step [24/244], loss=8.8852
	step [25/244], loss=9.9040
	step [26/244], loss=10.3310
	step [27/244], loss=12.9090
	step [28/244], loss=8.4311
	step [29/244], loss=10.6394
	step [30/244], loss=11.0022
	step [31/244], loss=10.1819
	step [32/244], loss=9.8543
	step [33/244], loss=10.1217
	step [34/244], loss=9.2251
	step [35/244], loss=9.9092
	step [36/244], loss=10.3759
	step [37/244], loss=12.6864
	step [38/244], loss=11.3983
	step [39/244], loss=9.7267
	step [40/244], loss=11.3096
	step [41/244], loss=9.8637
	step [42/244], loss=10.0227
	step [43/244], loss=8.2007
	step [44/244], loss=10.3501
	step [45/244], loss=9.4486
	step [46/244], loss=9.4275
	step [47/244], loss=10.3431
	step [48/244], loss=10.6191
	step [49/244], loss=10.5517
	step [50/244], loss=8.0807
	step [51/244], loss=14.0242
	step [52/244], loss=8.9228
	step [53/244], loss=12.7969
	step [54/244], loss=10.0057
	step [55/244], loss=10.9722
	step [56/244], loss=10.0276
	step [57/244], loss=9.9186
	step [58/244], loss=9.6508
	step [59/244], loss=10.2829
	step [60/244], loss=10.1874
	step [61/244], loss=10.9657
	step [62/244], loss=9.6311
	step [63/244], loss=10.7854
	step [64/244], loss=11.7650
	step [65/244], loss=10.2935
	step [66/244], loss=10.7413
	step [67/244], loss=11.2309
	step [68/244], loss=10.0621
	step [69/244], loss=12.3717
	step [70/244], loss=10.3143
	step [71/244], loss=8.7989
	step [72/244], loss=8.5215
	step [73/244], loss=9.0662
	step [74/244], loss=11.1550
	step [75/244], loss=10.9833
	step [76/244], loss=10.0545
	step [77/244], loss=11.0562
	step [78/244], loss=9.4452
	step [79/244], loss=11.4272
	step [80/244], loss=9.7284
	step [81/244], loss=8.8095
	step [82/244], loss=9.5648
	step [83/244], loss=11.7403
	step [84/244], loss=12.8045
	step [85/244], loss=11.4444
	step [86/244], loss=9.3586
	step [87/244], loss=8.7301
	step [88/244], loss=9.7402
	step [89/244], loss=8.6070
	step [90/244], loss=9.8203
	step [91/244], loss=8.6249
	step [92/244], loss=9.2468
	step [93/244], loss=12.1429
	step [94/244], loss=9.5146
	step [95/244], loss=10.6602
	step [96/244], loss=11.3599
	step [97/244], loss=9.8080
	step [98/244], loss=8.6533
	step [99/244], loss=9.9442
	step [100/244], loss=9.1334
	step [101/244], loss=10.5714
	step [102/244], loss=10.3361
	step [103/244], loss=9.4072
	step [104/244], loss=10.7552
	step [105/244], loss=10.6288
	step [106/244], loss=10.5439
	step [107/244], loss=10.1490
	step [108/244], loss=9.3238
	step [109/244], loss=10.0590
	step [110/244], loss=10.0063
	step [111/244], loss=8.4592
	step [112/244], loss=9.9022
	step [113/244], loss=9.8140
	step [114/244], loss=13.1618
	step [115/244], loss=10.8619
	step [116/244], loss=10.0548
	step [117/244], loss=11.2799
	step [118/244], loss=10.4396
	step [119/244], loss=9.4307
	step [120/244], loss=10.1166
	step [121/244], loss=9.3515
	step [122/244], loss=8.3487
	step [123/244], loss=9.5230
	step [124/244], loss=12.0089
	step [125/244], loss=11.6179
	step [126/244], loss=9.0126
	step [127/244], loss=9.7442
	step [128/244], loss=10.8064
	step [129/244], loss=11.0527
	step [130/244], loss=10.8468
	step [131/244], loss=11.0263
	step [132/244], loss=10.5324
	step [133/244], loss=10.7532
	step [134/244], loss=9.3656
	step [135/244], loss=9.3584
	step [136/244], loss=7.6105
	step [137/244], loss=10.1519
	step [138/244], loss=10.5154
	step [139/244], loss=8.9197
	step [140/244], loss=11.4827
	step [141/244], loss=10.0380
	step [142/244], loss=10.3785
	step [143/244], loss=11.6143
	step [144/244], loss=10.2452
	step [145/244], loss=8.3592
	step [146/244], loss=7.7633
	step [147/244], loss=10.7246
	step [148/244], loss=10.4980
	step [149/244], loss=9.3115
	step [150/244], loss=10.5093
	step [151/244], loss=10.4703
	step [152/244], loss=10.7510
	step [153/244], loss=10.0159
	step [154/244], loss=12.3040
	step [155/244], loss=9.6174
	step [156/244], loss=9.8612
	step [157/244], loss=8.4857
	step [158/244], loss=9.6386
	step [159/244], loss=10.6438
	step [160/244], loss=9.7817
	step [161/244], loss=9.4447
	step [162/244], loss=10.6138
	step [163/244], loss=9.7826
	step [164/244], loss=9.7274
	step [165/244], loss=9.2635
	step [166/244], loss=9.1226
	step [167/244], loss=10.7180
	step [168/244], loss=8.9001
	step [169/244], loss=10.7920
	step [170/244], loss=8.5173
	step [171/244], loss=10.6488
	step [172/244], loss=8.0951
	step [173/244], loss=10.5042
	step [174/244], loss=10.4628
	step [175/244], loss=10.0069
	step [176/244], loss=10.7732
	step [177/244], loss=13.7501
	step [178/244], loss=10.0348
	step [179/244], loss=12.5619
	step [180/244], loss=10.9154
	step [181/244], loss=12.2126
	step [182/244], loss=9.9172
	step [183/244], loss=10.4329
	step [184/244], loss=9.9558
	step [185/244], loss=10.6429
	step [186/244], loss=10.7503
	step [187/244], loss=10.0759
	step [188/244], loss=8.4064
	step [189/244], loss=8.4874
	step [190/244], loss=9.9449
	step [191/244], loss=10.1013
	step [192/244], loss=9.0019
	step [193/244], loss=8.0572
	step [194/244], loss=9.8294
	step [195/244], loss=8.0446
	step [196/244], loss=10.4585
	step [197/244], loss=9.5289
	step [198/244], loss=9.2213
	step [199/244], loss=10.7699
	step [200/244], loss=10.2487
	step [201/244], loss=10.7906
	step [202/244], loss=8.2520
	step [203/244], loss=10.1451
	step [204/244], loss=8.7906
	step [205/244], loss=10.3038
	step [206/244], loss=10.7098
	step [207/244], loss=11.0725
	step [208/244], loss=10.4472
	step [209/244], loss=6.6442
	step [210/244], loss=8.5977
	step [211/244], loss=10.0762
	step [212/244], loss=10.2861
	step [213/244], loss=11.0347
	step [214/244], loss=9.8674
	step [215/244], loss=9.3456
	step [216/244], loss=10.7450
	step [217/244], loss=10.7056
	step [218/244], loss=8.5096
	step [219/244], loss=11.0865
	step [220/244], loss=8.6560
	step [221/244], loss=9.6083
	step [222/244], loss=9.7169
	step [223/244], loss=8.4543
	step [224/244], loss=7.9972
	step [225/244], loss=9.9331
	step [226/244], loss=9.8873
	step [227/244], loss=10.1337
	step [228/244], loss=12.1280
	step [229/244], loss=10.6066
	step [230/244], loss=10.0810
	step [231/244], loss=9.9033
	step [232/244], loss=9.1800
	step [233/244], loss=9.5293
	step [234/244], loss=10.2275
	step [235/244], loss=8.9286
	step [236/244], loss=8.4746
	step [237/244], loss=12.1444
	step [238/244], loss=11.7487
	step [239/244], loss=8.4946
	step [240/244], loss=11.7114
	step [241/244], loss=10.2049
	step [242/244], loss=8.3024
	step [243/244], loss=10.7477
	step [244/244], loss=0.6897
	Evaluating
	loss=0.0339, precision=0.1594, recall=0.9983, f1=0.2748
Training epoch 16
	step [1/244], loss=12.6375
	step [2/244], loss=9.0398
	step [3/244], loss=7.8475
	step [4/244], loss=10.5456
	step [5/244], loss=10.4526
	step [6/244], loss=9.9170
	step [7/244], loss=9.5139
	step [8/244], loss=9.0023
	step [9/244], loss=10.5961
	step [10/244], loss=8.9196
	step [11/244], loss=9.4093
	step [12/244], loss=10.1545
	step [13/244], loss=10.4425
	step [14/244], loss=9.0119
	step [15/244], loss=9.9830
	step [16/244], loss=11.1570
	step [17/244], loss=11.2689
	step [18/244], loss=9.9385
	step [19/244], loss=9.9573
	step [20/244], loss=8.8541
	step [21/244], loss=9.2773
	step [22/244], loss=8.3604
	step [23/244], loss=9.3570
	step [24/244], loss=8.8830
	step [25/244], loss=10.4064
	step [26/244], loss=9.0828
	step [27/244], loss=11.9853
	step [28/244], loss=10.5856
	step [29/244], loss=8.1516
	step [30/244], loss=10.7981
	step [31/244], loss=9.5766
	step [32/244], loss=9.3242
	step [33/244], loss=7.9874
	step [34/244], loss=10.9375
	step [35/244], loss=9.6635
	step [36/244], loss=8.1844
	step [37/244], loss=10.3280
	step [38/244], loss=10.1583
	step [39/244], loss=9.1914
	step [40/244], loss=12.4494
	step [41/244], loss=9.3310
	step [42/244], loss=8.3424
	step [43/244], loss=12.0561
	step [44/244], loss=8.3867
	step [45/244], loss=10.4074
	step [46/244], loss=10.2102
	step [47/244], loss=9.2903
	step [48/244], loss=7.6536
	step [49/244], loss=11.0209
	step [50/244], loss=9.4351
	step [51/244], loss=9.5296
	step [52/244], loss=8.7578
	step [53/244], loss=10.5189
	step [54/244], loss=11.0132
	step [55/244], loss=9.2515
	step [56/244], loss=8.2613
	step [57/244], loss=9.5044
	step [58/244], loss=9.4227
	step [59/244], loss=10.0534
	step [60/244], loss=9.9025
	step [61/244], loss=10.8051
	step [62/244], loss=9.5060
	step [63/244], loss=9.7622
	step [64/244], loss=13.8008
	step [65/244], loss=10.4590
	step [66/244], loss=9.6260
	step [67/244], loss=10.0940
	step [68/244], loss=9.3216
	step [69/244], loss=9.5100
	step [70/244], loss=10.0310
	step [71/244], loss=12.2130
	step [72/244], loss=9.4165
	step [73/244], loss=10.5770
	step [74/244], loss=8.7077
	step [75/244], loss=8.0479
	step [76/244], loss=8.6386
	step [77/244], loss=9.1432
	step [78/244], loss=10.8616
	step [79/244], loss=8.5213
	step [80/244], loss=9.6526
	step [81/244], loss=11.0786
	step [82/244], loss=9.2021
	step [83/244], loss=8.3381
	step [84/244], loss=9.6444
	step [85/244], loss=9.4145
	step [86/244], loss=8.7730
	step [87/244], loss=8.9022
	step [88/244], loss=8.5135
	step [89/244], loss=9.3128
	step [90/244], loss=10.5637
	step [91/244], loss=9.2610
	step [92/244], loss=8.9118
	step [93/244], loss=8.6614
	step [94/244], loss=11.1769
	step [95/244], loss=11.0624
	step [96/244], loss=9.4446
	step [97/244], loss=10.4437
	step [98/244], loss=9.1042
	step [99/244], loss=11.2328
	step [100/244], loss=7.8273
	step [101/244], loss=8.4556
	step [102/244], loss=9.1242
	step [103/244], loss=10.1187
	step [104/244], loss=8.4480
	step [105/244], loss=9.4562
	step [106/244], loss=8.8779
	step [107/244], loss=9.2596
	step [108/244], loss=8.9326
	step [109/244], loss=8.1917
	step [110/244], loss=8.9476
	step [111/244], loss=9.7618
	step [112/244], loss=10.1797
	step [113/244], loss=8.1987
	step [114/244], loss=10.5362
	step [115/244], loss=8.6833
	step [116/244], loss=10.9967
	step [117/244], loss=14.1192
	step [118/244], loss=8.7092
	step [119/244], loss=10.2109
	step [120/244], loss=9.7368
	step [121/244], loss=9.0643
	step [122/244], loss=7.4337
	step [123/244], loss=10.4009
	step [124/244], loss=9.7846
	step [125/244], loss=8.9296
	step [126/244], loss=8.5519
	step [127/244], loss=7.7104
	step [128/244], loss=9.7404
	step [129/244], loss=8.2624
	step [130/244], loss=10.2735
	step [131/244], loss=7.6676
	step [132/244], loss=8.5457
	step [133/244], loss=9.2309
	step [134/244], loss=10.8380
	step [135/244], loss=9.9991
	step [136/244], loss=9.4425
	step [137/244], loss=10.8447
	step [138/244], loss=9.2118
	step [139/244], loss=9.3731
	step [140/244], loss=9.4079
	step [141/244], loss=8.1509
	step [142/244], loss=10.4866
	step [143/244], loss=9.6684
	step [144/244], loss=8.7275
	step [145/244], loss=8.9798
	step [146/244], loss=9.9253
	step [147/244], loss=10.0663
	step [148/244], loss=7.9828
	step [149/244], loss=10.2523
	step [150/244], loss=10.8744
	step [151/244], loss=8.6414
	step [152/244], loss=8.4878
	step [153/244], loss=9.6897
	step [154/244], loss=11.0392
	step [155/244], loss=10.0077
	step [156/244], loss=8.7646
	step [157/244], loss=11.2826
	step [158/244], loss=9.3419
	step [159/244], loss=9.5541
	step [160/244], loss=8.5996
	step [161/244], loss=9.8638
	step [162/244], loss=11.4985
	step [163/244], loss=7.5391
	step [164/244], loss=8.6295
	step [165/244], loss=11.9411
	step [166/244], loss=9.9187
	step [167/244], loss=8.7456
	step [168/244], loss=10.7860
	step [169/244], loss=8.1614
	step [170/244], loss=7.7353
	step [171/244], loss=10.2273
	step [172/244], loss=9.8711
	step [173/244], loss=14.0739
	step [174/244], loss=8.7307
	step [175/244], loss=8.8484
	step [176/244], loss=8.1041
	step [177/244], loss=10.4148
	step [178/244], loss=9.4993
	step [179/244], loss=9.1418
	step [180/244], loss=10.0479
	step [181/244], loss=10.3718
	step [182/244], loss=9.4673
	step [183/244], loss=8.7985
	step [184/244], loss=9.3308
	step [185/244], loss=10.7995
	step [186/244], loss=9.6348
	step [187/244], loss=9.6166
	step [188/244], loss=9.5044
	step [189/244], loss=9.4231
	step [190/244], loss=8.0744
	step [191/244], loss=8.6145
	step [192/244], loss=9.8788
	step [193/244], loss=9.2092
	step [194/244], loss=10.8863
	step [195/244], loss=8.5645
	step [196/244], loss=10.3834
	step [197/244], loss=8.5232
	step [198/244], loss=9.2233
	step [199/244], loss=9.7103
	step [200/244], loss=7.9024
	step [201/244], loss=8.6673
	step [202/244], loss=9.9811
	step [203/244], loss=9.5770
	step [204/244], loss=9.1948
	step [205/244], loss=9.2081
	step [206/244], loss=9.2378
	step [207/244], loss=9.6980
	step [208/244], loss=9.1388
	step [209/244], loss=9.5830
	step [210/244], loss=8.9359
	step [211/244], loss=9.2159
	step [212/244], loss=8.6466
	step [213/244], loss=10.3199
	step [214/244], loss=8.1320
	step [215/244], loss=9.2833
	step [216/244], loss=10.7173
	step [217/244], loss=9.8686
	step [218/244], loss=9.0621
	step [219/244], loss=8.1552
	step [220/244], loss=10.0277
	step [221/244], loss=9.6957
	step [222/244], loss=10.4268
	step [223/244], loss=7.5909
	step [224/244], loss=9.0845
	step [225/244], loss=9.4759
	step [226/244], loss=8.3183
	step [227/244], loss=8.7374
	step [228/244], loss=10.3021
	step [229/244], loss=11.5491
	step [230/244], loss=9.4095
	step [231/244], loss=10.0823
	step [232/244], loss=8.9486
	step [233/244], loss=11.7577
	step [234/244], loss=9.9381
	step [235/244], loss=9.1885
	step [236/244], loss=9.0445
	step [237/244], loss=8.7312
	step [238/244], loss=8.8517
	step [239/244], loss=8.5434
	step [240/244], loss=9.0129
	step [241/244], loss=8.4109
	step [242/244], loss=8.6724
	step [243/244], loss=8.3702
	step [244/244], loss=0.2820
	Evaluating
	loss=0.0276, precision=0.1743, recall=0.9978, f1=0.2967
Training epoch 17
	step [1/244], loss=8.7863
	step [2/244], loss=10.3808
	step [3/244], loss=8.4697
	step [4/244], loss=9.3790
	step [5/244], loss=9.8859
	step [6/244], loss=8.3154
	step [7/244], loss=10.5055
	step [8/244], loss=9.0319
	step [9/244], loss=9.2591
	step [10/244], loss=8.7447
	step [11/244], loss=8.1899
	step [12/244], loss=10.1772
	step [13/244], loss=8.8193
	step [14/244], loss=9.4218
	step [15/244], loss=10.6405
	step [16/244], loss=8.4526
	step [17/244], loss=8.5163
	step [18/244], loss=8.8957
	step [19/244], loss=8.4834
	step [20/244], loss=7.8593
	step [21/244], loss=8.7500
	step [22/244], loss=9.4976
	step [23/244], loss=11.9335
	step [24/244], loss=7.6676
	step [25/244], loss=9.2027
	step [26/244], loss=8.4182
	step [27/244], loss=11.1734
	step [28/244], loss=9.6962
	step [29/244], loss=8.1682
	step [30/244], loss=8.3936
	step [31/244], loss=10.8125
	step [32/244], loss=9.1622
	step [33/244], loss=10.8368
	step [34/244], loss=9.2754
	step [35/244], loss=9.7929
	step [36/244], loss=8.4930
	step [37/244], loss=7.8283
	step [38/244], loss=10.2640
	step [39/244], loss=10.3859
	step [40/244], loss=9.8892
	step [41/244], loss=8.0190
	step [42/244], loss=8.9385
	step [43/244], loss=9.1671
	step [44/244], loss=8.2825
	step [45/244], loss=11.1260
	step [46/244], loss=8.2356
	step [47/244], loss=10.5289
	step [48/244], loss=9.8070
	step [49/244], loss=7.6798
	step [50/244], loss=8.6861
	step [51/244], loss=8.0890
	step [52/244], loss=7.7754
	step [53/244], loss=9.1558
	step [54/244], loss=11.4870
	step [55/244], loss=9.5933
	step [56/244], loss=8.7268
	step [57/244], loss=8.6587
	step [58/244], loss=8.6271
	step [59/244], loss=9.2453
	step [60/244], loss=9.2877
	step [61/244], loss=7.9510
	step [62/244], loss=9.6090
	step [63/244], loss=9.5161
	step [64/244], loss=7.6677
	step [65/244], loss=8.4072
	step [66/244], loss=8.7488
	step [67/244], loss=10.8197
	step [68/244], loss=9.4699
	step [69/244], loss=9.8954
	step [70/244], loss=9.5553
	step [71/244], loss=9.5214
	step [72/244], loss=9.3078
	step [73/244], loss=8.8607
	step [74/244], loss=8.4292
	step [75/244], loss=8.2314
	step [76/244], loss=9.3679
	step [77/244], loss=7.8162
	step [78/244], loss=9.7301
	step [79/244], loss=9.1773
	step [80/244], loss=8.9781
	step [81/244], loss=9.4847
	step [82/244], loss=10.0013
	step [83/244], loss=8.0921
	step [84/244], loss=9.0095
	step [85/244], loss=9.0915
	step [86/244], loss=9.2536
	step [87/244], loss=10.0203
	step [88/244], loss=9.6563
	step [89/244], loss=7.8735
	step [90/244], loss=9.0147
	step [91/244], loss=11.2971
	step [92/244], loss=9.5091
	step [93/244], loss=9.8591
	step [94/244], loss=9.1661
	step [95/244], loss=9.1139
	step [96/244], loss=8.3860
	step [97/244], loss=10.1562
	step [98/244], loss=9.6333
	step [99/244], loss=10.2654
	step [100/244], loss=10.5967
	step [101/244], loss=10.6446
	step [102/244], loss=8.4735
	step [103/244], loss=9.5285
	step [104/244], loss=8.9193
	step [105/244], loss=9.0964
	step [106/244], loss=9.6820
	step [107/244], loss=8.2547
	step [108/244], loss=8.7523
	step [109/244], loss=10.0490
	step [110/244], loss=10.0554
	step [111/244], loss=7.6752
	step [112/244], loss=10.4150
	step [113/244], loss=10.5821
	step [114/244], loss=9.4860
	step [115/244], loss=8.4662
	step [116/244], loss=9.0938
	step [117/244], loss=8.5285
	step [118/244], loss=11.1710
	step [119/244], loss=10.3158
	step [120/244], loss=11.0354
	step [121/244], loss=7.6553
	step [122/244], loss=9.0328
	step [123/244], loss=9.5076
	step [124/244], loss=11.1871
	step [125/244], loss=10.0173
	step [126/244], loss=9.5314
	step [127/244], loss=8.4948
	step [128/244], loss=9.8971
	step [129/244], loss=9.0247
	step [130/244], loss=8.1834
	step [131/244], loss=9.2616
	step [132/244], loss=10.0361
	step [133/244], loss=8.2560
	step [134/244], loss=7.9171
	step [135/244], loss=11.0931
	step [136/244], loss=7.3447
	step [137/244], loss=7.0969
	step [138/244], loss=9.5044
	step [139/244], loss=9.1347
	step [140/244], loss=9.8702
	step [141/244], loss=9.5258
	step [142/244], loss=11.6575
	step [143/244], loss=8.2602
	step [144/244], loss=7.5932
	step [145/244], loss=9.6449
	step [146/244], loss=7.4572
	step [147/244], loss=9.1374
	step [148/244], loss=7.4066
	step [149/244], loss=7.3747
	step [150/244], loss=10.3572
	step [151/244], loss=8.9152
	step [152/244], loss=9.0458
	step [153/244], loss=7.7353
	step [154/244], loss=10.4292
	step [155/244], loss=10.5091
	step [156/244], loss=8.4617
	step [157/244], loss=9.0637
	step [158/244], loss=9.9299
	step [159/244], loss=8.8065
	step [160/244], loss=8.6634
	step [161/244], loss=10.5727
	step [162/244], loss=9.4313
	step [163/244], loss=10.0663
	step [164/244], loss=9.0548
	step [165/244], loss=7.3627
	step [166/244], loss=8.3253
	step [167/244], loss=10.0150
	step [168/244], loss=10.3862
	step [169/244], loss=7.7904
	step [170/244], loss=9.0888
	step [171/244], loss=9.3764
	step [172/244], loss=10.7541
	step [173/244], loss=8.7944
	step [174/244], loss=8.2871
	step [175/244], loss=9.3015
	step [176/244], loss=9.4136
	step [177/244], loss=9.4189
	step [178/244], loss=7.9873
	step [179/244], loss=9.0549
	step [180/244], loss=7.4847
	step [181/244], loss=8.4495
	step [182/244], loss=8.9829
	step [183/244], loss=7.7616
	step [184/244], loss=8.4168
	step [185/244], loss=8.8363
	step [186/244], loss=13.2090
	step [187/244], loss=9.6779
	step [188/244], loss=6.6132
	step [189/244], loss=9.1323
	step [190/244], loss=7.8441
	step [191/244], loss=9.9093
	step [192/244], loss=7.6104
	step [193/244], loss=9.0634
	step [194/244], loss=10.2217
	step [195/244], loss=10.7196
	step [196/244], loss=8.8973
	step [197/244], loss=11.2429
	step [198/244], loss=7.9312
	step [199/244], loss=8.5589
	step [200/244], loss=10.2252
	step [201/244], loss=9.9510
	step [202/244], loss=9.2153
	step [203/244], loss=8.2324
	step [204/244], loss=7.6986
	step [205/244], loss=9.9360
	step [206/244], loss=7.8371
	step [207/244], loss=10.3343
	step [208/244], loss=7.6135
	step [209/244], loss=8.0224
	step [210/244], loss=8.4208
	step [211/244], loss=10.1280
	step [212/244], loss=8.7132
	step [213/244], loss=9.6292
	step [214/244], loss=10.6652
	step [215/244], loss=8.6159
	step [216/244], loss=7.4274
	step [217/244], loss=8.8350
	step [218/244], loss=9.8586
	step [219/244], loss=10.2287
	step [220/244], loss=9.1152
	step [221/244], loss=8.6409
	step [222/244], loss=10.2290
	step [223/244], loss=8.2811
	step [224/244], loss=7.4907
	step [225/244], loss=10.8818
	step [226/244], loss=8.0121
	step [227/244], loss=9.4837
	step [228/244], loss=9.4077
	step [229/244], loss=10.6447
	step [230/244], loss=9.2416
	step [231/244], loss=7.7112
	step [232/244], loss=10.6189
	step [233/244], loss=8.6731
	step [234/244], loss=9.3674
	step [235/244], loss=7.6259
	step [236/244], loss=9.2442
	step [237/244], loss=10.2653
	step [238/244], loss=8.5498
	step [239/244], loss=8.1864
	step [240/244], loss=7.4598
	step [241/244], loss=10.0216
	step [242/244], loss=8.8484
	step [243/244], loss=7.0390
	step [244/244], loss=0.7352
	Evaluating
	loss=0.0241, precision=0.2141, recall=0.9971, f1=0.3525
saving model as: 2_saved_model.pth
Training epoch 18
	step [1/244], loss=10.3323
	step [2/244], loss=8.2592
	step [3/244], loss=9.9207
	step [4/244], loss=7.5183
	step [5/244], loss=8.1172
	step [6/244], loss=9.9675
	step [7/244], loss=9.6035
	step [8/244], loss=6.7650
	step [9/244], loss=9.7175
	step [10/244], loss=8.8781
	step [11/244], loss=9.7254
	step [12/244], loss=8.6901
	step [13/244], loss=9.2933
	step [14/244], loss=9.0880
	step [15/244], loss=11.4649
	step [16/244], loss=8.4349
	step [17/244], loss=8.2741
	step [18/244], loss=9.7820
	step [19/244], loss=9.8444
	step [20/244], loss=9.1580
	step [21/244], loss=8.7271
	step [22/244], loss=9.2745
	step [23/244], loss=7.5599
	step [24/244], loss=8.3434
	step [25/244], loss=7.0517
	step [26/244], loss=9.6070
	step [27/244], loss=9.5164
	step [28/244], loss=7.9223
	step [29/244], loss=8.8564
	step [30/244], loss=9.0635
	step [31/244], loss=8.8375
	step [32/244], loss=8.5273
	step [33/244], loss=9.3477
	step [34/244], loss=8.3812
	step [35/244], loss=8.3177
	step [36/244], loss=10.1248
	step [37/244], loss=8.5616
	step [38/244], loss=8.3036
	step [39/244], loss=8.7870
	step [40/244], loss=7.8990
	step [41/244], loss=7.7164
	step [42/244], loss=7.2546
	step [43/244], loss=8.6554
	step [44/244], loss=9.5756
	step [45/244], loss=9.1164
	step [46/244], loss=9.2355
	step [47/244], loss=8.9731
	step [48/244], loss=8.9612
	step [49/244], loss=8.2639
	step [50/244], loss=8.9624
	step [51/244], loss=7.7877
	step [52/244], loss=8.4219
	step [53/244], loss=9.5875
	step [54/244], loss=9.4125
	step [55/244], loss=7.4581
	step [56/244], loss=10.2309
	step [57/244], loss=8.1919
	step [58/244], loss=9.1357
	step [59/244], loss=8.5033
	step [60/244], loss=8.3574
	step [61/244], loss=9.5105
	step [62/244], loss=9.7826
	step [63/244], loss=9.3674
	step [64/244], loss=10.7339
	step [65/244], loss=7.5500
	step [66/244], loss=8.7890
	step [67/244], loss=9.2193
	step [68/244], loss=8.8680
	step [69/244], loss=9.3642
	step [70/244], loss=8.9307
	step [71/244], loss=8.3092
	step [72/244], loss=10.3146
	step [73/244], loss=8.7734
	step [74/244], loss=9.5922
	step [75/244], loss=9.1471
	step [76/244], loss=9.1228
	step [77/244], loss=7.9884
	step [78/244], loss=9.5815
	step [79/244], loss=9.5066
	step [80/244], loss=8.8188
	step [81/244], loss=7.9454
	step [82/244], loss=8.3541
	step [83/244], loss=9.1742
	step [84/244], loss=8.3757
	step [85/244], loss=9.5524
	step [86/244], loss=6.9563
	step [87/244], loss=8.0478
	step [88/244], loss=8.3625
	step [89/244], loss=9.1980
	step [90/244], loss=7.9319
	step [91/244], loss=8.3201
	step [92/244], loss=7.7379
	step [93/244], loss=8.1440
	step [94/244], loss=10.0076
	step [95/244], loss=9.1593
	step [96/244], loss=9.1545
	step [97/244], loss=8.5998
	step [98/244], loss=8.4888
	step [99/244], loss=7.7050
	step [100/244], loss=8.6362
	step [101/244], loss=7.8559
	step [102/244], loss=9.1747
	step [103/244], loss=8.9673
	step [104/244], loss=8.1638
	step [105/244], loss=7.9592
	step [106/244], loss=7.9860
	step [107/244], loss=9.4052
	step [108/244], loss=8.7281
	step [109/244], loss=10.8952
	step [110/244], loss=8.5040
	step [111/244], loss=7.6679
	step [112/244], loss=8.9217
	step [113/244], loss=8.8244
	step [114/244], loss=8.9338
	step [115/244], loss=12.1730
	step [116/244], loss=8.7135
	step [117/244], loss=8.9587
	step [118/244], loss=8.8162
	step [119/244], loss=7.4404
	step [120/244], loss=8.6622
	step [121/244], loss=7.5824
	step [122/244], loss=8.3299
	step [123/244], loss=9.8492
	step [124/244], loss=7.4768
	step [125/244], loss=8.2366
	step [126/244], loss=8.9625
	step [127/244], loss=9.9788
	step [128/244], loss=8.3320
	step [129/244], loss=9.5427
	step [130/244], loss=9.3574
	step [131/244], loss=11.5589
	step [132/244], loss=8.9384
	step [133/244], loss=8.6430
	step [134/244], loss=9.5559
	step [135/244], loss=9.3711
	step [136/244], loss=10.6927
	step [137/244], loss=8.8892
	step [138/244], loss=8.9286
	step [139/244], loss=8.9995
	step [140/244], loss=8.5970
	step [141/244], loss=8.0579
	step [142/244], loss=7.7940
	step [143/244], loss=9.2472
	step [144/244], loss=9.1286
	step [145/244], loss=9.3840
	step [146/244], loss=8.5384
	step [147/244], loss=8.9057
	step [148/244], loss=9.9927
	step [149/244], loss=9.1414
	step [150/244], loss=6.8566
	step [151/244], loss=7.4866
	step [152/244], loss=8.5212
	step [153/244], loss=8.5665
	step [154/244], loss=8.3355
	step [155/244], loss=11.2310
	step [156/244], loss=7.3795
	step [157/244], loss=8.4731
	step [158/244], loss=7.7943
	step [159/244], loss=9.7762
	step [160/244], loss=10.0439
	step [161/244], loss=8.7047
	step [162/244], loss=7.2075
	step [163/244], loss=9.5641
	step [164/244], loss=10.2923
	step [165/244], loss=7.8085
	step [166/244], loss=8.3316
	step [167/244], loss=9.3283
	step [168/244], loss=8.1459
	step [169/244], loss=9.3485
	step [170/244], loss=9.0744
	step [171/244], loss=10.4637
	step [172/244], loss=7.1670
	step [173/244], loss=7.7683
	step [174/244], loss=8.1730
	step [175/244], loss=8.2622
	step [176/244], loss=8.4953
	step [177/244], loss=9.6502
	step [178/244], loss=8.2726
	step [179/244], loss=8.1426
	step [180/244], loss=9.6720
	step [181/244], loss=8.2422
	step [182/244], loss=8.9496
	step [183/244], loss=8.5513
	step [184/244], loss=9.1298
	step [185/244], loss=8.0535
	step [186/244], loss=8.0955
	step [187/244], loss=7.7152
	step [188/244], loss=7.9331
	step [189/244], loss=8.3127
	step [190/244], loss=6.8795
	step [191/244], loss=8.5756
	step [192/244], loss=9.6371
	step [193/244], loss=10.2258
	step [194/244], loss=7.5559
	step [195/244], loss=8.3110
	step [196/244], loss=8.1217
	step [197/244], loss=8.7980
	step [198/244], loss=7.7065
	step [199/244], loss=9.9738
	step [200/244], loss=7.9612
	step [201/244], loss=8.1278
	step [202/244], loss=9.5391
	step [203/244], loss=11.1221
	step [204/244], loss=9.4858
	step [205/244], loss=8.0928
	step [206/244], loss=8.9884
	step [207/244], loss=9.7725
	step [208/244], loss=7.6052
	step [209/244], loss=8.1340
	step [210/244], loss=9.6468
	step [211/244], loss=9.8134
	step [212/244], loss=7.5232
	step [213/244], loss=8.9893
	step [214/244], loss=8.6325
	step [215/244], loss=8.0271
	step [216/244], loss=9.1891
	step [217/244], loss=8.3456
	step [218/244], loss=7.9659
	step [219/244], loss=8.2462
	step [220/244], loss=8.5591
	step [221/244], loss=10.2106
	step [222/244], loss=10.4204
	step [223/244], loss=9.2964
	step [224/244], loss=8.8717
	step [225/244], loss=8.1510
	step [226/244], loss=9.6820
	step [227/244], loss=7.8122
	step [228/244], loss=8.2924
	step [229/244], loss=10.2779
	step [230/244], loss=8.4590
	step [231/244], loss=7.9116
	step [232/244], loss=8.9767
	step [233/244], loss=8.3219
	step [234/244], loss=8.3194
	step [235/244], loss=9.6792
	step [236/244], loss=7.7487
	step [237/244], loss=7.2759
	step [238/244], loss=8.5131
	step [239/244], loss=8.5276
	step [240/244], loss=8.1650
	step [241/244], loss=8.6900
	step [242/244], loss=9.5559
	step [243/244], loss=8.2320
	step [244/244], loss=0.5677
	Evaluating
	loss=0.0248, precision=0.2038, recall=0.9971, f1=0.3385
Training epoch 19
	step [1/244], loss=8.2622
	step [2/244], loss=9.5257
	step [3/244], loss=8.5172
	step [4/244], loss=7.5500
	step [5/244], loss=7.3778
	step [6/244], loss=10.6830
	step [7/244], loss=9.1856
	step [8/244], loss=8.4292
	step [9/244], loss=7.6915
	step [10/244], loss=7.6795
	step [11/244], loss=8.8589
	step [12/244], loss=7.8387
	step [13/244], loss=7.1660
	step [14/244], loss=7.6982
	step [15/244], loss=7.1258
	step [16/244], loss=7.8482
	step [17/244], loss=8.7111
	step [18/244], loss=8.1316
	step [19/244], loss=9.0456
	step [20/244], loss=8.4104
	step [21/244], loss=8.0031
	step [22/244], loss=7.7066
	step [23/244], loss=8.0734
	step [24/244], loss=7.5217
	step [25/244], loss=7.5136
	step [26/244], loss=8.4022
	step [27/244], loss=8.9316
	step [28/244], loss=7.2154
	step [29/244], loss=9.5176
	step [30/244], loss=8.2931
	step [31/244], loss=8.5531
	step [32/244], loss=9.0530
	step [33/244], loss=8.3290
	step [34/244], loss=8.5565
	step [35/244], loss=8.4538
	step [36/244], loss=9.5761
	step [37/244], loss=7.8951
	step [38/244], loss=10.4261
	step [39/244], loss=8.7685
	step [40/244], loss=8.2294
	step [41/244], loss=8.5196
	step [42/244], loss=9.3321
	step [43/244], loss=9.8137
	step [44/244], loss=8.0786
	step [45/244], loss=8.9848
	step [46/244], loss=8.7498
	step [47/244], loss=7.3594
	step [48/244], loss=7.7519
	step [49/244], loss=8.7218
	step [50/244], loss=10.1872
	step [51/244], loss=7.7016
	step [52/244], loss=9.1165
	step [53/244], loss=8.9784
	step [54/244], loss=7.3665
	step [55/244], loss=8.7961
	step [56/244], loss=8.4618
	step [57/244], loss=7.4215
	step [58/244], loss=8.1383
	step [59/244], loss=10.2072
	step [60/244], loss=10.0058
	step [61/244], loss=7.0558
	step [62/244], loss=7.6069
	step [63/244], loss=6.6782
	step [64/244], loss=8.7859
	step [65/244], loss=8.5371
	step [66/244], loss=8.0184
	step [67/244], loss=8.0538
	step [68/244], loss=8.2658
	step [69/244], loss=8.7162
	step [70/244], loss=6.1300
	step [71/244], loss=7.4959
	step [72/244], loss=8.1906
	step [73/244], loss=7.6488
	step [74/244], loss=7.5013
	step [75/244], loss=9.4023
	step [76/244], loss=9.1995
	step [77/244], loss=8.0677
	step [78/244], loss=8.1126
	step [79/244], loss=8.7106
	step [80/244], loss=8.7393
	step [81/244], loss=8.6586
	step [82/244], loss=8.8683
	step [83/244], loss=9.3767
	step [84/244], loss=8.6957
	step [85/244], loss=8.6447
	step [86/244], loss=9.2036
	step [87/244], loss=8.9418
	step [88/244], loss=7.5516
	step [89/244], loss=7.9144
	step [90/244], loss=9.9989
	step [91/244], loss=9.8339
	step [92/244], loss=8.7925
	step [93/244], loss=8.8889
	step [94/244], loss=8.5508
	step [95/244], loss=8.2356
	step [96/244], loss=9.7806
	step [97/244], loss=8.6817
	step [98/244], loss=7.1904
	step [99/244], loss=10.3835
	step [100/244], loss=8.6487
	step [101/244], loss=7.8908
	step [102/244], loss=8.5761
	step [103/244], loss=8.0619
	step [104/244], loss=7.5077
	step [105/244], loss=10.6785
	step [106/244], loss=9.8624
	step [107/244], loss=6.4072
	step [108/244], loss=9.2799
	step [109/244], loss=8.8646
	step [110/244], loss=8.1195
	step [111/244], loss=8.9366
	step [112/244], loss=8.2115
	step [113/244], loss=8.0979
	step [114/244], loss=7.0748
	step [115/244], loss=9.6265
	step [116/244], loss=7.2928
	step [117/244], loss=8.4107
	step [118/244], loss=10.2386
	step [119/244], loss=8.8241
	step [120/244], loss=6.7947
	step [121/244], loss=8.6638
	step [122/244], loss=7.5022
	step [123/244], loss=8.3605
	step [124/244], loss=7.9050
	step [125/244], loss=9.5182
	step [126/244], loss=10.0152
	step [127/244], loss=8.5661
	step [128/244], loss=6.8296
	step [129/244], loss=7.6031
	step [130/244], loss=9.2866
	step [131/244], loss=9.9669
	step [132/244], loss=7.5618
	step [133/244], loss=6.4480
	step [134/244], loss=7.9951
	step [135/244], loss=9.4849
	step [136/244], loss=7.3620
	step [137/244], loss=7.4074
	step [138/244], loss=7.4070
	step [139/244], loss=9.5647
	step [140/244], loss=9.4510
	step [141/244], loss=9.5157
	step [142/244], loss=9.8795
	step [143/244], loss=9.7266
	step [144/244], loss=9.2641
	step [145/244], loss=7.8229
	step [146/244], loss=10.7857
	step [147/244], loss=9.0194
	step [148/244], loss=7.9199
	step [149/244], loss=8.4797
	step [150/244], loss=8.0967
	step [151/244], loss=8.1230
	step [152/244], loss=8.3968
	step [153/244], loss=8.2265
	step [154/244], loss=10.4895
	step [155/244], loss=8.0475
	step [156/244], loss=7.7202
	step [157/244], loss=8.9415
	step [158/244], loss=7.5851
	step [159/244], loss=9.7128
	step [160/244], loss=8.6682
	step [161/244], loss=8.1874
	step [162/244], loss=7.9727
	step [163/244], loss=11.6246
	step [164/244], loss=8.9840
	step [165/244], loss=8.7523
	step [166/244], loss=9.8787
	step [167/244], loss=8.5874
	step [168/244], loss=10.3044
	step [169/244], loss=9.5968
	step [170/244], loss=8.9295
	step [171/244], loss=7.5189
	step [172/244], loss=8.4966
	step [173/244], loss=8.0554
	step [174/244], loss=11.3872
	step [175/244], loss=6.3095
	step [176/244], loss=8.5181
	step [177/244], loss=7.3640
	step [178/244], loss=8.9301
	step [179/244], loss=8.7736
	step [180/244], loss=9.2019
	step [181/244], loss=7.3916
	step [182/244], loss=8.7873
	step [183/244], loss=10.0816
	step [184/244], loss=8.0198
	step [185/244], loss=9.4535
	step [186/244], loss=7.5267
	step [187/244], loss=9.1221
	step [188/244], loss=8.5818
	step [189/244], loss=9.7988
	step [190/244], loss=8.0137
	step [191/244], loss=8.4178
	step [192/244], loss=7.4948
	step [193/244], loss=8.8212
	step [194/244], loss=7.0262
	step [195/244], loss=8.0507
	step [196/244], loss=9.7486
	step [197/244], loss=8.6850
	step [198/244], loss=9.0822
	step [199/244], loss=6.8769
	step [200/244], loss=7.6272
	step [201/244], loss=7.7708
	step [202/244], loss=8.5975
	step [203/244], loss=11.0564
	step [204/244], loss=8.5897
	step [205/244], loss=7.7657
	step [206/244], loss=8.6074
	step [207/244], loss=6.8955
	step [208/244], loss=7.3226
	step [209/244], loss=10.2030
	step [210/244], loss=9.2209
	step [211/244], loss=8.2083
	step [212/244], loss=8.3757
	step [213/244], loss=9.6755
	step [214/244], loss=8.3282
	step [215/244], loss=8.0027
	step [216/244], loss=9.1634
	step [217/244], loss=10.0672
	step [218/244], loss=7.8413
	step [219/244], loss=7.3170
	step [220/244], loss=8.7241
	step [221/244], loss=8.2384
	step [222/244], loss=8.3076
	step [223/244], loss=6.7886
	step [224/244], loss=10.1377
	step [225/244], loss=7.7960
	step [226/244], loss=8.8197
	step [227/244], loss=7.5382
	step [228/244], loss=8.3433
	step [229/244], loss=7.0160
	step [230/244], loss=8.7264
	step [231/244], loss=9.1294
	step [232/244], loss=6.2262
	step [233/244], loss=8.1530
	step [234/244], loss=8.6115
	step [235/244], loss=8.2390
	step [236/244], loss=8.2800
	step [237/244], loss=7.5150
	step [238/244], loss=8.0661
	step [239/244], loss=8.2271
	step [240/244], loss=7.4149
	step [241/244], loss=9.4861
	step [242/244], loss=9.2093
	step [243/244], loss=7.6123
	step [244/244], loss=0.5488
	Evaluating
	loss=0.0250, precision=0.1848, recall=0.9974, f1=0.3119
Training epoch 20
	step [1/244], loss=8.4626
	step [2/244], loss=8.9075
	step [3/244], loss=8.2223
	step [4/244], loss=7.6134
	step [5/244], loss=8.2932
	step [6/244], loss=9.2123
	step [7/244], loss=8.3131
	step [8/244], loss=7.9994
	step [9/244], loss=9.3222
	step [10/244], loss=8.3110
	step [11/244], loss=7.9177
	step [12/244], loss=9.0636
	step [13/244], loss=8.5405
	step [14/244], loss=7.8309
	step [15/244], loss=7.6972
	step [16/244], loss=10.7126
	step [17/244], loss=7.4962
	step [18/244], loss=8.8099
	step [19/244], loss=6.8677
	step [20/244], loss=7.4144
	step [21/244], loss=7.9817
	step [22/244], loss=7.0759
	step [23/244], loss=8.3995
	step [24/244], loss=8.6929
	step [25/244], loss=8.0533
	step [26/244], loss=8.5125
	step [27/244], loss=8.2085
	step [28/244], loss=9.0184
	step [29/244], loss=10.2509
	step [30/244], loss=7.3452
	step [31/244], loss=9.6114
	step [32/244], loss=7.8160
	step [33/244], loss=6.9431
	step [34/244], loss=9.1131
	step [35/244], loss=10.3556
	step [36/244], loss=7.8818
	step [37/244], loss=7.8646
	step [38/244], loss=8.1720
	step [39/244], loss=8.4411
	step [40/244], loss=7.0400
	step [41/244], loss=6.9937
	step [42/244], loss=8.5457
	step [43/244], loss=7.5692
	step [44/244], loss=9.0784
	step [45/244], loss=7.7609
	step [46/244], loss=7.6832
	step [47/244], loss=8.3126
	step [48/244], loss=9.6555
	step [49/244], loss=8.6983
	step [50/244], loss=8.0795
	step [51/244], loss=6.3538
	step [52/244], loss=8.7158
	step [53/244], loss=8.5935
	step [54/244], loss=6.9605
	step [55/244], loss=7.7490
	step [56/244], loss=7.7933
	step [57/244], loss=8.1942
	step [58/244], loss=10.2668
	step [59/244], loss=8.3814
	step [60/244], loss=8.3523
	step [61/244], loss=8.2375
	step [62/244], loss=8.7511
	step [63/244], loss=9.0765
	step [64/244], loss=7.7458
	step [65/244], loss=7.4722
	step [66/244], loss=9.1205
	step [67/244], loss=8.2631
	step [68/244], loss=8.4489
	step [69/244], loss=7.6492
	step [70/244], loss=9.0182
	step [71/244], loss=10.4415
	step [72/244], loss=9.7346
	step [73/244], loss=9.5131
	step [74/244], loss=9.5270
	step [75/244], loss=8.2687
	step [76/244], loss=8.2340
	step [77/244], loss=7.3931
	step [78/244], loss=8.5987
	step [79/244], loss=6.8245
	step [80/244], loss=9.7694
	step [81/244], loss=6.0828
	step [82/244], loss=7.8488
	step [83/244], loss=8.3716
	step [84/244], loss=7.9890
	step [85/244], loss=6.8512
	step [86/244], loss=7.8459
	step [87/244], loss=10.2879
	step [88/244], loss=7.3044
	step [89/244], loss=8.9959
	step [90/244], loss=8.9830
	step [91/244], loss=7.8749
	step [92/244], loss=7.4134
	step [93/244], loss=8.0166
	step [94/244], loss=8.7221
	step [95/244], loss=9.0758
	step [96/244], loss=7.5814
	step [97/244], loss=8.1623
	step [98/244], loss=9.2891
	step [99/244], loss=8.4345
	step [100/244], loss=6.4968
	step [101/244], loss=9.2368
	step [102/244], loss=9.5865
	step [103/244], loss=8.9720
	step [104/244], loss=8.3876
	step [105/244], loss=7.9294
	step [106/244], loss=7.4480
	step [107/244], loss=8.3242
	step [108/244], loss=8.9977
	step [109/244], loss=8.3518
	step [110/244], loss=8.4853
	step [111/244], loss=8.5629
	step [112/244], loss=7.5650
	step [113/244], loss=9.6890
	step [114/244], loss=7.3700
	step [115/244], loss=8.2909
	step [116/244], loss=7.1717
	step [117/244], loss=8.1826
	step [118/244], loss=7.5833
	step [119/244], loss=8.1263
	step [120/244], loss=8.5173
	step [121/244], loss=9.5380
	step [122/244], loss=8.5100
	step [123/244], loss=8.9927
	step [124/244], loss=8.1482
	step [125/244], loss=6.7854
	step [126/244], loss=7.9659
	step [127/244], loss=7.6821
	step [128/244], loss=7.4640
	step [129/244], loss=8.0104
	step [130/244], loss=8.9054
	step [131/244], loss=6.3509
	step [132/244], loss=8.3874
	step [133/244], loss=9.2388
	step [134/244], loss=6.9325
	step [135/244], loss=7.8955
	step [136/244], loss=7.5050
	step [137/244], loss=9.4832
	step [138/244], loss=8.1592
	step [139/244], loss=8.1294
	step [140/244], loss=8.1483
	step [141/244], loss=8.2342
	step [142/244], loss=7.5373
	step [143/244], loss=8.5209
	step [144/244], loss=7.3340
	step [145/244], loss=7.3382
	step [146/244], loss=7.7615
	step [147/244], loss=7.3897
	step [148/244], loss=7.8388
	step [149/244], loss=8.0565
	step [150/244], loss=8.4550
	step [151/244], loss=6.7165
	step [152/244], loss=7.3512
	step [153/244], loss=8.0825
	step [154/244], loss=6.2312
	step [155/244], loss=7.2101
	step [156/244], loss=9.3934
	step [157/244], loss=6.1063
	step [158/244], loss=8.0102
	step [159/244], loss=7.8958
	step [160/244], loss=8.5163
	step [161/244], loss=8.6562
	step [162/244], loss=7.4773
	step [163/244], loss=9.8595
	step [164/244], loss=9.1482
	step [165/244], loss=7.4617
	step [166/244], loss=6.1209
	step [167/244], loss=8.7638
	step [168/244], loss=8.1252
	step [169/244], loss=9.3807
	step [170/244], loss=8.0250
	step [171/244], loss=8.7155
	step [172/244], loss=6.8755
	step [173/244], loss=7.2654
	step [174/244], loss=6.9520
	step [175/244], loss=7.3558
	step [176/244], loss=8.7485
	step [177/244], loss=7.8857
	step [178/244], loss=10.0960
	step [179/244], loss=7.7553
	step [180/244], loss=9.6018
	step [181/244], loss=6.9717
	step [182/244], loss=9.5093
	step [183/244], loss=9.3392
	step [184/244], loss=8.2101
	step [185/244], loss=8.9423
	step [186/244], loss=10.0590
	step [187/244], loss=8.4379
	step [188/244], loss=8.9123
	step [189/244], loss=8.7121
	step [190/244], loss=7.4489
	step [191/244], loss=8.9585
	step [192/244], loss=7.7979
	step [193/244], loss=9.8444
	step [194/244], loss=6.7990
	step [195/244], loss=7.0503
	step [196/244], loss=7.9622
	step [197/244], loss=7.4858
	step [198/244], loss=7.3041
	step [199/244], loss=7.4548
	step [200/244], loss=8.8497
	step [201/244], loss=8.6283
	step [202/244], loss=8.5353
	step [203/244], loss=7.2567
	step [204/244], loss=6.2993
	step [205/244], loss=7.5950
	step [206/244], loss=7.5256
	step [207/244], loss=8.2128
	step [208/244], loss=8.2359
	step [209/244], loss=8.4485
	step [210/244], loss=6.1822
	step [211/244], loss=8.4816
	step [212/244], loss=8.2725
	step [213/244], loss=10.2004
	step [214/244], loss=8.5238
	step [215/244], loss=7.6947
	step [216/244], loss=8.7792
	step [217/244], loss=9.2899
	step [218/244], loss=8.6615
	step [219/244], loss=8.8284
	step [220/244], loss=7.9719
	step [221/244], loss=7.7030
	step [222/244], loss=8.7614
	step [223/244], loss=8.3698
	step [224/244], loss=7.2237
	step [225/244], loss=7.7851
	step [226/244], loss=7.0314
	step [227/244], loss=8.1053
	step [228/244], loss=8.0020
	step [229/244], loss=9.3943
	step [230/244], loss=7.8851
	step [231/244], loss=9.7396
	step [232/244], loss=6.7450
	step [233/244], loss=7.1941
	step [234/244], loss=9.3697
	step [235/244], loss=8.8608
	step [236/244], loss=7.2062
	step [237/244], loss=7.8307
	step [238/244], loss=7.1423
	step [239/244], loss=8.8572
	step [240/244], loss=7.8197
	step [241/244], loss=6.6088
	step [242/244], loss=9.0913
	step [243/244], loss=7.1921
	step [244/244], loss=0.8801
	Evaluating
	loss=0.0247, precision=0.2136, recall=0.9969, f1=0.3518
Training epoch 21
	step [1/244], loss=9.2418
	step [2/244], loss=8.2602
	step [3/244], loss=8.9398
	step [4/244], loss=9.2057
	step [5/244], loss=8.3776
	step [6/244], loss=7.3073
	step [7/244], loss=7.5411
	step [8/244], loss=8.0255
	step [9/244], loss=8.4099
	step [10/244], loss=8.6245
	step [11/244], loss=8.2548
	step [12/244], loss=6.8969
	step [13/244], loss=6.9554
	step [14/244], loss=6.3260
	step [15/244], loss=7.8407
	step [16/244], loss=7.0928
	step [17/244], loss=7.2238
	step [18/244], loss=8.5968
	step [19/244], loss=8.2206
	step [20/244], loss=9.4098
	step [21/244], loss=7.6629
	step [22/244], loss=8.7006
	step [23/244], loss=7.2093
	step [24/244], loss=7.8548
	step [25/244], loss=8.3113
	step [26/244], loss=7.2058
	step [27/244], loss=6.9187
	step [28/244], loss=7.7525
	step [29/244], loss=7.6319
	step [30/244], loss=6.2982
	step [31/244], loss=7.5473
	step [32/244], loss=8.0997
	step [33/244], loss=7.3536
	step [34/244], loss=8.2865
	step [35/244], loss=7.4914
	step [36/244], loss=8.4628
	step [37/244], loss=8.0269
	step [38/244], loss=9.4335
	step [39/244], loss=6.3987
	step [40/244], loss=9.4832
	step [41/244], loss=7.3039
	step [42/244], loss=8.2891
	step [43/244], loss=10.7629
	step [44/244], loss=8.6067
	step [45/244], loss=7.3926
	step [46/244], loss=8.6758
	step [47/244], loss=7.1218
	step [48/244], loss=6.6407
	step [49/244], loss=7.7137
	step [50/244], loss=8.8904
	step [51/244], loss=8.7480
	step [52/244], loss=9.1077
	step [53/244], loss=8.2591
	step [54/244], loss=6.3934
	step [55/244], loss=7.5345
	step [56/244], loss=9.0777
	step [57/244], loss=9.1490
	step [58/244], loss=8.5678
	step [59/244], loss=6.6354
	step [60/244], loss=7.8016
	step [61/244], loss=8.7454
	step [62/244], loss=10.0294
	step [63/244], loss=7.9534
	step [64/244], loss=7.5984
	step [65/244], loss=6.8441
	step [66/244], loss=6.4709
	step [67/244], loss=7.8507
	step [68/244], loss=6.6951
	step [69/244], loss=9.7298
	step [70/244], loss=6.6343
	step [71/244], loss=6.7836
	step [72/244], loss=9.2127
	step [73/244], loss=10.3693
	step [74/244], loss=8.2059
	step [75/244], loss=8.3450
	step [76/244], loss=7.8743
	step [77/244], loss=7.8284
	step [78/244], loss=8.6527
	step [79/244], loss=6.6412
	step [80/244], loss=9.3316
	step [81/244], loss=6.6992
	step [82/244], loss=8.0682
	step [83/244], loss=8.0740
	step [84/244], loss=8.9646
	step [85/244], loss=7.3210
	step [86/244], loss=8.2893
	step [87/244], loss=9.5119
	step [88/244], loss=9.0061
	step [89/244], loss=8.1984
	step [90/244], loss=7.3276
	step [91/244], loss=8.3439
	step [92/244], loss=7.7802
	step [93/244], loss=6.8499
	step [94/244], loss=7.8543
	step [95/244], loss=7.9298
	step [96/244], loss=8.8129
	step [97/244], loss=7.4251
	step [98/244], loss=7.5131
	step [99/244], loss=8.7648
	step [100/244], loss=8.8691
	step [101/244], loss=7.2028
	step [102/244], loss=8.0141
	step [103/244], loss=6.9240
	step [104/244], loss=7.5186
	step [105/244], loss=7.7133
	step [106/244], loss=9.3556
	step [107/244], loss=8.3341
	step [108/244], loss=8.6020
	step [109/244], loss=8.3463
	step [110/244], loss=10.2872
	step [111/244], loss=7.4831
	step [112/244], loss=8.3492
	step [113/244], loss=7.8835
	step [114/244], loss=7.4932
	step [115/244], loss=8.5653
	step [116/244], loss=8.7267
	step [117/244], loss=6.6414
	step [118/244], loss=8.3562
	step [119/244], loss=8.6237
	step [120/244], loss=6.9259
	step [121/244], loss=7.5562
	step [122/244], loss=5.8455
	step [123/244], loss=8.1499
	step [124/244], loss=8.9461
	step [125/244], loss=7.9064
	step [126/244], loss=8.5828
	step [127/244], loss=7.1120
	step [128/244], loss=7.1409
	step [129/244], loss=7.9088
	step [130/244], loss=7.9794
	step [131/244], loss=10.0489
	step [132/244], loss=7.9322
	step [133/244], loss=8.5402
	step [134/244], loss=8.8230
	step [135/244], loss=7.4339
	step [136/244], loss=7.3481
	step [137/244], loss=8.3477
	step [138/244], loss=8.1543
	step [139/244], loss=10.0826
	step [140/244], loss=8.6019
	step [141/244], loss=8.5497
	step [142/244], loss=6.9434
	step [143/244], loss=7.7486
	step [144/244], loss=7.3676
	step [145/244], loss=7.4675
	step [146/244], loss=7.1986
	step [147/244], loss=7.2691
	step [148/244], loss=6.7752
	step [149/244], loss=7.6284
	step [150/244], loss=8.2415
	step [151/244], loss=8.6246
	step [152/244], loss=8.2779
	step [153/244], loss=9.4117
	step [154/244], loss=8.5637
	step [155/244], loss=7.9702
	step [156/244], loss=7.9490
	step [157/244], loss=8.0799
	step [158/244], loss=8.3406
	step [159/244], loss=6.1615
	step [160/244], loss=7.2661
	step [161/244], loss=7.2080
	step [162/244], loss=8.5601
	step [163/244], loss=7.7611
	step [164/244], loss=8.2730
	step [165/244], loss=7.9729
	step [166/244], loss=9.2853
	step [167/244], loss=8.2184
	step [168/244], loss=7.5227
	step [169/244], loss=6.9303
	step [170/244], loss=8.0844
	step [171/244], loss=7.4393
	step [172/244], loss=8.3703
	step [173/244], loss=7.7615
	step [174/244], loss=6.4694
	step [175/244], loss=8.4202
	step [176/244], loss=7.5833
	step [177/244], loss=10.3396
	step [178/244], loss=7.5938
	step [179/244], loss=8.3702
	step [180/244], loss=7.1189
	step [181/244], loss=7.1158
	step [182/244], loss=6.4829
	step [183/244], loss=6.8978
	step [184/244], loss=8.0624
	step [185/244], loss=6.9460
	step [186/244], loss=9.7973
	step [187/244], loss=7.1421
	step [188/244], loss=6.7481
	step [189/244], loss=7.2837
	step [190/244], loss=6.7439
	step [191/244], loss=8.5308
	step [192/244], loss=8.2555
	step [193/244], loss=8.8232
	step [194/244], loss=8.5586
	step [195/244], loss=9.7561
	step [196/244], loss=10.5033
	step [197/244], loss=7.7471
	step [198/244], loss=6.7372
	step [199/244], loss=7.8155
	step [200/244], loss=7.5429
	step [201/244], loss=7.7709
	step [202/244], loss=7.0155
	step [203/244], loss=11.8852
	step [204/244], loss=8.7052
	step [205/244], loss=7.6911
	step [206/244], loss=8.5302
	step [207/244], loss=6.9975
	step [208/244], loss=7.5140
	step [209/244], loss=8.9640
	step [210/244], loss=6.9520
	step [211/244], loss=8.0571
	step [212/244], loss=7.3683
	step [213/244], loss=10.1332
	step [214/244], loss=7.2293
	step [215/244], loss=6.6971
	step [216/244], loss=8.1076
	step [217/244], loss=7.1363
	step [218/244], loss=7.4131
	step [219/244], loss=7.8433
	step [220/244], loss=8.2614
	step [221/244], loss=7.7238
	step [222/244], loss=6.8905
	step [223/244], loss=8.6492
	step [224/244], loss=8.1687
	step [225/244], loss=6.8105
	step [226/244], loss=8.2025
	step [227/244], loss=7.3357
	step [228/244], loss=8.3943
	step [229/244], loss=7.7293
	step [230/244], loss=7.8086
	step [231/244], loss=7.2436
	step [232/244], loss=8.6064
	step [233/244], loss=5.7720
	step [234/244], loss=7.0555
	step [235/244], loss=7.6581
	step [236/244], loss=8.4479
	step [237/244], loss=7.7204
	step [238/244], loss=7.6112
	step [239/244], loss=8.5400
	step [240/244], loss=8.9050
	step [241/244], loss=8.2074
	step [242/244], loss=7.4695
	step [243/244], loss=7.9870
	step [244/244], loss=0.3672
	Evaluating
	loss=0.0282, precision=0.1564, recall=0.9983, f1=0.2704
Training epoch 22
	step [1/244], loss=7.7686
	step [2/244], loss=9.0527
	step [3/244], loss=8.7600
	step [4/244], loss=8.2377
	step [5/244], loss=7.5362
	step [6/244], loss=8.5532
	step [7/244], loss=8.5144
	step [8/244], loss=6.8340
	step [9/244], loss=8.2700
	step [10/244], loss=7.2320
	step [11/244], loss=7.7517
	step [12/244], loss=7.4439
	step [13/244], loss=6.8176
	step [14/244], loss=6.2292
	step [15/244], loss=6.5162
	step [16/244], loss=8.2844
	step [17/244], loss=6.3906
	step [18/244], loss=8.0046
	step [19/244], loss=6.7413
	step [20/244], loss=6.8589
	step [21/244], loss=8.5466
	step [22/244], loss=8.0348
	step [23/244], loss=7.2313
	step [24/244], loss=8.6413
	step [25/244], loss=7.7903
	step [26/244], loss=8.6596
	step [27/244], loss=8.4790
	step [28/244], loss=7.4123
	step [29/244], loss=7.4038
	step [30/244], loss=7.8150
	step [31/244], loss=7.7803
	step [32/244], loss=7.4773
	step [33/244], loss=8.3572
	step [34/244], loss=9.6263
	step [35/244], loss=6.9329
	step [36/244], loss=7.0923
	step [37/244], loss=7.4155
	step [38/244], loss=8.7856
	step [39/244], loss=8.7180
	step [40/244], loss=7.0338
	step [41/244], loss=6.3631
	step [42/244], loss=7.5744
	step [43/244], loss=7.1529
	step [44/244], loss=8.5916
	step [45/244], loss=7.6114
	step [46/244], loss=7.0266
	step [47/244], loss=6.8631
	step [48/244], loss=8.2015
	step [49/244], loss=6.3519
	step [50/244], loss=7.2795
	step [51/244], loss=7.4499
	step [52/244], loss=8.7634
	step [53/244], loss=7.1809
	step [54/244], loss=6.2024
	step [55/244], loss=7.3449
	step [56/244], loss=8.6053
	step [57/244], loss=6.7881
	step [58/244], loss=8.5239
	step [59/244], loss=8.2498
	step [60/244], loss=8.8998
	step [61/244], loss=7.7298
	step [62/244], loss=9.5453
	step [63/244], loss=8.4348
	step [64/244], loss=8.2946
	step [65/244], loss=7.5357
	step [66/244], loss=7.7151
	step [67/244], loss=7.7217
	step [68/244], loss=6.9757
	step [69/244], loss=6.0191
	step [70/244], loss=8.5327
	step [71/244], loss=7.7478
	step [72/244], loss=9.9914
	step [73/244], loss=7.4546
	step [74/244], loss=7.8960
	step [75/244], loss=8.1283
	step [76/244], loss=8.1435
	step [77/244], loss=6.6470
	step [78/244], loss=7.1660
	step [79/244], loss=8.0849
	step [80/244], loss=7.8589
	step [81/244], loss=8.3573
	step [82/244], loss=7.6377
	step [83/244], loss=8.3015
	step [84/244], loss=7.9553
	step [85/244], loss=10.8984
	step [86/244], loss=8.8112
	step [87/244], loss=7.8649
	step [88/244], loss=8.3605
	step [89/244], loss=8.0034
	step [90/244], loss=7.8315
	step [91/244], loss=7.2737
	step [92/244], loss=7.9929
	step [93/244], loss=7.9719
	step [94/244], loss=8.2195
	step [95/244], loss=7.5700
	step [96/244], loss=7.1927
	step [97/244], loss=7.7364
	step [98/244], loss=7.4036
	step [99/244], loss=8.6817
	step [100/244], loss=7.6637
	step [101/244], loss=7.5790
	step [102/244], loss=6.8371
	step [103/244], loss=7.2435
	step [104/244], loss=7.5564
	step [105/244], loss=8.4305
	step [106/244], loss=7.0434
	step [107/244], loss=9.1357
	step [108/244], loss=6.5074
	step [109/244], loss=9.1657
	step [110/244], loss=6.7301
	step [111/244], loss=8.1313
	step [112/244], loss=9.0157
	step [113/244], loss=8.6074
	step [114/244], loss=7.8441
	step [115/244], loss=7.3483
	step [116/244], loss=7.5730
	step [117/244], loss=7.5770
	step [118/244], loss=9.6665
	step [119/244], loss=8.8565
	step [120/244], loss=7.7085
	step [121/244], loss=7.3031
	step [122/244], loss=8.7623
	step [123/244], loss=7.4848
	step [124/244], loss=6.6875
	step [125/244], loss=7.8619
	step [126/244], loss=7.1998
	step [127/244], loss=7.0864
	step [128/244], loss=7.0393
	step [129/244], loss=8.3408
	step [130/244], loss=7.3189
	step [131/244], loss=6.3877
	step [132/244], loss=7.1176
	step [133/244], loss=7.4052
	step [134/244], loss=7.0528
	step [135/244], loss=8.4820
	step [136/244], loss=8.5372
	step [137/244], loss=5.7328
	step [138/244], loss=8.0760
	step [139/244], loss=8.4228
	step [140/244], loss=7.0279
	step [141/244], loss=6.2721
	step [142/244], loss=6.5895
	step [143/244], loss=7.2313
	step [144/244], loss=7.1007
	step [145/244], loss=8.2869
	step [146/244], loss=7.2804
	step [147/244], loss=7.2179
	step [148/244], loss=7.6622
	step [149/244], loss=7.5487
	step [150/244], loss=8.7550
	step [151/244], loss=7.8966
	step [152/244], loss=9.8207
	step [153/244], loss=8.9009
	step [154/244], loss=7.5191
	step [155/244], loss=7.1670
	step [156/244], loss=6.3017
	step [157/244], loss=6.9178
	step [158/244], loss=7.1929
	step [159/244], loss=7.7536
	step [160/244], loss=7.7054
	step [161/244], loss=6.1040
	step [162/244], loss=8.2592
	step [163/244], loss=5.9610
	step [164/244], loss=6.9507
	step [165/244], loss=6.9074
	step [166/244], loss=8.3537
	step [167/244], loss=8.0019
	step [168/244], loss=7.5650
	step [169/244], loss=7.1390
	step [170/244], loss=7.1291
	step [171/244], loss=7.0806
	step [172/244], loss=8.1170
	step [173/244], loss=8.7879
	step [174/244], loss=6.5018
	step [175/244], loss=7.6638
	step [176/244], loss=7.8827
	step [177/244], loss=6.5557
	step [178/244], loss=8.1995
	step [179/244], loss=7.2904
	step [180/244], loss=7.2055
	step [181/244], loss=7.2559
	step [182/244], loss=8.3255
	step [183/244], loss=7.6373
	step [184/244], loss=10.5819
	step [185/244], loss=7.2297
	step [186/244], loss=6.9447
	step [187/244], loss=8.3218
	step [188/244], loss=7.6236
	step [189/244], loss=6.5329
	step [190/244], loss=6.8693
	step [191/244], loss=7.7587
	step [192/244], loss=7.9549
	step [193/244], loss=8.9096
	step [194/244], loss=8.6552
	step [195/244], loss=8.0980
	step [196/244], loss=7.4779
	step [197/244], loss=7.8513
	step [198/244], loss=9.4376
	step [199/244], loss=7.6088
	step [200/244], loss=8.0500
	step [201/244], loss=7.0983
	step [202/244], loss=6.7977
	step [203/244], loss=8.9169
	step [204/244], loss=6.3343
	step [205/244], loss=9.1004
	step [206/244], loss=7.7738
	step [207/244], loss=6.5186
	step [208/244], loss=7.9524
	step [209/244], loss=7.8345
	step [210/244], loss=7.2436
	step [211/244], loss=6.7008
	step [212/244], loss=8.9119
	step [213/244], loss=7.6091
	step [214/244], loss=7.0364
	step [215/244], loss=5.9846
	step [216/244], loss=7.4923
	step [217/244], loss=8.1883
	step [218/244], loss=7.5823
	step [219/244], loss=7.7720
	step [220/244], loss=6.9995
	step [221/244], loss=6.9829
	step [222/244], loss=8.0703
	step [223/244], loss=6.7658
	step [224/244], loss=8.5923
	step [225/244], loss=6.8338
	step [226/244], loss=8.5880
	step [227/244], loss=7.1617
	step [228/244], loss=7.8640
	step [229/244], loss=7.5522
	step [230/244], loss=8.2656
	step [231/244], loss=8.7316
	step [232/244], loss=6.9630
	step [233/244], loss=7.3585
	step [234/244], loss=7.5165
	step [235/244], loss=7.0053
	step [236/244], loss=7.2164
	step [237/244], loss=6.5164
	step [238/244], loss=8.0258
	step [239/244], loss=6.1339
	step [240/244], loss=9.0779
	step [241/244], loss=8.2523
	step [242/244], loss=7.8841
	step [243/244], loss=6.9671
	step [244/244], loss=0.2710
	Evaluating
	loss=0.0220, precision=0.2130, recall=0.9972, f1=0.3510
Training epoch 23
	step [1/244], loss=7.3522
	step [2/244], loss=7.7038
	step [3/244], loss=7.4868
	step [4/244], loss=6.5420
	step [5/244], loss=8.5482
	step [6/244], loss=7.7134
	step [7/244], loss=6.6587
	step [8/244], loss=7.0373
	step [9/244], loss=7.6473
	step [10/244], loss=6.6587
	step [11/244], loss=8.3163
	step [12/244], loss=7.3051
	step [13/244], loss=5.6996
	step [14/244], loss=9.1665
	step [15/244], loss=8.6442
	step [16/244], loss=6.8011
	step [17/244], loss=8.8404
	step [18/244], loss=8.2008
	step [19/244], loss=6.5156
	step [20/244], loss=8.2431
	step [21/244], loss=7.9803
	step [22/244], loss=7.2715
	step [23/244], loss=7.6975
	step [24/244], loss=6.7583
	step [25/244], loss=7.6120
	step [26/244], loss=8.3257
	step [27/244], loss=8.1755
	step [28/244], loss=7.9483
	step [29/244], loss=7.8161
	step [30/244], loss=5.0521
	step [31/244], loss=7.4996
	step [32/244], loss=6.9958
	step [33/244], loss=5.5376
	step [34/244], loss=8.3910
	step [35/244], loss=5.9634
	step [36/244], loss=6.8642
	step [37/244], loss=6.2888
	step [38/244], loss=7.7449
	step [39/244], loss=8.5121
	step [40/244], loss=9.0327
	step [41/244], loss=6.7693
	step [42/244], loss=6.7007
	step [43/244], loss=8.7765
	step [44/244], loss=8.2004
	step [45/244], loss=7.7424
	step [46/244], loss=8.4634
	step [47/244], loss=6.9886
	step [48/244], loss=7.2523
	step [49/244], loss=6.9947
	step [50/244], loss=7.8226
	step [51/244], loss=7.0432
	step [52/244], loss=5.8144
	step [53/244], loss=7.4867
	step [54/244], loss=7.7579
	step [55/244], loss=7.4078
	step [56/244], loss=7.4255
	step [57/244], loss=6.7133
	step [58/244], loss=6.9072
	step [59/244], loss=7.7379
	step [60/244], loss=8.0671
	step [61/244], loss=9.5774
	step [62/244], loss=7.8951
	step [63/244], loss=7.2579
	step [64/244], loss=10.4128
	step [65/244], loss=6.3587
	step [66/244], loss=6.2115
	step [67/244], loss=6.9796
	step [68/244], loss=6.6861
	step [69/244], loss=7.8428
	step [70/244], loss=9.4325
	step [71/244], loss=8.2188
	step [72/244], loss=7.5254
	step [73/244], loss=6.6268
	step [74/244], loss=7.0988
	step [75/244], loss=8.6865
	step [76/244], loss=6.8672
	step [77/244], loss=6.8086
	step [78/244], loss=7.2895
	step [79/244], loss=5.9105
	step [80/244], loss=6.2603
	step [81/244], loss=7.7883
	step [82/244], loss=8.0774
	step [83/244], loss=6.1254
	step [84/244], loss=6.4269
	step [85/244], loss=6.9179
	step [86/244], loss=7.1583
	step [87/244], loss=8.6762
	step [88/244], loss=7.2997
	step [89/244], loss=7.0141
	step [90/244], loss=7.2392
	step [91/244], loss=7.8651
	step [92/244], loss=7.5070
	step [93/244], loss=8.7048
	step [94/244], loss=6.7511
	step [95/244], loss=7.8457
	step [96/244], loss=6.8618
	step [97/244], loss=7.6292
	step [98/244], loss=7.3279
	step [99/244], loss=6.6638
	step [100/244], loss=8.1896
	step [101/244], loss=6.6344
	step [102/244], loss=9.2806
	step [103/244], loss=10.1002
	step [104/244], loss=7.6235
	step [105/244], loss=8.1939
	step [106/244], loss=6.7143
	step [107/244], loss=6.7470
	step [108/244], loss=7.6334
	step [109/244], loss=7.3599
	step [110/244], loss=6.3686
	step [111/244], loss=8.4700
	step [112/244], loss=7.5152
	step [113/244], loss=5.9699
	step [114/244], loss=6.5695
	step [115/244], loss=8.4422
	step [116/244], loss=7.6750
	step [117/244], loss=7.5808
	step [118/244], loss=8.5201
	step [119/244], loss=7.6127
	step [120/244], loss=6.2023
	step [121/244], loss=6.4759
	step [122/244], loss=8.7237
	step [123/244], loss=6.8315
	step [124/244], loss=6.5520
	step [125/244], loss=9.6178
	step [126/244], loss=7.7783
	step [127/244], loss=7.5197
	step [128/244], loss=8.5792
	step [129/244], loss=6.8280
	step [130/244], loss=6.7513
	step [131/244], loss=7.7420
	step [132/244], loss=6.7986
	step [133/244], loss=7.9862
	step [134/244], loss=9.5530
	step [135/244], loss=7.8407
	step [136/244], loss=7.5485
	step [137/244], loss=7.4733
	step [138/244], loss=7.5258
	step [139/244], loss=7.4366
	step [140/244], loss=6.0902
	step [141/244], loss=6.4456
	step [142/244], loss=6.3862
	step [143/244], loss=8.9345
	step [144/244], loss=7.3873
	step [145/244], loss=7.3362
	step [146/244], loss=8.7710
	step [147/244], loss=7.6854
	step [148/244], loss=7.7324
	step [149/244], loss=7.5673
	step [150/244], loss=5.9318
	step [151/244], loss=8.5505
	step [152/244], loss=6.6269
	step [153/244], loss=8.3866
	step [154/244], loss=8.0764
	step [155/244], loss=7.9700
	step [156/244], loss=7.6700
	step [157/244], loss=7.0576
	step [158/244], loss=7.3225
	step [159/244], loss=6.6240
	step [160/244], loss=7.2393
	step [161/244], loss=6.7481
	step [162/244], loss=8.0761
	step [163/244], loss=6.9240
	step [164/244], loss=8.4508
	step [165/244], loss=6.9003
	step [166/244], loss=7.4858
	step [167/244], loss=8.8333
	step [168/244], loss=9.1024
	step [169/244], loss=6.2782
	step [170/244], loss=7.5773
	step [171/244], loss=6.6015
	step [172/244], loss=7.5711
	step [173/244], loss=6.3771
	step [174/244], loss=7.1554
	step [175/244], loss=7.9385
	step [176/244], loss=6.6379
	step [177/244], loss=7.3538
	step [178/244], loss=8.3980
	step [179/244], loss=8.8979
	step [180/244], loss=6.3342
	step [181/244], loss=7.0102
	step [182/244], loss=8.2618
	step [183/244], loss=6.8541
	step [184/244], loss=6.4638
	step [185/244], loss=7.5578
	step [186/244], loss=6.6146
	step [187/244], loss=6.9779
	step [188/244], loss=6.9757
	step [189/244], loss=5.7672
	step [190/244], loss=7.0468
	step [191/244], loss=6.4835
	step [192/244], loss=6.2683
	step [193/244], loss=6.5877
	step [194/244], loss=6.3541
	step [195/244], loss=6.8585
	step [196/244], loss=8.0635
	step [197/244], loss=6.9611
	step [198/244], loss=6.9499
	step [199/244], loss=7.2468
	step [200/244], loss=5.6161
	step [201/244], loss=6.4601
	step [202/244], loss=7.3958
	step [203/244], loss=6.2087
	step [204/244], loss=6.5531
	step [205/244], loss=7.5622
	step [206/244], loss=7.3361
	step [207/244], loss=7.6322
	step [208/244], loss=8.2858
	step [209/244], loss=9.4236
	step [210/244], loss=7.7465
	step [211/244], loss=7.6040
	step [212/244], loss=7.5220
	step [213/244], loss=8.0445
	step [214/244], loss=7.6121
	step [215/244], loss=7.3736
	step [216/244], loss=6.7337
	step [217/244], loss=8.1283
	step [218/244], loss=7.3798
	step [219/244], loss=6.0893
	step [220/244], loss=6.3910
	step [221/244], loss=6.7067
	step [222/244], loss=7.7948
	step [223/244], loss=7.1906
	step [224/244], loss=6.4552
	step [225/244], loss=7.4818
	step [226/244], loss=7.8275
	step [227/244], loss=7.2467
	step [228/244], loss=6.1432
	step [229/244], loss=9.3176
	step [230/244], loss=7.1272
	step [231/244], loss=8.6816
	step [232/244], loss=8.2674
	step [233/244], loss=7.6015
	step [234/244], loss=7.8364
	step [235/244], loss=7.5422
	step [236/244], loss=7.4931
	step [237/244], loss=7.6438
	step [238/244], loss=8.5524
	step [239/244], loss=6.4487
	step [240/244], loss=8.6113
	step [241/244], loss=6.4019
	step [242/244], loss=6.4265
	step [243/244], loss=7.2358
	step [244/244], loss=0.4281
	Evaluating
	loss=0.0193, precision=0.2130, recall=0.9969, f1=0.3510
Training epoch 24
	step [1/244], loss=6.6423
	step [2/244], loss=8.7816
	step [3/244], loss=7.6101
	step [4/244], loss=6.4516
	step [5/244], loss=6.8332
	step [6/244], loss=8.7821
	step [7/244], loss=5.4124
	step [8/244], loss=8.2676
	step [9/244], loss=7.6798
	step [10/244], loss=6.7598
	step [11/244], loss=8.4034
	step [12/244], loss=8.4851
	step [13/244], loss=8.3911
	step [14/244], loss=6.8837
	step [15/244], loss=6.2912
	step [16/244], loss=6.3460
	step [17/244], loss=6.6000
	step [18/244], loss=8.5530
	step [19/244], loss=7.2018
	step [20/244], loss=7.3836
	step [21/244], loss=6.7254
	step [22/244], loss=6.4392
	step [23/244], loss=7.5946
	step [24/244], loss=6.1449
	step [25/244], loss=8.2945
	step [26/244], loss=8.7528
	step [27/244], loss=8.2475
	step [28/244], loss=6.4417
	step [29/244], loss=6.7252
	step [30/244], loss=8.5667
	step [31/244], loss=6.0133
	step [32/244], loss=7.3626
	step [33/244], loss=7.3540
	step [34/244], loss=8.5292
	step [35/244], loss=6.4838
	step [36/244], loss=6.6928
	step [37/244], loss=7.4178
	step [38/244], loss=7.3228
	step [39/244], loss=6.9118
	step [40/244], loss=6.5871
	step [41/244], loss=6.3413
	step [42/244], loss=7.6664
	step [43/244], loss=8.1131
	step [44/244], loss=5.8897
	step [45/244], loss=6.1395
	step [46/244], loss=8.4759
	step [47/244], loss=6.2890
	step [48/244], loss=6.0239
	step [49/244], loss=7.0140
	step [50/244], loss=5.9589
	step [51/244], loss=8.3392
	step [52/244], loss=6.8261
	step [53/244], loss=8.4217
	step [54/244], loss=8.0076
	step [55/244], loss=7.3958
	step [56/244], loss=7.3566
	step [57/244], loss=6.1877
	step [58/244], loss=7.8678
	step [59/244], loss=7.6181
	step [60/244], loss=8.4913
	step [61/244], loss=7.2210
	step [62/244], loss=7.2988
	step [63/244], loss=7.0133
	step [64/244], loss=6.7524
	step [65/244], loss=8.1521
	step [66/244], loss=7.2873
	step [67/244], loss=6.9951
	step [68/244], loss=7.1781
	step [69/244], loss=8.3835
	step [70/244], loss=7.3391
	step [71/244], loss=7.4439
	step [72/244], loss=5.9413
	step [73/244], loss=6.8833
	step [74/244], loss=7.3412
	step [75/244], loss=5.2299
	step [76/244], loss=6.4796
	step [77/244], loss=6.4294
	step [78/244], loss=6.7607
	step [79/244], loss=7.6511
	step [80/244], loss=6.4931
	step [81/244], loss=6.6313
	step [82/244], loss=7.2943
	step [83/244], loss=7.0686
	step [84/244], loss=7.3174
	step [85/244], loss=6.2618
	step [86/244], loss=7.4327
	step [87/244], loss=7.9019
	step [88/244], loss=8.3627
	step [89/244], loss=7.9455
	step [90/244], loss=8.4654
	step [91/244], loss=7.2215
	step [92/244], loss=6.4611
	step [93/244], loss=5.9041
	step [94/244], loss=7.8136
	step [95/244], loss=7.4376
	step [96/244], loss=7.9963
	step [97/244], loss=7.2233
	step [98/244], loss=5.8392
	step [99/244], loss=6.3707
	step [100/244], loss=6.6015
	step [101/244], loss=9.2891
	step [102/244], loss=6.9744
	step [103/244], loss=8.5475
	step [104/244], loss=6.3389
	step [105/244], loss=7.7443
	step [106/244], loss=6.7858
	step [107/244], loss=8.4009
	step [108/244], loss=6.8538
	step [109/244], loss=6.8539
	step [110/244], loss=7.2361
	step [111/244], loss=8.4320
	step [112/244], loss=7.1165
	step [113/244], loss=7.0154
	step [114/244], loss=7.3057
	step [115/244], loss=9.0507
	step [116/244], loss=7.0459
	step [117/244], loss=6.3105
	step [118/244], loss=8.7327
	step [119/244], loss=7.3879
	step [120/244], loss=8.1607
	step [121/244], loss=6.4324
	step [122/244], loss=6.4342
	step [123/244], loss=7.1552
	step [124/244], loss=8.5674
	step [125/244], loss=7.2449
	step [126/244], loss=6.3959
	step [127/244], loss=7.2969
	step [128/244], loss=6.9899
	step [129/244], loss=7.4142
	step [130/244], loss=7.5557
	step [131/244], loss=7.1096
	step [132/244], loss=5.6827
	step [133/244], loss=7.7397
	step [134/244], loss=6.2283
	step [135/244], loss=6.5038
	step [136/244], loss=7.7792
	step [137/244], loss=6.2035
	step [138/244], loss=7.8412
	step [139/244], loss=6.2265
	step [140/244], loss=7.8452
	step [141/244], loss=6.7439
	step [142/244], loss=7.7167
	step [143/244], loss=9.0066
	step [144/244], loss=8.1026
	step [145/244], loss=6.6488
	step [146/244], loss=6.9345
	step [147/244], loss=6.0644
	step [148/244], loss=7.3653
	step [149/244], loss=6.8032
	step [150/244], loss=9.5809
	step [151/244], loss=6.5076
	step [152/244], loss=9.2738
	step [153/244], loss=6.4456
	step [154/244], loss=8.4597
	step [155/244], loss=6.4322
	step [156/244], loss=5.6816
	step [157/244], loss=8.5460
	step [158/244], loss=8.7093
	step [159/244], loss=8.3086
	step [160/244], loss=7.1383
	step [161/244], loss=5.6217
	step [162/244], loss=7.7062
	step [163/244], loss=7.2030
	step [164/244], loss=6.4850
	step [165/244], loss=5.8329
	step [166/244], loss=7.2871
	step [167/244], loss=5.6150
	step [168/244], loss=6.2864
	step [169/244], loss=9.5330
	step [170/244], loss=7.3618
	step [171/244], loss=6.1589
	step [172/244], loss=7.0729
	step [173/244], loss=7.5544
	step [174/244], loss=7.4005
	step [175/244], loss=6.9287
	step [176/244], loss=6.5794
	step [177/244], loss=10.3053
	step [178/244], loss=6.0229
	step [179/244], loss=7.0221
	step [180/244], loss=7.3900
	step [181/244], loss=7.5216
	step [182/244], loss=7.5367
	step [183/244], loss=7.7926
	step [184/244], loss=5.8529
	step [185/244], loss=6.0466
	step [186/244], loss=8.0596
	step [187/244], loss=7.4455
	step [188/244], loss=8.0795
	step [189/244], loss=7.1539
	step [190/244], loss=6.5536
	step [191/244], loss=6.9912
	step [192/244], loss=8.9735
	step [193/244], loss=7.4717
	step [194/244], loss=8.8126
	step [195/244], loss=5.8326
	step [196/244], loss=6.9489
	step [197/244], loss=6.6577
	step [198/244], loss=5.8543
	step [199/244], loss=7.5804
	step [200/244], loss=6.9490
	step [201/244], loss=6.5965
	step [202/244], loss=7.3621
	step [203/244], loss=7.1892
	step [204/244], loss=6.6964
	step [205/244], loss=6.5342
	step [206/244], loss=5.7219
	step [207/244], loss=6.7594
	step [208/244], loss=6.1319
	step [209/244], loss=9.2877
	step [210/244], loss=8.1906
	step [211/244], loss=7.8890
	step [212/244], loss=6.1169
	step [213/244], loss=7.5689
	step [214/244], loss=6.3345
	step [215/244], loss=6.6246
	step [216/244], loss=6.1153
	step [217/244], loss=6.7246
	step [218/244], loss=8.0015
	step [219/244], loss=8.2487
	step [220/244], loss=6.1682
	step [221/244], loss=7.7870
	step [222/244], loss=6.8506
	step [223/244], loss=6.4549
	step [224/244], loss=6.4571
	step [225/244], loss=6.8844
	step [226/244], loss=8.4529
	step [227/244], loss=5.9677
	step [228/244], loss=5.9708
	step [229/244], loss=6.3141
	step [230/244], loss=7.9543
	step [231/244], loss=8.2225
	step [232/244], loss=7.4468
	step [233/244], loss=7.8377
	step [234/244], loss=7.1011
	step [235/244], loss=7.6103
	step [236/244], loss=6.1754
	step [237/244], loss=7.4690
	step [238/244], loss=7.5842
	step [239/244], loss=7.7864
	step [240/244], loss=6.6054
	step [241/244], loss=7.5421
	step [242/244], loss=7.2128
	step [243/244], loss=6.3454
	step [244/244], loss=0.4724
	Evaluating
	loss=0.0192, precision=0.2106, recall=0.9971, f1=0.3477
Training epoch 25
	step [1/244], loss=7.6765
	step [2/244], loss=6.5711
	step [3/244], loss=6.9142
	step [4/244], loss=7.2700
	step [5/244], loss=6.8818
	step [6/244], loss=7.7911
	step [7/244], loss=7.3237
	step [8/244], loss=7.2249
	step [9/244], loss=6.9375
	step [10/244], loss=7.5116
	step [11/244], loss=7.4960
	step [12/244], loss=7.2343
	step [13/244], loss=8.7439
	step [14/244], loss=6.8347
	step [15/244], loss=6.9045
	step [16/244], loss=6.5350
	step [17/244], loss=7.9322
	step [18/244], loss=7.2830
	step [19/244], loss=6.2795
	step [20/244], loss=5.9970
	step [21/244], loss=6.1844
	step [22/244], loss=6.2285
	step [23/244], loss=7.0960
	step [24/244], loss=6.3982
	step [25/244], loss=7.4981
	step [26/244], loss=6.6030
	step [27/244], loss=7.4082
	step [28/244], loss=5.8809
	step [29/244], loss=6.8440
	step [30/244], loss=5.8433
	step [31/244], loss=5.8313
	step [32/244], loss=6.8569
	step [33/244], loss=6.8700
	step [34/244], loss=6.7801
	step [35/244], loss=6.8628
	step [36/244], loss=6.3339
	step [37/244], loss=6.4397
	step [38/244], loss=7.5347
	step [39/244], loss=7.8974
	step [40/244], loss=6.3075
	step [41/244], loss=9.9775
	step [42/244], loss=7.5183
	step [43/244], loss=7.6265
	step [44/244], loss=7.1513
	step [45/244], loss=7.3848
	step [46/244], loss=6.6315
	step [47/244], loss=6.8338
	step [48/244], loss=8.1942
	step [49/244], loss=8.2064
	step [50/244], loss=9.0846
	step [51/244], loss=7.5355
	step [52/244], loss=6.5268
	step [53/244], loss=8.6690
	step [54/244], loss=6.7996
	step [55/244], loss=7.0600
	step [56/244], loss=7.8507
	step [57/244], loss=6.2284
	step [58/244], loss=9.0642
	step [59/244], loss=7.1047
	step [60/244], loss=7.5165
	step [61/244], loss=7.1554
	step [62/244], loss=7.2996
	step [63/244], loss=7.1728
	step [64/244], loss=6.1674
	step [65/244], loss=6.6618
	step [66/244], loss=8.2596
	step [67/244], loss=6.5448
	step [68/244], loss=6.8796
	step [69/244], loss=6.4921
	step [70/244], loss=7.5656
	step [71/244], loss=6.4925
	step [72/244], loss=6.5652
	step [73/244], loss=5.8300
	step [74/244], loss=7.1041
	step [75/244], loss=5.5503
	step [76/244], loss=6.8354
	step [77/244], loss=6.2006
	step [78/244], loss=5.5465
	step [79/244], loss=7.9388
	step [80/244], loss=6.7436
	step [81/244], loss=7.7432
	step [82/244], loss=6.7346
	step [83/244], loss=6.5131
	step [84/244], loss=8.6777
	step [85/244], loss=6.8567
	step [86/244], loss=7.2243
	step [87/244], loss=6.5045
	step [88/244], loss=7.7899
	step [89/244], loss=7.3925
	step [90/244], loss=7.1330
	step [91/244], loss=6.9498
	step [92/244], loss=8.7709
	step [93/244], loss=6.2485
	step [94/244], loss=7.2428
	step [95/244], loss=7.8902
	step [96/244], loss=5.3866
	step [97/244], loss=7.0470
	step [98/244], loss=7.3343
	step [99/244], loss=7.5388
	step [100/244], loss=9.3184
	step [101/244], loss=6.4310
	step [102/244], loss=6.5680
	step [103/244], loss=6.3762
	step [104/244], loss=7.3288
	step [105/244], loss=6.1161
	step [106/244], loss=6.9848
	step [107/244], loss=9.3726
	step [108/244], loss=5.7543
	step [109/244], loss=7.4470
	step [110/244], loss=6.5564
	step [111/244], loss=7.3714
	step [112/244], loss=5.4205
	step [113/244], loss=7.7526
	step [114/244], loss=6.5653
	step [115/244], loss=7.8591
	step [116/244], loss=6.1467
	step [117/244], loss=7.0775
	step [118/244], loss=7.7354
	step [119/244], loss=7.2246
	step [120/244], loss=7.8219
	step [121/244], loss=6.1658
	step [122/244], loss=5.8322
	step [123/244], loss=6.9663
	step [124/244], loss=7.6609
	step [125/244], loss=7.8926
	step [126/244], loss=6.9691
	step [127/244], loss=6.3826
	step [128/244], loss=7.2925
	step [129/244], loss=7.3075
	step [130/244], loss=5.5611
	step [131/244], loss=7.7832
	step [132/244], loss=6.7877
	step [133/244], loss=6.4613
	step [134/244], loss=7.3453
	step [135/244], loss=7.0349
	step [136/244], loss=7.2449
	step [137/244], loss=7.9501
	step [138/244], loss=7.1347
	step [139/244], loss=7.2654
	step [140/244], loss=6.5441
	step [141/244], loss=8.3722
	step [142/244], loss=8.0677
	step [143/244], loss=8.5750
	step [144/244], loss=6.1204
	step [145/244], loss=7.8992
	step [146/244], loss=7.5392
	step [147/244], loss=6.7482
	step [148/244], loss=6.2940
	step [149/244], loss=7.5291
	step [150/244], loss=7.9844
	step [151/244], loss=7.2386
	step [152/244], loss=6.9317
	step [153/244], loss=5.5563
	step [154/244], loss=6.9605
	step [155/244], loss=7.8565
	step [156/244], loss=5.9832
	step [157/244], loss=8.0307
	step [158/244], loss=7.0310
	step [159/244], loss=8.0264
	step [160/244], loss=7.9194
	step [161/244], loss=5.8708
	step [162/244], loss=6.4508
	step [163/244], loss=6.6191
	step [164/244], loss=7.5889
	step [165/244], loss=6.6115
	step [166/244], loss=7.0494
	step [167/244], loss=8.7666
	step [168/244], loss=7.5674
	step [169/244], loss=6.3631
	step [170/244], loss=7.3749
	step [171/244], loss=6.7252
	step [172/244], loss=7.7512
	step [173/244], loss=6.9125
	step [174/244], loss=6.5917
	step [175/244], loss=6.0061
	step [176/244], loss=7.2004
	step [177/244], loss=6.6497
	step [178/244], loss=7.0056
	step [179/244], loss=6.3791
	step [180/244], loss=6.8206
	step [181/244], loss=6.0416
	step [182/244], loss=6.2555
	step [183/244], loss=6.2477
	step [184/244], loss=6.4901
	step [185/244], loss=6.1093
	step [186/244], loss=8.2181
	step [187/244], loss=7.1875
	step [188/244], loss=6.3085
	step [189/244], loss=6.5850
	step [190/244], loss=7.7254
	step [191/244], loss=7.1476
	step [192/244], loss=7.1318
	step [193/244], loss=8.0356
	step [194/244], loss=7.8738
	step [195/244], loss=7.9642
	step [196/244], loss=6.9615
	step [197/244], loss=6.3611
	step [198/244], loss=6.0565
	step [199/244], loss=6.3490
	step [200/244], loss=8.6360
	step [201/244], loss=5.8388
	step [202/244], loss=10.7688
	step [203/244], loss=6.2831
	step [204/244], loss=6.0384
	step [205/244], loss=6.8238
	step [206/244], loss=6.8652
	step [207/244], loss=6.5050
	step [208/244], loss=6.7072
	step [209/244], loss=6.0896
	step [210/244], loss=6.5933
	step [211/244], loss=7.6240
	step [212/244], loss=7.6666
	step [213/244], loss=7.0353
	step [214/244], loss=7.0812
	step [215/244], loss=7.8833
	step [216/244], loss=6.2005
	step [217/244], loss=6.7589
	step [218/244], loss=6.6191
	step [219/244], loss=5.7915
	step [220/244], loss=6.0542
	step [221/244], loss=5.9816
	step [222/244], loss=8.0246
	step [223/244], loss=7.6061
	step [224/244], loss=6.6867
	step [225/244], loss=5.9056
	step [226/244], loss=6.6536
	step [227/244], loss=7.8208
	step [228/244], loss=7.6325
	step [229/244], loss=7.2970
	step [230/244], loss=5.7376
	step [231/244], loss=6.6526
	step [232/244], loss=7.0950
	step [233/244], loss=6.9744
	step [234/244], loss=7.8976
	step [235/244], loss=9.3462
	step [236/244], loss=5.9195
	step [237/244], loss=6.9137
	step [238/244], loss=8.4325
	step [239/244], loss=7.3150
	step [240/244], loss=7.0891
	step [241/244], loss=6.2969
	step [242/244], loss=8.0993
	step [243/244], loss=6.6701
	step [244/244], loss=0.5463
	Evaluating
	loss=0.0201, precision=0.1980, recall=0.9973, f1=0.3304
Training epoch 26
	step [1/244], loss=6.2463
	step [2/244], loss=6.5458
	step [3/244], loss=7.2739
	step [4/244], loss=7.9644
	step [5/244], loss=8.2647
	step [6/244], loss=5.6850
	step [7/244], loss=6.1583
	step [8/244], loss=6.2160
	step [9/244], loss=7.4763
	step [10/244], loss=7.0967
	step [11/244], loss=5.5436
	step [12/244], loss=7.1572
	step [13/244], loss=6.2457
	step [14/244], loss=6.0938
	step [15/244], loss=7.9590
	step [16/244], loss=5.6409
	step [17/244], loss=6.7329
	step [18/244], loss=7.0384
	step [19/244], loss=8.0286
	step [20/244], loss=7.4983
	step [21/244], loss=7.2632
	step [22/244], loss=6.8871
	step [23/244], loss=6.6132
	step [24/244], loss=6.7921
	step [25/244], loss=7.5747
	step [26/244], loss=5.8456
	step [27/244], loss=8.2152
	step [28/244], loss=7.7011
	step [29/244], loss=5.9530
	step [30/244], loss=6.5648
	step [31/244], loss=7.8372
	step [32/244], loss=5.7488
	step [33/244], loss=6.1615
	step [34/244], loss=7.5584
	step [35/244], loss=5.6333
	step [36/244], loss=7.5726
	step [37/244], loss=5.7628
	step [38/244], loss=5.8202
	step [39/244], loss=9.2675
	step [40/244], loss=8.0675
	step [41/244], loss=7.1920
	step [42/244], loss=6.1555
	step [43/244], loss=7.7972
	step [44/244], loss=5.7589
	step [45/244], loss=7.1609
	step [46/244], loss=7.0174
	step [47/244], loss=6.6085
	step [48/244], loss=7.4249
	step [49/244], loss=6.9290
	step [50/244], loss=5.8113
	step [51/244], loss=7.5711
	step [52/244], loss=6.9435
	step [53/244], loss=6.9778
	step [54/244], loss=5.6063
	step [55/244], loss=6.5454
	step [56/244], loss=5.9995
	step [57/244], loss=6.2535
	step [58/244], loss=6.9664
	step [59/244], loss=6.8582
	step [60/244], loss=6.5281
	step [61/244], loss=7.3175
	step [62/244], loss=5.9317
	step [63/244], loss=6.5515
	step [64/244], loss=7.2401
	step [65/244], loss=6.2382
	step [66/244], loss=7.2653
	step [67/244], loss=6.9110
	step [68/244], loss=5.9736
	step [69/244], loss=8.8422
	step [70/244], loss=6.4172
	step [71/244], loss=6.4697
	step [72/244], loss=7.0316
	step [73/244], loss=6.9551
	step [74/244], loss=7.1744
	step [75/244], loss=5.3933
	step [76/244], loss=6.5859
	step [77/244], loss=7.8571
	step [78/244], loss=6.8366
	step [79/244], loss=5.6276
	step [80/244], loss=5.0811
	step [81/244], loss=5.9702
	step [82/244], loss=6.0247
	step [83/244], loss=5.5765
	step [84/244], loss=5.6546
	step [85/244], loss=7.2476
	step [86/244], loss=6.7388
	step [87/244], loss=7.1635
	step [88/244], loss=7.7031
	step [89/244], loss=7.8678
	step [90/244], loss=6.1410
	step [91/244], loss=7.1549
	step [92/244], loss=7.0063
	step [93/244], loss=6.4715
	step [94/244], loss=6.7574
	step [95/244], loss=7.8626
	step [96/244], loss=9.1951
	step [97/244], loss=6.9528
	step [98/244], loss=6.7197
	step [99/244], loss=6.9291
	step [100/244], loss=8.0632
	step [101/244], loss=6.1111
	step [102/244], loss=6.6824
	step [103/244], loss=6.7426
	step [104/244], loss=7.1092
	step [105/244], loss=7.2445
	step [106/244], loss=5.2822
	step [107/244], loss=6.8452
	step [108/244], loss=7.1171
	step [109/244], loss=6.7337
	step [110/244], loss=6.6792
	step [111/244], loss=6.6869
	step [112/244], loss=6.8148
	step [113/244], loss=6.8814
	step [114/244], loss=6.1840
	step [115/244], loss=6.0695
	step [116/244], loss=6.6542
	step [117/244], loss=7.2272
	step [118/244], loss=6.4166
	step [119/244], loss=7.0519
	step [120/244], loss=7.3672
	step [121/244], loss=6.9459
	step [122/244], loss=6.7727
	step [123/244], loss=6.4315
	step [124/244], loss=7.1983
	step [125/244], loss=7.3315
	step [126/244], loss=7.0640
	step [127/244], loss=8.0454
	step [128/244], loss=6.7265
	step [129/244], loss=5.4893
	step [130/244], loss=6.4776
	step [131/244], loss=6.6489
	step [132/244], loss=7.1388
	step [133/244], loss=6.9019
	step [134/244], loss=6.1460
	step [135/244], loss=4.9621
	step [136/244], loss=6.5800
	step [137/244], loss=8.0293
	step [138/244], loss=5.8175
	step [139/244], loss=6.0390
	step [140/244], loss=5.6328
	step [141/244], loss=6.9243
	step [142/244], loss=9.6230
	step [143/244], loss=5.9098
	step [144/244], loss=6.1630
	step [145/244], loss=6.4390
	step [146/244], loss=6.2207
	step [147/244], loss=8.6870
	step [148/244], loss=6.1422
	step [149/244], loss=7.5019
	step [150/244], loss=6.2231
	step [151/244], loss=6.8939
	step [152/244], loss=7.3207
	step [153/244], loss=6.6786
	step [154/244], loss=6.7074
	step [155/244], loss=8.0118
	step [156/244], loss=7.0934
	step [157/244], loss=6.1684
	step [158/244], loss=6.5811
	step [159/244], loss=5.8665
	step [160/244], loss=6.8327
	step [161/244], loss=7.0454
	step [162/244], loss=6.8786
	step [163/244], loss=6.7743
	step [164/244], loss=7.3065
	step [165/244], loss=6.7491
	step [166/244], loss=5.6427
	step [167/244], loss=5.1078
	step [168/244], loss=6.7621
	step [169/244], loss=8.7909
	step [170/244], loss=7.0252
	step [171/244], loss=6.2532
	step [172/244], loss=8.9143
	step [173/244], loss=5.7520
	step [174/244], loss=7.4672
	step [175/244], loss=6.0173
	step [176/244], loss=6.2510
	step [177/244], loss=6.4697
	step [178/244], loss=7.7660
	step [179/244], loss=8.7241
	step [180/244], loss=9.6384
	step [181/244], loss=7.4810
	step [182/244], loss=6.4378
	step [183/244], loss=8.0481
	step [184/244], loss=6.2574
	step [185/244], loss=6.6513
	step [186/244], loss=7.4036
	step [187/244], loss=7.0670
	step [188/244], loss=7.5501
	step [189/244], loss=7.0825
	step [190/244], loss=7.7192
	step [191/244], loss=8.5974
	step [192/244], loss=6.7456
	step [193/244], loss=7.4095
	step [194/244], loss=6.4157
	step [195/244], loss=6.2926
	step [196/244], loss=6.8971
	step [197/244], loss=7.1188
	step [198/244], loss=6.1322
	step [199/244], loss=5.8413
	step [200/244], loss=7.7456
	step [201/244], loss=6.8658
	step [202/244], loss=8.0426
	step [203/244], loss=7.4443
	step [204/244], loss=5.5780
	step [205/244], loss=7.1042
	step [206/244], loss=6.6100
	step [207/244], loss=7.3868
	step [208/244], loss=7.4046
	step [209/244], loss=7.2717
	step [210/244], loss=6.2250
	step [211/244], loss=5.9899
	step [212/244], loss=6.4881
	step [213/244], loss=7.7676
	step [214/244], loss=9.5683
	step [215/244], loss=7.9927
	step [216/244], loss=6.1814
	step [217/244], loss=7.1894
	step [218/244], loss=6.5327
	step [219/244], loss=6.7056
	step [220/244], loss=8.2755
	step [221/244], loss=7.5461
	step [222/244], loss=6.8415
	step [223/244], loss=7.1241
	step [224/244], loss=6.3909
	step [225/244], loss=6.2443
	step [226/244], loss=5.8278
	step [227/244], loss=6.6749
	step [228/244], loss=6.4122
	step [229/244], loss=6.3276
	step [230/244], loss=6.8592
	step [231/244], loss=7.8231
	step [232/244], loss=6.4166
	step [233/244], loss=7.6008
	step [234/244], loss=5.9125
	step [235/244], loss=6.8171
	step [236/244], loss=5.6162
	step [237/244], loss=6.0244
	step [238/244], loss=7.7434
	step [239/244], loss=7.4965
	step [240/244], loss=6.2379
	step [241/244], loss=7.5321
	step [242/244], loss=6.7628
	step [243/244], loss=4.7257
	step [244/244], loss=0.5878
	Evaluating
	loss=0.0190, precision=0.2304, recall=0.9965, f1=0.3743
saving model as: 2_saved_model.pth
Training epoch 27
	step [1/244], loss=7.3856
	step [2/244], loss=8.3425
	step [3/244], loss=6.9760
	step [4/244], loss=6.6177
	step [5/244], loss=6.1307
	step [6/244], loss=5.9455
	step [7/244], loss=6.3177
	step [8/244], loss=5.9001
	step [9/244], loss=6.6890
	step [10/244], loss=7.4577
	step [11/244], loss=6.4929
	step [12/244], loss=8.6799
	step [13/244], loss=5.1160
	step [14/244], loss=6.4177
	step [15/244], loss=6.1892
	step [16/244], loss=6.0312
	step [17/244], loss=7.2318
	step [18/244], loss=7.4624
	step [19/244], loss=7.7751
	step [20/244], loss=7.2492
	step [21/244], loss=7.5878
	step [22/244], loss=6.1970
	step [23/244], loss=6.6595
	step [24/244], loss=6.3444
	step [25/244], loss=6.5178
	step [26/244], loss=6.1800
	step [27/244], loss=6.7784
	step [28/244], loss=5.9151
	step [29/244], loss=5.6886
	step [30/244], loss=6.7636
	step [31/244], loss=8.9403
	step [32/244], loss=7.5655
	step [33/244], loss=6.2673
	step [34/244], loss=6.9599
	step [35/244], loss=6.4052
	step [36/244], loss=7.4662
	step [37/244], loss=7.0664
	step [38/244], loss=6.1137
	step [39/244], loss=6.6426
	step [40/244], loss=7.1303
	step [41/244], loss=6.2778
	step [42/244], loss=6.3644
	step [43/244], loss=5.4536
	step [44/244], loss=5.7906
	step [45/244], loss=7.8027
	step [46/244], loss=7.5637
	step [47/244], loss=6.6095
	step [48/244], loss=6.1886
	step [49/244], loss=6.2492
	step [50/244], loss=6.9658
	step [51/244], loss=7.1230
	step [52/244], loss=5.8368
	step [53/244], loss=7.3764
	step [54/244], loss=10.6976
	step [55/244], loss=7.2911
	step [56/244], loss=7.5174
	step [57/244], loss=7.5324
	step [58/244], loss=6.3645
	step [59/244], loss=7.7046
	step [60/244], loss=5.7999
	step [61/244], loss=8.3191
	step [62/244], loss=7.5145
	step [63/244], loss=8.9471
	step [64/244], loss=6.8059
	step [65/244], loss=7.3886
	step [66/244], loss=6.6221
	step [67/244], loss=9.0324
	step [68/244], loss=7.4565
	step [69/244], loss=6.7770
	step [70/244], loss=7.2955
	step [71/244], loss=6.4779
	step [72/244], loss=6.9343
	step [73/244], loss=6.8694
	step [74/244], loss=7.1484
	step [75/244], loss=6.3469
	step [76/244], loss=8.0418
	step [77/244], loss=7.7984
	step [78/244], loss=5.6566
	step [79/244], loss=8.4056
	step [80/244], loss=7.5686
	step [81/244], loss=7.5978
	step [82/244], loss=7.1540
	step [83/244], loss=6.4491
	step [84/244], loss=6.9283
	step [85/244], loss=7.1506
	step [86/244], loss=9.8827
	step [87/244], loss=6.6393
	step [88/244], loss=7.4534
	step [89/244], loss=7.3609
	step [90/244], loss=8.2692
	step [91/244], loss=7.2720
	step [92/244], loss=7.5906
	step [93/244], loss=6.4595
	step [94/244], loss=7.8454
	step [95/244], loss=8.0718
	step [96/244], loss=6.9227
	step [97/244], loss=6.8653
	step [98/244], loss=6.6008
	step [99/244], loss=7.3839
	step [100/244], loss=7.9256
	step [101/244], loss=6.7379
	step [102/244], loss=9.0843
	step [103/244], loss=6.9237
	step [104/244], loss=7.4220
	step [105/244], loss=5.8777
	step [106/244], loss=6.4547
	step [107/244], loss=6.2364
	step [108/244], loss=5.9732
	step [109/244], loss=7.9672
	step [110/244], loss=5.8725
	step [111/244], loss=6.8168
	step [112/244], loss=7.6669
	step [113/244], loss=7.1224
	step [114/244], loss=5.8369
	step [115/244], loss=5.9951
	step [116/244], loss=6.5300
	step [117/244], loss=7.6291
	step [118/244], loss=7.2271
	step [119/244], loss=5.1892
	step [120/244], loss=6.0812
	step [121/244], loss=6.6995
	step [122/244], loss=7.2144
	step [123/244], loss=6.4939
	step [124/244], loss=7.0744
	step [125/244], loss=6.8205
	step [126/244], loss=5.8806
	step [127/244], loss=5.8517
	step [128/244], loss=7.8282
	step [129/244], loss=6.4295
	step [130/244], loss=6.1650
	step [131/244], loss=6.2241
	step [132/244], loss=7.9080
	step [133/244], loss=7.5258
	step [134/244], loss=6.5019
	step [135/244], loss=6.2725
	step [136/244], loss=6.7552
	step [137/244], loss=6.7580
	step [138/244], loss=5.9621
	step [139/244], loss=6.0528
	step [140/244], loss=6.5976
	step [141/244], loss=6.5459
	step [142/244], loss=5.8170
	step [143/244], loss=6.1516
	step [144/244], loss=8.3740
	step [145/244], loss=5.7250
	step [146/244], loss=6.0880
	step [147/244], loss=7.1339
	step [148/244], loss=7.9664
	step [149/244], loss=7.0835
	step [150/244], loss=6.0297
	step [151/244], loss=6.3991
	step [152/244], loss=7.6805
	step [153/244], loss=6.2428
	step [154/244], loss=5.9151
	step [155/244], loss=6.4060
	step [156/244], loss=8.4068
	step [157/244], loss=5.4233
	step [158/244], loss=6.3150
	step [159/244], loss=6.9459
	step [160/244], loss=6.7438
	step [161/244], loss=6.1990
	step [162/244], loss=5.8788
	step [163/244], loss=7.3250
	step [164/244], loss=6.4419
	step [165/244], loss=6.4704
	step [166/244], loss=6.9006
	step [167/244], loss=6.4594
	step [168/244], loss=6.3452
	step [169/244], loss=6.1299
	step [170/244], loss=6.7213
	step [171/244], loss=6.8024
	step [172/244], loss=5.8751
	step [173/244], loss=6.0122
	step [174/244], loss=7.4163
	step [175/244], loss=5.6617
	step [176/244], loss=6.2914
	step [177/244], loss=5.4333
	step [178/244], loss=5.4426
	step [179/244], loss=6.3348
	step [180/244], loss=5.9614
	step [181/244], loss=6.0906
	step [182/244], loss=5.7196
	step [183/244], loss=5.8541
	step [184/244], loss=7.8060
	step [185/244], loss=6.9393
	step [186/244], loss=6.8919
	step [187/244], loss=6.4587
	step [188/244], loss=6.1440
	step [189/244], loss=8.3580
	step [190/244], loss=6.2401
	step [191/244], loss=7.3220
	step [192/244], loss=6.1589
	step [193/244], loss=7.2421
	step [194/244], loss=4.9708
	step [195/244], loss=5.1944
	step [196/244], loss=6.5212
	step [197/244], loss=6.4998
	step [198/244], loss=6.1089
	step [199/244], loss=6.1473
	step [200/244], loss=6.3039
	step [201/244], loss=7.0234
	step [202/244], loss=6.9542
	step [203/244], loss=4.7984
	step [204/244], loss=6.9014
	step [205/244], loss=5.4842
	step [206/244], loss=8.5207
	step [207/244], loss=6.8369
	step [208/244], loss=5.8328
	step [209/244], loss=6.7789
	step [210/244], loss=5.7769
	step [211/244], loss=6.9807
	step [212/244], loss=6.1751
	step [213/244], loss=7.9360
	step [214/244], loss=6.3277
	step [215/244], loss=7.3799
	step [216/244], loss=7.0988
	step [217/244], loss=6.1772
	step [218/244], loss=6.3244
	step [219/244], loss=7.1855
	step [220/244], loss=6.3954
	step [221/244], loss=6.7331
	step [222/244], loss=6.5108
	step [223/244], loss=5.7743
	step [224/244], loss=6.5785
	step [225/244], loss=8.6093
	step [226/244], loss=6.1535
	step [227/244], loss=5.8786
	step [228/244], loss=6.1671
	step [229/244], loss=6.6907
	step [230/244], loss=6.3069
	step [231/244], loss=6.1116
	step [232/244], loss=6.2718
	step [233/244], loss=6.5320
	step [234/244], loss=5.8374
	step [235/244], loss=7.0719
	step [236/244], loss=6.5206
	step [237/244], loss=6.1092
	step [238/244], loss=5.6432
	step [239/244], loss=5.8129
	step [240/244], loss=6.0650
	step [241/244], loss=5.8069
	step [242/244], loss=6.1011
	step [243/244], loss=6.0176
	step [244/244], loss=0.3729
	Evaluating
	loss=0.0172, precision=0.2279, recall=0.9964, f1=0.3710
Training epoch 28
	step [1/244], loss=7.1803
	step [2/244], loss=6.1865
	step [3/244], loss=6.3292
	step [4/244], loss=5.7004
	step [5/244], loss=6.5197
	step [6/244], loss=5.8165
	step [7/244], loss=6.5410
	step [8/244], loss=6.2179
	step [9/244], loss=7.0902
	step [10/244], loss=5.6900
	step [11/244], loss=6.0689
	step [12/244], loss=6.3578
	step [13/244], loss=7.2419
	step [14/244], loss=6.4112
	step [15/244], loss=6.3773
	step [16/244], loss=6.5244
	step [17/244], loss=6.5742
	step [18/244], loss=6.2104
	step [19/244], loss=7.3425
	step [20/244], loss=4.7522
	step [21/244], loss=6.9458
	step [22/244], loss=7.1677
	step [23/244], loss=7.6728
	step [24/244], loss=7.3834
	step [25/244], loss=6.1457
	step [26/244], loss=7.8100
	step [27/244], loss=7.2006
	step [28/244], loss=7.4430
	step [29/244], loss=5.5741
	step [30/244], loss=6.0928
	step [31/244], loss=6.0458
	step [32/244], loss=8.0864
	step [33/244], loss=5.7139
	step [34/244], loss=6.5830
	step [35/244], loss=6.7091
	step [36/244], loss=5.2814
	step [37/244], loss=5.8174
	step [38/244], loss=6.1257
	step [39/244], loss=5.6601
	step [40/244], loss=6.2510
	step [41/244], loss=6.5622
	step [42/244], loss=6.2340
	step [43/244], loss=7.4321
	step [44/244], loss=6.9771
	step [45/244], loss=6.4556
	step [46/244], loss=6.3817
	step [47/244], loss=7.4505
	step [48/244], loss=5.4005
	step [49/244], loss=6.5515
	step [50/244], loss=5.7831
	step [51/244], loss=5.6447
	step [52/244], loss=6.3815
	step [53/244], loss=6.3347
	step [54/244], loss=6.6359
	step [55/244], loss=6.3320
	step [56/244], loss=6.2800
	step [57/244], loss=5.7783
	step [58/244], loss=6.9636
	step [59/244], loss=5.6437
	step [60/244], loss=4.9481
	step [61/244], loss=7.6461
	step [62/244], loss=5.3440
	step [63/244], loss=5.9764
	step [64/244], loss=7.3779
	step [65/244], loss=6.2917
	step [66/244], loss=6.0797
	step [67/244], loss=5.8286
	step [68/244], loss=5.9855
	step [69/244], loss=6.0028
	step [70/244], loss=7.8307
	step [71/244], loss=6.6383
	step [72/244], loss=5.3897
	step [73/244], loss=7.5632
	step [74/244], loss=6.0079
	step [75/244], loss=5.8961
	step [76/244], loss=7.3468
	step [77/244], loss=6.2388
	step [78/244], loss=6.3879
	step [79/244], loss=6.5154
	step [80/244], loss=5.5595
	step [81/244], loss=6.7493
	step [82/244], loss=7.0969
	step [83/244], loss=6.1325
	step [84/244], loss=7.1073
	step [85/244], loss=6.0301
	step [86/244], loss=7.2176
	step [87/244], loss=5.5148
	step [88/244], loss=6.2450
	step [89/244], loss=8.6130
	step [90/244], loss=5.1664
	step [91/244], loss=6.2687
	step [92/244], loss=6.6965
	step [93/244], loss=5.7971
	step [94/244], loss=6.5103
	step [95/244], loss=6.1334
	step [96/244], loss=5.5349
	step [97/244], loss=6.3769
	step [98/244], loss=6.5463
	step [99/244], loss=7.5896
	step [100/244], loss=6.4173
	step [101/244], loss=7.9899
	step [102/244], loss=6.1705
	step [103/244], loss=6.9798
	step [104/244], loss=6.3617
	step [105/244], loss=7.0641
	step [106/244], loss=5.7766
	step [107/244], loss=8.9388
	step [108/244], loss=5.4161
	step [109/244], loss=7.4066
	step [110/244], loss=7.7476
	step [111/244], loss=5.8878
	step [112/244], loss=7.3196
	step [113/244], loss=6.7428
	step [114/244], loss=6.9225
	step [115/244], loss=5.9045
	step [116/244], loss=6.0257
	step [117/244], loss=5.8417
	step [118/244], loss=5.8340
	step [119/244], loss=8.0734
	step [120/244], loss=5.4253
	step [121/244], loss=6.7824
	step [122/244], loss=6.4334
	step [123/244], loss=6.0981
	step [124/244], loss=6.6991
	step [125/244], loss=7.4605
	step [126/244], loss=4.9108
	step [127/244], loss=5.9672
	step [128/244], loss=5.4474
	step [129/244], loss=6.4905
	step [130/244], loss=7.0109
	step [131/244], loss=6.0398
	step [132/244], loss=6.8830
	step [133/244], loss=4.7224
	step [134/244], loss=5.3063
	step [135/244], loss=6.2958
	step [136/244], loss=7.0696
	step [137/244], loss=5.9720
	step [138/244], loss=7.3960
	step [139/244], loss=6.6627
	step [140/244], loss=6.5112
	step [141/244], loss=5.5779
	step [142/244], loss=5.9472
	step [143/244], loss=5.8360
	step [144/244], loss=5.0307
	step [145/244], loss=6.7521
	step [146/244], loss=7.6033
	step [147/244], loss=6.4306
	step [148/244], loss=7.4568
	step [149/244], loss=7.4724
	step [150/244], loss=6.2030
	step [151/244], loss=6.8182
	step [152/244], loss=7.9343
	step [153/244], loss=6.9480
	step [154/244], loss=6.4644
	step [155/244], loss=6.0619
	step [156/244], loss=6.6407
	step [157/244], loss=7.0040
	step [158/244], loss=6.2938
	step [159/244], loss=6.1044
	step [160/244], loss=7.0376
	step [161/244], loss=5.9404
	step [162/244], loss=6.0043
	step [163/244], loss=6.4181
	step [164/244], loss=6.0619
	step [165/244], loss=5.5210
	step [166/244], loss=5.7646
	step [167/244], loss=5.5384
	step [168/244], loss=7.3739
	step [169/244], loss=5.6613
	step [170/244], loss=7.1831
	step [171/244], loss=6.7645
	step [172/244], loss=5.7944
	step [173/244], loss=6.2493
	step [174/244], loss=7.2291
	step [175/244], loss=6.0078
	step [176/244], loss=7.5732
	step [177/244], loss=5.7230
	step [178/244], loss=6.8779
	step [179/244], loss=7.6812
	step [180/244], loss=5.3943
	step [181/244], loss=5.8018
	step [182/244], loss=6.3227
	step [183/244], loss=5.6094
	step [184/244], loss=7.5177
	step [185/244], loss=6.2662
	step [186/244], loss=6.7870
	step [187/244], loss=6.6850
	step [188/244], loss=6.7343
	step [189/244], loss=7.3907
	step [190/244], loss=5.9309
	step [191/244], loss=7.4219
	step [192/244], loss=6.4134
	step [193/244], loss=6.0650
	step [194/244], loss=6.2137
	step [195/244], loss=6.2310
	step [196/244], loss=6.7959
	step [197/244], loss=6.9541
	step [198/244], loss=5.8247
	step [199/244], loss=6.5114
	step [200/244], loss=5.5392
	step [201/244], loss=7.7859
	step [202/244], loss=5.8588
	step [203/244], loss=5.7263
	step [204/244], loss=6.0312
	step [205/244], loss=7.2211
	step [206/244], loss=6.8170
	step [207/244], loss=7.2898
	step [208/244], loss=7.1492
	step [209/244], loss=7.7483
	step [210/244], loss=5.9554
	step [211/244], loss=8.0938
	step [212/244], loss=7.3886
	step [213/244], loss=7.4542
	step [214/244], loss=6.7424
	step [215/244], loss=5.6996
	step [216/244], loss=5.8388
	step [217/244], loss=6.0840
	step [218/244], loss=6.7625
	step [219/244], loss=7.1624
	step [220/244], loss=5.3578
	step [221/244], loss=6.7150
	step [222/244], loss=5.3425
	step [223/244], loss=5.4854
	step [224/244], loss=6.5396
	step [225/244], loss=5.6699
	step [226/244], loss=6.7628
	step [227/244], loss=6.2893
	step [228/244], loss=6.9550
	step [229/244], loss=6.9594
	step [230/244], loss=6.7879
	step [231/244], loss=6.5737
	step [232/244], loss=6.1627
	step [233/244], loss=6.9846
	step [234/244], loss=6.5100
	step [235/244], loss=5.2529
	step [236/244], loss=5.8179
	step [237/244], loss=6.8941
	step [238/244], loss=5.7303
	step [239/244], loss=4.8790
	step [240/244], loss=5.8497
	step [241/244], loss=7.3740
	step [242/244], loss=5.5673
	step [243/244], loss=6.5345
	step [244/244], loss=0.3252
	Evaluating
	loss=0.0177, precision=0.2301, recall=0.9966, f1=0.3739
Training epoch 29
	step [1/244], loss=5.9477
	step [2/244], loss=7.7561
	step [3/244], loss=6.1708
	step [4/244], loss=6.1452
	step [5/244], loss=6.2781
	step [6/244], loss=5.9641
	step [7/244], loss=6.5549
	step [8/244], loss=6.4127
	step [9/244], loss=6.5773
	step [10/244], loss=5.9964
	step [11/244], loss=6.2101
	step [12/244], loss=6.8965
	step [13/244], loss=5.5936
	step [14/244], loss=7.4166
	step [15/244], loss=6.4987
	step [16/244], loss=6.7545
	step [17/244], loss=6.5825
	step [18/244], loss=6.4930
	step [19/244], loss=5.5690
	step [20/244], loss=6.6489
	step [21/244], loss=7.7830
	step [22/244], loss=6.4609
	step [23/244], loss=6.2397
	step [24/244], loss=7.6613
	step [25/244], loss=7.5980
	step [26/244], loss=6.7650
	step [27/244], loss=7.4423
	step [28/244], loss=5.3563
	step [29/244], loss=6.2897
	step [30/244], loss=7.9546
	step [31/244], loss=6.4102
	step [32/244], loss=6.3689
	step [33/244], loss=7.3680
	step [34/244], loss=5.0711
	step [35/244], loss=6.6807
	step [36/244], loss=6.3986
	step [37/244], loss=6.2320
	step [38/244], loss=6.0833
	step [39/244], loss=6.0898
	step [40/244], loss=7.4640
	step [41/244], loss=5.7957
	step [42/244], loss=6.0382
	step [43/244], loss=6.1700
	step [44/244], loss=6.0100
	step [45/244], loss=7.3343
	step [46/244], loss=5.8141
	step [47/244], loss=5.4125
	step [48/244], loss=6.9749
	step [49/244], loss=6.6005
	step [50/244], loss=7.6615
	step [51/244], loss=6.6467
	step [52/244], loss=6.1790
	step [53/244], loss=6.4905
	step [54/244], loss=6.8053
	step [55/244], loss=7.2809
	step [56/244], loss=6.5713
	step [57/244], loss=5.3079
	step [58/244], loss=6.4809
	step [59/244], loss=6.4948
	step [60/244], loss=7.4398
	step [61/244], loss=5.9056
	step [62/244], loss=5.7680
	step [63/244], loss=5.3060
	step [64/244], loss=7.0128
	step [65/244], loss=5.9264
	step [66/244], loss=6.6999
	step [67/244], loss=5.2660
	step [68/244], loss=5.3077
	step [69/244], loss=6.0472
	step [70/244], loss=5.5232
	step [71/244], loss=5.6739
	step [72/244], loss=6.8938
	step [73/244], loss=7.7719
	step [74/244], loss=6.5668
	step [75/244], loss=5.6392
	step [76/244], loss=6.2549
	step [77/244], loss=6.0967
	step [78/244], loss=8.6972
	step [79/244], loss=7.1385
	step [80/244], loss=8.3569
	step [81/244], loss=5.8651
	step [82/244], loss=6.4286
	step [83/244], loss=6.1581
	step [84/244], loss=6.7725
	step [85/244], loss=7.0603
	step [86/244], loss=5.8602
	step [87/244], loss=5.5048
	step [88/244], loss=4.9969
	step [89/244], loss=6.6405
	step [90/244], loss=7.3244
	step [91/244], loss=6.5700
	step [92/244], loss=5.9064
	step [93/244], loss=5.5438
	step [94/244], loss=7.4543
	step [95/244], loss=6.1553
	step [96/244], loss=6.7346
	step [97/244], loss=5.5914
	step [98/244], loss=6.3136
	step [99/244], loss=5.7851
	step [100/244], loss=5.8136
	step [101/244], loss=7.1999
	step [102/244], loss=5.5165
	step [103/244], loss=6.3293
	step [104/244], loss=7.1988
	step [105/244], loss=7.6026
	step [106/244], loss=5.4358
	step [107/244], loss=6.8684
	step [108/244], loss=5.6813
	step [109/244], loss=5.8427
	step [110/244], loss=6.1514
	step [111/244], loss=5.9088
	step [112/244], loss=7.1550
	step [113/244], loss=6.3388
	step [114/244], loss=6.9260
	step [115/244], loss=6.8598
	step [116/244], loss=5.4581
	step [117/244], loss=6.9764
	step [118/244], loss=5.5489
	step [119/244], loss=5.4706
	step [120/244], loss=5.5170
	step [121/244], loss=5.7017
	step [122/244], loss=6.1161
	step [123/244], loss=5.5791
	step [124/244], loss=6.5224
	step [125/244], loss=8.3646
	step [126/244], loss=5.4878
	step [127/244], loss=5.7758
	step [128/244], loss=7.4148
	step [129/244], loss=5.6897
	step [130/244], loss=5.2481
	step [131/244], loss=7.3904
	step [132/244], loss=4.9734
	step [133/244], loss=6.4740
	step [134/244], loss=5.8664
	step [135/244], loss=5.7663
	step [136/244], loss=6.7445
	step [137/244], loss=6.7909
	step [138/244], loss=6.8675
	step [139/244], loss=6.8037
	step [140/244], loss=5.7914
	step [141/244], loss=7.9800
	step [142/244], loss=6.5051
	step [143/244], loss=7.5094
	step [144/244], loss=6.6630
	step [145/244], loss=7.4940
	step [146/244], loss=5.7113
	step [147/244], loss=5.3738
	step [148/244], loss=5.0075
	step [149/244], loss=6.7776
	step [150/244], loss=5.5381
	step [151/244], loss=6.5645
	step [152/244], loss=6.7011
	step [153/244], loss=5.8268
	step [154/244], loss=6.5029
	step [155/244], loss=5.9347
	step [156/244], loss=6.1603
	step [157/244], loss=5.9168
	step [158/244], loss=4.6569
	step [159/244], loss=6.5146
	step [160/244], loss=7.1410
	step [161/244], loss=5.8495
	step [162/244], loss=7.3398
	step [163/244], loss=5.3256
	step [164/244], loss=6.2091
	step [165/244], loss=5.7406
	step [166/244], loss=6.1008
	step [167/244], loss=5.7828
	step [168/244], loss=5.7010
	step [169/244], loss=5.8049
	step [170/244], loss=7.6459
	step [171/244], loss=7.0143
	step [172/244], loss=6.3093
	step [173/244], loss=6.1919
	step [174/244], loss=6.8835
	step [175/244], loss=7.1376
	step [176/244], loss=5.9868
	step [177/244], loss=6.3414
	step [178/244], loss=9.0264
	step [179/244], loss=6.5814
	step [180/244], loss=5.9306
	step [181/244], loss=6.4177
	step [182/244], loss=7.1740
	step [183/244], loss=6.8880
	step [184/244], loss=5.3415
	step [185/244], loss=5.4903
	step [186/244], loss=5.7670
	step [187/244], loss=6.8735
	step [188/244], loss=7.5242
	step [189/244], loss=6.8701
	step [190/244], loss=5.4299
	step [191/244], loss=5.5690
	step [192/244], loss=5.3804
	step [193/244], loss=5.6410
	step [194/244], loss=6.0223
	step [195/244], loss=5.9107
	step [196/244], loss=4.9778
	step [197/244], loss=5.9789
	step [198/244], loss=6.2347
	step [199/244], loss=5.2619
	step [200/244], loss=7.9739
	step [201/244], loss=5.9895
	step [202/244], loss=6.4562
	step [203/244], loss=6.9442
	step [204/244], loss=7.4038
	step [205/244], loss=7.6743
	step [206/244], loss=7.1367
	step [207/244], loss=7.7721
	step [208/244], loss=5.0212
	step [209/244], loss=6.6037
	step [210/244], loss=5.6639
	step [211/244], loss=6.4077
	step [212/244], loss=6.9608
	step [213/244], loss=5.8400
	step [214/244], loss=7.4648
	step [215/244], loss=5.5996
	step [216/244], loss=6.6521
	step [217/244], loss=6.2363
	step [218/244], loss=4.1346
	step [219/244], loss=5.2769
	step [220/244], loss=5.3481
	step [221/244], loss=6.9656
	step [222/244], loss=5.5266
	step [223/244], loss=5.7662
	step [224/244], loss=5.8951
	step [225/244], loss=5.7353
	step [226/244], loss=6.4441
	step [227/244], loss=6.4199
	step [228/244], loss=6.5702
	step [229/244], loss=6.7011
	step [230/244], loss=6.6880
	step [231/244], loss=5.9072
	step [232/244], loss=6.6439
	step [233/244], loss=6.4915
	step [234/244], loss=7.2615
	step [235/244], loss=6.5767
	step [236/244], loss=6.3333
	step [237/244], loss=7.5930
	step [238/244], loss=6.2803
	step [239/244], loss=6.9074
	step [240/244], loss=6.2989
	step [241/244], loss=7.1855
	step [242/244], loss=6.3126
	step [243/244], loss=6.2104
	step [244/244], loss=0.2063
	Evaluating
	loss=0.0201, precision=0.2048, recall=0.9973, f1=0.3398
Training epoch 30
	step [1/244], loss=5.9823
	step [2/244], loss=7.0589
	step [3/244], loss=8.0536
	step [4/244], loss=7.6783
	step [5/244], loss=6.1490
	step [6/244], loss=5.9873
	step [7/244], loss=6.3511
	step [8/244], loss=6.2924
	step [9/244], loss=5.0034
	step [10/244], loss=6.1832
	step [11/244], loss=6.6949
	step [12/244], loss=6.6417
	step [13/244], loss=9.5747
	step [14/244], loss=5.5771
	step [15/244], loss=6.6751
	step [16/244], loss=6.3279
	step [17/244], loss=6.2194
	step [18/244], loss=6.6646
	step [19/244], loss=5.8953
	step [20/244], loss=6.4489
	step [21/244], loss=6.2593
	step [22/244], loss=6.6694
	step [23/244], loss=6.7613
	step [24/244], loss=5.8674
	step [25/244], loss=6.0390
	step [26/244], loss=6.0680
	step [27/244], loss=6.8223
	step [28/244], loss=5.5643
	step [29/244], loss=6.5902
	step [30/244], loss=6.3167
	step [31/244], loss=6.3811
	step [32/244], loss=6.8491
	step [33/244], loss=6.6373
	step [34/244], loss=5.2852
	step [35/244], loss=5.3399
	step [36/244], loss=6.2163
	step [37/244], loss=6.1849
	step [38/244], loss=5.8683
	step [39/244], loss=6.2749
	step [40/244], loss=6.3054
	step [41/244], loss=6.1469
	step [42/244], loss=6.2241
	step [43/244], loss=6.2568
	step [44/244], loss=6.0146
	step [45/244], loss=5.6205
	step [46/244], loss=6.9538
	step [47/244], loss=7.4915
	step [48/244], loss=6.7703
	step [49/244], loss=4.9161
	step [50/244], loss=6.7955
	step [51/244], loss=5.7869
	step [52/244], loss=5.7932
	step [53/244], loss=6.3422
	step [54/244], loss=5.8018
	step [55/244], loss=7.2112
	step [56/244], loss=6.6560
	step [57/244], loss=5.5569
	step [58/244], loss=6.5364
	step [59/244], loss=6.3940
	step [60/244], loss=6.7724
	step [61/244], loss=5.1849
	step [62/244], loss=6.3223
	step [63/244], loss=6.2325
	step [64/244], loss=5.8248
	step [65/244], loss=5.8175
	step [66/244], loss=6.2957
	step [67/244], loss=6.6902
	step [68/244], loss=6.4884
	step [69/244], loss=8.0274
	step [70/244], loss=5.4959
	step [71/244], loss=6.0472
	step [72/244], loss=6.8757
	step [73/244], loss=7.3006
	step [74/244], loss=5.7484
	step [75/244], loss=7.0097
	step [76/244], loss=6.0813
	step [77/244], loss=6.0055
	step [78/244], loss=5.4195
	step [79/244], loss=6.0520
	step [80/244], loss=7.1769
	step [81/244], loss=5.2938
	step [82/244], loss=6.9258
	step [83/244], loss=5.3592
	step [84/244], loss=5.9088
	step [85/244], loss=7.3236
	step [86/244], loss=6.0361
	step [87/244], loss=5.0677
	step [88/244], loss=5.5626
	step [89/244], loss=5.9731
	step [90/244], loss=7.6498
	step [91/244], loss=5.6262
	step [92/244], loss=6.7088
	step [93/244], loss=5.7811
	step [94/244], loss=6.4891
	step [95/244], loss=7.2761
	step [96/244], loss=6.8507
	step [97/244], loss=5.6251
	step [98/244], loss=5.6689
	step [99/244], loss=5.8767
	step [100/244], loss=6.5686
	step [101/244], loss=7.6498
	step [102/244], loss=6.2686
	step [103/244], loss=6.8488
	step [104/244], loss=5.6292
	step [105/244], loss=7.0010
	step [106/244], loss=5.4766
	step [107/244], loss=4.6866
	step [108/244], loss=6.8147
	step [109/244], loss=5.7863
	step [110/244], loss=5.4023
	step [111/244], loss=5.4847
	step [112/244], loss=5.6961
	step [113/244], loss=5.6114
	step [114/244], loss=5.7656
	step [115/244], loss=5.1706
	step [116/244], loss=7.3244
	step [117/244], loss=6.7252
	step [118/244], loss=6.3254
	step [119/244], loss=5.6570
	step [120/244], loss=6.5186
	step [121/244], loss=6.5151
	step [122/244], loss=6.6246
	step [123/244], loss=6.6049
	step [124/244], loss=6.9317
	step [125/244], loss=5.4718
	step [126/244], loss=6.9861
	step [127/244], loss=5.7206
	step [128/244], loss=5.4722
	step [129/244], loss=4.7208
	step [130/244], loss=5.7314
	step [131/244], loss=5.8234
	step [132/244], loss=5.9526
	step [133/244], loss=6.5285
	step [134/244], loss=5.9499
	step [135/244], loss=6.1724
	step [136/244], loss=6.3069
	step [137/244], loss=4.8897
	step [138/244], loss=6.6113
	step [139/244], loss=6.9206
	step [140/244], loss=6.3058
	step [141/244], loss=6.0565
	step [142/244], loss=5.9677
	step [143/244], loss=5.1991
	step [144/244], loss=7.4744
	step [145/244], loss=5.1939
	step [146/244], loss=6.3544
	step [147/244], loss=5.7498
	step [148/244], loss=5.4667
	step [149/244], loss=5.9906
	step [150/244], loss=5.9172
	step [151/244], loss=7.1117
	step [152/244], loss=5.0566
	step [153/244], loss=6.5597
	step [154/244], loss=5.4837
	step [155/244], loss=6.5852
	step [156/244], loss=5.5883
	step [157/244], loss=6.0465
	step [158/244], loss=7.2564
	step [159/244], loss=5.6984
	step [160/244], loss=5.9360
	step [161/244], loss=6.3009
	step [162/244], loss=4.9971
	step [163/244], loss=6.3825
	step [164/244], loss=5.6072
	step [165/244], loss=6.2044
	step [166/244], loss=6.0276
	step [167/244], loss=5.7189
	step [168/244], loss=6.4191
	step [169/244], loss=6.8265
	step [170/244], loss=6.7418
	step [171/244], loss=5.8833
	step [172/244], loss=6.4691
	step [173/244], loss=5.7079
	step [174/244], loss=7.0222
	step [175/244], loss=5.6213
	step [176/244], loss=5.8671
	step [177/244], loss=7.5584
	step [178/244], loss=5.8982
	step [179/244], loss=6.3271
	step [180/244], loss=6.9288
	step [181/244], loss=6.0907
	step [182/244], loss=6.6523
	step [183/244], loss=6.6863
	step [184/244], loss=5.4582
	step [185/244], loss=5.8608
	step [186/244], loss=6.2119
	step [187/244], loss=6.5061
	step [188/244], loss=6.0035
	step [189/244], loss=6.7657
	step [190/244], loss=5.9963
	step [191/244], loss=5.6045
	step [192/244], loss=6.9527
	step [193/244], loss=6.9194
	step [194/244], loss=6.0521
	step [195/244], loss=5.3761
	step [196/244], loss=6.0406
	step [197/244], loss=6.2850
	step [198/244], loss=6.4895
	step [199/244], loss=6.1792
	step [200/244], loss=7.1469
	step [201/244], loss=5.8190
	step [202/244], loss=5.7732
	step [203/244], loss=7.3499
	step [204/244], loss=5.8416
	step [205/244], loss=5.7207
	step [206/244], loss=7.7714
	step [207/244], loss=5.6614
	step [208/244], loss=5.6833
	step [209/244], loss=5.8768
	step [210/244], loss=5.9567
	step [211/244], loss=5.7767
	step [212/244], loss=6.9539
	step [213/244], loss=4.8640
	step [214/244], loss=7.3357
	step [215/244], loss=6.3697
	step [216/244], loss=7.3779
	step [217/244], loss=6.0521
	step [218/244], loss=6.4454
	step [219/244], loss=5.2572
	step [220/244], loss=5.3575
	step [221/244], loss=5.9036
	step [222/244], loss=5.1118
	step [223/244], loss=5.2567
	step [224/244], loss=6.1349
	step [225/244], loss=6.0283
	step [226/244], loss=5.5882
	step [227/244], loss=6.5193
	step [228/244], loss=6.4180
	step [229/244], loss=6.5039
	step [230/244], loss=6.8102
	step [231/244], loss=6.0608
	step [232/244], loss=5.4930
	step [233/244], loss=6.6024
	step [234/244], loss=5.0385
	step [235/244], loss=7.3577
	step [236/244], loss=5.5230
	step [237/244], loss=6.5204
	step [238/244], loss=5.2992
	step [239/244], loss=7.6260
	step [240/244], loss=6.3207
	step [241/244], loss=5.8302
	step [242/244], loss=6.4065
	step [243/244], loss=6.7222
	step [244/244], loss=0.1405
	Evaluating
	loss=0.0179, precision=0.2311, recall=0.9961, f1=0.3752
saving model as: 2_saved_model.pth
Training finished
best_f1: 0.3752087601407261
directing: Z rim_enhanced: False test_id 2
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12033 # image files with weight 12007
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3486 # image files with weight 3477
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Z 12007
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/188], loss=1252.1577
	step [2/188], loss=835.5924
	step [3/188], loss=552.3269
	step [4/188], loss=305.3858
	step [5/188], loss=199.9123
	step [6/188], loss=176.0533
	step [7/188], loss=163.6040
	step [8/188], loss=129.1762
	step [9/188], loss=105.4219
	step [10/188], loss=106.0118
	step [11/188], loss=105.5249
	step [12/188], loss=101.3207
	step [13/188], loss=98.5014
	step [14/188], loss=96.1498
	step [15/188], loss=95.7173
	step [16/188], loss=95.0351
	step [17/188], loss=90.1367
	step [18/188], loss=95.7066
	step [19/188], loss=89.9154
	step [20/188], loss=93.1198
	step [21/188], loss=92.4230
	step [22/188], loss=94.7710
	step [23/188], loss=94.1553
	step [24/188], loss=86.2977
	step [25/188], loss=91.3301
	step [26/188], loss=89.5188
	step [27/188], loss=85.7990
	step [28/188], loss=86.2151
	step [29/188], loss=85.1985
	step [30/188], loss=87.0839
	step [31/188], loss=87.4952
	step [32/188], loss=80.7216
	step [33/188], loss=82.0643
	step [34/188], loss=82.8301
	step [35/188], loss=84.1897
	step [36/188], loss=84.5637
	step [37/188], loss=82.0057
	step [38/188], loss=81.3028
	step [39/188], loss=82.2482
	step [40/188], loss=82.5063
	step [41/188], loss=80.2136
	step [42/188], loss=81.1920
	step [43/188], loss=79.9010
	step [44/188], loss=80.1731
	step [45/188], loss=77.3944
	step [46/188], loss=76.7133
	step [47/188], loss=78.1365
	step [48/188], loss=78.0571
	step [49/188], loss=76.5983
	step [50/188], loss=75.6904
	step [51/188], loss=74.4545
	step [52/188], loss=76.8007
	step [53/188], loss=76.7171
	step [54/188], loss=73.8295
	step [55/188], loss=75.3631
	step [56/188], loss=75.0835
	step [57/188], loss=73.9445
	step [58/188], loss=73.5798
	step [59/188], loss=76.4703
	step [60/188], loss=71.7040
	step [61/188], loss=72.0621
	step [62/188], loss=75.3688
	step [63/188], loss=72.2962
	step [64/188], loss=73.6327
	step [65/188], loss=74.8550
	step [66/188], loss=71.1201
	step [67/188], loss=70.1784
	step [68/188], loss=71.2422
	step [69/188], loss=71.5055
	step [70/188], loss=69.0948
	step [71/188], loss=69.8411
	step [72/188], loss=69.1829
	step [73/188], loss=70.3714
	step [74/188], loss=68.3553
	step [75/188], loss=69.5923
	step [76/188], loss=66.4226
	step [77/188], loss=67.5760
	step [78/188], loss=67.7838
	step [79/188], loss=64.2079
	step [80/188], loss=69.5244
	step [81/188], loss=65.1294
	step [82/188], loss=67.5339
	step [83/188], loss=65.6134
	step [84/188], loss=65.0986
	step [85/188], loss=66.5008
	step [86/188], loss=65.1726
	step [87/188], loss=69.6163
	step [88/188], loss=65.7276
	step [89/188], loss=67.3922
	step [90/188], loss=64.6812
	step [91/188], loss=67.5525
	step [92/188], loss=68.1563
	step [93/188], loss=66.4098
	step [94/188], loss=65.1034
	step [95/188], loss=64.1743
	step [96/188], loss=64.6491
	step [97/188], loss=62.1165
	step [98/188], loss=63.0829
	step [99/188], loss=61.2868
	step [100/188], loss=60.8412
	step [101/188], loss=62.5996
	step [102/188], loss=60.8355
	step [103/188], loss=63.4444
	step [104/188], loss=63.6849
	step [105/188], loss=62.4107
	step [106/188], loss=61.0603
	step [107/188], loss=61.7687
	step [108/188], loss=61.1588
	step [109/188], loss=62.5989
	step [110/188], loss=62.1862
	step [111/188], loss=59.0562
	step [112/188], loss=62.5058
	step [113/188], loss=59.5585
	step [114/188], loss=57.6719
	step [115/188], loss=62.1754
	step [116/188], loss=61.7097
	step [117/188], loss=60.2200
	step [118/188], loss=58.9121
	step [119/188], loss=60.6549
	step [120/188], loss=57.1470
	step [121/188], loss=57.7147
	step [122/188], loss=60.8301
	step [123/188], loss=58.6194
	step [124/188], loss=57.7117
	step [125/188], loss=57.8179
	step [126/188], loss=59.5122
	step [127/188], loss=59.2194
	step [128/188], loss=57.8838
	step [129/188], loss=57.2772
	step [130/188], loss=58.1603
	step [131/188], loss=56.9112
	step [132/188], loss=56.9354
	step [133/188], loss=57.0363
	step [134/188], loss=58.4921
	step [135/188], loss=56.0782
	step [136/188], loss=58.3171
	step [137/188], loss=56.8825
	step [138/188], loss=58.0212
	step [139/188], loss=57.3142
	step [140/188], loss=59.7381
	step [141/188], loss=55.5630
	step [142/188], loss=56.4536
	step [143/188], loss=54.0199
	step [144/188], loss=55.3774
	step [145/188], loss=56.9456
	step [146/188], loss=57.7852
	step [147/188], loss=57.0757
	step [148/188], loss=53.8058
	step [149/188], loss=56.4452
	step [150/188], loss=54.4334
	step [151/188], loss=56.1936
	step [152/188], loss=55.3785
	step [153/188], loss=53.8946
	step [154/188], loss=53.8477
	step [155/188], loss=52.6289
	step [156/188], loss=53.7503
	step [157/188], loss=54.5055
	step [158/188], loss=54.4328
	step [159/188], loss=53.2729
	step [160/188], loss=54.7938
	step [161/188], loss=53.2424
	step [162/188], loss=52.9474
	step [163/188], loss=58.4997
	step [164/188], loss=54.1839
	step [165/188], loss=53.5566
	step [166/188], loss=53.6333
	step [167/188], loss=55.4167
	step [168/188], loss=54.6758
	step [169/188], loss=52.4235
	step [170/188], loss=52.7755
	step [171/188], loss=53.2418
	step [172/188], loss=51.5054
	step [173/188], loss=52.2023
	step [174/188], loss=51.0723
	step [175/188], loss=52.4807
	step [176/188], loss=51.0978
	step [177/188], loss=51.6446
	step [178/188], loss=51.0844
	step [179/188], loss=51.7494
	step [180/188], loss=49.8020
	step [181/188], loss=49.5705
	step [182/188], loss=50.4236
	step [183/188], loss=51.2370
	step [184/188], loss=49.3746
	step [185/188], loss=50.7170
	step [186/188], loss=51.1006
	step [187/188], loss=49.5314
	step [188/188], loss=31.7706
	Evaluating
	loss=0.1854, precision=0.1798, recall=0.9964, f1=0.3046
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/188], loss=50.4341
	step [2/188], loss=52.0736
	step [3/188], loss=52.0195
	step [4/188], loss=51.4211
	step [5/188], loss=50.7819
	step [6/188], loss=54.1104
	step [7/188], loss=50.4801
	step [8/188], loss=51.1223
	step [9/188], loss=47.4816
	step [10/188], loss=48.7009
	step [11/188], loss=51.1451
	step [12/188], loss=48.3017
	step [13/188], loss=49.9545
	step [14/188], loss=50.7548
	step [15/188], loss=49.0224
	step [16/188], loss=50.6537
	step [17/188], loss=48.1005
	step [18/188], loss=49.7895
	step [19/188], loss=49.1049
	step [20/188], loss=49.6858
	step [21/188], loss=49.3133
	step [22/188], loss=48.0244
	step [23/188], loss=46.5335
	step [24/188], loss=48.4731
	step [25/188], loss=48.0854
	step [26/188], loss=45.9179
	step [27/188], loss=45.9756
	step [28/188], loss=47.6559
	step [29/188], loss=45.7650
	step [30/188], loss=48.9691
	step [31/188], loss=48.2354
	step [32/188], loss=49.0681
	step [33/188], loss=47.4483
	step [34/188], loss=45.2639
	step [35/188], loss=47.5043
	step [36/188], loss=47.9919
	step [37/188], loss=47.9620
	step [38/188], loss=47.2664
	step [39/188], loss=48.1849
	step [40/188], loss=46.7215
	step [41/188], loss=47.9044
	step [42/188], loss=47.6310
	step [43/188], loss=46.3251
	step [44/188], loss=48.5871
	step [45/188], loss=46.3262
	step [46/188], loss=48.4115
	step [47/188], loss=46.2136
	step [48/188], loss=45.5579
	step [49/188], loss=46.3350
	step [50/188], loss=45.6043
	step [51/188], loss=43.6842
	step [52/188], loss=44.9558
	step [53/188], loss=44.5018
	step [54/188], loss=47.8289
	step [55/188], loss=44.8806
	step [56/188], loss=44.1459
	step [57/188], loss=44.7359
	step [58/188], loss=44.8499
	step [59/188], loss=44.1987
	step [60/188], loss=47.3617
	step [61/188], loss=46.5272
	step [62/188], loss=46.8565
	step [63/188], loss=45.4077
	step [64/188], loss=44.9772
	step [65/188], loss=47.5608
	step [66/188], loss=45.2704
	step [67/188], loss=44.3348
	step [68/188], loss=45.1447
	step [69/188], loss=44.1501
	step [70/188], loss=44.8828
	step [71/188], loss=46.6839
	step [72/188], loss=44.1610
	step [73/188], loss=44.6947
	step [74/188], loss=41.6928
	step [75/188], loss=44.8983
	step [76/188], loss=45.0006
	step [77/188], loss=44.2025
	step [78/188], loss=45.4384
	step [79/188], loss=44.0583
	step [80/188], loss=42.9523
	step [81/188], loss=44.2241
	step [82/188], loss=43.8496
	step [83/188], loss=44.0233
	step [84/188], loss=45.4026
	step [85/188], loss=43.3343
	step [86/188], loss=44.4991
	step [87/188], loss=43.0304
	step [88/188], loss=42.4564
	step [89/188], loss=43.1513
	step [90/188], loss=42.3432
	step [91/188], loss=42.7411
	step [92/188], loss=44.2618
	step [93/188], loss=40.6080
	step [94/188], loss=41.1978
	step [95/188], loss=43.0998
	step [96/188], loss=43.1922
	step [97/188], loss=42.3320
	step [98/188], loss=43.3517
	step [99/188], loss=41.4644
	step [100/188], loss=44.7178
	step [101/188], loss=43.6598
	step [102/188], loss=44.0418
	step [103/188], loss=42.6135
	step [104/188], loss=42.5498
	step [105/188], loss=45.8296
	step [106/188], loss=39.7194
	step [107/188], loss=44.3694
	step [108/188], loss=41.7457
	step [109/188], loss=40.5346
	step [110/188], loss=43.1626
	step [111/188], loss=40.6382
	step [112/188], loss=41.3569
	step [113/188], loss=42.0511
	step [114/188], loss=42.3374
	step [115/188], loss=39.7797
	step [116/188], loss=40.3106
	step [117/188], loss=39.9794
	step [118/188], loss=38.9531
	step [119/188], loss=40.0113
	step [120/188], loss=41.1206
	step [121/188], loss=41.5729
	step [122/188], loss=41.2706
	step [123/188], loss=40.9865
	step [124/188], loss=39.7197
	step [125/188], loss=42.2754
	step [126/188], loss=41.3357
	step [127/188], loss=39.7234
	step [128/188], loss=39.9362
	step [129/188], loss=37.9117
	step [130/188], loss=40.9578
	step [131/188], loss=40.4042
	step [132/188], loss=41.2230
	step [133/188], loss=40.0091
	step [134/188], loss=42.5714
	step [135/188], loss=40.2488
	step [136/188], loss=39.8102
	step [137/188], loss=39.8651
	step [138/188], loss=40.8904
	step [139/188], loss=38.4487
	step [140/188], loss=41.4933
	step [141/188], loss=41.3672
	step [142/188], loss=39.3376
	step [143/188], loss=39.3145
	step [144/188], loss=41.2807
	step [145/188], loss=37.2375
	step [146/188], loss=39.3663
	step [147/188], loss=39.0860
	step [148/188], loss=40.9087
	step [149/188], loss=39.6279
	step [150/188], loss=41.2261
	step [151/188], loss=43.3972
	step [152/188], loss=39.5688
	step [153/188], loss=40.1460
	step [154/188], loss=38.6459
	step [155/188], loss=40.0181
	step [156/188], loss=38.0468
	step [157/188], loss=35.8238
	step [158/188], loss=39.5224
	step [159/188], loss=38.1633
	step [160/188], loss=41.0834
	step [161/188], loss=37.8054
	step [162/188], loss=39.3625
	step [163/188], loss=39.4818
	step [164/188], loss=37.7351
	step [165/188], loss=40.2403
	step [166/188], loss=38.3427
	step [167/188], loss=37.3852
	step [168/188], loss=36.7585
	step [169/188], loss=37.8630
	step [170/188], loss=39.1992
	step [171/188], loss=37.2315
	step [172/188], loss=36.6360
	step [173/188], loss=38.9573
	step [174/188], loss=37.2499
	step [175/188], loss=37.2337
	step [176/188], loss=39.7404
	step [177/188], loss=37.6417
	step [178/188], loss=36.0653
	step [179/188], loss=38.0237
	step [180/188], loss=37.7877
	step [181/188], loss=36.4336
	step [182/188], loss=38.3202
	step [183/188], loss=37.8301
	step [184/188], loss=37.0168
	step [185/188], loss=39.6920
	step [186/188], loss=37.0441
	step [187/188], loss=36.9673
	step [188/188], loss=22.5434
	Evaluating
	loss=0.1309, precision=0.1922, recall=0.9965, f1=0.3223
saving model as: 2_saved_model.pth
Training epoch 3
	step [1/188], loss=36.2903
	step [2/188], loss=36.5255
	step [3/188], loss=36.0175
	step [4/188], loss=35.5004
	step [5/188], loss=37.3491
	step [6/188], loss=38.3553
	step [7/188], loss=37.7702
	step [8/188], loss=37.5179
	step [9/188], loss=38.1952
	step [10/188], loss=37.5219
	step [11/188], loss=36.9120
	step [12/188], loss=37.5596
	step [13/188], loss=34.4129
	step [14/188], loss=36.3008
	step [15/188], loss=35.7281
	step [16/188], loss=36.8366
	step [17/188], loss=35.6258
	step [18/188], loss=34.9362
	step [19/188], loss=35.3101
	step [20/188], loss=35.7041
	step [21/188], loss=37.1328
	step [22/188], loss=35.8271
	step [23/188], loss=38.0251
	step [24/188], loss=37.3460
	step [25/188], loss=35.6934
	step [26/188], loss=35.4873
	step [27/188], loss=35.7238
	step [28/188], loss=37.5016
	step [29/188], loss=35.8369
	step [30/188], loss=34.6445
	step [31/188], loss=34.9271
	step [32/188], loss=34.3707
	step [33/188], loss=36.2961
	step [34/188], loss=36.6176
	step [35/188], loss=33.8966
	step [36/188], loss=35.4983
	step [37/188], loss=35.2370
	step [38/188], loss=35.5224
	step [39/188], loss=36.0716
	step [40/188], loss=34.1510
	step [41/188], loss=33.9060
	step [42/188], loss=35.8157
	step [43/188], loss=36.6455
	step [44/188], loss=36.0503
	step [45/188], loss=32.9201
	step [46/188], loss=34.7769
	step [47/188], loss=38.0876
	step [48/188], loss=32.5434
	step [49/188], loss=34.5888
	step [50/188], loss=33.8957
	step [51/188], loss=34.9015
	step [52/188], loss=34.9538
	step [53/188], loss=31.4537
	step [54/188], loss=33.0066
	step [55/188], loss=38.0715
	step [56/188], loss=32.5729
	step [57/188], loss=34.6360
	step [58/188], loss=35.0235
	step [59/188], loss=34.9755
	step [60/188], loss=33.7484
	step [61/188], loss=34.5758
	step [62/188], loss=33.0568
	step [63/188], loss=34.8208
	step [64/188], loss=37.1946
	step [65/188], loss=33.2222
	step [66/188], loss=33.1716
	step [67/188], loss=34.6206
	step [68/188], loss=34.8933
	step [69/188], loss=34.5819
	step [70/188], loss=35.9061
	step [71/188], loss=36.6975
	step [72/188], loss=35.4096
	step [73/188], loss=34.2642
	step [74/188], loss=35.0614
	step [75/188], loss=37.5359
	step [76/188], loss=33.4669
	step [77/188], loss=33.6785
	step [78/188], loss=34.9124
	step [79/188], loss=33.5111
	step [80/188], loss=34.4124
	step [81/188], loss=34.7694
	step [82/188], loss=32.5539
	step [83/188], loss=34.8051
	step [84/188], loss=35.2407
	step [85/188], loss=34.2074
	step [86/188], loss=32.0979
	step [87/188], loss=32.3908
	step [88/188], loss=31.9665
	step [89/188], loss=33.3234
	step [90/188], loss=32.9666
	step [91/188], loss=31.4360
	step [92/188], loss=32.6590
	step [93/188], loss=32.7147
	step [94/188], loss=33.0505
	step [95/188], loss=32.1593
	step [96/188], loss=32.0316
	step [97/188], loss=32.1930
	step [98/188], loss=33.8502
	step [99/188], loss=32.1852
	step [100/188], loss=32.6003
	step [101/188], loss=33.0190
	step [102/188], loss=32.0505
	step [103/188], loss=35.4505
	step [104/188], loss=32.2066
	step [105/188], loss=32.1441
	step [106/188], loss=32.0650
	step [107/188], loss=31.8056
	step [108/188], loss=31.9014
	step [109/188], loss=34.4703
	step [110/188], loss=33.4265
	step [111/188], loss=33.2717
	step [112/188], loss=32.3940
	step [113/188], loss=33.9683
	step [114/188], loss=33.8428
	step [115/188], loss=31.8908
	step [116/188], loss=32.5815
	step [117/188], loss=32.9536
	step [118/188], loss=32.6335
	step [119/188], loss=31.7070
	step [120/188], loss=32.8962
	step [121/188], loss=32.0146
	step [122/188], loss=32.4027
	step [123/188], loss=32.2896
	step [124/188], loss=32.4025
	step [125/188], loss=31.8209
	step [126/188], loss=32.8731
	step [127/188], loss=32.7519
	step [128/188], loss=31.8364
	step [129/188], loss=29.9937
	step [130/188], loss=30.6117
	step [131/188], loss=31.6016
	step [132/188], loss=32.4098
	step [133/188], loss=33.8226
	step [134/188], loss=31.1684
	step [135/188], loss=32.0973
	step [136/188], loss=35.4571
	step [137/188], loss=32.8372
	step [138/188], loss=32.2483
	step [139/188], loss=29.8129
	step [140/188], loss=31.5464
	step [141/188], loss=31.0436
	step [142/188], loss=31.8410
	step [143/188], loss=31.8683
	step [144/188], loss=31.4720
	step [145/188], loss=29.6154
	step [146/188], loss=29.3543
	step [147/188], loss=32.5453
	step [148/188], loss=29.7082
	step [149/188], loss=31.2691
	step [150/188], loss=32.2316
	step [151/188], loss=30.3973
	step [152/188], loss=29.0911
	step [153/188], loss=30.7685
	step [154/188], loss=29.7880
	step [155/188], loss=30.4103
	step [156/188], loss=30.4879
	step [157/188], loss=30.1268
	step [158/188], loss=32.9018
	step [159/188], loss=32.8553
	step [160/188], loss=31.6910
	step [161/188], loss=32.4455
	step [162/188], loss=32.2475
	step [163/188], loss=30.8994
	step [164/188], loss=30.9680
	step [165/188], loss=30.0306
	step [166/188], loss=31.6740
	step [167/188], loss=32.2380
	step [168/188], loss=29.6662
	step [169/188], loss=31.6108
	step [170/188], loss=30.5377
	step [171/188], loss=28.9192
	step [172/188], loss=29.8273
	step [173/188], loss=31.4131
	step [174/188], loss=30.0863
	step [175/188], loss=29.8398
	step [176/188], loss=31.4483
	step [177/188], loss=29.8731
	step [178/188], loss=29.7387
	step [179/188], loss=30.0459
	step [180/188], loss=29.4290
	step [181/188], loss=29.7962
	step [182/188], loss=33.2939
	step [183/188], loss=26.7708
	step [184/188], loss=28.5116
	step [185/188], loss=30.0406
	step [186/188], loss=27.2064
	step [187/188], loss=28.3590
	step [188/188], loss=17.8878
	Evaluating
	loss=0.1031, precision=0.1857, recall=0.9966, f1=0.3131
Training epoch 4
	step [1/188], loss=30.5464
	step [2/188], loss=28.2338
	step [3/188], loss=27.0874
	step [4/188], loss=29.9191
	step [5/188], loss=27.7614
	step [6/188], loss=29.2237
	step [7/188], loss=31.5182
	step [8/188], loss=28.1971
	step [9/188], loss=27.8018
	step [10/188], loss=30.5122
	step [11/188], loss=30.4748
	step [12/188], loss=28.1568
	step [13/188], loss=30.4521
	step [14/188], loss=27.8342
	step [15/188], loss=28.4952
	step [16/188], loss=29.5424
	step [17/188], loss=29.2794
	step [18/188], loss=30.2141
	step [19/188], loss=29.5465
	step [20/188], loss=30.0827
	step [21/188], loss=26.5224
	step [22/188], loss=26.3674
	step [23/188], loss=28.4376
	step [24/188], loss=29.1065
	step [25/188], loss=31.3960
	step [26/188], loss=30.9668
	step [27/188], loss=28.0142
	step [28/188], loss=31.6767
	step [29/188], loss=29.9898
	step [30/188], loss=29.4506
	step [31/188], loss=29.0368
	step [32/188], loss=30.0593
	step [33/188], loss=29.2175
	step [34/188], loss=27.4021
	step [35/188], loss=30.1471
	step [36/188], loss=28.8189
	step [37/188], loss=25.6577
	step [38/188], loss=28.8832
	step [39/188], loss=28.2191
	step [40/188], loss=26.5865
	step [41/188], loss=30.3928
	step [42/188], loss=29.8769
	step [43/188], loss=30.2264
	step [44/188], loss=28.9613
	step [45/188], loss=27.3258
	step [46/188], loss=29.6281
	step [47/188], loss=25.9341
	step [48/188], loss=28.2015
	step [49/188], loss=28.7303
	step [50/188], loss=26.6786
	step [51/188], loss=30.0452
	step [52/188], loss=29.0448
	step [53/188], loss=27.5907
	step [54/188], loss=29.0298
	step [55/188], loss=26.0596
	step [56/188], loss=29.2763
	step [57/188], loss=28.4079
	step [58/188], loss=28.3806
	step [59/188], loss=27.1578
	step [60/188], loss=26.5501
	step [61/188], loss=26.2212
	step [62/188], loss=28.7417
	step [63/188], loss=28.5258
	step [64/188], loss=27.2084
	step [65/188], loss=27.9060
	step [66/188], loss=29.0218
	step [67/188], loss=30.6034
	step [68/188], loss=27.3040
	step [69/188], loss=25.7523
	step [70/188], loss=26.8201
	step [71/188], loss=29.2748
	step [72/188], loss=28.3073
	step [73/188], loss=26.2185
	step [74/188], loss=25.4471
	step [75/188], loss=26.1110
	step [76/188], loss=25.9333
	step [77/188], loss=28.1264
	step [78/188], loss=26.6687
	step [79/188], loss=28.1596
	step [80/188], loss=26.1387
	step [81/188], loss=25.9462
	step [82/188], loss=27.9032
	step [83/188], loss=29.4313
	step [84/188], loss=25.5257
	step [85/188], loss=30.1788
	step [86/188], loss=26.3172
	step [87/188], loss=26.4037
	step [88/188], loss=27.5958
	step [89/188], loss=27.8532
	step [90/188], loss=27.6692
	step [91/188], loss=27.7536
	step [92/188], loss=27.0101
	step [93/188], loss=25.3296
	step [94/188], loss=27.0388
	step [95/188], loss=23.5011
	step [96/188], loss=25.7495
	step [97/188], loss=27.8225
	step [98/188], loss=26.9243
	step [99/188], loss=25.0822
	step [100/188], loss=28.0427
	step [101/188], loss=26.4050
	step [102/188], loss=27.2422
	step [103/188], loss=27.8077
	step [104/188], loss=29.9205
	step [105/188], loss=27.9847
	step [106/188], loss=27.0381
	step [107/188], loss=26.6245
	step [108/188], loss=28.1466
	step [109/188], loss=25.8219
	step [110/188], loss=26.8236
	step [111/188], loss=27.6968
	step [112/188], loss=26.6603
	step [113/188], loss=26.1301
	step [114/188], loss=27.2495
	step [115/188], loss=29.1310
	step [116/188], loss=26.4242
	step [117/188], loss=28.0319
	step [118/188], loss=27.5826
	step [119/188], loss=25.9429
	step [120/188], loss=25.5481
	step [121/188], loss=25.5885
	step [122/188], loss=26.4856
	step [123/188], loss=26.9932
	step [124/188], loss=27.3969
	step [125/188], loss=26.1607
	step [126/188], loss=26.6820
	step [127/188], loss=24.7351
	step [128/188], loss=25.1714
	step [129/188], loss=25.7179
	step [130/188], loss=24.9716
	step [131/188], loss=30.8278
	step [132/188], loss=25.3159
	step [133/188], loss=25.3364
	step [134/188], loss=24.9123
	step [135/188], loss=27.9162
	step [136/188], loss=24.9739
	step [137/188], loss=24.8795
	step [138/188], loss=26.5944
	step [139/188], loss=26.5712
	step [140/188], loss=24.8345
	step [141/188], loss=26.8762
	step [142/188], loss=27.6910
	step [143/188], loss=25.9075
	step [144/188], loss=24.6040
	step [145/188], loss=25.9684
	step [146/188], loss=25.8760
	step [147/188], loss=25.9233
	step [148/188], loss=26.3029
	step [149/188], loss=26.3606
	step [150/188], loss=26.0146
	step [151/188], loss=26.5536
	step [152/188], loss=25.2373
	step [153/188], loss=23.8633
	step [154/188], loss=25.5080
	step [155/188], loss=22.9347
	step [156/188], loss=22.8062
	step [157/188], loss=23.9947
	step [158/188], loss=26.7438
	step [159/188], loss=26.3903
	step [160/188], loss=27.8373
	step [161/188], loss=28.7918
	step [162/188], loss=24.5808
	step [163/188], loss=25.9622
	step [164/188], loss=23.1066
	step [165/188], loss=26.5109
	step [166/188], loss=25.5500
	step [167/188], loss=24.1995
	step [168/188], loss=22.7504
	step [169/188], loss=24.4151
	step [170/188], loss=23.8844
	step [171/188], loss=27.2198
	step [172/188], loss=23.8962
	step [173/188], loss=24.3669
	step [174/188], loss=25.4033
	step [175/188], loss=25.4046
	step [176/188], loss=24.9139
	step [177/188], loss=26.5533
	step [178/188], loss=25.7945
	step [179/188], loss=24.8132
	step [180/188], loss=26.1884
	step [181/188], loss=24.5231
	step [182/188], loss=24.0950
	step [183/188], loss=22.9778
	step [184/188], loss=26.0861
	step [185/188], loss=25.2724
	step [186/188], loss=24.4902
	step [187/188], loss=24.4302
	step [188/188], loss=17.4727
	Evaluating
	loss=0.0938, precision=0.1339, recall=0.9978, f1=0.2362
Training epoch 5
	step [1/188], loss=24.9387
	step [2/188], loss=23.3223
	step [3/188], loss=24.3263
	step [4/188], loss=25.0926
	step [5/188], loss=25.0487
	step [6/188], loss=27.1663
	step [7/188], loss=24.3522
	step [8/188], loss=22.8743
	step [9/188], loss=24.9926
	step [10/188], loss=23.9428
	step [11/188], loss=23.7150
	step [12/188], loss=26.1393
	step [13/188], loss=24.6840
	step [14/188], loss=22.6241
	step [15/188], loss=22.9243
	step [16/188], loss=24.0692
	step [17/188], loss=24.5135
	step [18/188], loss=26.3514
	step [19/188], loss=26.3359
	step [20/188], loss=26.2929
	step [21/188], loss=23.0433
	step [22/188], loss=25.5553
	step [23/188], loss=22.7546
	step [24/188], loss=23.5263
	step [25/188], loss=26.4075
	step [26/188], loss=23.9365
	step [27/188], loss=22.5697
	step [28/188], loss=22.1487
	step [29/188], loss=23.7852
	step [30/188], loss=25.7147
	step [31/188], loss=22.7052
	step [32/188], loss=24.9204
	step [33/188], loss=25.5114
	step [34/188], loss=22.7452
	step [35/188], loss=23.8646
	step [36/188], loss=23.4606
	step [37/188], loss=23.1838
	step [38/188], loss=26.2422
	step [39/188], loss=23.0017
	step [40/188], loss=24.4538
	step [41/188], loss=25.2078
	step [42/188], loss=23.3350
	step [43/188], loss=23.8733
	step [44/188], loss=23.9720
	step [45/188], loss=22.4848
	step [46/188], loss=21.1694
	step [47/188], loss=23.4790
	step [48/188], loss=24.6456
	step [49/188], loss=21.6911
	step [50/188], loss=22.1197
	step [51/188], loss=24.0588
	step [52/188], loss=22.9816
	step [53/188], loss=23.3032
	step [54/188], loss=22.9004
	step [55/188], loss=24.8370
	step [56/188], loss=22.6385
	step [57/188], loss=23.1646
	step [58/188], loss=22.1716
	step [59/188], loss=25.5917
	step [60/188], loss=22.0357
	step [61/188], loss=23.5968
	step [62/188], loss=23.8924
	step [63/188], loss=23.7205
	step [64/188], loss=23.6387
	step [65/188], loss=24.6416
	step [66/188], loss=23.5258
	step [67/188], loss=24.1149
	step [68/188], loss=22.6014
	step [69/188], loss=20.5714
	step [70/188], loss=23.9131
	step [71/188], loss=22.5668
	step [72/188], loss=22.9194
	step [73/188], loss=24.0565
	step [74/188], loss=21.9722
	step [75/188], loss=22.2306
	step [76/188], loss=23.7200
	step [77/188], loss=20.0018
	step [78/188], loss=25.2171
	step [79/188], loss=23.1494
	step [80/188], loss=22.1684
	step [81/188], loss=22.8379
	step [82/188], loss=20.9883
	step [83/188], loss=22.2169
	step [84/188], loss=22.5470
	step [85/188], loss=23.8557
	step [86/188], loss=23.4088
	step [87/188], loss=22.9558
	step [88/188], loss=24.3139
	step [89/188], loss=24.2726
	step [90/188], loss=24.1624
	step [91/188], loss=22.2671
	step [92/188], loss=21.7432
	step [93/188], loss=21.4989
	step [94/188], loss=23.4244
	step [95/188], loss=22.7385
	step [96/188], loss=24.5396
	step [97/188], loss=25.3847
	step [98/188], loss=21.5743
	step [99/188], loss=22.7727
	step [100/188], loss=23.4413
	step [101/188], loss=23.5914
	step [102/188], loss=21.1179
	step [103/188], loss=21.1063
	step [104/188], loss=21.6688
	step [105/188], loss=22.5439
	step [106/188], loss=21.8695
	step [107/188], loss=22.1514
	step [108/188], loss=21.1845
	step [109/188], loss=23.9151
	step [110/188], loss=23.7708
	step [111/188], loss=23.2426
	step [112/188], loss=23.7696
	step [113/188], loss=24.2477
	step [114/188], loss=24.1886
	step [115/188], loss=22.7756
	step [116/188], loss=22.7607
	step [117/188], loss=21.2887
	step [118/188], loss=22.2918
	step [119/188], loss=22.3621
	step [120/188], loss=22.2951
	step [121/188], loss=21.0709
	step [122/188], loss=25.5708
	step [123/188], loss=24.5136
	step [124/188], loss=21.3460
	step [125/188], loss=22.8198
	step [126/188], loss=25.0830
	step [127/188], loss=21.8583
	step [128/188], loss=22.4749
	step [129/188], loss=22.0218
	step [130/188], loss=24.7334
	step [131/188], loss=23.3166
	step [132/188], loss=25.6385
	step [133/188], loss=24.3600
	step [134/188], loss=22.6613
	step [135/188], loss=23.1830
	step [136/188], loss=21.5625
	step [137/188], loss=24.1096
	step [138/188], loss=23.5881
	step [139/188], loss=22.2026
	step [140/188], loss=20.9773
	step [141/188], loss=22.4725
	step [142/188], loss=21.3256
	step [143/188], loss=21.6325
	step [144/188], loss=21.1779
	step [145/188], loss=21.6789
	step [146/188], loss=20.4756
	step [147/188], loss=21.0912
	step [148/188], loss=23.1467
	step [149/188], loss=23.1998
	step [150/188], loss=23.4347
	step [151/188], loss=20.5933
	step [152/188], loss=21.5058
	step [153/188], loss=20.9267
	step [154/188], loss=22.5869
	step [155/188], loss=21.9340
	step [156/188], loss=22.1526
	step [157/188], loss=21.3956
	step [158/188], loss=19.1227
	step [159/188], loss=24.4177
	step [160/188], loss=19.4152
	step [161/188], loss=20.3840
	step [162/188], loss=20.8544
	step [163/188], loss=21.9971
	step [164/188], loss=21.6471
	step [165/188], loss=21.3480
	step [166/188], loss=21.0236
	step [167/188], loss=20.9312
	step [168/188], loss=21.5339
	step [169/188], loss=22.9462
	step [170/188], loss=20.4283
	step [171/188], loss=21.6662
	step [172/188], loss=20.9817
	step [173/188], loss=22.8769
	step [174/188], loss=24.6260
	step [175/188], loss=25.1063
	step [176/188], loss=23.3212
	step [177/188], loss=23.2428
	step [178/188], loss=20.0755
	step [179/188], loss=20.4950
	step [180/188], loss=22.1502
	step [181/188], loss=22.0774
	step [182/188], loss=23.6253
	step [183/188], loss=21.1331
	step [184/188], loss=22.0150
	step [185/188], loss=20.0207
	step [186/188], loss=24.4837
	step [187/188], loss=21.2455
	step [188/188], loss=11.6217
	Evaluating
	loss=0.0674, precision=0.2049, recall=0.9964, f1=0.3399
saving model as: 2_saved_model.pth
Training epoch 6
	step [1/188], loss=22.8968
	step [2/188], loss=19.8717
	step [3/188], loss=21.0313
	step [4/188], loss=23.6839
	step [5/188], loss=21.5391
	step [6/188], loss=20.5932
	step [7/188], loss=20.6074
	step [8/188], loss=20.9017
	step [9/188], loss=21.8010
	step [10/188], loss=22.0985
	step [11/188], loss=20.7363
	step [12/188], loss=22.4135
	step [13/188], loss=21.1612
	step [14/188], loss=20.2332
	step [15/188], loss=21.6734
	step [16/188], loss=20.6319
	step [17/188], loss=22.4204
	step [18/188], loss=21.0947
	step [19/188], loss=21.6059
	step [20/188], loss=19.7268
	step [21/188], loss=22.7927
	step [22/188], loss=19.1917
	step [23/188], loss=21.9455
	step [24/188], loss=20.4713
	step [25/188], loss=20.3455
	step [26/188], loss=20.4828
	step [27/188], loss=20.0693
	step [28/188], loss=19.9290
	step [29/188], loss=19.6396
	step [30/188], loss=19.8780
	step [31/188], loss=20.2291
	step [32/188], loss=19.2238
	step [33/188], loss=22.6799
	step [34/188], loss=23.6158
	step [35/188], loss=21.8221
	step [36/188], loss=19.9584
	step [37/188], loss=20.3322
	step [38/188], loss=21.3667
	step [39/188], loss=18.8923
	step [40/188], loss=20.7615
	step [41/188], loss=20.8539
	step [42/188], loss=19.9435
	step [43/188], loss=22.4469
	step [44/188], loss=22.6227
	step [45/188], loss=21.0682
	step [46/188], loss=19.5345
	step [47/188], loss=21.6000
	step [48/188], loss=19.8196
	step [49/188], loss=21.4948
	step [50/188], loss=18.5511
	step [51/188], loss=19.7660
	step [52/188], loss=20.2101
	step [53/188], loss=20.9359
	step [54/188], loss=19.2151
	step [55/188], loss=19.5575
	step [56/188], loss=23.0163
	step [57/188], loss=19.0890
	step [58/188], loss=23.1129
	step [59/188], loss=20.9134
	step [60/188], loss=20.0049
	step [61/188], loss=22.0872
	step [62/188], loss=22.7760
	step [63/188], loss=21.5226
	step [64/188], loss=18.6617
	step [65/188], loss=20.9387
	step [66/188], loss=18.5346
	step [67/188], loss=21.0286
	step [68/188], loss=20.8843
	step [69/188], loss=22.6402
	step [70/188], loss=20.4300
	step [71/188], loss=19.6518
	step [72/188], loss=22.3576
	step [73/188], loss=19.6586
	step [74/188], loss=18.5527
	step [75/188], loss=18.6001
	step [76/188], loss=19.5701
	step [77/188], loss=23.1510
	step [78/188], loss=20.7924
	step [79/188], loss=19.8249
	step [80/188], loss=21.1607
	step [81/188], loss=21.5716
	step [82/188], loss=19.6456
	step [83/188], loss=17.4678
	step [84/188], loss=20.8990
	step [85/188], loss=20.1315
	step [86/188], loss=20.3422
	step [87/188], loss=19.2072
	step [88/188], loss=18.1394
	step [89/188], loss=19.5888
	step [90/188], loss=22.3283
	step [91/188], loss=19.2539
	step [92/188], loss=18.1301
	step [93/188], loss=22.6077
	step [94/188], loss=21.2732
	step [95/188], loss=17.8294
	step [96/188], loss=18.6045
	step [97/188], loss=19.4806
	step [98/188], loss=20.2926
	step [99/188], loss=18.6233
	step [100/188], loss=20.4640
	step [101/188], loss=20.4115
	step [102/188], loss=21.5540
	step [103/188], loss=18.1477
	step [104/188], loss=19.3487
	step [105/188], loss=19.7868
	step [106/188], loss=20.2295
	step [107/188], loss=21.6039
	step [108/188], loss=19.1168
	step [109/188], loss=19.4615
	step [110/188], loss=18.4815
	step [111/188], loss=19.1362
	step [112/188], loss=19.7018
	step [113/188], loss=18.5579
	step [114/188], loss=19.6940
	step [115/188], loss=21.8087
	step [116/188], loss=18.4208
	step [117/188], loss=17.9869
	step [118/188], loss=21.1771
	step [119/188], loss=20.3438
	step [120/188], loss=19.3159
	step [121/188], loss=18.7554
	step [122/188], loss=20.2051
	step [123/188], loss=20.2728
	step [124/188], loss=20.5183
	step [125/188], loss=22.8630
	step [126/188], loss=22.4207
	step [127/188], loss=19.1766
	step [128/188], loss=17.6147
	step [129/188], loss=20.4744
	step [130/188], loss=19.3751
	step [131/188], loss=16.6976
	step [132/188], loss=21.2747
	step [133/188], loss=18.9504
	step [134/188], loss=18.4715
	step [135/188], loss=20.5201
	step [136/188], loss=19.9481
	step [137/188], loss=19.8769
	step [138/188], loss=19.0928
	step [139/188], loss=21.2532
	step [140/188], loss=20.4315
	step [141/188], loss=19.0754
	step [142/188], loss=19.7869
	step [143/188], loss=20.5916
	step [144/188], loss=17.7652
	step [145/188], loss=18.4848
	step [146/188], loss=20.3796
	step [147/188], loss=20.0649
	step [148/188], loss=19.7493
	step [149/188], loss=19.8716
	step [150/188], loss=21.5102
	step [151/188], loss=18.7900
	step [152/188], loss=18.3824
	step [153/188], loss=18.7789
	step [154/188], loss=18.3974
	step [155/188], loss=19.1561
	step [156/188], loss=18.7706
	step [157/188], loss=18.5774
	step [158/188], loss=19.8333
	step [159/188], loss=17.0145
	step [160/188], loss=17.0365
	step [161/188], loss=18.3722
	step [162/188], loss=19.1783
	step [163/188], loss=18.6392
	step [164/188], loss=21.1014
	step [165/188], loss=19.3699
	step [166/188], loss=20.1345
	step [167/188], loss=20.2415
	step [168/188], loss=20.5521
	step [169/188], loss=17.9742
	step [170/188], loss=21.1382
	step [171/188], loss=17.9401
	step [172/188], loss=18.0849
	step [173/188], loss=20.2605
	step [174/188], loss=19.7886
	step [175/188], loss=19.6233
	step [176/188], loss=19.4305
	step [177/188], loss=18.1133
	step [178/188], loss=17.2140
	step [179/188], loss=16.7853
	step [180/188], loss=17.2844
	step [181/188], loss=18.8598
	step [182/188], loss=20.4271
	step [183/188], loss=20.1710
	step [184/188], loss=19.1907
	step [185/188], loss=20.6638
	step [186/188], loss=17.7005
	step [187/188], loss=18.7683
	step [188/188], loss=10.9453
	Evaluating
	loss=0.0638, precision=0.1675, recall=0.9972, f1=0.2869
Training epoch 7
	step [1/188], loss=18.9739
	step [2/188], loss=18.2992
	step [3/188], loss=20.2116
	step [4/188], loss=19.9363
	step [5/188], loss=18.3232
	step [6/188], loss=18.8617
	step [7/188], loss=18.7097
	step [8/188], loss=19.6954
	step [9/188], loss=19.3789
	step [10/188], loss=17.7988
	step [11/188], loss=19.2514
	step [12/188], loss=19.4238
	step [13/188], loss=19.5914
	step [14/188], loss=18.1567
	step [15/188], loss=19.0157
	step [16/188], loss=19.7929
	step [17/188], loss=17.6449
	step [18/188], loss=18.4674
	step [19/188], loss=19.6712
	step [20/188], loss=19.6253
	step [21/188], loss=19.4951
	step [22/188], loss=20.3053
	step [23/188], loss=20.5442
	step [24/188], loss=19.0947
	step [25/188], loss=16.6001
	step [26/188], loss=21.2408
	step [27/188], loss=17.9831
	step [28/188], loss=17.9138
	step [29/188], loss=18.5946
	step [30/188], loss=18.3142
	step [31/188], loss=19.0057
	step [32/188], loss=18.9438
	step [33/188], loss=17.2366
	step [34/188], loss=16.8839
	step [35/188], loss=17.3514
	step [36/188], loss=19.3404
	step [37/188], loss=16.6436
	step [38/188], loss=18.2506
	step [39/188], loss=20.0991
	step [40/188], loss=19.1371
	step [41/188], loss=18.7873
	step [42/188], loss=20.1230
	step [43/188], loss=18.3868
	step [44/188], loss=17.9464
	step [45/188], loss=17.8037
	step [46/188], loss=18.2647
	step [47/188], loss=20.1634
	step [48/188], loss=17.4217
	step [49/188], loss=19.2291
	step [50/188], loss=19.6150
	step [51/188], loss=18.7580
	step [52/188], loss=17.2942
	step [53/188], loss=16.9539
	step [54/188], loss=17.5216
	step [55/188], loss=17.3741
	step [56/188], loss=16.3709
	step [57/188], loss=19.6898
	step [58/188], loss=17.9001
	step [59/188], loss=19.2045
	step [60/188], loss=17.6845
	step [61/188], loss=19.7613
	step [62/188], loss=16.5577
	step [63/188], loss=17.8230
	step [64/188], loss=18.6237
	step [65/188], loss=18.2328
	step [66/188], loss=18.9472
	step [67/188], loss=19.3216
	step [68/188], loss=17.3462
	step [69/188], loss=19.3214
	step [70/188], loss=18.8803
	step [71/188], loss=16.3705
	step [72/188], loss=18.2012
	step [73/188], loss=21.7949
	step [74/188], loss=16.4017
	step [75/188], loss=17.4356
	step [76/188], loss=16.8899
	step [77/188], loss=16.8370
	step [78/188], loss=16.4615
	step [79/188], loss=21.2494
	step [80/188], loss=17.3180
	step [81/188], loss=20.1963
	step [82/188], loss=19.3088
	step [83/188], loss=17.1275
	step [84/188], loss=19.0545
	step [85/188], loss=18.4867
	step [86/188], loss=17.5156
	step [87/188], loss=18.0320
	step [88/188], loss=18.7579
	step [89/188], loss=18.9158
	step [90/188], loss=20.2313
	step [91/188], loss=17.6810
	step [92/188], loss=18.1617
	step [93/188], loss=18.4419
	step [94/188], loss=18.5492
	step [95/188], loss=18.6894
	step [96/188], loss=16.3591
	step [97/188], loss=16.8195
	step [98/188], loss=18.3520
	step [99/188], loss=17.5105
	step [100/188], loss=18.8112
	step [101/188], loss=17.3880
	step [102/188], loss=17.4139
	step [103/188], loss=16.2158
	step [104/188], loss=18.0223
	step [105/188], loss=17.9603
	step [106/188], loss=20.3638
	step [107/188], loss=18.5475
	step [108/188], loss=16.5183
	step [109/188], loss=18.6550
	step [110/188], loss=18.5096
	step [111/188], loss=15.7414
	step [112/188], loss=18.4864
	step [113/188], loss=21.2516
	step [114/188], loss=18.7186
	step [115/188], loss=19.3782
	step [116/188], loss=15.6219
	step [117/188], loss=19.3871
	step [118/188], loss=16.1631
	step [119/188], loss=15.5880
	step [120/188], loss=17.8125
	step [121/188], loss=17.6766
	step [122/188], loss=15.6732
	step [123/188], loss=15.8546
	step [124/188], loss=22.8273
	step [125/188], loss=16.5506
	step [126/188], loss=16.0132
	step [127/188], loss=17.9173
	step [128/188], loss=17.5474
	step [129/188], loss=17.4798
	step [130/188], loss=16.8240
	step [131/188], loss=16.8252
	step [132/188], loss=15.9710
	step [133/188], loss=17.1546
	step [134/188], loss=15.8040
	step [135/188], loss=19.2676
	step [136/188], loss=18.1217
	step [137/188], loss=16.1299
	step [138/188], loss=15.6892
	step [139/188], loss=17.4990
	step [140/188], loss=17.2685
	step [141/188], loss=18.4781
	step [142/188], loss=15.7551
	step [143/188], loss=16.6645
	step [144/188], loss=17.2859
	step [145/188], loss=18.7837
	step [146/188], loss=17.3437
	step [147/188], loss=16.4102
	step [148/188], loss=17.4168
	step [149/188], loss=16.9403
	step [150/188], loss=16.6834
	step [151/188], loss=18.8870
	step [152/188], loss=17.3343
	step [153/188], loss=18.7440
	step [154/188], loss=16.3427
	step [155/188], loss=17.6647
	step [156/188], loss=16.3389
	step [157/188], loss=16.1847
	step [158/188], loss=18.5746
	step [159/188], loss=15.6132
	step [160/188], loss=15.8431
	step [161/188], loss=17.2373
	step [162/188], loss=17.4314
	step [163/188], loss=16.7652
	step [164/188], loss=16.5890
	step [165/188], loss=15.5990
	step [166/188], loss=17.3155
	step [167/188], loss=16.1696
	step [168/188], loss=16.7279
	step [169/188], loss=16.0182
	step [170/188], loss=16.0631
	step [171/188], loss=17.1788
	step [172/188], loss=15.5717
	step [173/188], loss=15.6649
	step [174/188], loss=16.6169
	step [175/188], loss=16.4483
	step [176/188], loss=15.7062
	step [177/188], loss=18.6952
	step [178/188], loss=15.3626
	step [179/188], loss=17.7142
	step [180/188], loss=20.0540
	step [181/188], loss=17.2450
	step [182/188], loss=17.1634
	step [183/188], loss=18.4245
	step [184/188], loss=16.5201
	step [185/188], loss=16.5011
	step [186/188], loss=16.9492
	step [187/188], loss=15.6209
	step [188/188], loss=12.5836
	Evaluating
	loss=0.0535, precision=0.1961, recall=0.9968, f1=0.3278
Training epoch 8
	step [1/188], loss=16.2908
	step [2/188], loss=17.2837
	step [3/188], loss=18.6325
	step [4/188], loss=18.6840
	step [5/188], loss=14.8152
	step [6/188], loss=16.8536
	step [7/188], loss=15.3969
	step [8/188], loss=16.2358
	step [9/188], loss=15.1892
	step [10/188], loss=15.1820
	step [11/188], loss=16.4829
	step [12/188], loss=16.9493
	step [13/188], loss=18.4856
	step [14/188], loss=16.9374
	step [15/188], loss=15.3926
	step [16/188], loss=17.4987
	step [17/188], loss=17.8622
	step [18/188], loss=17.6888
	step [19/188], loss=15.9469
	step [20/188], loss=16.8572
	step [21/188], loss=16.8769
	step [22/188], loss=17.4138
	step [23/188], loss=19.4552
	step [24/188], loss=14.3137
	step [25/188], loss=19.9568
	step [26/188], loss=17.8337
	step [27/188], loss=15.9669
	step [28/188], loss=15.8809
	step [29/188], loss=15.4102
	step [30/188], loss=16.6037
	step [31/188], loss=15.5403
	step [32/188], loss=16.5670
	step [33/188], loss=15.6378
	step [34/188], loss=17.5749
	step [35/188], loss=16.8728
	step [36/188], loss=16.3624
	step [37/188], loss=17.6937
	step [38/188], loss=17.1082
	step [39/188], loss=16.3750
	step [40/188], loss=14.4132
	step [41/188], loss=15.1926
	step [42/188], loss=18.7393
	step [43/188], loss=15.0679
	step [44/188], loss=15.9931
	step [45/188], loss=14.1515
	step [46/188], loss=17.0871
	step [47/188], loss=15.7997
	step [48/188], loss=17.2468
	step [49/188], loss=15.5749
	step [50/188], loss=17.2502
	step [51/188], loss=15.9134
	step [52/188], loss=15.5255
	step [53/188], loss=16.9310
	step [54/188], loss=16.5905
	step [55/188], loss=14.8987
	step [56/188], loss=17.3112
	step [57/188], loss=17.3657
	step [58/188], loss=18.0118
	step [59/188], loss=17.2356
	step [60/188], loss=17.3518
	step [61/188], loss=15.3386
	step [62/188], loss=19.2353
	step [63/188], loss=16.8450
	step [64/188], loss=15.2290
	step [65/188], loss=17.5921
	step [66/188], loss=19.0421
	step [67/188], loss=15.7972
	step [68/188], loss=17.2990
	step [69/188], loss=14.8611
	step [70/188], loss=15.9208
	step [71/188], loss=16.6686
	step [72/188], loss=16.7698
	step [73/188], loss=19.2925
	step [74/188], loss=16.6473
	step [75/188], loss=17.9490
	step [76/188], loss=15.6510
	step [77/188], loss=17.0367
	step [78/188], loss=16.8811
	step [79/188], loss=15.5157
	step [80/188], loss=15.2246
	step [81/188], loss=14.8651
	step [82/188], loss=16.8607
	step [83/188], loss=17.4694
	step [84/188], loss=17.8107
	step [85/188], loss=19.9790
	step [86/188], loss=18.3277
	step [87/188], loss=15.0608
	step [88/188], loss=18.0361
	step [89/188], loss=16.2051
	step [90/188], loss=16.0046
	step [91/188], loss=17.8930
	step [92/188], loss=15.4657
	step [93/188], loss=15.9676
	step [94/188], loss=15.6514
	step [95/188], loss=15.1078
	step [96/188], loss=19.7200
	step [97/188], loss=14.8321
	step [98/188], loss=18.2481
	step [99/188], loss=17.3265
	step [100/188], loss=17.0006
	step [101/188], loss=14.2964
	step [102/188], loss=18.6630
	step [103/188], loss=15.3330
	step [104/188], loss=15.9881
	step [105/188], loss=15.1991
	step [106/188], loss=15.0237
	step [107/188], loss=14.6304
	step [108/188], loss=15.4449
	step [109/188], loss=16.0439
	step [110/188], loss=16.4444
	step [111/188], loss=15.7039
	step [112/188], loss=15.1835
	step [113/188], loss=16.8895
	step [114/188], loss=16.7196
	step [115/188], loss=18.1921
	step [116/188], loss=16.4218
	step [117/188], loss=15.2159
	step [118/188], loss=16.5441
	step [119/188], loss=18.2873
	step [120/188], loss=12.0382
	step [121/188], loss=13.8707
	step [122/188], loss=15.6920
	step [123/188], loss=15.3177
	step [124/188], loss=16.0643
	step [125/188], loss=15.8131
	step [126/188], loss=15.8288
	step [127/188], loss=15.6469
	step [128/188], loss=16.6069
	step [129/188], loss=14.8641
	step [130/188], loss=14.8424
	step [131/188], loss=15.7092
	step [132/188], loss=18.6405
	step [133/188], loss=15.3742
	step [134/188], loss=16.6459
	step [135/188], loss=15.9634
	step [136/188], loss=14.7317
	step [137/188], loss=16.0631
	step [138/188], loss=15.2381
	step [139/188], loss=17.0058
	step [140/188], loss=15.4873
	step [141/188], loss=16.5523
	step [142/188], loss=17.5225
	step [143/188], loss=16.1315
	step [144/188], loss=15.0612
	step [145/188], loss=17.0252
	step [146/188], loss=16.1355
	step [147/188], loss=14.3694
	step [148/188], loss=16.1077
	step [149/188], loss=13.9360
	step [150/188], loss=16.8741
	step [151/188], loss=14.9143
	step [152/188], loss=14.8532
	step [153/188], loss=16.3011
	step [154/188], loss=16.7730
	step [155/188], loss=17.5275
	step [156/188], loss=14.3100
	step [157/188], loss=14.6760
	step [158/188], loss=15.8152
	step [159/188], loss=14.9365
	step [160/188], loss=16.5997
	step [161/188], loss=16.7782
	step [162/188], loss=15.2647
	step [163/188], loss=16.4535
	step [164/188], loss=14.4830
	step [165/188], loss=16.4220
	step [166/188], loss=16.3395
	step [167/188], loss=16.2210
	step [168/188], loss=15.2982
	step [169/188], loss=17.9362
	step [170/188], loss=17.5485
	step [171/188], loss=17.8416
	step [172/188], loss=16.2838
	step [173/188], loss=16.1767
	step [174/188], loss=16.7666
	step [175/188], loss=16.1021
	step [176/188], loss=14.9256
	step [177/188], loss=15.2445
	step [178/188], loss=15.3314
	step [179/188], loss=16.1255
	step [180/188], loss=15.0487
	step [181/188], loss=17.1038
	step [182/188], loss=16.9579
	step [183/188], loss=17.9809
	step [184/188], loss=14.1428
	step [185/188], loss=16.3388
	step [186/188], loss=16.9876
	step [187/188], loss=13.9111
	step [188/188], loss=8.9596
	Evaluating
	loss=0.0456, precision=0.2120, recall=0.9963, f1=0.3497
saving model as: 2_saved_model.pth
Training epoch 9
	step [1/188], loss=15.5045
	step [2/188], loss=15.2385
	step [3/188], loss=15.3648
	step [4/188], loss=15.0325
	step [5/188], loss=15.6999
	step [6/188], loss=15.6587
	step [7/188], loss=17.6611
	step [8/188], loss=14.5436
	step [9/188], loss=16.5434
	step [10/188], loss=16.9524
	step [11/188], loss=15.0748
	step [12/188], loss=14.3470
	step [13/188], loss=16.4061
	step [14/188], loss=14.6709
	step [15/188], loss=15.5347
	step [16/188], loss=16.1690
	step [17/188], loss=16.7042
	step [18/188], loss=14.2180
	step [19/188], loss=18.4819
	step [20/188], loss=13.4028
	step [21/188], loss=15.1498
	step [22/188], loss=14.8730
	step [23/188], loss=15.5587
	step [24/188], loss=16.3275
	step [25/188], loss=15.3561
	step [26/188], loss=15.6354
	step [27/188], loss=15.0596
	step [28/188], loss=14.7588
	step [29/188], loss=14.5048
	step [30/188], loss=14.8432
	step [31/188], loss=13.4484
	step [32/188], loss=17.3499
	step [33/188], loss=14.2129
	step [34/188], loss=19.7844
	step [35/188], loss=16.3238
	step [36/188], loss=13.7037
	step [37/188], loss=16.1920
	step [38/188], loss=12.4164
	step [39/188], loss=12.5545
	step [40/188], loss=14.1428
	step [41/188], loss=13.4158
	step [42/188], loss=16.3052
	step [43/188], loss=17.8749
	step [44/188], loss=16.7512
	step [45/188], loss=15.9898
	step [46/188], loss=16.7288
	step [47/188], loss=14.8834
	step [48/188], loss=13.2011
	step [49/188], loss=15.6312
	step [50/188], loss=15.6797
	step [51/188], loss=16.2603
	step [52/188], loss=14.0680
	step [53/188], loss=12.7713
	step [54/188], loss=14.2036
	step [55/188], loss=18.5525
	step [56/188], loss=13.6106
	step [57/188], loss=15.0570
	step [58/188], loss=14.1331
	step [59/188], loss=14.1340
	step [60/188], loss=12.7051
	step [61/188], loss=16.0101
	step [62/188], loss=16.6913
	step [63/188], loss=15.1103
	step [64/188], loss=13.7751
	step [65/188], loss=15.3768
	step [66/188], loss=15.2397
	step [67/188], loss=14.5097
	step [68/188], loss=16.1723
	step [69/188], loss=16.2135
	step [70/188], loss=14.8029
	step [71/188], loss=15.3579
	step [72/188], loss=15.0280
	step [73/188], loss=16.2012
	step [74/188], loss=14.3151
	step [75/188], loss=16.4902
	step [76/188], loss=14.7237
	step [77/188], loss=14.4233
	step [78/188], loss=15.3435
	step [79/188], loss=15.9263
	step [80/188], loss=15.2093
	step [81/188], loss=17.0590
	step [82/188], loss=15.0208
	step [83/188], loss=14.6858
	step [84/188], loss=14.9558
	step [85/188], loss=15.1283
	step [86/188], loss=17.3243
	step [87/188], loss=14.2266
	step [88/188], loss=15.4131
	step [89/188], loss=16.4869
	step [90/188], loss=15.4782
	step [91/188], loss=14.0755
	step [92/188], loss=14.4176
	step [93/188], loss=14.5901
	step [94/188], loss=16.9612
	step [95/188], loss=16.7047
	step [96/188], loss=14.7799
	step [97/188], loss=15.0578
	step [98/188], loss=15.6833
	step [99/188], loss=12.4113
	step [100/188], loss=13.6353
	step [101/188], loss=15.1336
	step [102/188], loss=15.7147
	step [103/188], loss=14.9170
	step [104/188], loss=16.3722
	step [105/188], loss=16.5695
	step [106/188], loss=16.5962
	step [107/188], loss=15.2724
	step [108/188], loss=15.4253
	step [109/188], loss=13.5603
	step [110/188], loss=18.4479
	step [111/188], loss=15.3932
	step [112/188], loss=12.6629
	step [113/188], loss=14.4780
	step [114/188], loss=15.3483
	step [115/188], loss=14.2951
	step [116/188], loss=15.4218
	step [117/188], loss=14.2300
	step [118/188], loss=16.0389
	step [119/188], loss=14.6306
	step [120/188], loss=15.5397
	step [121/188], loss=16.7669
	step [122/188], loss=13.6038
	step [123/188], loss=14.4861
	step [124/188], loss=14.5789
	step [125/188], loss=16.8071
	step [126/188], loss=14.8068
	step [127/188], loss=16.8588
	step [128/188], loss=17.0296
	step [129/188], loss=14.2293
	step [130/188], loss=14.5646
	step [131/188], loss=13.9099
	step [132/188], loss=16.2796
	step [133/188], loss=15.5719
	step [134/188], loss=15.2005
	step [135/188], loss=14.8387
	step [136/188], loss=14.0626
	step [137/188], loss=16.1864
	step [138/188], loss=14.7550
	step [139/188], loss=15.3023
	step [140/188], loss=16.7627
	step [141/188], loss=15.0256
	step [142/188], loss=13.8672
	step [143/188], loss=13.9968
	step [144/188], loss=15.7624
	step [145/188], loss=14.0126
	step [146/188], loss=15.8339
	step [147/188], loss=12.3872
	step [148/188], loss=13.1694
	step [149/188], loss=15.1538
	step [150/188], loss=13.8463
	step [151/188], loss=14.0409
	step [152/188], loss=13.4164
	step [153/188], loss=14.4083
	step [154/188], loss=15.1131
	step [155/188], loss=14.4683
	step [156/188], loss=15.8683
	step [157/188], loss=15.0154
	step [158/188], loss=13.3820
	step [159/188], loss=14.0132
	step [160/188], loss=14.3877
	step [161/188], loss=15.1872
	step [162/188], loss=14.5354
	step [163/188], loss=15.2435
	step [164/188], loss=14.2356
	step [165/188], loss=13.1097
	step [166/188], loss=15.7638
	step [167/188], loss=14.5787
	step [168/188], loss=15.4096
	step [169/188], loss=15.0079
	step [170/188], loss=15.5683
	step [171/188], loss=15.7126
	step [172/188], loss=14.4156
	step [173/188], loss=13.1180
	step [174/188], loss=14.6348
	step [175/188], loss=12.0381
	step [176/188], loss=14.4086
	step [177/188], loss=15.6714
	step [178/188], loss=14.2989
	step [179/188], loss=16.1676
	step [180/188], loss=16.8176
	step [181/188], loss=15.8034
	step [182/188], loss=14.1202
	step [183/188], loss=15.7558
	step [184/188], loss=13.2979
	step [185/188], loss=13.7085
	step [186/188], loss=15.5107
	step [187/188], loss=14.1261
	step [188/188], loss=9.1329
	Evaluating
	loss=0.0389, precision=0.2300, recall=0.9955, f1=0.3736
saving model as: 2_saved_model.pth
Training epoch 10
	step [1/188], loss=14.4845
	step [2/188], loss=15.5722
	step [3/188], loss=14.7104
	step [4/188], loss=14.4372
	step [5/188], loss=14.6932
	step [6/188], loss=12.5940
	step [7/188], loss=13.8727
	step [8/188], loss=13.9434
	step [9/188], loss=12.9978
	step [10/188], loss=14.2631
	step [11/188], loss=14.0899
	step [12/188], loss=14.9699
	step [13/188], loss=13.2265
	step [14/188], loss=15.8534
	step [15/188], loss=13.8926
	step [16/188], loss=13.7223
	step [17/188], loss=14.1973
	step [18/188], loss=14.6101
	step [19/188], loss=15.1345
	step [20/188], loss=15.1735
	step [21/188], loss=16.4272
	step [22/188], loss=14.5358
	step [23/188], loss=15.3459
	step [24/188], loss=13.6131
	step [25/188], loss=13.3297
	step [26/188], loss=14.9546
	step [27/188], loss=14.2973
	step [28/188], loss=16.5295
	step [29/188], loss=12.4371
	step [30/188], loss=14.4767
	step [31/188], loss=16.2624
	step [32/188], loss=12.7681
	step [33/188], loss=13.6527
	step [34/188], loss=16.9984
	step [35/188], loss=14.8999
	step [36/188], loss=15.2916
	step [37/188], loss=13.9858
	step [38/188], loss=15.1064
	step [39/188], loss=13.3976
	step [40/188], loss=12.1203
	step [41/188], loss=16.4634
	step [42/188], loss=15.4524
	step [43/188], loss=12.1817
	step [44/188], loss=11.6949
	step [45/188], loss=12.4199
	step [46/188], loss=13.9706
	step [47/188], loss=16.5123
	step [48/188], loss=15.0758
	step [49/188], loss=15.6969
	step [50/188], loss=15.0225
	step [51/188], loss=14.0812
	step [52/188], loss=16.2353
	step [53/188], loss=14.7890
	step [54/188], loss=15.5923
	step [55/188], loss=14.5638
	step [56/188], loss=13.4442
	step [57/188], loss=14.6378
	step [58/188], loss=13.4632
	step [59/188], loss=16.4554
	step [60/188], loss=19.4392
	step [61/188], loss=14.6168
	step [62/188], loss=13.9513
	step [63/188], loss=14.3386
	step [64/188], loss=13.7332
	step [65/188], loss=13.7955
	step [66/188], loss=13.9526
	step [67/188], loss=12.8228
	step [68/188], loss=16.2935
	step [69/188], loss=12.5083
	step [70/188], loss=14.5438
	step [71/188], loss=16.8847
	step [72/188], loss=13.7722
	step [73/188], loss=15.7466
	step [74/188], loss=13.8128
	step [75/188], loss=14.3200
	step [76/188], loss=14.2040
	step [77/188], loss=12.2809
	step [78/188], loss=15.4949
	step [79/188], loss=13.2990
	step [80/188], loss=11.4971
	step [81/188], loss=13.1659
	step [82/188], loss=13.1919
	step [83/188], loss=17.0062
	step [84/188], loss=14.8373
	step [85/188], loss=16.1139
	step [86/188], loss=13.9284
	step [87/188], loss=12.0658
	step [88/188], loss=14.0663
	step [89/188], loss=15.4176
	step [90/188], loss=12.4771
	step [91/188], loss=13.8560
	step [92/188], loss=13.8848
	step [93/188], loss=12.3574
	step [94/188], loss=12.1881
	step [95/188], loss=12.2686
	step [96/188], loss=14.8393
	step [97/188], loss=14.0318
	step [98/188], loss=14.5079
	step [99/188], loss=15.1129
	step [100/188], loss=13.4829
	step [101/188], loss=15.1659
	step [102/188], loss=13.5782
	step [103/188], loss=13.4028
	step [104/188], loss=15.4873
	step [105/188], loss=11.2118
	step [106/188], loss=15.2933
	step [107/188], loss=12.8501
	step [108/188], loss=15.0660
	step [109/188], loss=14.4526
	step [110/188], loss=13.6601
	step [111/188], loss=14.7138
	step [112/188], loss=12.5155
	step [113/188], loss=14.0090
	step [114/188], loss=14.0836
	step [115/188], loss=12.6625
	step [116/188], loss=15.6773
	step [117/188], loss=11.9415
	step [118/188], loss=14.5880
	step [119/188], loss=13.3434
	step [120/188], loss=14.0258
	step [121/188], loss=12.6498
	step [122/188], loss=12.4539
	step [123/188], loss=11.8264
	step [124/188], loss=14.3431
	step [125/188], loss=13.0288
	step [126/188], loss=13.4817
	step [127/188], loss=13.3798
	step [128/188], loss=13.5321
	step [129/188], loss=16.7726
	step [130/188], loss=14.4564
	step [131/188], loss=13.0750
	step [132/188], loss=11.7048
	step [133/188], loss=13.9167
	step [134/188], loss=14.4180
	step [135/188], loss=16.2847
	step [136/188], loss=12.6675
	step [137/188], loss=15.5072
	step [138/188], loss=15.5515
	step [139/188], loss=15.1504
	step [140/188], loss=11.4142
	step [141/188], loss=14.7528
	step [142/188], loss=14.2169
	step [143/188], loss=13.0562
	step [144/188], loss=14.8474
	step [145/188], loss=13.4239
	step [146/188], loss=14.8556
	step [147/188], loss=14.6367
	step [148/188], loss=12.4149
	step [149/188], loss=14.4244
	step [150/188], loss=12.8073
	step [151/188], loss=17.0156
	step [152/188], loss=16.4335
	step [153/188], loss=12.9751
	step [154/188], loss=13.3047
	step [155/188], loss=14.1148
	step [156/188], loss=13.4163
	step [157/188], loss=12.0637
	step [158/188], loss=13.4984
	step [159/188], loss=14.3506
	step [160/188], loss=14.1222
	step [161/188], loss=13.5719
	step [162/188], loss=13.1016
	step [163/188], loss=13.1248
	step [164/188], loss=14.6292
	step [165/188], loss=11.8858
	step [166/188], loss=12.5739
	step [167/188], loss=13.5405
	step [168/188], loss=13.0759
	step [169/188], loss=12.1621
	step [170/188], loss=14.4620
	step [171/188], loss=11.4865
	step [172/188], loss=13.2036
	step [173/188], loss=13.3192
	step [174/188], loss=15.7254
	step [175/188], loss=13.2945
	step [176/188], loss=12.3384
	step [177/188], loss=14.1418
	step [178/188], loss=11.9123
	step [179/188], loss=13.8684
	step [180/188], loss=14.8106
	step [181/188], loss=12.8378
	step [182/188], loss=15.3950
	step [183/188], loss=14.0942
	step [184/188], loss=13.2459
	step [185/188], loss=15.5210
	step [186/188], loss=12.0618
	step [187/188], loss=11.0498
	step [188/188], loss=7.3396
	Evaluating
	loss=0.0396, precision=0.1884, recall=0.9962, f1=0.3169
Training epoch 11
	step [1/188], loss=13.6976
	step [2/188], loss=12.3111
	step [3/188], loss=11.7108
	step [4/188], loss=15.7386
	step [5/188], loss=14.3052
	step [6/188], loss=15.7794
	step [7/188], loss=12.5534
	step [8/188], loss=12.1919
	step [9/188], loss=13.6163
	step [10/188], loss=13.1346
	step [11/188], loss=12.8961
	step [12/188], loss=13.0652
	step [13/188], loss=13.9954
	step [14/188], loss=14.0973
	step [15/188], loss=13.3980
	step [16/188], loss=14.7244
	step [17/188], loss=15.9039
	step [18/188], loss=12.7862
	step [19/188], loss=13.6386
	step [20/188], loss=14.3587
	step [21/188], loss=12.9494
	step [22/188], loss=13.2143
	step [23/188], loss=12.0011
	step [24/188], loss=13.4821
	step [25/188], loss=12.0468
	step [26/188], loss=13.7161
	step [27/188], loss=16.7418
	step [28/188], loss=11.9439
	step [29/188], loss=13.7873
	step [30/188], loss=17.3125
	step [31/188], loss=14.2194
	step [32/188], loss=12.3845
	step [33/188], loss=12.8148
	step [34/188], loss=12.7846
	step [35/188], loss=15.4765
	step [36/188], loss=16.0907
	step [37/188], loss=14.3215
	step [38/188], loss=12.7101
	step [39/188], loss=14.0782
	step [40/188], loss=15.0566
	step [41/188], loss=14.3889
	step [42/188], loss=13.2387
	step [43/188], loss=14.7759
	step [44/188], loss=14.7072
	step [45/188], loss=11.9539
	step [46/188], loss=12.5073
	step [47/188], loss=14.0450
	step [48/188], loss=15.4268
	step [49/188], loss=13.6636
	step [50/188], loss=13.6276
	step [51/188], loss=11.5980
	step [52/188], loss=12.6859
	step [53/188], loss=13.1849
	step [54/188], loss=14.2472
	step [55/188], loss=11.4185
	step [56/188], loss=13.3364
	step [57/188], loss=12.2068
	step [58/188], loss=12.5203
	step [59/188], loss=13.1171
	step [60/188], loss=12.2816
	step [61/188], loss=14.0163
	step [62/188], loss=11.1405
	step [63/188], loss=13.0163
	step [64/188], loss=12.8172
	step [65/188], loss=14.2600
	step [66/188], loss=12.7121
	step [67/188], loss=14.4687
	step [68/188], loss=12.1408
	step [69/188], loss=12.9697
	step [70/188], loss=13.7028
	step [71/188], loss=13.2398
	step [72/188], loss=13.0572
	step [73/188], loss=10.7297
	step [74/188], loss=12.5045
	step [75/188], loss=12.8073
	step [76/188], loss=14.5793
	step [77/188], loss=12.4611
	step [78/188], loss=12.9846
	step [79/188], loss=13.6762
	step [80/188], loss=12.2052
	step [81/188], loss=12.1722
	step [82/188], loss=15.0756
	step [83/188], loss=12.6419
	step [84/188], loss=14.7826
	step [85/188], loss=11.4140
	step [86/188], loss=11.6919
	step [87/188], loss=14.6373
	step [88/188], loss=14.4827
	step [89/188], loss=13.8261
	step [90/188], loss=11.0605
	step [91/188], loss=13.0226
	step [92/188], loss=12.5379
	step [93/188], loss=12.8016
	step [94/188], loss=12.2387
	step [95/188], loss=12.2617
	step [96/188], loss=16.0023
	step [97/188], loss=14.5615
	step [98/188], loss=14.9998
	step [99/188], loss=12.6416
	step [100/188], loss=15.7602
	step [101/188], loss=12.9654
	step [102/188], loss=11.6299
	step [103/188], loss=12.4717
	step [104/188], loss=14.5723
	step [105/188], loss=15.3204
	step [106/188], loss=14.5345
	step [107/188], loss=12.1200
	step [108/188], loss=11.8694
	step [109/188], loss=12.9003
	step [110/188], loss=14.7848
	step [111/188], loss=15.3766
	step [112/188], loss=13.6642
	step [113/188], loss=13.9657
	step [114/188], loss=12.2396
	step [115/188], loss=13.4012
	step [116/188], loss=12.4188
	step [117/188], loss=12.1011
	step [118/188], loss=12.7050
	step [119/188], loss=12.7941
	step [120/188], loss=13.5047
	step [121/188], loss=13.5173
	step [122/188], loss=14.5857
	step [123/188], loss=13.5946
	step [124/188], loss=14.1082
	step [125/188], loss=11.8388
	step [126/188], loss=12.8719
	step [127/188], loss=11.4734
	step [128/188], loss=14.0520
	step [129/188], loss=13.0170
	step [130/188], loss=15.0410
	step [131/188], loss=13.6909
	step [132/188], loss=15.9293
	step [133/188], loss=12.9423
	step [134/188], loss=12.8538
	step [135/188], loss=13.0925
	step [136/188], loss=14.2296
	step [137/188], loss=13.3811
	step [138/188], loss=10.9115
	step [139/188], loss=12.9683
	step [140/188], loss=12.7224
	step [141/188], loss=12.0295
	step [142/188], loss=14.3632
	step [143/188], loss=13.3503
	step [144/188], loss=12.8545
	step [145/188], loss=14.3873
	step [146/188], loss=14.7910
	step [147/188], loss=13.2024
	step [148/188], loss=15.3516
	step [149/188], loss=12.6915
	step [150/188], loss=13.2353
	step [151/188], loss=13.4322
	step [152/188], loss=12.3370
	step [153/188], loss=13.7889
	step [154/188], loss=15.5905
	step [155/188], loss=11.6616
	step [156/188], loss=13.3744
	step [157/188], loss=12.9192
	step [158/188], loss=12.4384
	step [159/188], loss=14.6778
	step [160/188], loss=12.8192
	step [161/188], loss=11.3010
	step [162/188], loss=13.2823
	step [163/188], loss=13.6883
	step [164/188], loss=12.2472
	step [165/188], loss=11.6844
	step [166/188], loss=11.0801
	step [167/188], loss=12.7325
	step [168/188], loss=12.7430
	step [169/188], loss=13.0839
	step [170/188], loss=10.3308
	step [171/188], loss=14.4768
	step [172/188], loss=11.1092
	step [173/188], loss=11.9629
	step [174/188], loss=16.6178
	step [175/188], loss=12.1798
	step [176/188], loss=13.1275
	step [177/188], loss=12.7734
	step [178/188], loss=13.7112
	step [179/188], loss=12.6505
	step [180/188], loss=13.9658
	step [181/188], loss=12.5385
	step [182/188], loss=14.3858
	step [183/188], loss=12.2892
	step [184/188], loss=11.7888
	step [185/188], loss=11.2748
	step [186/188], loss=11.9682
	step [187/188], loss=13.7789
	step [188/188], loss=7.5751
	Evaluating
	loss=0.0453, precision=0.1529, recall=0.9978, f1=0.2651
Training epoch 12
	step [1/188], loss=11.1526
	step [2/188], loss=11.8272
	step [3/188], loss=13.5202
	step [4/188], loss=12.6373
	step [5/188], loss=10.9524
	step [6/188], loss=10.9715
	step [7/188], loss=13.3919
	step [8/188], loss=12.2758
	step [9/188], loss=12.1491
	step [10/188], loss=11.2358
	step [11/188], loss=11.7945
	step [12/188], loss=12.7262
	step [13/188], loss=11.3414
	step [14/188], loss=14.2841
	step [15/188], loss=12.1001
	step [16/188], loss=12.5809
	step [17/188], loss=14.8614
	step [18/188], loss=11.7270
	step [19/188], loss=13.2507
	step [20/188], loss=12.4955
	step [21/188], loss=11.8048
	step [22/188], loss=12.7909
	step [23/188], loss=12.1757
	step [24/188], loss=12.1565
	step [25/188], loss=13.4419
	step [26/188], loss=13.6377
	step [27/188], loss=10.2506
	step [28/188], loss=12.5567
	step [29/188], loss=11.4967
	step [30/188], loss=15.2167
	step [31/188], loss=15.1915
	step [32/188], loss=12.4000
	step [33/188], loss=13.1994
	step [34/188], loss=15.8454
	step [35/188], loss=12.7905
	step [36/188], loss=14.6488
	step [37/188], loss=11.6898
	step [38/188], loss=12.0893
	step [39/188], loss=11.5058
	step [40/188], loss=12.5513
	step [41/188], loss=15.2665
	step [42/188], loss=12.1530
	step [43/188], loss=13.2106
	step [44/188], loss=10.6441
	step [45/188], loss=14.1014
	step [46/188], loss=13.7611
	step [47/188], loss=11.7690
	step [48/188], loss=11.0977
	step [49/188], loss=14.3904
	step [50/188], loss=12.2128
	step [51/188], loss=11.7589
	step [52/188], loss=14.5367
	step [53/188], loss=12.6901
	step [54/188], loss=13.5966
	step [55/188], loss=15.3059
	step [56/188], loss=12.0666
	step [57/188], loss=12.1501
	step [58/188], loss=11.6440
	step [59/188], loss=13.5849
	step [60/188], loss=12.5481
	step [61/188], loss=12.3741
	step [62/188], loss=11.0892
	step [63/188], loss=12.3881
	step [64/188], loss=15.5309
	step [65/188], loss=14.0219
	step [66/188], loss=13.3421
	step [67/188], loss=12.6306
	step [68/188], loss=11.2371
	step [69/188], loss=12.0017
	step [70/188], loss=14.4201
	step [71/188], loss=11.0032
	step [72/188], loss=13.6241
	step [73/188], loss=10.9680
	step [74/188], loss=11.2564
	step [75/188], loss=14.7830
	step [76/188], loss=9.9621
	step [77/188], loss=11.9137
	step [78/188], loss=12.2314
	step [79/188], loss=11.5420
	step [80/188], loss=15.5844
	step [81/188], loss=13.4742
	step [82/188], loss=15.6494
	step [83/188], loss=11.4642
	step [84/188], loss=11.9617
	step [85/188], loss=12.0559
	step [86/188], loss=12.3417
	step [87/188], loss=14.4168
	step [88/188], loss=13.5103
	step [89/188], loss=12.9819
	step [90/188], loss=12.2502
	step [91/188], loss=12.4754
	step [92/188], loss=13.4525
	step [93/188], loss=16.1517
	step [94/188], loss=12.2616
	step [95/188], loss=12.7259
	step [96/188], loss=12.4932
	step [97/188], loss=13.4330
	step [98/188], loss=12.7301
	step [99/188], loss=12.3609
	step [100/188], loss=13.3999
	step [101/188], loss=11.8596
	step [102/188], loss=12.6899
	step [103/188], loss=11.9555
	step [104/188], loss=10.5981
	step [105/188], loss=11.8905
	step [106/188], loss=12.2261
	step [107/188], loss=11.7665
	step [108/188], loss=10.9010
	step [109/188], loss=14.5621
	step [110/188], loss=11.3708
	step [111/188], loss=11.0850
	step [112/188], loss=12.8164
	step [113/188], loss=13.7423
	step [114/188], loss=11.4257
	step [115/188], loss=12.8866
	step [116/188], loss=10.2453
	step [117/188], loss=13.4232
	step [118/188], loss=13.1448
	step [119/188], loss=13.3691
	step [120/188], loss=14.0310
	step [121/188], loss=15.8085
	step [122/188], loss=12.9344
	step [123/188], loss=11.6469
	step [124/188], loss=11.7825
	step [125/188], loss=9.8724
	step [126/188], loss=14.4625
	step [127/188], loss=14.9727
	step [128/188], loss=10.4069
	step [129/188], loss=12.4952
	step [130/188], loss=11.0762
	step [131/188], loss=11.4269
	step [132/188], loss=11.2178
	step [133/188], loss=12.5605
	step [134/188], loss=12.6903
	step [135/188], loss=9.8973
	step [136/188], loss=10.1885
	step [137/188], loss=14.1479
	step [138/188], loss=14.4715
	step [139/188], loss=12.9465
	step [140/188], loss=12.8720
	step [141/188], loss=12.1065
	step [142/188], loss=11.6012
	step [143/188], loss=11.9215
	step [144/188], loss=11.9526
	step [145/188], loss=11.8164
	step [146/188], loss=10.5268
	step [147/188], loss=15.0145
	step [148/188], loss=16.0603
	step [149/188], loss=13.8449
	step [150/188], loss=12.8765
	step [151/188], loss=11.9075
	step [152/188], loss=14.1047
	step [153/188], loss=12.8911
	step [154/188], loss=14.2689
	step [155/188], loss=11.1705
	step [156/188], loss=11.6973
	step [157/188], loss=10.3337
	step [158/188], loss=14.0330
	step [159/188], loss=13.0436
	step [160/188], loss=11.1998
	step [161/188], loss=13.2906
	step [162/188], loss=13.2947
	step [163/188], loss=13.6593
	step [164/188], loss=11.0801
	step [165/188], loss=13.3415
	step [166/188], loss=11.8395
	step [167/188], loss=12.7474
	step [168/188], loss=10.9481
	step [169/188], loss=12.1153
	step [170/188], loss=11.6753
	step [171/188], loss=11.3575
	step [172/188], loss=12.2459
	step [173/188], loss=14.1226
	step [174/188], loss=11.9306
	step [175/188], loss=10.7029
	step [176/188], loss=14.3737
	step [177/188], loss=14.8384
	step [178/188], loss=13.2453
	step [179/188], loss=12.0029
	step [180/188], loss=14.8737
	step [181/188], loss=12.3822
	step [182/188], loss=11.3845
	step [183/188], loss=12.3464
	step [184/188], loss=12.7482
	step [185/188], loss=13.2556
	step [186/188], loss=11.9531
	step [187/188], loss=12.6541
	step [188/188], loss=7.3026
	Evaluating
	loss=0.0376, precision=0.1728, recall=0.9974, f1=0.2946
Training epoch 13
	step [1/188], loss=11.8785
	step [2/188], loss=13.6885
	step [3/188], loss=12.4773
	step [4/188], loss=11.0080
	step [5/188], loss=11.9908
	step [6/188], loss=14.1828
	step [7/188], loss=10.8627
	step [8/188], loss=11.8939
	step [9/188], loss=14.0431
	step [10/188], loss=12.2988
	step [11/188], loss=12.4280
	step [12/188], loss=10.9293
	step [13/188], loss=13.6523
	step [14/188], loss=11.8042
	step [15/188], loss=11.8632
	step [16/188], loss=13.5534
	step [17/188], loss=12.2782
	step [18/188], loss=14.5167
	step [19/188], loss=11.7853
	step [20/188], loss=13.3692
	step [21/188], loss=13.6630
	step [22/188], loss=12.1344
	step [23/188], loss=11.7842
	step [24/188], loss=10.9893
	step [25/188], loss=10.0974
	step [26/188], loss=12.6171
	step [27/188], loss=10.8488
	step [28/188], loss=9.9491
	step [29/188], loss=14.1961
	step [30/188], loss=11.9227
	step [31/188], loss=11.0268
	step [32/188], loss=12.7189
	step [33/188], loss=11.1741
	step [34/188], loss=14.0955
	step [35/188], loss=11.0106
	step [36/188], loss=14.0189
	step [37/188], loss=12.0685
	step [38/188], loss=11.0441
	step [39/188], loss=11.9776
	step [40/188], loss=10.2920
	step [41/188], loss=11.0141
	step [42/188], loss=12.1302
	step [43/188], loss=12.8264
	step [44/188], loss=12.8583
	step [45/188], loss=12.8970
	step [46/188], loss=11.0798
	step [47/188], loss=13.3863
	step [48/188], loss=11.6751
	step [49/188], loss=11.4989
	step [50/188], loss=12.4745
	step [51/188], loss=12.9791
	step [52/188], loss=14.1670
	step [53/188], loss=12.0422
	step [54/188], loss=12.1273
	step [55/188], loss=12.1856
	step [56/188], loss=10.6340
	step [57/188], loss=11.4381
	step [58/188], loss=12.6615
	step [59/188], loss=10.3014
	step [60/188], loss=11.6906
	step [61/188], loss=11.0979
	step [62/188], loss=11.1245
	step [63/188], loss=15.7611
	step [64/188], loss=14.9325
	step [65/188], loss=12.8644
	step [66/188], loss=12.0168
	step [67/188], loss=12.5303
	step [68/188], loss=12.9459
	step [69/188], loss=13.1104
	step [70/188], loss=10.8707
	step [71/188], loss=12.2899
	step [72/188], loss=12.1921
	step [73/188], loss=11.1440
	step [74/188], loss=11.5251
	step [75/188], loss=11.4973
	step [76/188], loss=13.3850
	step [77/188], loss=12.3699
	step [78/188], loss=13.6890
	step [79/188], loss=13.9411
	step [80/188], loss=13.4772
	step [81/188], loss=12.3534
	step [82/188], loss=11.2722
	step [83/188], loss=13.4981
	step [84/188], loss=13.3393
	step [85/188], loss=12.9758
	step [86/188], loss=11.0854
	step [87/188], loss=12.8620
	step [88/188], loss=13.2696
	step [89/188], loss=12.3641
	step [90/188], loss=13.5050
	step [91/188], loss=13.1016
	step [92/188], loss=12.7562
	step [93/188], loss=11.5654
	step [94/188], loss=11.1025
	step [95/188], loss=15.3248
	step [96/188], loss=10.6146
	step [97/188], loss=13.1643
	step [98/188], loss=10.0621
	step [99/188], loss=12.5174
	step [100/188], loss=12.4715
	step [101/188], loss=10.5006
	step [102/188], loss=10.5431
	step [103/188], loss=11.9699
	step [104/188], loss=11.3826
	step [105/188], loss=11.3541
	step [106/188], loss=13.0734
	step [107/188], loss=13.2120
	step [108/188], loss=11.4303
	step [109/188], loss=11.6542
	step [110/188], loss=12.3826
	step [111/188], loss=13.2599
	step [112/188], loss=10.1175
	step [113/188], loss=13.5444
	step [114/188], loss=14.7128
	step [115/188], loss=10.6443
	step [116/188], loss=11.7401
	step [117/188], loss=10.3140
	step [118/188], loss=12.1683
	step [119/188], loss=11.4517
	step [120/188], loss=11.8729
	step [121/188], loss=13.1044
	step [122/188], loss=10.4374
	step [123/188], loss=11.9092
	step [124/188], loss=13.0141
	step [125/188], loss=11.9479
	step [126/188], loss=11.0348
	step [127/188], loss=11.7442
	step [128/188], loss=12.5158
	step [129/188], loss=10.0213
	step [130/188], loss=10.2423
	step [131/188], loss=10.7245
	step [132/188], loss=12.7342
	step [133/188], loss=10.3033
	step [134/188], loss=14.1428
	step [135/188], loss=10.9274
	step [136/188], loss=13.5495
	step [137/188], loss=10.7956
	step [138/188], loss=10.7347
	step [139/188], loss=11.2944
	step [140/188], loss=10.7147
	step [141/188], loss=11.2465
	step [142/188], loss=12.3501
	step [143/188], loss=11.3864
	step [144/188], loss=12.7497
	step [145/188], loss=10.7714
	step [146/188], loss=11.2909
	step [147/188], loss=12.8758
	step [148/188], loss=11.0898
	step [149/188], loss=12.2184
	step [150/188], loss=10.4637
	step [151/188], loss=13.0687
	step [152/188], loss=11.9637
	step [153/188], loss=12.7609
	step [154/188], loss=12.1143
	step [155/188], loss=13.5731
	step [156/188], loss=12.5750
	step [157/188], loss=10.5816
	step [158/188], loss=10.9533
	step [159/188], loss=10.6114
	step [160/188], loss=14.3612
	step [161/188], loss=12.4674
	step [162/188], loss=11.0021
	step [163/188], loss=10.7043
	step [164/188], loss=9.9815
	step [165/188], loss=12.0220
	step [166/188], loss=13.3256
	step [167/188], loss=11.5919
	step [168/188], loss=11.9597
	step [169/188], loss=9.9951
	step [170/188], loss=11.3058
	step [171/188], loss=13.1697
	step [172/188], loss=12.9622
	step [173/188], loss=11.8998
	step [174/188], loss=11.9826
	step [175/188], loss=10.7897
	step [176/188], loss=12.5443
	step [177/188], loss=13.3218
	step [178/188], loss=11.6549
	step [179/188], loss=11.0944
	step [180/188], loss=10.7262
	step [181/188], loss=10.6041
	step [182/188], loss=10.3318
	step [183/188], loss=10.5005
	step [184/188], loss=12.3694
	step [185/188], loss=13.5870
	step [186/188], loss=13.7559
	step [187/188], loss=10.8682
	step [188/188], loss=7.7867
	Evaluating
	loss=0.0442, precision=0.1400, recall=0.9979, f1=0.2456
Training epoch 14
	step [1/188], loss=14.2131
	step [2/188], loss=12.1952
	step [3/188], loss=11.9516
	step [4/188], loss=13.9331
	step [5/188], loss=10.9380
	step [6/188], loss=11.4777
	step [7/188], loss=11.6605
	step [8/188], loss=14.1734
	step [9/188], loss=10.6706
	step [10/188], loss=15.3410
	step [11/188], loss=11.3639
	step [12/188], loss=13.6673
	step [13/188], loss=14.0594
	step [14/188], loss=11.7621
	step [15/188], loss=12.5170
	step [16/188], loss=11.4431
	step [17/188], loss=12.5270
	step [18/188], loss=12.5240
	step [19/188], loss=11.7196
	step [20/188], loss=13.1631
	step [21/188], loss=11.2668
	step [22/188], loss=12.8775
	step [23/188], loss=10.9655
	step [24/188], loss=13.8571
	step [25/188], loss=10.5526
	step [26/188], loss=11.0246
	step [27/188], loss=10.8236
	step [28/188], loss=10.5793
	step [29/188], loss=12.2498
	step [30/188], loss=13.6581
	step [31/188], loss=10.5337
	step [32/188], loss=10.9008
	step [33/188], loss=12.5026
	step [34/188], loss=12.0527
	step [35/188], loss=12.6189
	step [36/188], loss=10.7235
	step [37/188], loss=12.8357
	step [38/188], loss=10.2181
	step [39/188], loss=11.2796
	step [40/188], loss=10.8853
	step [41/188], loss=10.2166
	step [42/188], loss=10.1492
	step [43/188], loss=13.0763
	step [44/188], loss=11.9696
	step [45/188], loss=10.3876
	step [46/188], loss=10.3195
	step [47/188], loss=12.9316
	step [48/188], loss=10.9214
	step [49/188], loss=12.1708
	step [50/188], loss=13.8995
	step [51/188], loss=10.6394
	step [52/188], loss=12.2851
	step [53/188], loss=11.2248
	step [54/188], loss=9.9537
	step [55/188], loss=12.9857
	step [56/188], loss=13.9693
	step [57/188], loss=9.8558
	step [58/188], loss=10.3065
	step [59/188], loss=10.8031
	step [60/188], loss=11.4138
	step [61/188], loss=11.4736
	step [62/188], loss=11.1665
	step [63/188], loss=12.1208
	step [64/188], loss=10.5588
	step [65/188], loss=11.1800
	step [66/188], loss=12.2014
	step [67/188], loss=10.4409
	step [68/188], loss=10.7713
	step [69/188], loss=13.5542
	step [70/188], loss=11.9307
	step [71/188], loss=10.4579
	step [72/188], loss=11.6545
	step [73/188], loss=14.7371
	step [74/188], loss=10.4520
	step [75/188], loss=9.5194
	step [76/188], loss=12.1394
	step [77/188], loss=12.4517
	step [78/188], loss=12.8938
	step [79/188], loss=10.8747
	step [80/188], loss=14.0820
	step [81/188], loss=9.5114
	step [82/188], loss=11.6171
	step [83/188], loss=11.8887
	step [84/188], loss=13.5941
	step [85/188], loss=11.9718
	step [86/188], loss=11.7103
	step [87/188], loss=10.5639
	step [88/188], loss=10.7280
	step [89/188], loss=11.3655
	step [90/188], loss=11.0878
	step [91/188], loss=11.1291
	step [92/188], loss=10.6998
	step [93/188], loss=10.3458
	step [94/188], loss=13.2007
	step [95/188], loss=11.6456
	step [96/188], loss=14.0538
	step [97/188], loss=10.6878
	step [98/188], loss=9.5253
	step [99/188], loss=12.7912
	step [100/188], loss=10.9798
	step [101/188], loss=11.3517
	step [102/188], loss=12.4138
	step [103/188], loss=11.4944
	step [104/188], loss=12.4740
	step [105/188], loss=12.8817
	step [106/188], loss=9.9254
	step [107/188], loss=10.7411
	step [108/188], loss=11.6989
	step [109/188], loss=10.8379
	step [110/188], loss=11.5126
	step [111/188], loss=12.3214
	step [112/188], loss=12.4122
	step [113/188], loss=11.7837
	step [114/188], loss=10.3934
	step [115/188], loss=11.0592
	step [116/188], loss=12.0321
	step [117/188], loss=16.4377
	step [118/188], loss=10.1514
	step [119/188], loss=12.1651
	step [120/188], loss=11.9115
	step [121/188], loss=13.6604
	step [122/188], loss=10.7516
	step [123/188], loss=11.6036
	step [124/188], loss=11.2462
	step [125/188], loss=10.6438
	step [126/188], loss=10.8224
	step [127/188], loss=11.9168
	step [128/188], loss=10.6412
	step [129/188], loss=14.3813
	step [130/188], loss=11.1527
	step [131/188], loss=11.4352
	step [132/188], loss=13.6525
	step [133/188], loss=11.1266
	step [134/188], loss=10.6686
	step [135/188], loss=9.9134
	step [136/188], loss=11.3567
	step [137/188], loss=9.3883
	step [138/188], loss=9.9576
	step [139/188], loss=12.0562
	step [140/188], loss=11.0235
	step [141/188], loss=10.0613
	step [142/188], loss=11.5409
	step [143/188], loss=9.8211
	step [144/188], loss=11.7710
	step [145/188], loss=13.0066
	step [146/188], loss=11.7460
	step [147/188], loss=12.7524
	step [148/188], loss=10.8674
	step [149/188], loss=11.5879
	step [150/188], loss=11.5759
	step [151/188], loss=10.1482
	step [152/188], loss=12.3592
	step [153/188], loss=13.3926
	step [154/188], loss=10.4188
	step [155/188], loss=11.4173
	step [156/188], loss=10.0468
	step [157/188], loss=14.3004
	step [158/188], loss=11.7764
	step [159/188], loss=12.1533
	step [160/188], loss=13.6610
	step [161/188], loss=9.9235
	step [162/188], loss=12.4798
	step [163/188], loss=11.4810
	step [164/188], loss=12.2431
	step [165/188], loss=10.7700
	step [166/188], loss=11.7227
	step [167/188], loss=12.3552
	step [168/188], loss=9.8263
	step [169/188], loss=11.2694
	step [170/188], loss=11.3964
	step [171/188], loss=12.0500
	step [172/188], loss=10.7480
	step [173/188], loss=13.5859
	step [174/188], loss=11.1471
	step [175/188], loss=13.9481
	step [176/188], loss=13.1824
	step [177/188], loss=10.9936
	step [178/188], loss=9.7981
	step [179/188], loss=10.3497
	step [180/188], loss=13.7525
	step [181/188], loss=9.9522
	step [182/188], loss=12.4618
	step [183/188], loss=12.4392
	step [184/188], loss=11.2949
	step [185/188], loss=10.2271
	step [186/188], loss=12.0993
	step [187/188], loss=12.1867
	step [188/188], loss=6.5818
	Evaluating
	loss=0.0346, precision=0.1801, recall=0.9972, f1=0.3051
Training epoch 15
	step [1/188], loss=11.3123
	step [2/188], loss=8.9173
	step [3/188], loss=10.1816
	step [4/188], loss=12.9933
	step [5/188], loss=12.1000
	step [6/188], loss=12.4158
	step [7/188], loss=12.2036
	step [8/188], loss=12.0422
	step [9/188], loss=9.0997
	step [10/188], loss=12.8046
	step [11/188], loss=12.1739
	step [12/188], loss=12.9238
	step [13/188], loss=11.5664
	step [14/188], loss=12.6892
	step [15/188], loss=13.5992
	step [16/188], loss=9.7429
	step [17/188], loss=9.4074
	step [18/188], loss=14.0190
	step [19/188], loss=11.1558
	step [20/188], loss=11.3900
	step [21/188], loss=11.9214
	step [22/188], loss=10.6859
	step [23/188], loss=9.9663
	step [24/188], loss=11.1794
	step [25/188], loss=10.4362
	step [26/188], loss=10.4875
	step [27/188], loss=10.1309
	step [28/188], loss=13.3131
	step [29/188], loss=11.3944
	step [30/188], loss=12.1796
	step [31/188], loss=10.2583
	step [32/188], loss=13.4397
	step [33/188], loss=11.9946
	step [34/188], loss=11.7064
	step [35/188], loss=10.8910
	step [36/188], loss=11.1471
	step [37/188], loss=9.4175
	step [38/188], loss=11.9486
	step [39/188], loss=13.6439
	step [40/188], loss=11.3805
	step [41/188], loss=12.8721
	step [42/188], loss=11.7826
	step [43/188], loss=12.0924
	step [44/188], loss=11.1405
	step [45/188], loss=10.1764
	step [46/188], loss=11.1619
	step [47/188], loss=10.7424
	step [48/188], loss=10.5286
	step [49/188], loss=12.5133
	step [50/188], loss=13.7536
	step [51/188], loss=9.6434
	step [52/188], loss=11.1453
	step [53/188], loss=12.2516
	step [54/188], loss=10.3268
	step [55/188], loss=13.3849
	step [56/188], loss=10.6862
	step [57/188], loss=10.3719
	step [58/188], loss=11.8467
	step [59/188], loss=12.8617
	step [60/188], loss=8.6444
	step [61/188], loss=11.0708
	step [62/188], loss=12.1876
	step [63/188], loss=10.8286
	step [64/188], loss=10.8416
	step [65/188], loss=12.3983
	step [66/188], loss=12.2461
	step [67/188], loss=10.1399
	step [68/188], loss=11.0816
	step [69/188], loss=9.9710
	step [70/188], loss=9.8164
	step [71/188], loss=13.7628
	step [72/188], loss=10.3719
	step [73/188], loss=10.8219
	step [74/188], loss=11.3971
	step [75/188], loss=11.7549
	step [76/188], loss=12.3485
	step [77/188], loss=12.2027
	step [78/188], loss=9.9309
	step [79/188], loss=9.4164
	step [80/188], loss=10.2338
	step [81/188], loss=9.6819
	step [82/188], loss=9.5639
	step [83/188], loss=9.0077
	step [84/188], loss=10.9428
	step [85/188], loss=9.7053
	step [86/188], loss=11.9703
	step [87/188], loss=13.2913
	step [88/188], loss=10.1454
	step [89/188], loss=11.0966
	step [90/188], loss=10.2623
	step [91/188], loss=10.6946
	step [92/188], loss=9.6033
	step [93/188], loss=9.2983
	step [94/188], loss=12.4155
	step [95/188], loss=11.8232
	step [96/188], loss=8.3554
	step [97/188], loss=11.7432
	step [98/188], loss=10.8084
	step [99/188], loss=13.0349
	step [100/188], loss=9.9872
	step [101/188], loss=13.4583
	step [102/188], loss=13.2763
	step [103/188], loss=11.2177
	step [104/188], loss=11.6010
	step [105/188], loss=13.0777
	step [106/188], loss=9.3790
	step [107/188], loss=11.2005
	step [108/188], loss=11.0933
	step [109/188], loss=10.7348
	step [110/188], loss=11.0084
	step [111/188], loss=9.7248
	step [112/188], loss=12.2199
	step [113/188], loss=11.5866
	step [114/188], loss=11.7506
	step [115/188], loss=8.6531
	step [116/188], loss=12.0249
	step [117/188], loss=12.3620
	step [118/188], loss=9.8935
	step [119/188], loss=10.6077
	step [120/188], loss=11.3157
	step [121/188], loss=12.0825
	step [122/188], loss=10.7002
	step [123/188], loss=10.2518
	step [124/188], loss=10.8695
	step [125/188], loss=11.5299
	step [126/188], loss=10.5342
	step [127/188], loss=10.0371
	step [128/188], loss=10.8314
	step [129/188], loss=13.4944
	step [130/188], loss=11.2070
	step [131/188], loss=11.8852
	step [132/188], loss=10.8291
	step [133/188], loss=12.2912
	step [134/188], loss=10.1760
	step [135/188], loss=9.8204
	step [136/188], loss=10.3211
	step [137/188], loss=11.7453
	step [138/188], loss=10.1738
	step [139/188], loss=10.9191
	step [140/188], loss=11.5185
	step [141/188], loss=12.3161
	step [142/188], loss=10.3253
	step [143/188], loss=10.5991
	step [144/188], loss=13.3623
	step [145/188], loss=12.5501
	step [146/188], loss=10.2213
	step [147/188], loss=11.5286
	step [148/188], loss=10.6667
	step [149/188], loss=12.5574
	step [150/188], loss=12.3393
	step [151/188], loss=10.0465
	step [152/188], loss=12.5253
	step [153/188], loss=10.7465
	step [154/188], loss=10.6418
	step [155/188], loss=10.0181
	step [156/188], loss=12.7889
	step [157/188], loss=12.5485
	step [158/188], loss=10.5006
	step [159/188], loss=9.7380
	step [160/188], loss=10.8572
	step [161/188], loss=11.7676
	step [162/188], loss=9.4305
	step [163/188], loss=10.9988
	step [164/188], loss=10.2942
	step [165/188], loss=8.9991
	step [166/188], loss=9.8958
	step [167/188], loss=10.1293
	step [168/188], loss=14.6320
	step [169/188], loss=10.0648
	step [170/188], loss=11.3247
	step [171/188], loss=12.2379
	step [172/188], loss=12.2679
	step [173/188], loss=9.8715
	step [174/188], loss=11.2029
	step [175/188], loss=11.3358
	step [176/188], loss=12.2955
	step [177/188], loss=10.9996
	step [178/188], loss=10.3163
	step [179/188], loss=12.6268
	step [180/188], loss=12.5378
	step [181/188], loss=9.5327
	step [182/188], loss=13.0304
	step [183/188], loss=10.4722
	step [184/188], loss=11.7256
	step [185/188], loss=9.5952
	step [186/188], loss=12.9064
	step [187/188], loss=11.0789
	step [188/188], loss=7.2694
	Evaluating
	loss=0.0265, precision=0.2255, recall=0.9957, f1=0.3677
Training epoch 16
	step [1/188], loss=12.0607
	step [2/188], loss=11.9763
	step [3/188], loss=10.5628
	step [4/188], loss=10.2784
	step [5/188], loss=8.8743
	step [6/188], loss=11.4170
	step [7/188], loss=11.3465
	step [8/188], loss=10.9363
	step [9/188], loss=10.2514
	step [10/188], loss=10.3119
	step [11/188], loss=10.8290
	step [12/188], loss=11.0357
	step [13/188], loss=11.6580
	step [14/188], loss=10.2184
	step [15/188], loss=12.6922
	step [16/188], loss=13.5254
	step [17/188], loss=10.3209
	step [18/188], loss=10.6898
	step [19/188], loss=10.5075
	step [20/188], loss=10.3873
	step [21/188], loss=13.1078
	step [22/188], loss=10.3309
	step [23/188], loss=10.1299
	step [24/188], loss=11.8158
	step [25/188], loss=12.3907
	step [26/188], loss=11.9073
	step [27/188], loss=11.5881
	step [28/188], loss=12.2041
	step [29/188], loss=10.7634
	step [30/188], loss=12.2271
	step [31/188], loss=11.7148
	step [32/188], loss=10.3683
	step [33/188], loss=11.2432
	step [34/188], loss=10.6434
	step [35/188], loss=8.5844
	step [36/188], loss=10.3253
	step [37/188], loss=9.4189
	step [38/188], loss=10.3550
	step [39/188], loss=11.4472
	step [40/188], loss=11.0399
	step [41/188], loss=12.9743
	step [42/188], loss=11.3994
	step [43/188], loss=10.5162
	step [44/188], loss=8.6307
	step [45/188], loss=12.0954
	step [46/188], loss=9.0923
	step [47/188], loss=10.5863
	step [48/188], loss=13.3002
	step [49/188], loss=11.6620
	step [50/188], loss=10.9522
	step [51/188], loss=10.1694
	step [52/188], loss=10.1373
	step [53/188], loss=8.7001
	step [54/188], loss=11.3169
	step [55/188], loss=10.2827
	step [56/188], loss=12.6770
	step [57/188], loss=9.8578
	step [58/188], loss=10.7739
	step [59/188], loss=10.4660
	step [60/188], loss=9.6751
	step [61/188], loss=12.2973
	step [62/188], loss=11.5944
	step [63/188], loss=9.7011
	step [64/188], loss=10.7662
	step [65/188], loss=14.4491
	step [66/188], loss=12.0759
	step [67/188], loss=10.4804
	step [68/188], loss=9.7051
	step [69/188], loss=10.7973
	step [70/188], loss=10.6837
	step [71/188], loss=12.6016
	step [72/188], loss=9.5779
	step [73/188], loss=10.7329
	step [74/188], loss=10.8538
	step [75/188], loss=12.7411
	step [76/188], loss=12.5812
	step [77/188], loss=10.1865
	step [78/188], loss=9.8597
	step [79/188], loss=8.7020
	step [80/188], loss=9.6437
	step [81/188], loss=10.8509
	step [82/188], loss=9.9335
	step [83/188], loss=9.9469
	step [84/188], loss=11.2959
	step [85/188], loss=9.4818
	step [86/188], loss=11.3850
	step [87/188], loss=10.8177
	step [88/188], loss=9.7337
	step [89/188], loss=11.4003
	step [90/188], loss=9.1647
	step [91/188], loss=11.3306
	step [92/188], loss=11.0092
	step [93/188], loss=10.9549
	step [94/188], loss=10.7279
	step [95/188], loss=12.4005
	step [96/188], loss=12.6543
	step [97/188], loss=12.9802
	step [98/188], loss=10.4435
	step [99/188], loss=13.6338
	step [100/188], loss=11.5096
	step [101/188], loss=11.6963
	step [102/188], loss=11.8147
	step [103/188], loss=12.5848
	step [104/188], loss=11.8257
	step [105/188], loss=11.9854
	step [106/188], loss=11.5046
	step [107/188], loss=8.9929
	step [108/188], loss=9.2721
	step [109/188], loss=11.6800
	step [110/188], loss=9.8390
	step [111/188], loss=10.8261
	step [112/188], loss=9.9968
	step [113/188], loss=8.6819
	step [114/188], loss=11.2649
	step [115/188], loss=11.2811
	step [116/188], loss=12.6621
	step [117/188], loss=11.3430
	step [118/188], loss=9.7662
	step [119/188], loss=10.5048
	step [120/188], loss=11.0257
	step [121/188], loss=11.3582
	step [122/188], loss=9.4942
	step [123/188], loss=9.6343
	step [124/188], loss=12.5222
	step [125/188], loss=8.3462
	step [126/188], loss=10.3747
	step [127/188], loss=11.9518
	step [128/188], loss=9.0562
	step [129/188], loss=11.6632
	step [130/188], loss=9.7293
	step [131/188], loss=9.6661
	step [132/188], loss=10.9007
	step [133/188], loss=10.9608
	step [134/188], loss=12.1173
	step [135/188], loss=12.0215
	step [136/188], loss=10.1750
	step [137/188], loss=10.3776
	step [138/188], loss=11.2507
	step [139/188], loss=8.5790
	step [140/188], loss=10.8143
	step [141/188], loss=12.5911
	step [142/188], loss=10.6006
	step [143/188], loss=11.8645
	step [144/188], loss=11.1118
	step [145/188], loss=9.6938
	step [146/188], loss=10.7077
	step [147/188], loss=10.6675
	step [148/188], loss=10.4130
	step [149/188], loss=10.1916
	step [150/188], loss=9.4063
	step [151/188], loss=9.9586
	step [152/188], loss=13.2279
	step [153/188], loss=9.8223
	step [154/188], loss=11.8609
	step [155/188], loss=9.9335
	step [156/188], loss=11.2726
	step [157/188], loss=13.6654
	step [158/188], loss=10.2014
	step [159/188], loss=11.9088
	step [160/188], loss=11.8653
	step [161/188], loss=10.0044
	step [162/188], loss=11.5179
	step [163/188], loss=12.0874
	step [164/188], loss=11.7798
	step [165/188], loss=11.5344
	step [166/188], loss=9.8659
	step [167/188], loss=11.4742
	step [168/188], loss=11.3096
	step [169/188], loss=9.6054
	step [170/188], loss=10.3369
	step [171/188], loss=10.6563
	step [172/188], loss=9.6537
	step [173/188], loss=9.8816
	step [174/188], loss=9.1521
	step [175/188], loss=10.0663
	step [176/188], loss=10.8077
	step [177/188], loss=12.0761
	step [178/188], loss=10.5755
	step [179/188], loss=9.9773
	step [180/188], loss=12.2940
	step [181/188], loss=10.9724
	step [182/188], loss=12.9457
	step [183/188], loss=9.5662
	step [184/188], loss=13.0401
	step [185/188], loss=12.5967
	step [186/188], loss=11.3588
	step [187/188], loss=9.1450
	step [188/188], loss=5.5245
	Evaluating
	loss=0.0301, precision=0.1965, recall=0.9968, f1=0.3283
Training epoch 17
	step [1/188], loss=8.6348
	step [2/188], loss=9.7016
	step [3/188], loss=11.4200
	step [4/188], loss=10.2979
	step [5/188], loss=10.3073
	step [6/188], loss=8.5403
	step [7/188], loss=8.5492
	step [8/188], loss=11.6166
	step [9/188], loss=11.7059
	step [10/188], loss=9.6969
	step [11/188], loss=8.8409
	step [12/188], loss=10.4524
	step [13/188], loss=9.4949
	step [14/188], loss=10.4598
	step [15/188], loss=13.0829
	step [16/188], loss=11.9150
	step [17/188], loss=9.5827
	step [18/188], loss=11.2799
	step [19/188], loss=11.4794
	step [20/188], loss=9.3327
	step [21/188], loss=9.1452
	step [22/188], loss=11.0700
	step [23/188], loss=10.3178
	step [24/188], loss=11.6449
	step [25/188], loss=11.2275
	step [26/188], loss=10.6879
	step [27/188], loss=8.3541
	step [28/188], loss=12.7407
	step [29/188], loss=10.5391
	step [30/188], loss=10.6797
	step [31/188], loss=11.1792
	step [32/188], loss=9.2180
	step [33/188], loss=11.3939
	step [34/188], loss=10.0525
	step [35/188], loss=12.0547
	step [36/188], loss=12.0979
	step [37/188], loss=9.3545
	step [38/188], loss=8.8416
	step [39/188], loss=11.7443
	step [40/188], loss=12.2312
	step [41/188], loss=10.6063
	step [42/188], loss=10.3768
	step [43/188], loss=9.3487
	step [44/188], loss=9.8295
	step [45/188], loss=12.8278
	step [46/188], loss=10.5119
	step [47/188], loss=10.1944
	step [48/188], loss=9.3878
	step [49/188], loss=11.6625
	step [50/188], loss=11.9437
	step [51/188], loss=10.1699
	step [52/188], loss=12.4664
	step [53/188], loss=11.0193
	step [54/188], loss=11.8883
	step [55/188], loss=10.5403
	step [56/188], loss=10.8160
	step [57/188], loss=10.6867
	step [58/188], loss=10.8092
	step [59/188], loss=10.7618
	step [60/188], loss=10.5847
	step [61/188], loss=11.2652
	step [62/188], loss=13.0731
	step [63/188], loss=9.0868
	step [64/188], loss=7.5683
	step [65/188], loss=10.5470
	step [66/188], loss=9.5264
	step [67/188], loss=10.2004
	step [68/188], loss=11.0417
	step [69/188], loss=10.5423
	step [70/188], loss=10.4174
	step [71/188], loss=10.9078
	step [72/188], loss=10.3536
	step [73/188], loss=9.9947
	step [74/188], loss=11.5861
	step [75/188], loss=9.1476
	step [76/188], loss=12.5710
	step [77/188], loss=9.0615
	step [78/188], loss=9.1923
	step [79/188], loss=9.2266
	step [80/188], loss=10.5256
	step [81/188], loss=11.7190
	step [82/188], loss=10.8694
	step [83/188], loss=9.8207
	step [84/188], loss=10.0508
	step [85/188], loss=9.7151
	step [86/188], loss=9.1471
	step [87/188], loss=10.8184
	step [88/188], loss=9.6018
	step [89/188], loss=12.0029
	step [90/188], loss=9.5410
	step [91/188], loss=9.9557
	step [92/188], loss=8.7342
	step [93/188], loss=9.1241
	step [94/188], loss=10.4586
	step [95/188], loss=10.3585
	step [96/188], loss=9.5300
	step [97/188], loss=11.3536
	step [98/188], loss=11.1987
	step [99/188], loss=10.6871
	step [100/188], loss=9.8347
	step [101/188], loss=10.0733
	step [102/188], loss=10.4282
	step [103/188], loss=12.3368
	step [104/188], loss=11.3772
	step [105/188], loss=9.8694
	step [106/188], loss=12.1512
	step [107/188], loss=10.0747
	step [108/188], loss=13.2844
	step [109/188], loss=11.2463
	step [110/188], loss=13.2589
	step [111/188], loss=9.5327
	step [112/188], loss=10.9793
	step [113/188], loss=9.8943
	step [114/188], loss=11.3768
	step [115/188], loss=10.4552
	step [116/188], loss=9.3346
	step [117/188], loss=10.8190
	step [118/188], loss=8.8555
	step [119/188], loss=9.9882
	step [120/188], loss=10.2640
	step [121/188], loss=10.7121
	step [122/188], loss=11.9725
	step [123/188], loss=8.7763
	step [124/188], loss=10.4234
	step [125/188], loss=12.5634
	step [126/188], loss=10.2558
	step [127/188], loss=13.5087
	step [128/188], loss=12.4667
	step [129/188], loss=10.0482
	step [130/188], loss=11.8856
	step [131/188], loss=8.8178
	step [132/188], loss=10.0309
	step [133/188], loss=10.3874
	step [134/188], loss=9.9822
	step [135/188], loss=9.3472
	step [136/188], loss=11.2124
	step [137/188], loss=9.7187
	step [138/188], loss=7.5353
	step [139/188], loss=9.6504
	step [140/188], loss=9.7078
	step [141/188], loss=10.3610
	step [142/188], loss=11.6263
	step [143/188], loss=11.6596
	step [144/188], loss=9.2709
	step [145/188], loss=9.4963
	step [146/188], loss=12.4694
	step [147/188], loss=11.9842
	step [148/188], loss=10.1522
	step [149/188], loss=10.9898
	step [150/188], loss=11.2626
	step [151/188], loss=11.0970
	step [152/188], loss=8.0717
	step [153/188], loss=11.3963
	step [154/188], loss=9.4591
	step [155/188], loss=9.7024
	step [156/188], loss=11.7113
	step [157/188], loss=8.9533
	step [158/188], loss=9.7434
	step [159/188], loss=11.5938
	step [160/188], loss=11.0316
	step [161/188], loss=9.3196
	step [162/188], loss=13.8307
	step [163/188], loss=11.7913
	step [164/188], loss=10.5813
	step [165/188], loss=9.6046
	step [166/188], loss=10.1043
	step [167/188], loss=8.7407
	step [168/188], loss=9.8221
	step [169/188], loss=11.1991
	step [170/188], loss=11.8560
	step [171/188], loss=10.9829
	step [172/188], loss=10.7437
	step [173/188], loss=11.8056
	step [174/188], loss=10.5019
	step [175/188], loss=10.5691
	step [176/188], loss=11.1175
	step [177/188], loss=11.8674
	step [178/188], loss=11.3137
	step [179/188], loss=10.2672
	step [180/188], loss=10.7057
	step [181/188], loss=12.1165
	step [182/188], loss=9.6141
	step [183/188], loss=13.7700
	step [184/188], loss=9.5405
	step [185/188], loss=10.5489
	step [186/188], loss=12.3313
	step [187/188], loss=8.5587
	step [188/188], loss=5.9870
	Evaluating
	loss=0.0231, precision=0.2471, recall=0.9950, f1=0.3959
saving model as: 2_saved_model.pth
Training epoch 18
	step [1/188], loss=9.2798
	step [2/188], loss=8.8037
	step [3/188], loss=10.4090
	step [4/188], loss=10.5906
	step [5/188], loss=10.1959
	step [6/188], loss=10.2711
	step [7/188], loss=10.5110
	step [8/188], loss=9.2925
	step [9/188], loss=10.4472
	step [10/188], loss=10.2884
	step [11/188], loss=10.1002
	step [12/188], loss=8.9115
	step [13/188], loss=9.0053
	step [14/188], loss=9.3232
	step [15/188], loss=10.9964
	step [16/188], loss=8.9660
	step [17/188], loss=10.6518
	step [18/188], loss=11.8324
	step [19/188], loss=11.0988
	step [20/188], loss=10.0002
	step [21/188], loss=10.6941
	step [22/188], loss=9.7214
	step [23/188], loss=11.4009
	step [24/188], loss=9.4710
	step [25/188], loss=11.3527
	step [26/188], loss=9.8282
	step [27/188], loss=10.1215
	step [28/188], loss=9.6113
	step [29/188], loss=9.2374
	step [30/188], loss=8.1571
	step [31/188], loss=10.6007
	step [32/188], loss=10.5273
	step [33/188], loss=9.6636
	step [34/188], loss=10.8963
	step [35/188], loss=11.5757
	step [36/188], loss=10.0322
	step [37/188], loss=10.5886
	step [38/188], loss=11.6951
	step [39/188], loss=9.2137
	step [40/188], loss=10.3307
	step [41/188], loss=13.1579
	step [42/188], loss=9.1595
	step [43/188], loss=9.5964
	step [44/188], loss=8.7788
	step [45/188], loss=9.9251
	step [46/188], loss=10.4192
	step [47/188], loss=10.0907
	step [48/188], loss=9.9493
	step [49/188], loss=11.3856
	step [50/188], loss=9.1621
	step [51/188], loss=10.9916
	step [52/188], loss=8.3234
	step [53/188], loss=11.5742
	step [54/188], loss=9.5532
	step [55/188], loss=10.0148
	step [56/188], loss=9.3698
	step [57/188], loss=9.8271
	step [58/188], loss=10.9736
	step [59/188], loss=9.6506
	step [60/188], loss=10.4175
	step [61/188], loss=11.0910
	step [62/188], loss=10.6233
	step [63/188], loss=10.1554
	step [64/188], loss=10.5279
	step [65/188], loss=9.2957
	step [66/188], loss=11.1899
	step [67/188], loss=11.3741
	step [68/188], loss=9.9549
	step [69/188], loss=10.3630
	step [70/188], loss=12.9092
	step [71/188], loss=9.7412
	step [72/188], loss=8.7158
	step [73/188], loss=10.2768
	step [74/188], loss=8.7528
	step [75/188], loss=12.2857
	step [76/188], loss=11.1537
	step [77/188], loss=9.8726
	step [78/188], loss=10.3631
	step [79/188], loss=10.3821
	step [80/188], loss=7.8082
	step [81/188], loss=9.3031
	step [82/188], loss=10.0681
	step [83/188], loss=8.7903
	step [84/188], loss=11.5126
	step [85/188], loss=9.7920
	step [86/188], loss=9.5281
	step [87/188], loss=8.7885
	step [88/188], loss=10.3443
	step [89/188], loss=11.4729
	step [90/188], loss=12.1078
	step [91/188], loss=9.3088
	step [92/188], loss=9.3927
	step [93/188], loss=9.6149
	step [94/188], loss=10.2725
	step [95/188], loss=12.7970
	step [96/188], loss=9.6158
	step [97/188], loss=9.8639
	step [98/188], loss=8.6199
	step [99/188], loss=9.8743
	step [100/188], loss=12.0775
	step [101/188], loss=12.1356
	step [102/188], loss=11.0812
	step [103/188], loss=10.5078
	step [104/188], loss=11.0415
	step [105/188], loss=10.9455
	step [106/188], loss=10.5209
	step [107/188], loss=11.3045
	step [108/188], loss=9.6094
	step [109/188], loss=9.5954
	step [110/188], loss=11.3938
	step [111/188], loss=9.3488
	step [112/188], loss=13.2316
	step [113/188], loss=10.8641
	step [114/188], loss=10.4124
	step [115/188], loss=11.3376
	step [116/188], loss=10.6324
	step [117/188], loss=10.1865
	step [118/188], loss=11.4026
	step [119/188], loss=12.0101
	step [120/188], loss=10.4183
	step [121/188], loss=10.3473
	step [122/188], loss=10.5617
	step [123/188], loss=11.4466
	step [124/188], loss=10.4652
	step [125/188], loss=10.0208
	step [126/188], loss=10.0532
	step [127/188], loss=11.4892
	step [128/188], loss=10.0797
	step [129/188], loss=8.9273
	step [130/188], loss=9.7114
	step [131/188], loss=11.0255
	step [132/188], loss=10.8728
	step [133/188], loss=9.5425
	step [134/188], loss=11.5848
	step [135/188], loss=9.9365
	step [136/188], loss=10.2979
	step [137/188], loss=10.5616
	step [138/188], loss=9.7065
	step [139/188], loss=10.5158
	step [140/188], loss=11.5315
	step [141/188], loss=12.4076
	step [142/188], loss=13.5680
	step [143/188], loss=10.0657
	step [144/188], loss=10.4191
	step [145/188], loss=8.7191
	step [146/188], loss=10.7710
	step [147/188], loss=11.8914
	step [148/188], loss=10.9615
	step [149/188], loss=9.4814
	step [150/188], loss=11.1563
	step [151/188], loss=7.5770
	step [152/188], loss=10.3873
	step [153/188], loss=10.3022
	step [154/188], loss=9.6154
	step [155/188], loss=10.8340
	step [156/188], loss=8.3673
	step [157/188], loss=10.6267
	step [158/188], loss=10.4261
	step [159/188], loss=10.8693
	step [160/188], loss=9.8216
	step [161/188], loss=9.5975
	step [162/188], loss=11.2356
	step [163/188], loss=10.6802
	step [164/188], loss=8.1915
	step [165/188], loss=10.5219
	step [166/188], loss=11.1593
	step [167/188], loss=8.2985
	step [168/188], loss=9.5814
	step [169/188], loss=12.3001
	step [170/188], loss=10.4434
	step [171/188], loss=10.2402
	step [172/188], loss=9.6400
	step [173/188], loss=8.8285
	step [174/188], loss=8.9104
	step [175/188], loss=9.5626
	step [176/188], loss=10.8525
	step [177/188], loss=9.2752
	step [178/188], loss=8.4875
	step [179/188], loss=10.1522
	step [180/188], loss=9.9459
	step [181/188], loss=10.3691
	step [182/188], loss=11.4608
	step [183/188], loss=10.7064
	step [184/188], loss=11.1550
	step [185/188], loss=10.7102
	step [186/188], loss=8.7402
	step [187/188], loss=9.6274
	step [188/188], loss=5.8878
	Evaluating
	loss=0.0296, precision=0.1797, recall=0.9970, f1=0.3045
Training epoch 19
	step [1/188], loss=8.5839
	step [2/188], loss=12.0429
	step [3/188], loss=11.0387
	step [4/188], loss=9.7366
	step [5/188], loss=9.7106
	step [6/188], loss=9.7147
	step [7/188], loss=9.5578
	step [8/188], loss=10.5217
	step [9/188], loss=11.5651
	step [10/188], loss=9.0708
	step [11/188], loss=10.6022
	step [12/188], loss=9.7576
	step [13/188], loss=10.1709
	step [14/188], loss=10.2951
	step [15/188], loss=9.7999
	step [16/188], loss=9.9691
	step [17/188], loss=8.4164
	step [18/188], loss=13.4068
	step [19/188], loss=11.1919
	step [20/188], loss=10.0551
	step [21/188], loss=10.7427
	step [22/188], loss=11.1064
	step [23/188], loss=10.7082
	step [24/188], loss=11.3564
	step [25/188], loss=12.3285
	step [26/188], loss=9.0152
	step [27/188], loss=11.5796
	step [28/188], loss=9.1092
	step [29/188], loss=9.8198
	step [30/188], loss=9.5273
	step [31/188], loss=9.4235
	step [32/188], loss=8.8536
	step [33/188], loss=11.3059
	step [34/188], loss=8.3327
	step [35/188], loss=7.9091
	step [36/188], loss=10.2244
	step [37/188], loss=9.9002
	step [38/188], loss=11.1688
	step [39/188], loss=8.6593
	step [40/188], loss=9.8717
	step [41/188], loss=10.5603
	step [42/188], loss=10.5069
	step [43/188], loss=9.9787
	step [44/188], loss=10.6599
	step [45/188], loss=7.8108
	step [46/188], loss=10.9395
	step [47/188], loss=8.3140
	step [48/188], loss=12.2763
	step [49/188], loss=8.9796
	step [50/188], loss=8.8910
	step [51/188], loss=10.1642
	step [52/188], loss=7.9076
	step [53/188], loss=10.2249
	step [54/188], loss=11.3002
	step [55/188], loss=11.8724
	step [56/188], loss=11.7837
	step [57/188], loss=9.4896
	step [58/188], loss=11.1517
	step [59/188], loss=10.8360
	step [60/188], loss=8.9984
	step [61/188], loss=10.2913
	step [62/188], loss=9.1034
	step [63/188], loss=9.4594
	step [64/188], loss=9.1133
	step [65/188], loss=10.8373
	step [66/188], loss=8.0913
	step [67/188], loss=10.8722
	step [68/188], loss=11.0796
	step [69/188], loss=10.5484
	step [70/188], loss=9.8208
	step [71/188], loss=9.5184
	step [72/188], loss=9.1842
	step [73/188], loss=10.1882
	step [74/188], loss=8.2991
	step [75/188], loss=10.9256
	step [76/188], loss=9.1890
	step [77/188], loss=11.1154
	step [78/188], loss=8.9391
	step [79/188], loss=12.5929
	step [80/188], loss=11.5211
	step [81/188], loss=8.1458
	step [82/188], loss=8.6204
	step [83/188], loss=9.9079
	step [84/188], loss=8.8126
	step [85/188], loss=8.4943
	step [86/188], loss=8.6879
	step [87/188], loss=10.4615
	step [88/188], loss=8.6792
	step [89/188], loss=11.2583
	step [90/188], loss=14.0804
	step [91/188], loss=9.2046
	step [92/188], loss=8.5772
	step [93/188], loss=9.4326
	step [94/188], loss=9.3765
	step [95/188], loss=11.5557
	step [96/188], loss=10.8979
	step [97/188], loss=11.0131
	step [98/188], loss=10.4815
	step [99/188], loss=9.6150
	step [100/188], loss=8.2626
	step [101/188], loss=9.2823
	step [102/188], loss=9.9530
	step [103/188], loss=8.2932
	step [104/188], loss=8.4259
	step [105/188], loss=10.2624
	step [106/188], loss=9.5283
	step [107/188], loss=10.2084
	step [108/188], loss=9.8284
	step [109/188], loss=9.8512
	step [110/188], loss=9.2704
	step [111/188], loss=10.2953
	step [112/188], loss=8.7099
	step [113/188], loss=10.0180
	step [114/188], loss=8.9146
	step [115/188], loss=9.6602
	step [116/188], loss=9.5345
	step [117/188], loss=10.5033
	step [118/188], loss=7.0799
	step [119/188], loss=10.2174
	step [120/188], loss=9.9727
	step [121/188], loss=9.5074
	step [122/188], loss=10.1758
	step [123/188], loss=9.0516
	step [124/188], loss=11.2484
	step [125/188], loss=9.4046
	step [126/188], loss=8.8705
	step [127/188], loss=9.8312
	step [128/188], loss=11.9884
	step [129/188], loss=9.8524
	step [130/188], loss=10.1408
	step [131/188], loss=8.7405
	step [132/188], loss=8.6899
	step [133/188], loss=9.8243
	step [134/188], loss=11.1829
	step [135/188], loss=8.8741
	step [136/188], loss=9.2957
	step [137/188], loss=12.4089
	step [138/188], loss=9.1541
	step [139/188], loss=9.4415
	step [140/188], loss=9.0583
	step [141/188], loss=10.0478
	step [142/188], loss=8.7038
	step [143/188], loss=10.7928
	step [144/188], loss=11.3127
	step [145/188], loss=9.1434
	step [146/188], loss=8.5260
	step [147/188], loss=9.4238
	step [148/188], loss=10.0357
	step [149/188], loss=10.3876
	step [150/188], loss=10.0780
	step [151/188], loss=10.5205
	step [152/188], loss=8.8549
	step [153/188], loss=8.2792
	step [154/188], loss=9.5272
	step [155/188], loss=9.1142
	step [156/188], loss=13.2779
	step [157/188], loss=11.9949
	step [158/188], loss=8.4107
	step [159/188], loss=10.1000
	step [160/188], loss=9.4955
	step [161/188], loss=10.7873
	step [162/188], loss=10.9530
	step [163/188], loss=11.1615
	step [164/188], loss=9.5751
	step [165/188], loss=9.7224
	step [166/188], loss=11.3207
	step [167/188], loss=10.5013
	step [168/188], loss=11.2264
	step [169/188], loss=9.6442
	step [170/188], loss=10.5994
	step [171/188], loss=10.9396
	step [172/188], loss=9.9264
	step [173/188], loss=10.2829
	step [174/188], loss=10.8520
	step [175/188], loss=8.7898
	step [176/188], loss=9.8927
	step [177/188], loss=9.8241
	step [178/188], loss=8.5675
	step [179/188], loss=10.5204
	step [180/188], loss=10.4296
	step [181/188], loss=8.1740
	step [182/188], loss=11.1953
	step [183/188], loss=8.7008
	step [184/188], loss=8.6930
	step [185/188], loss=10.2837
	step [186/188], loss=9.4768
	step [187/188], loss=10.5669
	step [188/188], loss=7.8504
	Evaluating
	loss=0.0345, precision=0.1547, recall=0.9977, f1=0.2679
Training epoch 20
	step [1/188], loss=9.7764
	step [2/188], loss=8.8308
	step [3/188], loss=10.5125
	step [4/188], loss=10.7824
	step [5/188], loss=9.6950
	step [6/188], loss=12.2258
	step [7/188], loss=9.6403
	step [8/188], loss=11.0378
	step [9/188], loss=10.4251
	step [10/188], loss=7.8160
	step [11/188], loss=11.0942
	step [12/188], loss=10.0508
	step [13/188], loss=9.3386
	step [14/188], loss=8.8003
	step [15/188], loss=10.9558
	step [16/188], loss=11.3175
	step [17/188], loss=11.6528
	step [18/188], loss=9.4151
	step [19/188], loss=8.6138
	step [20/188], loss=9.1787
	step [21/188], loss=10.1699
	step [22/188], loss=9.6722
	step [23/188], loss=9.8243
	step [24/188], loss=10.6943
	step [25/188], loss=9.7029
	step [26/188], loss=7.3730
	step [27/188], loss=11.0651
	step [28/188], loss=9.3359
	step [29/188], loss=8.0192
	step [30/188], loss=10.2935
	step [31/188], loss=10.7540
	step [32/188], loss=10.8673
	step [33/188], loss=9.0619
	step [34/188], loss=9.8450
	step [35/188], loss=8.0792
	step [36/188], loss=9.4763
	step [37/188], loss=11.4400
	step [38/188], loss=7.1883
	step [39/188], loss=7.5170
	step [40/188], loss=10.9833
	step [41/188], loss=10.5674
	step [42/188], loss=8.6629
	step [43/188], loss=9.4694
	step [44/188], loss=10.1403
	step [45/188], loss=9.5717
	step [46/188], loss=11.3083
	step [47/188], loss=10.5172
	step [48/188], loss=9.9395
	step [49/188], loss=8.4366
	step [50/188], loss=9.0158
	step [51/188], loss=9.1784
	step [52/188], loss=11.4394
	step [53/188], loss=10.3864
	step [54/188], loss=10.7924
	step [55/188], loss=8.8599
	step [56/188], loss=9.3140
	step [57/188], loss=8.6714
	step [58/188], loss=10.7472
	step [59/188], loss=9.6461
	step [60/188], loss=7.1685
	step [61/188], loss=9.1425
	step [62/188], loss=7.9949
	step [63/188], loss=12.3263
	step [64/188], loss=11.0481
	step [65/188], loss=9.1754
	step [66/188], loss=11.7963
	step [67/188], loss=9.5448
	step [68/188], loss=9.1811
	step [69/188], loss=11.9993
	step [70/188], loss=10.2685
	step [71/188], loss=9.7241
	step [72/188], loss=9.5643
	step [73/188], loss=10.1944
	step [74/188], loss=9.3036
	step [75/188], loss=9.2603
	step [76/188], loss=7.3999
	step [77/188], loss=9.4249
	step [78/188], loss=8.6502
	step [79/188], loss=12.2731
	step [80/188], loss=8.8274
	step [81/188], loss=9.9836
	step [82/188], loss=12.0864
	step [83/188], loss=10.3577
	step [84/188], loss=10.4459
	step [85/188], loss=11.2403
	step [86/188], loss=10.6033
	step [87/188], loss=8.6583
	step [88/188], loss=10.1711
	step [89/188], loss=8.8359
	step [90/188], loss=11.1962
	step [91/188], loss=10.5621
	step [92/188], loss=9.9125
	step [93/188], loss=8.5809
	step [94/188], loss=8.7179
	step [95/188], loss=9.2028
	step [96/188], loss=8.5753
	step [97/188], loss=9.8194
	step [98/188], loss=11.0867
	step [99/188], loss=10.4159
	step [100/188], loss=8.4400
	step [101/188], loss=10.7430
	step [102/188], loss=9.8832
	step [103/188], loss=10.9983
	step [104/188], loss=11.5066
	step [105/188], loss=8.9495
	step [106/188], loss=10.1803
	step [107/188], loss=8.6675
	step [108/188], loss=8.2591
	step [109/188], loss=8.7920
	step [110/188], loss=10.4272
	step [111/188], loss=10.7013
	step [112/188], loss=8.0441
	step [113/188], loss=10.6139
	step [114/188], loss=9.9079
	step [115/188], loss=9.2740
	step [116/188], loss=9.9858
	step [117/188], loss=9.8893
	step [118/188], loss=9.2089
	step [119/188], loss=8.9666
	step [120/188], loss=11.0108
	step [121/188], loss=8.9355
	step [122/188], loss=8.8341
	step [123/188], loss=9.1195
	step [124/188], loss=10.3810
	step [125/188], loss=10.6729
	step [126/188], loss=11.9219
	step [127/188], loss=11.6507
	step [128/188], loss=10.8171
	step [129/188], loss=9.9526
	step [130/188], loss=8.3495
	step [131/188], loss=10.7278
	step [132/188], loss=9.0142
	step [133/188], loss=10.8901
	step [134/188], loss=9.5379
	step [135/188], loss=10.8666
	step [136/188], loss=8.2363
	step [137/188], loss=8.0833
	step [138/188], loss=11.0955
	step [139/188], loss=9.4455
	step [140/188], loss=9.4753
	step [141/188], loss=12.2032
	step [142/188], loss=10.6544
	step [143/188], loss=11.0962
	step [144/188], loss=10.5627
	step [145/188], loss=9.9546
	step [146/188], loss=10.1206
	step [147/188], loss=10.8764
	step [148/188], loss=9.6771
	step [149/188], loss=8.4880
	step [150/188], loss=9.8911
	step [151/188], loss=10.2545
	step [152/188], loss=11.4876
	step [153/188], loss=11.4481
	step [154/188], loss=9.2030
	step [155/188], loss=7.3510
	step [156/188], loss=8.9297
	step [157/188], loss=9.1696
	step [158/188], loss=9.5521
	step [159/188], loss=9.1217
	step [160/188], loss=12.2999
	step [161/188], loss=9.8567
	step [162/188], loss=10.8710
	step [163/188], loss=9.9787
	step [164/188], loss=8.0392
	step [165/188], loss=9.6683
	step [166/188], loss=9.7778
	step [167/188], loss=9.3966
	step [168/188], loss=9.1529
	step [169/188], loss=9.2106
	step [170/188], loss=10.3637
	step [171/188], loss=8.0766
	step [172/188], loss=10.7226
	step [173/188], loss=8.7630
	step [174/188], loss=8.3123
	step [175/188], loss=8.4087
	step [176/188], loss=10.6559
	step [177/188], loss=10.0565
	step [178/188], loss=10.0249
	step [179/188], loss=9.4022
	step [180/188], loss=9.8943
	step [181/188], loss=10.5137
	step [182/188], loss=7.9780
	step [183/188], loss=9.4634
	step [184/188], loss=9.1176
	step [185/188], loss=8.7792
	step [186/188], loss=10.3672
	step [187/188], loss=9.3817
	step [188/188], loss=6.4190
	Evaluating
	loss=0.0261, precision=0.2075, recall=0.9964, f1=0.3435
Training epoch 21
	step [1/188], loss=9.4549
	step [2/188], loss=9.7836
	step [3/188], loss=9.2448
	step [4/188], loss=8.4768
	step [5/188], loss=9.5768
	step [6/188], loss=10.7188
	step [7/188], loss=9.3838
	step [8/188], loss=8.2309
	step [9/188], loss=9.0033
	step [10/188], loss=10.1174
	step [11/188], loss=9.0508
	step [12/188], loss=8.2347
	step [13/188], loss=8.5087
	step [14/188], loss=10.9438
	step [15/188], loss=11.4223
	step [16/188], loss=10.2961
	step [17/188], loss=10.3762
	step [18/188], loss=10.7078
	step [19/188], loss=9.2165
	step [20/188], loss=9.3552
	step [21/188], loss=8.8654
	step [22/188], loss=10.4671
	step [23/188], loss=9.3801
	step [24/188], loss=9.7637
	step [25/188], loss=7.8923
	step [26/188], loss=9.9546
	step [27/188], loss=7.1353
	step [28/188], loss=7.1221
	step [29/188], loss=7.0966
	step [30/188], loss=8.0707
	step [31/188], loss=7.8338
	step [32/188], loss=8.7603
	step [33/188], loss=11.0523
	step [34/188], loss=8.4559
	step [35/188], loss=8.4709
	step [36/188], loss=10.4564
	step [37/188], loss=10.4785
	step [38/188], loss=9.1803
	step [39/188], loss=9.8287
	step [40/188], loss=9.8932
	step [41/188], loss=8.4341
	step [42/188], loss=9.9647
	step [43/188], loss=9.5560
	step [44/188], loss=10.6898
	step [45/188], loss=10.1385
	step [46/188], loss=11.2765
	step [47/188], loss=10.9780
	step [48/188], loss=8.4560
	step [49/188], loss=8.3957
	step [50/188], loss=10.6336
	step [51/188], loss=11.6379
	step [52/188], loss=9.3871
	step [53/188], loss=8.1907
	step [54/188], loss=11.0763
	step [55/188], loss=9.9888
	step [56/188], loss=8.8274
	step [57/188], loss=10.6954
	step [58/188], loss=9.9240
	step [59/188], loss=8.3869
	step [60/188], loss=10.7390
	step [61/188], loss=11.7730
	step [62/188], loss=9.3000
	step [63/188], loss=8.8594
	step [64/188], loss=9.0317
	step [65/188], loss=9.2246
	step [66/188], loss=9.0854
	step [67/188], loss=9.4094
	step [68/188], loss=9.0948
	step [69/188], loss=10.8742
	step [70/188], loss=13.7604
	step [71/188], loss=11.6590
	step [72/188], loss=9.2949
	step [73/188], loss=9.3860
	step [74/188], loss=9.4667
	step [75/188], loss=9.8080
	step [76/188], loss=8.3854
	step [77/188], loss=11.1564
	step [78/188], loss=8.8999
	step [79/188], loss=8.9256
	step [80/188], loss=9.1553
	step [81/188], loss=8.5925
	step [82/188], loss=8.8764
	step [83/188], loss=11.5282
	step [84/188], loss=10.2004
	step [85/188], loss=9.3999
	step [86/188], loss=8.0612
	step [87/188], loss=10.8114
	step [88/188], loss=9.7714
	step [89/188], loss=11.1109
	step [90/188], loss=8.5512
	step [91/188], loss=10.2732
	step [92/188], loss=7.6016
	step [93/188], loss=9.8773
	step [94/188], loss=8.9101
	step [95/188], loss=9.1983
	step [96/188], loss=8.8621
	step [97/188], loss=12.0456
	step [98/188], loss=7.6963
	step [99/188], loss=10.9890
	step [100/188], loss=9.2353
	step [101/188], loss=9.7189
	step [102/188], loss=8.6230
	step [103/188], loss=10.3991
	step [104/188], loss=11.3023
	step [105/188], loss=10.5029
	step [106/188], loss=9.8320
	step [107/188], loss=9.1700
	step [108/188], loss=9.9838
	step [109/188], loss=9.3399
	step [110/188], loss=7.8617
	step [111/188], loss=8.0977
	step [112/188], loss=11.2964
	step [113/188], loss=9.2556
	step [114/188], loss=10.2683
	step [115/188], loss=11.1237
	step [116/188], loss=12.6907
	step [117/188], loss=8.9748
	step [118/188], loss=10.1170
	step [119/188], loss=10.2785
	step [120/188], loss=8.1084
	step [121/188], loss=9.3657
	step [122/188], loss=9.7099
	step [123/188], loss=7.5259
	step [124/188], loss=9.4994
	step [125/188], loss=10.7033
	step [126/188], loss=8.8228
	step [127/188], loss=9.6695
	step [128/188], loss=9.5835
	step [129/188], loss=9.6310
	step [130/188], loss=9.4965
	step [131/188], loss=10.6412
	step [132/188], loss=8.9845
	step [133/188], loss=7.5130
	step [134/188], loss=9.0166
	step [135/188], loss=8.5691
	step [136/188], loss=10.1691
	step [137/188], loss=10.9725
	step [138/188], loss=10.9933
	step [139/188], loss=9.0837
	step [140/188], loss=10.5754
	step [141/188], loss=8.1675
	step [142/188], loss=9.4965
	step [143/188], loss=10.0909
	step [144/188], loss=9.2792
	step [145/188], loss=10.1593
	step [146/188], loss=8.7829
	step [147/188], loss=8.5739
	step [148/188], loss=9.5632
	step [149/188], loss=7.1776
	step [150/188], loss=8.2864
	step [151/188], loss=9.7373
	step [152/188], loss=9.1745
	step [153/188], loss=8.8316
	step [154/188], loss=8.9573
	step [155/188], loss=9.7658
	step [156/188], loss=6.9436
	step [157/188], loss=12.9229
	step [158/188], loss=9.4587
	step [159/188], loss=9.1256
	step [160/188], loss=9.9558
	step [161/188], loss=9.6246
	step [162/188], loss=7.4863
	step [163/188], loss=8.6102
	step [164/188], loss=8.4328
	step [165/188], loss=7.7808
	step [166/188], loss=9.1800
	step [167/188], loss=11.8770
	step [168/188], loss=7.8493
	step [169/188], loss=7.5724
	step [170/188], loss=8.8018
	step [171/188], loss=8.2435
	step [172/188], loss=8.7095
	step [173/188], loss=10.6533
	step [174/188], loss=10.5421
	step [175/188], loss=12.6739
	step [176/188], loss=8.6116
	step [177/188], loss=10.2580
	step [178/188], loss=9.2569
	step [179/188], loss=10.6640
	step [180/188], loss=8.5312
	step [181/188], loss=8.4048
	step [182/188], loss=11.6867
	step [183/188], loss=10.8304
	step [184/188], loss=7.4596
	step [185/188], loss=10.5142
	step [186/188], loss=9.0977
	step [187/188], loss=9.2967
	step [188/188], loss=4.4267
	Evaluating
	loss=0.0303, precision=0.1761, recall=0.9975, f1=0.2993
Training epoch 22
	step [1/188], loss=9.3480
	step [2/188], loss=10.5749
	step [3/188], loss=9.5741
	step [4/188], loss=9.0050
	step [5/188], loss=10.4111
	step [6/188], loss=7.9420
	step [7/188], loss=7.7595
	step [8/188], loss=8.4490
	step [9/188], loss=8.1469
	step [10/188], loss=8.7226
	step [11/188], loss=9.3691
	step [12/188], loss=11.6007
	step [13/188], loss=10.1228
	step [14/188], loss=9.1033
	step [15/188], loss=9.7655
	step [16/188], loss=9.7680
	step [17/188], loss=9.2434
	step [18/188], loss=10.0074
	step [19/188], loss=7.8764
	step [20/188], loss=9.4098
	step [21/188], loss=11.6606
	step [22/188], loss=9.2059
	step [23/188], loss=12.3302
	step [24/188], loss=10.4246
	step [25/188], loss=7.6668
	step [26/188], loss=9.5480
	step [27/188], loss=7.5186
	step [28/188], loss=9.2283
	step [29/188], loss=8.6542
	step [30/188], loss=8.6599
	step [31/188], loss=10.3082
	step [32/188], loss=10.2158
	step [33/188], loss=9.8826
	step [34/188], loss=11.1700
	step [35/188], loss=10.4702
	step [36/188], loss=8.3672
	step [37/188], loss=10.3699
	step [38/188], loss=9.4996
	step [39/188], loss=9.8463
	step [40/188], loss=9.8795
	step [41/188], loss=9.9182
	step [42/188], loss=9.4939
	step [43/188], loss=6.8887
	step [44/188], loss=8.2279
	step [45/188], loss=9.5405
	step [46/188], loss=9.1582
	step [47/188], loss=8.1062
	step [48/188], loss=10.3701
	step [49/188], loss=9.7689
	step [50/188], loss=9.4112
	step [51/188], loss=7.4899
	step [52/188], loss=8.9815
	step [53/188], loss=7.9516
	step [54/188], loss=8.0741
	step [55/188], loss=9.8357
	step [56/188], loss=9.0207
	step [57/188], loss=8.4974
	step [58/188], loss=10.4867
	step [59/188], loss=9.7267
	step [60/188], loss=8.7028
	step [61/188], loss=11.8250
	step [62/188], loss=10.2133
	step [63/188], loss=9.0587
	step [64/188], loss=8.1705
	step [65/188], loss=9.2277
	step [66/188], loss=9.8325
	step [67/188], loss=7.5589
	step [68/188], loss=9.2193
	step [69/188], loss=11.6441
	step [70/188], loss=10.8599
	step [71/188], loss=8.5019
	step [72/188], loss=10.4276
	step [73/188], loss=8.5612
	step [74/188], loss=10.5356
	step [75/188], loss=10.3015
	step [76/188], loss=8.8016
	step [77/188], loss=9.2923
	step [78/188], loss=7.9116
	step [79/188], loss=10.8459
	step [80/188], loss=8.9778
	step [81/188], loss=9.4418
	step [82/188], loss=8.7705
	step [83/188], loss=10.7644
	step [84/188], loss=9.1248
	step [85/188], loss=8.9669
	step [86/188], loss=9.7973
	step [87/188], loss=10.0904
	step [88/188], loss=9.4415
	step [89/188], loss=8.9879
	step [90/188], loss=9.3095
	step [91/188], loss=10.3577
	step [92/188], loss=7.6011
	step [93/188], loss=10.5747
	step [94/188], loss=7.8944
	step [95/188], loss=9.0198
	step [96/188], loss=8.6759
	step [97/188], loss=8.1631
	step [98/188], loss=8.7248
	step [99/188], loss=9.2134
	step [100/188], loss=10.0046
	step [101/188], loss=11.9373
	step [102/188], loss=8.7385
	step [103/188], loss=8.8424
	step [104/188], loss=7.1888
	step [105/188], loss=8.4861
	step [106/188], loss=6.9595
	step [107/188], loss=9.5206
	step [108/188], loss=10.0509
	step [109/188], loss=10.6173
	step [110/188], loss=9.7240
	step [111/188], loss=8.6066
	step [112/188], loss=8.3766
	step [113/188], loss=9.8960
	step [114/188], loss=8.9798
	step [115/188], loss=9.2690
	step [116/188], loss=9.7265
	step [117/188], loss=9.2382
	step [118/188], loss=8.7204
	step [119/188], loss=8.6266
	step [120/188], loss=9.0055
	step [121/188], loss=7.7205
	step [122/188], loss=10.4325
	step [123/188], loss=8.5797
	step [124/188], loss=9.6951
	step [125/188], loss=9.1438
	step [126/188], loss=9.0814
	step [127/188], loss=9.0269
	step [128/188], loss=9.1702
	step [129/188], loss=9.1059
	step [130/188], loss=8.7509
	step [131/188], loss=8.9309
	step [132/188], loss=13.3311
	step [133/188], loss=9.5812
	step [134/188], loss=9.9498
	step [135/188], loss=8.1619
	step [136/188], loss=8.6408
	step [137/188], loss=9.4692
	step [138/188], loss=8.0546
	step [139/188], loss=9.4960
	step [140/188], loss=11.3237
	step [141/188], loss=9.1724
	step [142/188], loss=9.7169
	step [143/188], loss=9.5070
	step [144/188], loss=8.6859
	step [145/188], loss=10.3474
	step [146/188], loss=8.5135
	step [147/188], loss=9.0005
	step [148/188], loss=8.7941
	step [149/188], loss=9.5951
	step [150/188], loss=9.2376
	step [151/188], loss=8.8277
	step [152/188], loss=12.3518
	step [153/188], loss=9.2540
	step [154/188], loss=11.1329
	step [155/188], loss=8.3803
	step [156/188], loss=8.0521
	step [157/188], loss=8.5340
	step [158/188], loss=8.3621
	step [159/188], loss=9.6136
	step [160/188], loss=8.7144
	step [161/188], loss=8.9430
	step [162/188], loss=10.3882
	step [163/188], loss=10.2503
	step [164/188], loss=7.8466
	step [165/188], loss=8.9304
	step [166/188], loss=9.5344
	step [167/188], loss=11.1011
	step [168/188], loss=10.5542
	step [169/188], loss=10.2325
	step [170/188], loss=9.1003
	step [171/188], loss=8.5578
	step [172/188], loss=8.1968
	step [173/188], loss=9.6719
	step [174/188], loss=7.2173
	step [175/188], loss=10.3393
	step [176/188], loss=10.0156
	step [177/188], loss=8.8974
	step [178/188], loss=9.4739
	step [179/188], loss=10.3772
	step [180/188], loss=9.1155
	step [181/188], loss=10.5087
	step [182/188], loss=9.1317
	step [183/188], loss=9.3533
	step [184/188], loss=9.1536
	step [185/188], loss=7.7225
	step [186/188], loss=8.7343
	step [187/188], loss=10.2902
	step [188/188], loss=7.0698
	Evaluating
	loss=0.0259, precision=0.1967, recall=0.9968, f1=0.3286
Training epoch 23
	step [1/188], loss=9.3592
	step [2/188], loss=10.0298
	step [3/188], loss=9.2820
	step [4/188], loss=8.8437
	step [5/188], loss=8.4237
	step [6/188], loss=9.0140
	step [7/188], loss=8.6496
	step [8/188], loss=8.5575
	step [9/188], loss=8.9389
	step [10/188], loss=10.0538
	step [11/188], loss=8.4639
	step [12/188], loss=9.4551
	step [13/188], loss=8.5529
	step [14/188], loss=9.9137
	step [15/188], loss=7.6298
	step [16/188], loss=8.7806
	step [17/188], loss=6.9402
	step [18/188], loss=9.4988
	step [19/188], loss=7.4351
	step [20/188], loss=8.2399
	step [21/188], loss=7.7611
	step [22/188], loss=7.9972
	step [23/188], loss=9.4278
	step [24/188], loss=9.0844
	step [25/188], loss=10.0489
	step [26/188], loss=9.3359
	step [27/188], loss=8.8058
	step [28/188], loss=9.2640
	step [29/188], loss=8.0043
	step [30/188], loss=8.4500
	step [31/188], loss=9.0824
	step [32/188], loss=9.2803
	step [33/188], loss=10.1373
	step [34/188], loss=7.4470
	step [35/188], loss=9.0241
	step [36/188], loss=8.4029
	step [37/188], loss=7.6712
	step [38/188], loss=8.8359
	step [39/188], loss=9.3089
	step [40/188], loss=10.1445
	step [41/188], loss=10.8846
	step [42/188], loss=8.8857
	step [43/188], loss=9.8075
	step [44/188], loss=8.3932
	step [45/188], loss=10.6135
	step [46/188], loss=8.6372
	step [47/188], loss=9.1394
	step [48/188], loss=9.5574
	step [49/188], loss=9.6786
	step [50/188], loss=11.4730
	step [51/188], loss=9.3939
	step [52/188], loss=7.6307
	step [53/188], loss=8.0097
	step [54/188], loss=9.4540
	step [55/188], loss=10.0067
	step [56/188], loss=8.2009
	step [57/188], loss=9.0013
	step [58/188], loss=8.8520
	step [59/188], loss=8.2444
	step [60/188], loss=9.8687
	step [61/188], loss=9.3330
	step [62/188], loss=9.5776
	step [63/188], loss=9.8124
	step [64/188], loss=9.9249
	step [65/188], loss=10.2104
	step [66/188], loss=9.3523
	step [67/188], loss=8.3462
	step [68/188], loss=8.2627
	step [69/188], loss=8.4405
	step [70/188], loss=7.9281
	step [71/188], loss=7.9537
	step [72/188], loss=8.5794
	step [73/188], loss=8.6873
	step [74/188], loss=7.3088
	step [75/188], loss=9.4565
	step [76/188], loss=9.0735
	step [77/188], loss=7.4883
	step [78/188], loss=8.5595
	step [79/188], loss=9.3761
	step [80/188], loss=8.9510
	step [81/188], loss=7.9490
	step [82/188], loss=9.1464
	step [83/188], loss=7.8747
	step [84/188], loss=8.0480
	step [85/188], loss=11.0843
	step [86/188], loss=10.8021
	step [87/188], loss=8.6772
	step [88/188], loss=9.3615
	step [89/188], loss=8.7717
	step [90/188], loss=7.4841
	step [91/188], loss=9.7151
	step [92/188], loss=7.2668
	step [93/188], loss=9.8352
	step [94/188], loss=8.9629
	step [95/188], loss=10.0252
	step [96/188], loss=9.8485
	step [97/188], loss=7.7780
	step [98/188], loss=9.6379
	step [99/188], loss=9.6555
	step [100/188], loss=9.0116
	step [101/188], loss=10.9453
	step [102/188], loss=7.5478
	step [103/188], loss=9.0267
	step [104/188], loss=7.9941
	step [105/188], loss=9.1103
	step [106/188], loss=9.7976
	step [107/188], loss=9.2681
	step [108/188], loss=8.6407
	step [109/188], loss=7.4851
	step [110/188], loss=9.0966
	step [111/188], loss=8.5052
	step [112/188], loss=10.2476
	step [113/188], loss=8.0633
	step [114/188], loss=9.4278
	step [115/188], loss=7.9445
	step [116/188], loss=8.0765
	step [117/188], loss=9.6196
	step [118/188], loss=10.1421
	step [119/188], loss=10.1761
	step [120/188], loss=8.6110
	step [121/188], loss=10.1037
	step [122/188], loss=9.4722
	step [123/188], loss=7.9593
	step [124/188], loss=12.7081
	step [125/188], loss=8.9867
	step [126/188], loss=10.9615
	step [127/188], loss=8.3663
	step [128/188], loss=8.6179
	step [129/188], loss=9.9689
	step [130/188], loss=8.5750
	step [131/188], loss=8.8264
	step [132/188], loss=10.2537
	step [133/188], loss=8.7018
	step [134/188], loss=8.0320
	step [135/188], loss=9.0264
	step [136/188], loss=9.4761
	step [137/188], loss=10.7666
	step [138/188], loss=7.9506
	step [139/188], loss=10.6320
	step [140/188], loss=9.5739
	step [141/188], loss=7.2551
	step [142/188], loss=7.7452
	step [143/188], loss=9.7887
	step [144/188], loss=8.0453
	step [145/188], loss=7.4050
	step [146/188], loss=10.7533
	step [147/188], loss=11.3408
	step [148/188], loss=10.3088
	step [149/188], loss=12.1365
	step [150/188], loss=8.2746
	step [151/188], loss=8.6507
	step [152/188], loss=8.3563
	step [153/188], loss=7.9998
	step [154/188], loss=8.9762
	step [155/188], loss=10.7296
	step [156/188], loss=11.6017
	step [157/188], loss=9.0501
	step [158/188], loss=9.0278
	step [159/188], loss=10.4732
	step [160/188], loss=8.0029
	step [161/188], loss=11.6795
	step [162/188], loss=9.7489
	step [163/188], loss=7.3003
	step [164/188], loss=9.1491
	step [165/188], loss=10.4836
	step [166/188], loss=9.7857
	step [167/188], loss=10.7813
	step [168/188], loss=8.8327
	step [169/188], loss=10.7183
	step [170/188], loss=10.0542
	step [171/188], loss=9.8056
	step [172/188], loss=11.0016
	step [173/188], loss=9.1975
	step [174/188], loss=9.3267
	step [175/188], loss=7.8085
	step [176/188], loss=9.0933
	step [177/188], loss=10.0827
	step [178/188], loss=7.2413
	step [179/188], loss=10.3498
	step [180/188], loss=7.5504
	step [181/188], loss=10.4901
	step [182/188], loss=7.6534
	step [183/188], loss=10.1544
	step [184/188], loss=10.4007
	step [185/188], loss=8.5377
	step [186/188], loss=6.6670
	step [187/188], loss=7.0846
	step [188/188], loss=6.1759
	Evaluating
	loss=0.0286, precision=0.1812, recall=0.9972, f1=0.3066
Training epoch 24
	step [1/188], loss=9.3561
	step [2/188], loss=9.3897
	step [3/188], loss=8.4228
	step [4/188], loss=11.0994
	step [5/188], loss=8.3201
	step [6/188], loss=7.0553
	step [7/188], loss=9.5717
	step [8/188], loss=7.8126
	step [9/188], loss=8.2622
	step [10/188], loss=7.0612
	step [11/188], loss=7.9516
	step [12/188], loss=9.2980
	step [13/188], loss=8.6279
	step [14/188], loss=7.9648
	step [15/188], loss=7.3598
	step [16/188], loss=7.6735
	step [17/188], loss=7.4973
	step [18/188], loss=9.4670
	step [19/188], loss=11.4572
	step [20/188], loss=8.2112
	step [21/188], loss=8.2662
	step [22/188], loss=8.9840
	step [23/188], loss=9.2523
	step [24/188], loss=8.1387
	step [25/188], loss=9.2224
	step [26/188], loss=8.2116
	step [27/188], loss=7.5399
	step [28/188], loss=8.4231
	step [29/188], loss=9.7781
	step [30/188], loss=8.9775
	step [31/188], loss=7.8111
	step [32/188], loss=7.1546
	step [33/188], loss=10.3494
	step [34/188], loss=9.4961
	step [35/188], loss=10.1323
	step [36/188], loss=9.3544
	step [37/188], loss=9.2577
	step [38/188], loss=8.8501
	step [39/188], loss=7.2273
	step [40/188], loss=8.9669
	step [41/188], loss=9.2930
	step [42/188], loss=9.5860
	step [43/188], loss=9.5109
	step [44/188], loss=8.2062
	step [45/188], loss=7.7211
	step [46/188], loss=9.0909
	step [47/188], loss=9.4785
	step [48/188], loss=7.6894
	step [49/188], loss=8.8621
	step [50/188], loss=9.9366
	step [51/188], loss=9.4866
	step [52/188], loss=8.6767
	step [53/188], loss=9.2400
	step [54/188], loss=7.1726
	step [55/188], loss=8.4488
	step [56/188], loss=7.7101
	step [57/188], loss=8.2463
	step [58/188], loss=9.7372
	step [59/188], loss=8.7950
	step [60/188], loss=9.9861
	step [61/188], loss=8.7102
	step [62/188], loss=8.0982
	step [63/188], loss=9.5609
	step [64/188], loss=11.1354
	step [65/188], loss=7.9357
	step [66/188], loss=8.3498
	step [67/188], loss=8.9152
	step [68/188], loss=7.7376
	step [69/188], loss=10.2982
	step [70/188], loss=8.1604
	step [71/188], loss=8.7602
	step [72/188], loss=7.7896
	step [73/188], loss=8.2062
	step [74/188], loss=7.6305
	step [75/188], loss=9.0864
	step [76/188], loss=8.7924
	step [77/188], loss=8.5296
	step [78/188], loss=7.7106
	step [79/188], loss=12.0774
	step [80/188], loss=8.9156
	step [81/188], loss=10.5863
	step [82/188], loss=8.5329
	step [83/188], loss=10.6402
	step [84/188], loss=8.8912
	step [85/188], loss=10.2925
	step [86/188], loss=9.7874
	step [87/188], loss=7.7720
	step [88/188], loss=8.1709
	step [89/188], loss=7.8938
	step [90/188], loss=9.6815
	step [91/188], loss=8.9977
	step [92/188], loss=8.6688
	step [93/188], loss=7.3819
	step [94/188], loss=7.9361
	step [95/188], loss=9.6206
	step [96/188], loss=11.0818
	step [97/188], loss=10.1895
	step [98/188], loss=7.8862
	step [99/188], loss=7.1650
	step [100/188], loss=9.7586
	step [101/188], loss=9.2436
	step [102/188], loss=7.6217
	step [103/188], loss=9.6838
	step [104/188], loss=7.7216
	step [105/188], loss=9.2215
	step [106/188], loss=9.3307
	step [107/188], loss=9.5591
	step [108/188], loss=8.1712
	step [109/188], loss=9.3516
	step [110/188], loss=7.7094
	step [111/188], loss=9.9046
	step [112/188], loss=10.5634
	step [113/188], loss=9.0570
	step [114/188], loss=9.4701
	step [115/188], loss=7.1821
	step [116/188], loss=8.9458
	step [117/188], loss=9.0581
	step [118/188], loss=9.9455
	step [119/188], loss=8.9134
	step [120/188], loss=8.4767
	step [121/188], loss=7.7097
	step [122/188], loss=9.7204
	step [123/188], loss=8.4796
	step [124/188], loss=8.9614
	step [125/188], loss=8.6930
	step [126/188], loss=9.6464
	step [127/188], loss=8.6922
	step [128/188], loss=8.9769
	step [129/188], loss=8.7162
	step [130/188], loss=11.2273
	step [131/188], loss=7.9275
	step [132/188], loss=9.3713
	step [133/188], loss=9.6769
	step [134/188], loss=8.0279
	step [135/188], loss=8.4701
	step [136/188], loss=8.7519
	step [137/188], loss=6.9499
	step [138/188], loss=8.8961
	step [139/188], loss=9.9293
	step [140/188], loss=8.9794
	step [141/188], loss=8.1985
	step [142/188], loss=9.6849
	step [143/188], loss=9.6811
	step [144/188], loss=8.8906
	step [145/188], loss=10.4975
	step [146/188], loss=9.9997
	step [147/188], loss=9.4926
	step [148/188], loss=8.3418
	step [149/188], loss=8.3309
	step [150/188], loss=7.2732
	step [151/188], loss=9.1413
	step [152/188], loss=9.0485
	step [153/188], loss=8.8670
	step [154/188], loss=10.7840
	step [155/188], loss=8.4418
	step [156/188], loss=10.2686
	step [157/188], loss=9.3233
	step [158/188], loss=9.4003
	step [159/188], loss=10.0199
	step [160/188], loss=9.3467
	step [161/188], loss=11.2442
	step [162/188], loss=9.8797
	step [163/188], loss=9.7953
	step [164/188], loss=9.7798
	step [165/188], loss=8.5328
	step [166/188], loss=10.7814
	step [167/188], loss=8.9425
	step [168/188], loss=7.4336
	step [169/188], loss=8.1759
	step [170/188], loss=9.8510
	step [171/188], loss=7.2915
	step [172/188], loss=8.8051
	step [173/188], loss=8.2903
	step [174/188], loss=7.9912
	step [175/188], loss=6.7436
	step [176/188], loss=7.0493
	step [177/188], loss=8.8654
	step [178/188], loss=8.8245
	step [179/188], loss=7.4127
	step [180/188], loss=12.1084
	step [181/188], loss=6.6575
	step [182/188], loss=8.1937
	step [183/188], loss=11.4384
	step [184/188], loss=9.5506
	step [185/188], loss=10.2852
	step [186/188], loss=11.0930
	step [187/188], loss=10.1452
	step [188/188], loss=5.1525
	Evaluating
	loss=0.0253, precision=0.1911, recall=0.9966, f1=0.3207
Training epoch 25
	step [1/188], loss=7.6818
	step [2/188], loss=7.7723
	step [3/188], loss=8.0468
	step [4/188], loss=9.0593
	step [5/188], loss=9.1803
	step [6/188], loss=7.8871
	step [7/188], loss=8.1148
	step [8/188], loss=7.5969
	step [9/188], loss=11.0828
	step [10/188], loss=8.7368
	step [11/188], loss=8.8569
	step [12/188], loss=7.9237
	step [13/188], loss=9.8030
	step [14/188], loss=8.5092
	step [15/188], loss=10.6475
	step [16/188], loss=7.9558
	step [17/188], loss=6.6591
	step [18/188], loss=7.7741
	step [19/188], loss=7.5669
	step [20/188], loss=9.9966
	step [21/188], loss=6.9861
	step [22/188], loss=7.3008
	step [23/188], loss=7.1247
	step [24/188], loss=8.9922
	step [25/188], loss=11.9585
	step [26/188], loss=8.2414
	step [27/188], loss=10.1162
	step [28/188], loss=8.1755
	step [29/188], loss=8.8655
	step [30/188], loss=9.4760
	step [31/188], loss=9.3596
	step [32/188], loss=7.4057
	step [33/188], loss=8.4131
	step [34/188], loss=8.7896
	step [35/188], loss=8.9696
	step [36/188], loss=10.8711
	step [37/188], loss=7.5537
	step [38/188], loss=8.2868
	step [39/188], loss=7.8039
	step [40/188], loss=8.4863
	step [41/188], loss=8.3370
	step [42/188], loss=10.1641
	step [43/188], loss=8.2780
	step [44/188], loss=8.3977
	step [45/188], loss=10.1264
	step [46/188], loss=9.7032
	step [47/188], loss=9.4518
	step [48/188], loss=8.2012
	step [49/188], loss=9.6651
	step [50/188], loss=10.6822
	step [51/188], loss=10.1140
	step [52/188], loss=8.3655
	step [53/188], loss=9.8768
	step [54/188], loss=9.7947
	step [55/188], loss=8.0189
	step [56/188], loss=8.3650
	step [57/188], loss=7.6274
	step [58/188], loss=12.0285
	step [59/188], loss=9.6215
	step [60/188], loss=9.7278
	step [61/188], loss=7.4914
	step [62/188], loss=7.8639
	step [63/188], loss=9.3433
	step [64/188], loss=8.7490
	step [65/188], loss=8.6667
	step [66/188], loss=8.3380
	step [67/188], loss=9.6380
	step [68/188], loss=8.4765
	step [69/188], loss=10.0331
	step [70/188], loss=8.4702
	step [71/188], loss=8.1428
	step [72/188], loss=9.1368
	step [73/188], loss=8.3851
	step [74/188], loss=7.1873
	step [75/188], loss=8.6341
	step [76/188], loss=8.0717
	step [77/188], loss=9.5349
	step [78/188], loss=8.8928
	step [79/188], loss=8.2950
	step [80/188], loss=9.6924
	step [81/188], loss=7.0514
	step [82/188], loss=7.3536
	step [83/188], loss=9.8670
	step [84/188], loss=8.8800
	step [85/188], loss=8.0552
	step [86/188], loss=8.2673
	step [87/188], loss=10.1423
	step [88/188], loss=7.4585
	step [89/188], loss=9.6365
	step [90/188], loss=8.5919
	step [91/188], loss=7.8490
	step [92/188], loss=9.3828
	step [93/188], loss=9.2211
	step [94/188], loss=8.3854
	step [95/188], loss=8.2586
	step [96/188], loss=7.3087
	step [97/188], loss=7.4770
	step [98/188], loss=7.9575
	step [99/188], loss=10.3234
	step [100/188], loss=6.7265
	step [101/188], loss=10.5971
	step [102/188], loss=8.3984
	step [103/188], loss=8.8270
	step [104/188], loss=10.6677
	step [105/188], loss=8.7733
	step [106/188], loss=8.6543
	step [107/188], loss=7.7732
	step [108/188], loss=8.4098
	step [109/188], loss=10.0438
	step [110/188], loss=7.8540
	step [111/188], loss=6.5613
	step [112/188], loss=8.3074
	step [113/188], loss=8.3962
	step [114/188], loss=8.1532
	step [115/188], loss=8.3444
	step [116/188], loss=6.7329
	step [117/188], loss=9.4876
	step [118/188], loss=12.0317
	step [119/188], loss=7.9215
	step [120/188], loss=7.3637
	step [121/188], loss=9.7422
	step [122/188], loss=9.5267
	step [123/188], loss=7.5109
	step [124/188], loss=9.6322
	step [125/188], loss=7.9065
	step [126/188], loss=7.0007
	step [127/188], loss=8.7425
	step [128/188], loss=9.9343
	step [129/188], loss=6.8754
	step [130/188], loss=8.8456
	step [131/188], loss=10.1361
	step [132/188], loss=9.5603
	step [133/188], loss=8.8869
	step [134/188], loss=7.8794
	step [135/188], loss=8.3771
	step [136/188], loss=8.0867
	step [137/188], loss=6.7529
	step [138/188], loss=9.0922
	step [139/188], loss=7.8069
	step [140/188], loss=10.2754
	step [141/188], loss=9.5264
	step [142/188], loss=9.4772
	step [143/188], loss=8.9632
	step [144/188], loss=7.9528
	step [145/188], loss=7.7670
	step [146/188], loss=10.1257
	step [147/188], loss=7.3331
	step [148/188], loss=8.6383
	step [149/188], loss=9.0007
	step [150/188], loss=8.1400
	step [151/188], loss=8.2484
	step [152/188], loss=8.3527
	step [153/188], loss=9.0284
	step [154/188], loss=8.2295
	step [155/188], loss=9.0531
	step [156/188], loss=7.6836
	step [157/188], loss=8.1342
	step [158/188], loss=7.1774
	step [159/188], loss=7.2954
	step [160/188], loss=8.5483
	step [161/188], loss=8.2320
	step [162/188], loss=9.4477
	step [163/188], loss=9.1626
	step [164/188], loss=8.8249
	step [165/188], loss=9.5303
	step [166/188], loss=9.1644
	step [167/188], loss=8.8300
	step [168/188], loss=7.8640
	step [169/188], loss=8.3596
	step [170/188], loss=10.7937
	step [171/188], loss=7.6890
	step [172/188], loss=10.2008
	step [173/188], loss=8.5062
	step [174/188], loss=6.9689
	step [175/188], loss=8.6807
	step [176/188], loss=9.0758
	step [177/188], loss=11.5178
	step [178/188], loss=10.5627
	step [179/188], loss=7.7338
	step [180/188], loss=8.8791
	step [181/188], loss=9.8064
	step [182/188], loss=7.9866
	step [183/188], loss=6.8737
	step [184/188], loss=8.0931
	step [185/188], loss=7.7857
	step [186/188], loss=9.4051
	step [187/188], loss=7.8407
	step [188/188], loss=5.2753
	Evaluating
	loss=0.0223, precision=0.2205, recall=0.9963, f1=0.3610
Training epoch 26
	step [1/188], loss=8.8813
	step [2/188], loss=8.9206
	step [3/188], loss=7.9976
	step [4/188], loss=6.9960
	step [5/188], loss=8.0675
	step [6/188], loss=8.1539
	step [7/188], loss=9.0764
	step [8/188], loss=8.4209
	step [9/188], loss=7.5649
	step [10/188], loss=9.3337
	step [11/188], loss=9.2942
	step [12/188], loss=7.9642
	step [13/188], loss=7.6347
	step [14/188], loss=7.5658
	step [15/188], loss=7.7807
	step [16/188], loss=8.3028
	step [17/188], loss=8.1587
	step [18/188], loss=7.8524
	step [19/188], loss=7.3316
	step [20/188], loss=6.9039
	step [21/188], loss=8.4545
	step [22/188], loss=8.3906
	step [23/188], loss=7.8499
	step [24/188], loss=9.9613
	step [25/188], loss=7.5188
	step [26/188], loss=8.0545
	step [27/188], loss=6.6359
	step [28/188], loss=8.3275
	step [29/188], loss=8.0726
	step [30/188], loss=8.9861
	step [31/188], loss=8.9471
	step [32/188], loss=8.1788
	step [33/188], loss=9.2831
	step [34/188], loss=9.4709
	step [35/188], loss=7.8294
	step [36/188], loss=7.7659
	step [37/188], loss=6.7138
	step [38/188], loss=7.3320
	step [39/188], loss=10.3357
	step [40/188], loss=8.1502
	step [41/188], loss=9.0721
	step [42/188], loss=8.5959
	step [43/188], loss=10.4740
	step [44/188], loss=7.8417
	step [45/188], loss=8.6548
	step [46/188], loss=8.7799
	step [47/188], loss=8.0817
	step [48/188], loss=8.6009
	step [49/188], loss=6.7961
	step [50/188], loss=7.6890
	step [51/188], loss=6.7358
	step [52/188], loss=6.9855
	step [53/188], loss=8.5690
	step [54/188], loss=6.5343
	step [55/188], loss=9.2295
	step [56/188], loss=10.0475
	step [57/188], loss=8.5762
	step [58/188], loss=7.1776
	step [59/188], loss=8.3144
	step [60/188], loss=8.5332
	step [61/188], loss=9.5286
	step [62/188], loss=9.3344
	step [63/188], loss=8.4576
	step [64/188], loss=8.0872
	step [65/188], loss=9.3303
	step [66/188], loss=7.9829
	step [67/188], loss=9.4577
	step [68/188], loss=8.3055
	step [69/188], loss=8.3686
	step [70/188], loss=8.1318
	step [71/188], loss=8.7860
	step [72/188], loss=9.4383
	step [73/188], loss=8.3501
	step [74/188], loss=7.8606
	step [75/188], loss=8.5363
	step [76/188], loss=7.6268
	step [77/188], loss=7.7664
	step [78/188], loss=8.8714
	step [79/188], loss=8.3851
	step [80/188], loss=8.5960
	step [81/188], loss=9.2629
	step [82/188], loss=9.2329
	step [83/188], loss=9.1307
	step [84/188], loss=9.5212
	step [85/188], loss=6.7279
	step [86/188], loss=9.0700
	step [87/188], loss=8.7444
	step [88/188], loss=7.5302
	step [89/188], loss=8.2190
	step [90/188], loss=8.0650
	step [91/188], loss=6.5988
	step [92/188], loss=8.1400
	step [93/188], loss=9.7812
	step [94/188], loss=10.6026
	step [95/188], loss=8.8607
	step [96/188], loss=8.2150
	step [97/188], loss=7.6147
	step [98/188], loss=9.7779
	step [99/188], loss=9.0031
	step [100/188], loss=10.3055
	step [101/188], loss=8.4913
	step [102/188], loss=8.2816
	step [103/188], loss=8.0108
	step [104/188], loss=10.0403
	step [105/188], loss=7.6161
	step [106/188], loss=7.9425
	step [107/188], loss=9.4202
	step [108/188], loss=7.2023
	step [109/188], loss=9.6002
	step [110/188], loss=9.6688
	step [111/188], loss=8.9888
	step [112/188], loss=8.5360
	step [113/188], loss=9.8291
	step [114/188], loss=7.5864
	step [115/188], loss=7.9236
	step [116/188], loss=10.3244
	step [117/188], loss=7.5982
	step [118/188], loss=6.7373
	step [119/188], loss=8.4754
	step [120/188], loss=7.0280
	step [121/188], loss=8.6815
	step [122/188], loss=8.3053
	step [123/188], loss=8.8474
	step [124/188], loss=8.3186
	step [125/188], loss=9.5301
	step [126/188], loss=9.1461
	step [127/188], loss=8.9135
	step [128/188], loss=8.6145
	step [129/188], loss=8.5373
	step [130/188], loss=13.3574
	step [131/188], loss=8.2570
	step [132/188], loss=8.3455
	step [133/188], loss=8.8643
	step [134/188], loss=9.8247
	step [135/188], loss=7.9319
	step [136/188], loss=9.2813
	step [137/188], loss=8.0277
	step [138/188], loss=8.6233
	step [139/188], loss=8.7817
	step [140/188], loss=7.6870
	step [141/188], loss=8.9368
	step [142/188], loss=8.9566
	step [143/188], loss=8.1292
	step [144/188], loss=9.6400
	step [145/188], loss=9.0103
	step [146/188], loss=8.4780
	step [147/188], loss=8.5453
	step [148/188], loss=7.8071
	step [149/188], loss=7.5533
	step [150/188], loss=8.1440
	step [151/188], loss=6.3064
	step [152/188], loss=7.9442
	step [153/188], loss=7.4785
	step [154/188], loss=9.4362
	step [155/188], loss=11.9654
	step [156/188], loss=7.5288
	step [157/188], loss=7.6128
	step [158/188], loss=7.4925
	step [159/188], loss=9.1190
	step [160/188], loss=9.4198
	step [161/188], loss=7.6278
	step [162/188], loss=8.1367
	step [163/188], loss=7.5594
	step [164/188], loss=11.0559
	step [165/188], loss=9.6804
	step [166/188], loss=10.4690
	step [167/188], loss=8.7075
	step [168/188], loss=8.4768
	step [169/188], loss=8.0582
	step [170/188], loss=6.0413
	step [171/188], loss=8.7069
	step [172/188], loss=11.4794
	step [173/188], loss=9.1245
	step [174/188], loss=9.6979
	step [175/188], loss=8.1910
	step [176/188], loss=7.9789
	step [177/188], loss=8.8926
	step [178/188], loss=7.9703
	step [179/188], loss=9.2469
	step [180/188], loss=7.4814
	step [181/188], loss=9.9501
	step [182/188], loss=8.0212
	step [183/188], loss=7.9233
	step [184/188], loss=9.0335
	step [185/188], loss=7.5305
	step [186/188], loss=8.0356
	step [187/188], loss=8.6153
	step [188/188], loss=3.6622
	Evaluating
	loss=0.0210, precision=0.2423, recall=0.9957, f1=0.3897
Training epoch 27
	step [1/188], loss=7.8314
	step [2/188], loss=7.1598
	step [3/188], loss=9.3378
	step [4/188], loss=10.6169
	step [5/188], loss=6.9577
	step [6/188], loss=8.6057
	step [7/188], loss=7.7097
	step [8/188], loss=7.3513
	step [9/188], loss=9.6837
	step [10/188], loss=8.2378
	step [11/188], loss=8.0751
	step [12/188], loss=6.3500
	step [13/188], loss=9.0761
	step [14/188], loss=7.3244
	step [15/188], loss=8.3919
	step [16/188], loss=8.2703
	step [17/188], loss=8.4710
	step [18/188], loss=8.8984
	step [19/188], loss=8.7049
	step [20/188], loss=9.4798
	step [21/188], loss=8.8120
	step [22/188], loss=7.2801
	step [23/188], loss=7.3542
	step [24/188], loss=6.8846
	step [25/188], loss=9.2621
	step [26/188], loss=7.9255
	step [27/188], loss=8.5546
	step [28/188], loss=7.2090
	step [29/188], loss=9.7172
	step [30/188], loss=8.7687
	step [31/188], loss=7.0160
	step [32/188], loss=7.9616
	step [33/188], loss=8.9490
	step [34/188], loss=8.3667
	step [35/188], loss=10.1120
	step [36/188], loss=8.7960
	step [37/188], loss=8.5331
	step [38/188], loss=11.5287
	step [39/188], loss=6.4765
	step [40/188], loss=9.8417
	step [41/188], loss=7.4812
	step [42/188], loss=7.8922
	step [43/188], loss=7.7967
	step [44/188], loss=7.3663
	step [45/188], loss=8.0971
	step [46/188], loss=9.3438
	step [47/188], loss=7.6910
	step [48/188], loss=9.2262
	step [49/188], loss=6.8804
	step [50/188], loss=7.3738
	step [51/188], loss=7.7606
	step [52/188], loss=8.8597
	step [53/188], loss=7.4792
	step [54/188], loss=8.6179
	step [55/188], loss=9.0009
	step [56/188], loss=8.3531
	step [57/188], loss=8.5371
	step [58/188], loss=8.3461
	step [59/188], loss=6.5248
	step [60/188], loss=8.0786
	step [61/188], loss=8.1416
	step [62/188], loss=8.3397
	step [63/188], loss=8.0971
	step [64/188], loss=7.9985
	step [65/188], loss=9.7198
	step [66/188], loss=9.2217
	step [67/188], loss=6.5059
	step [68/188], loss=7.4570
	step [69/188], loss=10.5244
	step [70/188], loss=9.1669
	step [71/188], loss=9.0767
	step [72/188], loss=8.1492
	step [73/188], loss=9.0268
	step [74/188], loss=8.5011
	step [75/188], loss=9.0798
	step [76/188], loss=7.0199
	step [77/188], loss=9.2689
	step [78/188], loss=6.0962
	step [79/188], loss=7.6640
	step [80/188], loss=9.9716
	step [81/188], loss=7.2792
	step [82/188], loss=9.8033
	step [83/188], loss=8.3493
	step [84/188], loss=8.5623
	step [85/188], loss=7.5073
	step [86/188], loss=8.4094
	step [87/188], loss=7.9601
	step [88/188], loss=10.5961
	step [89/188], loss=8.3807
	step [90/188], loss=6.9747
	step [91/188], loss=8.3962
	step [92/188], loss=8.0611
	step [93/188], loss=11.3029
	step [94/188], loss=7.1928
	step [95/188], loss=7.9144
	step [96/188], loss=8.3318
	step [97/188], loss=7.0450
	step [98/188], loss=7.2357
	step [99/188], loss=9.1548
	step [100/188], loss=8.6581
	step [101/188], loss=8.2520
	step [102/188], loss=8.9462
	step [103/188], loss=10.0565
	step [104/188], loss=8.1508
	step [105/188], loss=6.9264
	step [106/188], loss=8.7693
	step [107/188], loss=8.1600
	step [108/188], loss=6.9497
	step [109/188], loss=8.1169
	step [110/188], loss=8.4931
	step [111/188], loss=7.8245
	step [112/188], loss=6.5024
	step [113/188], loss=10.3938
	step [114/188], loss=9.5738
	step [115/188], loss=8.3033
	step [116/188], loss=7.6169
	step [117/188], loss=8.3384
	step [118/188], loss=8.4526
	step [119/188], loss=8.3340
	step [120/188], loss=7.0447
	step [121/188], loss=6.0083
	step [122/188], loss=8.6882
	step [123/188], loss=8.2736
	step [124/188], loss=8.5997
	step [125/188], loss=9.5138
	step [126/188], loss=7.6711
	step [127/188], loss=7.4619
	step [128/188], loss=7.1691
	step [129/188], loss=9.5614
	step [130/188], loss=8.2611
	step [131/188], loss=11.5617
	step [132/188], loss=7.5945
	step [133/188], loss=8.1376
	step [134/188], loss=9.9449
	step [135/188], loss=9.5569
	step [136/188], loss=7.9903
	step [137/188], loss=7.1887
	step [138/188], loss=7.8309
	step [139/188], loss=9.2892
	step [140/188], loss=6.7999
	step [141/188], loss=10.0205
	step [142/188], loss=8.5177
	step [143/188], loss=8.5156
	step [144/188], loss=7.8405
	step [145/188], loss=9.1208
	step [146/188], loss=8.1468
	step [147/188], loss=7.6068
	step [148/188], loss=8.4554
	step [149/188], loss=8.5709
	step [150/188], loss=8.4335
	step [151/188], loss=7.8314
	step [152/188], loss=9.2948
	step [153/188], loss=8.0684
	step [154/188], loss=9.0210
	step [155/188], loss=8.8463
	step [156/188], loss=8.8868
	step [157/188], loss=7.3131
	step [158/188], loss=7.8754
	step [159/188], loss=8.7593
	step [160/188], loss=7.5520
	step [161/188], loss=7.9941
	step [162/188], loss=8.0016
	step [163/188], loss=8.9480
	step [164/188], loss=10.1771
	step [165/188], loss=7.8566
	step [166/188], loss=9.9897
	step [167/188], loss=8.6736
	step [168/188], loss=7.5106
	step [169/188], loss=7.8049
	step [170/188], loss=7.6265
	step [171/188], loss=8.3378
	step [172/188], loss=8.3244
	step [173/188], loss=7.3487
	step [174/188], loss=8.4993
	step [175/188], loss=9.6887
	step [176/188], loss=8.5082
	step [177/188], loss=8.2349
	step [178/188], loss=10.4462
	step [179/188], loss=9.0511
	step [180/188], loss=9.8761
	step [181/188], loss=8.3218
	step [182/188], loss=7.6673
	step [183/188], loss=8.2267
	step [184/188], loss=7.9138
	step [185/188], loss=6.5363
	step [186/188], loss=8.2083
	step [187/188], loss=8.2627
	step [188/188], loss=6.1023
	Evaluating
	loss=0.0213, precision=0.2377, recall=0.9960, f1=0.3838
Training epoch 28
